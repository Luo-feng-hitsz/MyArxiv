{"2024-08-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2408.14471v1","updated":"2024-08-26T17:59:01Z","published":"2024-08-26T17:59:01Z","title":"A Practitioner's Guide to Continual Multimodal Pretraining","summary":"  Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux.\n","authors":["Karsten Roth","Vishaal Udandarao","Sebastian Dziadzio","Ameya Prabhu","Mehdi Cherti","Oriol Vinyals","Olivier HÃ©naff","Samuel Albanie","Matthias Bethge","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2408.14471v1.pdf","comment":"Technical Report. 52 pages"},{"id":"http://arxiv.org/abs/2408.14470v1","updated":"2024-08-26T17:58:53Z","published":"2024-08-26T17:58:53Z","title":"Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models","summary":"  Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification.\n","authors":["Aradhye Agarwal","Suhas K Ramesh","Ayan Sengupta","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2408.14470v1.pdf","comment":"15 pages, 7 tables, 9 figures"},{"id":"http://arxiv.org/abs/2408.14467v1","updated":"2024-08-26T17:58:17Z","published":"2024-08-26T17:58:17Z","title":"Explicit Inductive Inference using Large Language Models","summary":"  Large Language Models (LLMs) are reported to hold undesirable attestation\nbias on inference tasks: when asked to predict if a premise P entails a\nhypothesis H, instead of considering H's conditional truthfulness entailed by\nP, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In\nthis paper, we propose a pipeline that exploits this bias to do explicit\ninductive inference. Our pipeline uses an LLM to transform a premise into a set\nof attested alternatives, and then aggregate answers of the derived new\nentailment inquiries to support the original inference prediction. On a\ndirectional predicate entailment benchmark, we demonstrate that by applying\nthis simple pipeline, we can improve the overall performance of LLMs on\ninference and substantially alleviate the impact of their attestation bias.\n","authors":["Tianyang Liu","Tianyi Li","Liang Cheng","Mark Steedman"],"pdf_url":"https://arxiv.org/pdf/2408.14467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11796v2","updated":"2024-08-26T17:50:46Z","published":"2024-08-21T17:38:48Z","title":"LLM Pruning and Distillation in Practice: The Minitron Approach","summary":"  We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.\n","authors":["Sharath Turuvekere Sreenivas","Saurav Muralidharan","Raviraj Joshi","Marcin Chochowski","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2408.11796v2.pdf","comment":"v2: Added missing references. Cleaned up runtime performance section"},{"id":"http://arxiv.org/abs/2306.13840v3","updated":"2024-08-26T17:34:44Z","published":"2023-06-24T02:25:56Z","title":"Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data","summary":"  Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.\n","authors":["Brando Miranda","Alycia Lee","Sudharsan Sundar","Allison Casasola","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2306.13840v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10468v3","updated":"2024-08-26T17:28:23Z","published":"2024-08-20T00:40:49Z","title":"Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions","summary":"  The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.\n","authors":["Jinxin Liu","Zao Yang"],"pdf_url":"https://arxiv.org/pdf/2408.10468v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14438v1","updated":"2024-08-26T17:25:16Z","published":"2024-08-26T17:25:16Z","title":"Evaluating Large Language Models on Spatial Tasks: A Multi-Task\n  Benchmarking Study","summary":"  The advent of large language models such as ChatGPT, Gemini, and others has\nunderscored the importance of evaluating their diverse capabilities, ranging\nfrom natural language understanding to code generation. However, their\nperformance on spatial tasks has not been comprehensively assessed. This study\naddresses this gap by introducing a novel multi-task spatial evaluation\ndataset, designed to systematically explore and compare the performance of\nseveral advanced models on spatial tasks. The dataset encompasses twelve\ndistinct task types, including spatial understanding and path planning, each\nwith verified, accurate answers. We evaluated multiple models, including\nOpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase\ntesting approach. Initially, we conducted zero-shot testing, followed by\ncategorizing the dataset by difficulty and performing prompt tuning tests.\nResults indicate that gpt-4o achieved the highest overall accuracy in the first\nphase, with an average of 71.3%. Although moonshot-v1-8k slightly\nunderperformed overall, it surpassed gpt-4o in place name recognition tasks.\nThe study also highlights the impact of prompt strategies on model performance\nin specific tasks. For example, the Chain-of-Thought (COT) strategy increased\ngpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot\nstrategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to\n76.3%.\n","authors":["Liuchang Xu Shuo Zhao","Qingming Lin","Luyao Chen","Qianqian Luo","Sensen Wu","Xinyue Ye","Hailin Feng","Zhenhong Du"],"pdf_url":"https://arxiv.org/pdf/2408.14438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14419v1","updated":"2024-08-26T17:04:23Z","published":"2024-08-26T17:04:23Z","title":"CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language\n  Models","summary":"  We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large\nlanguage models. CHARTOM consists of specially designed data visualizing\ncharts. Given a chart, a language model needs to not only correctly comprehend\nthe chart (the FACT question) but also judge if the chart will be misleading to\na human reader (the MIND question). Both questions have significant societal\nbenefits. We detail the construction of the CHARTOM benchmark including its\ncalibration on human performance.\n","authors":["Shubham Bharti","Shiyun Cheng","Jihyun Rho","Martina Rao","Xiaojin Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.14419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14418v1","updated":"2024-08-26T17:04:00Z","published":"2024-08-26T17:04:00Z","title":"MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues","summary":"  Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization.\n","authors":["Kuluhan Binici","Abhinav Ramesh Kashyap","Viktor Schlegel","Andy T. Liu","Vijay Prakash Dwivedi","Thanh-Tung Nguyen","Xiaoxue Gao","Nancy F. Chen","Stefan Winkler"],"pdf_url":"https://arxiv.org/pdf/2408.14418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05720v2","updated":"2024-08-26T16:48:08Z","published":"2024-03-08T23:17:55Z","title":"A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models","summary":"  Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\n  Using clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation.\n","authors":["Asad Aali","Dave Van Veen","Yamin Ishraq Arefeen","Jason Hom","Christian Bluethgen","Eduardo Pontes Reis","Sergios Gatidis","Namuun Clifford","Joseph Daws","Arash S. Tehrani","Jangwon Kim","Akshay S. Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2403.05720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14398v1","updated":"2024-08-26T16:29:13Z","published":"2024-08-26T16:29:13Z","title":"Language-specific Calibration for Pruning Multilingual Language Models","summary":"  Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.\n","authors":["Simon Kurz","Zhixue Zhao","Jian-Jia Chen","Lucie Flek"],"pdf_url":"https://arxiv.org/pdf/2408.14398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14397v1","updated":"2024-08-26T16:28:56Z","published":"2024-08-26T16:28:56Z","title":"Uncovering Knowledge Gaps in Radiology Report Generation Models through\n  Knowledge Graphs","summary":"  Recent advancements in artificial intelligence have significantly improved\nthe automatic generation of radiology reports. However, existing evaluation\nmethods fail to reveal the models' understanding of radiological images and\ntheir capacity to achieve human-level granularity in descriptions. To bridge\nthis gap, we introduce a system, named ReXKG, which extracts structured\ninformation from processed reports to construct a comprehensive radiology\nknowledge graph. We then propose three metrics to evaluate the similarity of\nnodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs\n(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative\nanalysis of AI-generated and human-written radiology reports, assessing the\nperformance of both specialist and generalist models. Our study provides a\ndeeper understanding of the capabilities and limitations of current AI models\nin radiology report generation, offering valuable insights for improving model\nperformance and clinical applicability.\n","authors":["Xiaoman Zhang","JuliÃ¡n N. Acosta","Hong-Yu Zhou","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2408.14397v1.pdf","comment":"Code is available at: https://github.com/rajpurkarlab/ReXKG"},{"id":"http://arxiv.org/abs/2408.14380v1","updated":"2024-08-26T16:00:41Z","published":"2024-08-26T16:00:41Z","title":"Probing Causality Manipulation of Large Language Models","summary":"  Large language models (LLMs) have shown various ability on natural language\nprocessing, including problems about causality. It is not intuitive for LLMs to\ncommand causality, since pretrained models usually work on statistical\nassociations, and do not focus on causes and effects in sentences. So that\nprobing internal manipulation of causality is necessary for LLMs. This paper\nproposes a novel approach to probe causality manipulation hierarchically, by\nproviding different shortcuts to models and observe behaviors. We exploit\nretrieval augmented generation (RAG) and in-context learning (ICL) for models\non a designed causality classification task. We conduct experiments on\nmainstream LLMs, including GPT-4 and some smaller and domain-specific models.\nOur results suggest that LLMs can detect entities related to causality and\nrecognize direct causal relationships. However, LLMs lack specialized cognition\nfor causality, merely treating them as part of the global semantic of the\nsentence.\n","authors":["Chenyang Zhang","Haibo Tong","Bin Zhang","Dongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.14380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19178v2","updated":"2024-08-26T15:59:03Z","published":"2024-04-30T01:02:15Z","title":"Revenge of the Fallen? Recurrent Models Match Transformers at Predicting\n  Human Language Comprehension Metrics","summary":"  Transformers have generally supplanted recurrent neural networks as the\ndominant architecture for both natural language processing tasks and for\nmodelling the effect of predictability on online human language comprehension.\nHowever, two recently developed recurrent model architectures, RWKV and Mamba,\nappear to perform natural language tasks comparably to or better than\ntransformers of equivalent scale. In this paper, we show that contemporary\nrecurrent models are now also able to match - and in some cases, exceed - the\nperformance of comparably sized transformers at modeling online human language\ncomprehension. This suggests that transformer language models are not uniquely\nsuited to this task, and opens up new directions for debates about the extent\nto which architectural features of language models make them better or worse\nmodels of human language comprehension.\n","authors":["James A. Michaelov","Catherine Arnett","Benjamin K. Bergen"],"pdf_url":"https://arxiv.org/pdf/2404.19178v2.pdf","comment":"Accepted at COLM 2024"},{"id":"http://arxiv.org/abs/2408.14354v1","updated":"2024-08-26T15:30:05Z","published":"2024-08-26T15:30:05Z","title":"SWE-bench-java: A GitHub Issue Resolving Benchmark for Java","summary":"  GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming.\n","authors":["Daoguang Zan","Zhirong Huang","Ailun Yu","Shaoxin Lin","Yifan Shi","Wei Liu","Dong Chen","Zongshuai Qi","Hao Yu","Lei Yu","Dezhi Ran","Muhan Zeng","Bo Shen","Pan Bian","Guangtai Liang","Bei Guan","Pengjie Huang","Tao Xie","Yongji Wang","Qianxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14354v1.pdf","comment":"This work is in progress"},{"id":"http://arxiv.org/abs/2408.14352v1","updated":"2024-08-26T15:29:34Z","published":"2024-08-26T15:29:34Z","title":"Assessing Contamination in Large Language Models: Introducing the\n  LogProber method","summary":"  In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities.\n","authors":["Nicolas Yax","Pierre-Yves Oudeyer","Stefano Palminteri"],"pdf_url":"https://arxiv.org/pdf/2408.14352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14340v1","updated":"2024-08-26T15:13:14Z","published":"2024-08-26T15:13:14Z","title":"Foundation Models for Music: A Survey","summary":"  In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.\n","authors":["Yinghao Ma","Anders Ãland","Anton Ragni","Bleiz MacSen Del Sette","Charalampos Saitis","Chris Donahue","Chenghua Lin","Christos Plachouras","Emmanouil Benetos","Elio Quinton","Elona Shatri","Fabio Morreale","Ge Zhang","GyÃ¶rgy Fazekas","Gus Xia","Huan Zhang","Ilaria Manco","Jiawen Huang","Julien Guinot","Liwei Lin","Luca Marinelli","Max W. Y. Lam","Megha Sharma","Qiuqiang Kong","Roger B. Dannenberg","Ruibin Yuan","Shangda Wu","Shih-Lun Wu","Shuqi Dai","Shun Lei","Shiyin Kang","Simon Dixon","Wenhu Chen","Wehhao Huang","Xingjian Du","Xingwei Qu","Xu Tan","Yizhi Li","Zeyue Tian","Zhiyong Wu","Zhizheng Wu","Ziyang Ma","Ziyu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16528v2","updated":"2024-08-26T14:59:53Z","published":"2024-05-26T11:29:57Z","title":"LoQT: Low Rank Adapters for Quantized Training","summary":"  Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware.\n","authors":["Sebastian Loeschcke","Mads Toftrup","Michael J. Kastoryano","Serge Belongie","VÃ©steinn SnÃ¦bjarnarson"],"pdf_url":"https://arxiv.org/pdf/2405.16528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14317v1","updated":"2024-08-26T14:45:03Z","published":"2024-08-26T14:45:03Z","title":"Claim Verification in the Age of Large Language Models: A Survey","summary":"  The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.\n","authors":["Alphaeus Dmonte","Roland Oruche","Marcos Zampieri","Prasad Calyam","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2408.14317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14307v1","updated":"2024-08-26T14:38:19Z","published":"2024-08-26T14:38:19Z","title":"LLM-3D Print: Large Language Models To Monitor and Control 3D Printing","summary":"  Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention.\n","authors":["Yayati Jadhav","Peter Pak","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2408.14307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10594v3","updated":"2024-08-26T14:30:38Z","published":"2024-06-15T11:03:33Z","title":"BlockPruner: Fine-grained Pruning for Large Language Models","summary":"  With the rapid growth in the size and complexity of large language models\n(LLMs), the costs associated with their training and inference have escalated\nsignificantly. Research indicates that certain layers in LLMs harbor\nsubstantial redundancy, and pruning these layers has minimal impact on the\noverall performance. While various layer pruning methods have been developed\nbased on this insight, they generally overlook the finer-grained redundancies\nwithin the layers themselves. In this paper, we delve deeper into the\narchitecture of LLMs and demonstrate that finer-grained pruning can be achieved\nby targeting redundancies in multi-head attention (MHA) and multi-layer\nperceptron (MLP) blocks. We propose a novel, training-free structured pruning\napproach called BlockPruner. Unlike existing layer pruning methods, BlockPruner\nsegments each Transformer layer into MHA and MLP blocks. It then assesses the\nimportance of these blocks using perplexity measures and applies a heuristic\nsearch for iterative pruning. We applied BlockPruner to LLMs of various sizes\nand architectures and validated its performance across a wide range of\ndownstream tasks. Experimental results show that BlockPruner achieves more\ngranular and effective pruning compared to state-of-the-art baselines.\n","authors":["Longguang Zhong","Fanqi Wan","Ruijun Chen","Xiaojun Quan","Liangzhi Li"],"pdf_url":"https://arxiv.org/pdf/2406.10594v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14283v1","updated":"2024-08-26T14:09:28Z","published":"2024-08-26T14:09:28Z","title":"Predictability and Causality in Spanish and English Natural Language\n  Generation","summary":"  In recent years, the field of Natural Language Generation (NLG) has been\nboosted by the recent advances in deep learning technologies. Nonetheless,\nthese new data-intensive methods introduce language-dependent disparities in\nNLG as the main training data sets are in English. Also, most neural NLG\nsystems use decoder-only (causal) transformer language models, which work well\nfor English, but were not designed with other languages in mind. In this work\nwe depart from the hypothesis that they may introduce generation bias in target\nlanguages with less rigid word ordering, subject omission, or different\nattachment preferences for relative clauses, so that for these target languages\nother language generation strategies may be more desirable. This paper first\ncompares causal and non-causal language modeling for English and Spanish, two\nlanguages with different grammatical structures and over 1.5 billion and 0.5\nbillion speakers, respectively. For this purpose, we define a novel metric of\naverage causal and non-causal context-conditioned entropy of the grammatical\ncategory distribution for both languages as an information-theoretic a priori\napproach. The evaluation of natural text sources (such as training data) in\nboth languages reveals lower average non-causal conditional entropy in Spanish\nand lower causal conditional entropy in English. According to this experiment,\nSpanish is more predictable than English given a non-causal context. Then, by\napplying a conditional relative entropy metric to text generation experiments,\nwe obtain as insights that the best performance is respectively achieved with\ncausal NLG in English, and with non-causal NLG in Spanish. These insights\nsupport further research in NLG in Spanish using bidirectional transformer\nlanguage models.\n","authors":["Andrea Busto-CastiÃ±eira","Francisco J. GonzÃ¡lez-CastaÃ±o","Silvia GarcÃ­a-MÃ©ndez","Francisco de Arriba-PÃ©rez"],"pdf_url":"https://arxiv.org/pdf/2408.14283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09869v2","updated":"2024-08-26T13:55:59Z","published":"2024-08-19T10:20:06Z","title":"Docling Technical Report","summary":"  This technical report introduces Docling, an easy to use, self-contained,\nMIT-licensed open-source package for PDF document conversion. It is powered by\nstate-of-the-art specialized AI models for layout analysis (DocLayNet) and\ntable structure recognition (TableFormer), and runs efficiently on commodity\nhardware in a small resource budget. The code interface allows for easy\nextensibility and addition of new features and models.\n","authors":["Christoph Auer","Maksym Lysak","Ahmed Nassar","Michele Dolfi","Nikolaos Livathinos","Panos Vagenas","Cesar Berrospi Ramis","Matteo Omenetti","Fabian Lindlbauer","Kasper Dinkla","Valery Weber","Lucas Morin","Ingmar Meijer","Viktor Kuropiatnyk","Peter W. J. Staar"],"pdf_url":"https://arxiv.org/pdf/2408.09869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14277v1","updated":"2024-08-26T13:53:04Z","published":"2024-08-26T13:53:04Z","title":"Epidemic Information Extraction for Event-Based Surveillance using Large\n  Language Models","summary":"  This paper presents a novel approach to epidemic surveillance, leveraging the\npower of Artificial Intelligence and Large Language Models (LLMs) for effective\ninterpretation of unstructured big data sources, like the popular ProMED and\nWHO Disease Outbreak News. We explore several LLMs, evaluating their\ncapabilities in extracting valuable epidemic information. We further enhance\nthe capabilities of the LLMs using in-context learning, and test the\nperformance of an ensemble model incorporating multiple open-source LLMs. The\nfindings indicate that LLMs can significantly enhance the accuracy and\ntimeliness of epidemic modelling and forecasting, offering a promising tool for\nmanaging future pandemic events.\n","authors":["Sergio Consoli","Peter Markov","Nikolaos I. Stilianakis","Lorenzo Bertolini","Antonio Puertas Gallardo","Mario Ceresa"],"pdf_url":"https://arxiv.org/pdf/2408.14277v1.pdf","comment":"11 pages, 4 figures, Ninth International Congress on Information and\n  Communication Technology (ICICT 2024)"},{"id":"http://arxiv.org/abs/2408.14262v1","updated":"2024-08-26T13:29:25Z","published":"2024-08-26T13:29:25Z","title":"Self-supervised Speech Representations Still Struggle with African\n  American Vernacular English","summary":"  Underperformance of ASR systems for speakers of African American Vernacular\nEnglish (AAVE) and other marginalized language varieties is a well-documented\nphenomenon, and one that reinforces the stigmatization of these varieties. We\ninvestigate whether or not the recent wave of Self-Supervised Learning (SSL)\nspeech models can close the gap in ASR performance between AAVE and Mainstream\nAmerican English (MAE). We evaluate four SSL models (wav2vec 2.0, HuBERT,\nWavLM, and XLS-R) on zero-shot Automatic Speech Recognition (ASR) for these two\nvarieties and find that these models perpetuate the bias in performance against\nAAVE. Additionally, the models have higher word error rates on utterances with\nmore phonological and morphosyntactic features of AAVE. Despite the success of\nSSL speech models in improving ASR for low resource varieties, SSL pre-training\nalone may not bridge the gap between AAVE and MAE. Our code is publicly\navailable at https://github.com/cmu-llab/s3m-aave.\n","authors":["Kalvin Chang","Yi-Hui Chou","Jiatong Shi","Hsuan-Ming Chen","Nicole Holliday","Odette Scharenborg","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2408.14262v1.pdf","comment":"INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.20584v2","updated":"2024-08-26T13:19:48Z","published":"2024-07-30T06:33:44Z","title":"Pruning Large Language Models with Semi-Structural Adaptive Sparse\n  Training","summary":"  The tremendous success of Large Language Models (LLMs) across various complex\ntasks relies heavily on their substantial scale, which raises challenges during\nmodel deployment due to their large memory consumption. Recently, numerous\nstudies have attempted to compress LLMs using one-shot pruning methods.\nHowever, these methods often experience considerable performance degradation on\ncomplex language understanding tasks, calling into question the feasibility of\npruning in LLMs. To address this issue, we propose a pruning pipeline for\nsemi-structured sparse models via retraining, termed Adaptive Sparse Trainer\n(AST). Unlike previous one-shot pruning methods, AST incrementally transforms\ndense models into sparse ones by applying decay to masked weights while\nallowing the model to adaptively select masks throughout the training process.\nFurthermore, we observe that using distillation with a dense model as the\nteacher can prevent the sparse model from falling into local optima and\naccelerate convergence. In addition, we incorporate extra well-initialized\nparameters to further enhance model performance with minimal increase in memory\nfootprint. AST can significantly enhance model performance, approaching the\nlevel of dense models. When applied to the LLaMA2-7B model, AST reduces the\nzero-shot accuracy gap between dense and semi-structured sparse models to 1.12%\nacross multiple zero-shot tasks, utilizing less than 0.4% of the pretraining\ntokens. Our work demonstrates the feasibility of deploying semi-structured\nsparse large language models and introduces a novel method for achieving highly\ncompressed models when combined with existing quantization techniques.\n","authors":["Weiyu Huang","Yuezhou Hu","Guohao Jian","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2407.20584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14236v1","updated":"2024-08-26T12:50:27Z","published":"2024-08-26T12:50:27Z","title":"DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for\n  type classification","summary":"  We introduce semantic towers, an extrinsic knowledge representation method,\nand compare it to intrinsic knowledge in large language models for ontology\nlearning. Our experiments show a trade-off between performance and semantic\ngrounding for extrinsic knowledge compared to a fine-tuned model intrinsic\nknowledge. We report our findings on the Large Language Models for Ontology\nLearning (LLMs4OL) 2024 challenge.\n","authors":["Hanna Abi Akl"],"pdf_url":"https://arxiv.org/pdf/2408.14236v1.pdf","comment":"8 pages, 4 figures, accepted for the LLMs4OL challenge at the\n  International Semantic Web Conference (ISWC) 2024"},{"id":"http://arxiv.org/abs/2406.10265v2","updated":"2024-08-26T10:54:12Z","published":"2024-06-11T07:42:13Z","title":"Improving Language Models for Emotion Analysis: Insights from Cognitive\n  Science","summary":"  We propose leveraging cognitive science research on emotions and\ncommunication to improve language models for emotion analysis. First, we\npresent the main emotion theories in psychology and cognitive science. Then, we\nintroduce the main methods of emotion annotation in natural language processing\nand their connections to psychological theories. We also present the two main\ntypes of analyses of emotional communication in cognitive pragmatics. Finally,\nbased on the cognitive science research presented, we propose directions for\nimproving language models for emotion analysis. We suggest that these research\nefforts pave the way for constructing new annotation schemes, methods, and a\npossible benchmark for emotional understanding, considering different facets of\nhuman emotion and communication.\n","authors":["Constant Bonard","Gustave Cortal"],"pdf_url":"https://arxiv.org/pdf/2406.10265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05141v2","updated":"2024-08-26T10:53:28Z","published":"2024-08-09T15:53:55Z","title":"A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning","summary":"  Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.\n","authors":["Ye Yuan","Chengwu Liu","Jingyang Yuan","Gongbo Sun","Siqi Li","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.05141v2.pdf","comment":"Technical report for 3rd prize in Task 1 of Meta CRAG KDD Cup 2024"},{"id":"http://arxiv.org/abs/2312.03731v7","updated":"2024-08-26T10:11:45Z","published":"2023-11-28T02:36:53Z","title":"MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs","summary":"  Graphs can inherently model interconnected objects on the Web, thereby\nfacilitating a series of Web applications, such as web analyzing and content\nrecommendation. Recently, Graph Neural Networks (GNNs) have emerged as a\nmainstream technique for graph representation learning. However, their efficacy\nwithin an end-to-end supervised framework is significantly tied to the\navailabilityof task-specific labels. To mitigate labeling costs and enhance\nrobustness in few-shot settings, pre-training on self-supervised tasks has\nemerged as a promising method, while prompting has been proposed to further\nnarrow the objective gap between pretext and downstream tasks. Although there\nhas been some initial exploration of prompt-based learning on graphs, they\nprimarily leverage a single pretext task, resulting in a limited subset of\ngeneral knowledge that could be learned from the pre-training data. Hence, in\nthis paper, we propose MultiGPrompt, a novel multi-task pre-training and\nprompting framework to exploit multiple pretext tasks for more comprehensive\npre-trained knowledge. First, in pre-training, we design a set of pretext\ntokens to synergize multiple pretext tasks. Second, we propose a dual-prompt\nmechanism consisting of composed and open prompts to leverage task-specific and\nglobal pre-training knowledge, to guide downstream tasks in few-shot settings.\nFinally, we conduct extensive experiments on six public datasets to evaluate\nand analyze MultiGPrompt.\n","authors":["Xingtong Yu","Chang Zhou","Yuan Fang","Xinming Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.03731v7.pdf","comment":"WWW2024 research track"},{"id":"http://arxiv.org/abs/2408.14154v1","updated":"2024-08-26T09:57:19Z","published":"2024-08-26T09:57:19Z","title":"Investigating the effect of Mental Models in User Interaction with an\n  Adaptive Dialog Agent","summary":"  Mental models play an important role in whether user interaction with\nintelligent systems, such as dialog systems is successful or not. Adaptive\ndialog systems present the opportunity to align a dialog agent's behavior with\nheterogeneous user expectations. However, there has been little research into\nwhat mental models users form when interacting with a task-oriented dialog\nsystem, how these models affect users' interactions, or what role system\nadaptation can play in this process, making it challenging to avoid damage to\nhuman-AI partnership. In this work, we collect a new publicly available dataset\nfor exploring user mental models about information seeking dialog systems. We\ndemonstrate that users have a variety of conflicting mental models about such\nsystems, the validity of which directly impacts the success of their\ninteractions and perceived usability of system. Furthermore, we show that\nadapting a dialog agent's behavior to better align with users' mental models,\neven when done implicitly, can improve perceived usability, dialog efficiency,\nand success. To this end, we argue that implicit adaptation can be a valid\nstrategy for task-oriented dialog systems, so long as developers first have a\nsolid understanding of users' mental models.\n","authors":["Lindsey Vanderlyn","Dirk VÃ¤th","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2408.14154v1.pdf","comment":"submitted to COLING 2025"},{"id":"http://arxiv.org/abs/2408.14153v1","updated":"2024-08-26T09:55:34Z","published":"2024-08-26T09:55:34Z","title":"Explaining Vision-Language Similarities in Dual Encoders with\n  Feature-Pair Attributions","summary":"  Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and learn similarities between them. However, it is not\nunderstood how such models compare two inputs. Here, we address this research\ngap with two contributions. First, we derive a method to attribute predictions\nof any differentiable dual encoder onto feature-pair interactions between its\ninputs. Second, we apply our method to CLIP-type models and show that they\nlearn fine-grained correspondences between parts of captions and regions in\nimages. They match objects across input modes and also account for mismatches.\nHowever, this visual-linguistic grounding ability heavily varies between object\nclasses, depends on the training data distribution, and largely improves after\nin-domain training. Using our method we can identify knowledge gaps about\nspecific object classes in individual models and can monitor their improvement\nupon fine-tuning.\n","authors":["Lucas MÃ¶ller","Pascal Tilli","Ngoc Thang Vu","Sebastian PadÃ³"],"pdf_url":"https://arxiv.org/pdf/2408.14153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04660v3","updated":"2024-08-26T09:37:46Z","published":"2024-08-05T20:01:10Z","title":"XMainframe: A Large Language Model for Mainframe Modernization","summary":"  Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers.\n","authors":["Anh T. V. Dau","Hieu Trung Dao","Anh Tuan Nguyen","Hieu Trung Tran","Phong X. Nguyen","Nghi D. Q. Bui"],"pdf_url":"https://arxiv.org/pdf/2408.04660v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14141v1","updated":"2024-08-26T09:37:42Z","published":"2024-08-26T09:37:42Z","title":"Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in\n  Subjective Tasks?","summary":"  Subjective tasks in NLP have been mostly relegated to objective standards,\nwhere the gold label is decided by taking the majority vote. This obfuscates\nannotator disagreement and the inherent uncertainty of the label. We argue that\nsubjectivity should factor into model decisions and play a direct role via\ncalibration under a selective prediction setting. Specifically, instead of\ncalibrating confidence purely from the model's perspective, we calibrate models\nfor subjective tasks based on crowd worker agreement. Our method,\nCrowd-Calibrator, models the distance between the distribution of crowd worker\nlabels and the model's own distribution over labels to inform whether the model\nshould abstain from a decision. On two highly subjective tasks, hate speech\ndetection and natural language inference, our experiments show Crowd-Calibrator\neither outperforms or achieves competitive performance with existing selective\nprediction baselines. Our findings highlight the value of bringing human\ndecision-making into model predictions.\n","authors":["Urja Khurana","Eric Nalisnick","Antske Fokkens","Swabha Swayamdipta"],"pdf_url":"https://arxiv.org/pdf/2408.14141v1.pdf","comment":"Accepted at COLM 2024"},{"id":"http://arxiv.org/abs/2403.08564v2","updated":"2024-08-26T09:35:39Z","published":"2024-03-13T14:19:08Z","title":"Non-discrimination Criteria for Generative Language Models","summary":"  Generative AI, such as large language models, has undergone rapid development\nwithin recent years. As these models become increasingly available to the\npublic, concerns arise about perpetuating and amplifying harmful biases in\napplications. Gender stereotypes can be harmful and limiting for the\nindividuals they target, whether they consist of misrepresentation or\ndiscrimination. Recognizing gender bias as a pervasive societal construct, this\npaper studies how to uncover and quantify the presence of gender biases in\ngenerative language models. In particular, we derive generative AI analogues of\nthree well-known non-discrimination criteria from classification, namely\nindependence, separation and sufficiency. To demonstrate these criteria in\naction, we design prompts for each of the criteria with a focus on occupational\ngender stereotype, specifically utilizing the medical test to introduce the\nground truth in the generative AI context. Our results address the presence of\noccupational gender bias within such conversational language models.\n","authors":["Sara Sterlie","Nina Weng","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.08564v2.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.14137v1","updated":"2024-08-26T09:34:36Z","published":"2024-08-26T09:34:36Z","title":"Multi-Faceted Evaluation of Modeling Languages for Augmented Reality\n  Applications -- The Case of ARWFML","summary":"  The evaluation of modeling languages for augmented reality applications poses\nparticular challenges due to the three-dimensional environment they target. The\npreviously introduced Augmented Reality Workflow Modeling Language (ARWFML)\nenables the model-based creation of augmented reality scenarios without\nprogramming knowledge. Building upon the first design cycle of the language's\nspecification, this paper presents two further design iterations for refining\nthe language based on multi-faceted evaluations. These include a comparative\nevaluation of implementation options and workflow capabilities, the\nintroduction of a 3D notation, and the development of a new 3D modeling\nenvironment. On this basis, a comprehensibility study of the language was\nconducted. Thereby, we show how modeling languages for augmented reality can be\nevolved towards a maturity level suitable for empirical evaluations.\n","authors":["Fabian Muff","Hans-Georg Fill"],"pdf_url":"https://arxiv.org/pdf/2408.14137v1.pdf","comment":"Accepted manuscript for the 43rd International Conference on\n  Conceptual Modeling Conceptual Modeling, AI, and Beyond 28-31 October 2024 |\n  Pittsburgh, Pennsylvania, USA"},{"id":"http://arxiv.org/abs/2408.14119v1","updated":"2024-08-26T09:08:26Z","published":"2024-08-26T09:08:26Z","title":"Contrastive Learning Subspace for Text Clustering","summary":"  Contrastive learning has been frequently investigated to learn effective\nrepresentations for text clustering tasks. While existing contrastive\nlearning-based text clustering methods only focus on modeling instance-wise\nsemantic similarity relationships, they ignore contextual information and\nunderlying relationships among all instances that needs to be clustered. In\nthis paper, we propose a novel text clustering approach called Subspace\nContrastive Learning (SCL) which models cluster-wise relationships among\ninstances. Specifically, the proposed SCL consists of two main modules: (1) a\nself-expressive module that constructs virtual positive samples and (2) a\ncontrastive learning module that further learns a discriminative subspace to\ncapture task-specific cluster-wise relationships among texts. Experimental\nresults show that the proposed SCL method not only has achieved superior\nresults on multiple task clustering datasets but also has less complexity in\npositive sample construction.\n","authors":["Qian Yong","Chen Chen","Xiabing Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.14119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11534v6","updated":"2024-08-26T08:52:44Z","published":"2023-08-21T06:51:56Z","title":"PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator","summary":"  The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT dialogues, as evidenced by Vicuna. However, due to challenges in\ngathering dialogues involving human participation, current endeavors like Baize\nand UltraChat rely on ChatGPT conducting roleplay to simulate humans based on\ninstructions, resulting in overdependence on seeds, diminished human-likeness,\nlimited topic diversity, and an absence of genuine multi-round conversational\ndynamics. To address the above issues, we propose a paradigm to simulate human\nbehavior better and explore the benefits of incorporating more human-like\nquestions in multi-turn conversations. Specifically, we directly target human\nquestions extracted from genuine human-machine conversations as a learning goal\nand provide a novel user simulator called `Socratic'. The experimental results\nshow our response model, `PlatoLM', achieves SoTA performance among LLaMA-based\n7B models in MT-Bench. Our findings further demonstrate that our method\nintroduces highly human-like questioning patterns and rich topic structures,\nwhich can teach the response model better than previous works in multi-round\nconversations.\n","authors":["Chuyi Kong","Yaxin Fan","Xiang Wan","Feng Jiang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2308.11534v6.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2406.10833v2","updated":"2024-08-26T08:47:54Z","published":"2024-06-16T08:03:24Z","title":"A Comprehensive Survey of Scientific Large Language Models and Their\n  Applications in Scientific Discovery","summary":"  In many scientific fields, large language models (LLMs) have revolutionized\nthe way text and other modalities of data (e.g., molecules and proteins) are\nhandled, achieving superior performance in various applications and augmenting\nthe scientific discovery process. Nevertheless, previous surveys on scientific\nLLMs often concentrate on one or two fields or a single modality. In this\npaper, we aim to provide a more holistic view of the research landscape by\nunveiling cross-field and cross-modal connections between scientific LLMs\nregarding their architectures and pre-training techniques. To this end, we\ncomprehensively survey over 250 scientific LLMs, discuss their commonalities\nand differences, as well as summarize pre-training datasets and evaluation\ntasks for each field and modality. Moreover, we investigate how LLMs have been\ndeployed to benefit scientific discovery. Resources related to this survey are\navailable at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.\n","authors":["Yu Zhang","Xiusi Chen","Bowen Jin","Sheng Wang","Shuiwang Ji","Wei Wang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2406.10833v2.pdf","comment":"34 pages (GitHub:\n  https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models)"},{"id":"http://arxiv.org/abs/2403.11322v4","updated":"2024-08-26T08:25:01Z","published":"2024-03-17T19:54:16Z","title":"StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows","summary":"  It is a notable trend to use Large Language Models (LLMs) to tackle complex\ntasks, e.g., tasks that require a sequence of actions and dynamic interaction\nwith tools and external environments. In this paper, we propose StateFlow, a\nnovel LLM-based task-solving paradigm that conceptualizes complex task-solving\nprocesses as state machines. In StateFlow, we distinguish between \"process\ngrounding\" (via state and state transitions) and \"sub-task solving\" (through\nactions within a state), enhancing control and interpretability of the\ntask-solving procedure. A state represents the status of a running process. The\ntransitions between states are controlled by heuristic rules or decisions made\nby the LLM, allowing for a dynamic and adaptive progression. Upon entering a\nstate, a series of actions is executed, involving not only calling LLMs guided\nby different prompts, but also the utilization of external tools as needed. Our\nresults show that StateFlow significantly enhances LLMs' efficiency. For\ninstance, StateFlow achieves 13% and 28% higher success rates compared to ReAct\nin InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.\nWe also show that StateFlow can be combined with iterative refining methods\nlike Reflexion to further improve performance.\n","authors":["Yiran Wu","Tianwei Yue","Shaokun Zhang","Chi Wang","Qingyun Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11322v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03624v2","updated":"2024-08-26T08:09:39Z","published":"2024-07-04T04:19:50Z","title":"Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks","summary":"  Although LLMs have the potential to transform many fields, they still\nunderperform humans in reasoning tasks. Existing methods induce the model to\nproduce step-by-step calculations, but this research explores the question:\nDoes making the LLM analyze the question improve its performance? We propose a\nnovel prompting strategy called Question Analysis Prompting (QAP), in which the\nmodel is prompted to explain the question in $n$ words before solving. The\nvalue of $n$ influences the length of response generated by the model. QAP is\nevaluated on GPT 3.5 Turbo and GPT 4 Turbo on arithmetic datasets GSM8K, AQuA,\nand SAT and commonsense dataset StrategyQA. QAP is compared with other\nstate-of-the-art prompts including Chain-of-Thought (CoT), Plan and Solve\nPrompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all\nstate-of-the-art prompts on AQuA and SAT datasets on both GPT3.5 and GPT4. QAP\nconsistently ranks among the top-2 prompts on 75\\% of the tests. A key factor\nof QAP performance can be attributed to response length, where detailed\nresponses are beneficial when answering harder questions, but can negatively\naffect easy questions.\n","authors":["Dharunish Yugeswardeenoo","Kevin Zhu","Sean O'Brien"],"pdf_url":"https://arxiv.org/pdf/2407.03624v2.pdf","comment":"Accepted in Proceedings of the 62nd Annual Meeting of the Association\n  for Computational Linguistics: Student Research Workshop (ACL-SRW 2024) 11\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.09893v2","updated":"2024-08-26T07:54:27Z","published":"2024-07-13T13:58:24Z","title":"Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks","summary":"  Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART.\n","authors":["Shengbin Yue","Siyuan Wang","Wei Chen","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.09893v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04975v4","updated":"2024-08-26T07:48:19Z","published":"2024-08-09T09:56:30Z","title":"reCSE: Portable Reshaping Features for Sentence Embedding in\n  Self-supervised Contrastive Learning","summary":"  We propose reCSE, a self supervised contrastive learning sentence\nrepresentation framework based on feature reshaping. This framework is\ndifferent from the current advanced models that use discrete data augmentation\nmethods, but instead reshapes the input features of the original sentence,\naggregates the global information of each token in the sentence, and alleviates\nthe common problems of representation polarity and GPU memory consumption\nlinear increase in current advanced models. In addition, our reCSE has achieved\ncompetitive performance in semantic similarity tasks. And the experiment proves\nthat our proposed feature reshaping method has strong universality, which can\nbe transplanted to other self supervised contrastive learning frameworks and\nenhance their representation ability, even achieving state-of-the-art\nperformance. Our code is available at https://github.com/heavenhellchen/reCSE.\n","authors":["Fufangchen Zhao","Jian Gao","Danfeng Yan"],"pdf_url":"https://arxiv.org/pdf/2408.04975v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10903v3","updated":"2024-08-26T07:37:19Z","published":"2024-08-20T14:47:38Z","title":"BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model","summary":"  The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.\n","authors":["Yeyong Yu","Rusheng Yu","Haojie Wei","Zhanqiu Zhang","Quan Qian"],"pdf_url":"https://arxiv.org/pdf/2408.10903v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14053v1","updated":"2024-08-26T07:19:07Z","published":"2024-08-26T07:19:07Z","title":"Enhancing Depression Diagnosis with Chain-of-Thought Prompting","summary":"  When using AI to detect signs of depressive disorder, AI models habitually\ndraw preemptive conclusions. We theorize that using chain-of-thought (CoT)\nprompting to evaluate Patient Health Questionnaire-8 (PHQ-8) scores will\nimprove the accuracy of the scores determined by AI models. In our findings,\nwhen the models reasoned with CoT, the estimated PHQ-8 scores were consistently\ncloser on average to the accepted true scores reported by each participant\ncompared to when not using CoT. Our goal is to expand upon AI models'\nunderstanding of the intricacies of human conversation, allowing them to more\neffectively assess a patient's feelings and tone, therefore being able to more\naccurately discern mental disorder symptoms; ultimately, we hope to augment AI\nmodels' abilities, so that they can be widely accessible and used in the\nmedical field.\n","authors":["Elysia Shi","Adithri Manda","London Chowdhury","Runeema Arun","Kevin Zhu","Michael Lam"],"pdf_url":"https://arxiv.org/pdf/2408.14053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16535v3","updated":"2024-08-26T07:14:46Z","published":"2023-09-28T15:47:03Z","title":"KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language\n  Models","summary":"  Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches\nin changing factual knowledge stored in the Language models. However, there is\na lack of research on whether present locating methods can pinpoint the exact\nparameters embedding the desired knowledge. Moreover, although many researchers\nhave questioned the validity of locality hypothesis of factual knowledge, no\nmethod is provided to test the a hypothesis for more in-depth discussion and\nresearch. Therefore, we introduce KLoB, a benchmark examining three essential\nproperties that a reliable knowledge locating method should satisfy. KLoB can\nserve as a benchmark for evaluating existing locating methods in language\nmodels, and can contributes a method to reassessing the validity of locality\nhypothesis of factual knowledge. KLoB is publicly available at an anonymous\nGitHub: \\url{https://github.com/anon6662/KLoB}.\n","authors":["Yiming Ju","Xingrun Xing","Zhixiong Zeng"],"pdf_url":"https://arxiv.org/pdf/2309.16535v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03791v2","updated":"2024-08-26T07:13:47Z","published":"2024-07-04T09:55:04Z","title":"M5 -- A Diverse Benchmark to Assess the Performance of Large Multimodal\n  Models Across Multilingual and Multicultural Vision-Language Tasks","summary":"  Since the release of ChatGPT, the field of Natural Language Processing has\nexperienced rapid advancements, particularly in Large Language Models (LLMs)\nand their multimodal counterparts, Large Multimodal Models (LMMs). Despite\ntheir impressive capabilities, LLMs often exhibit significant performance\ndisparities across different languages and cultural contexts, as demonstrated\nby various text-only benchmarks. However, current research lacks such\nbenchmarks for multimodal visio-linguistic settings. This work fills this gap\nby introducing M5, the first comprehensive benchmark designed to evaluate LMMs\non diverse vision-language tasks within a multilingual and multicultural\ncontext. M5 includes eight datasets covering five tasks and $41$ languages,\nwith a focus on underrepresented languages and culturally diverse images.\nFurthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a\nnew Visio-Linguistic Outlier Detection task, in which all evaluated open-source\nmodels fail to significantly surpass the random baseline. Through extensive\nevaluation and analyses, we highlight substantial task-agnostic performance\ndisparities between high- and low-resource languages. Moreover, we show that\nlarger models do not necessarily outperform smaller ones in a multilingual\nsetting.\n","authors":["Florian Schneider","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2407.03791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06607v4","updated":"2024-08-26T06:57:51Z","published":"2023-11-11T16:37:41Z","title":"Monkey: Image Resolution and Text Label Are Important Things for Large\n  Multi-modal Models","summary":"  Large Multimodal Models (LMMs) have shown promise in vision-language tasks\nbut struggle with high-resolution input and detailed scene understanding.\nAddressing these challenges, we introduce Monkey to enhance LMM capabilities.\nFirstly, Monkey processes input images by dividing them into uniform patches,\neach matching the size (e.g., 448x448) used in the original training of the\nwell-trained vision encoder. Equipped with individual adapter for each patch,\nMonkey can handle higher resolutions up to 1344x896 pixels, enabling the\ndetailed capture of complex visual information. Secondly, it employs a\nmulti-level description generation method, enriching the context for\nscene-object associations. This two-part strategy ensures more effective\nlearning from generated data: the higher resolution allows for a more detailed\ncapture of visuals, which in turn enhances the effectiveness of comprehensive\ndescriptions. Extensive ablative results validate the effectiveness of our\ndesigns. Additionally, experiments on 18 datasets further demonstrate that\nMonkey surpasses existing LMMs in many tasks like Image Captioning and various\nVisual Question Answering formats. Specially, in qualitative tests focused on\ndense text question answering, Monkey has exhibited encouraging results\ncompared with GPT4V. Code is available at\nhttps://github.com/Yuliang-Liu/Monkey.\n","authors":["Zhang Li","Biao Yang","Qiang Liu","Zhiyin Ma","Shuo Zhang","Jingxu Yang","Yabo Sun","Yuliang Liu","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2311.06607v4.pdf","comment":"CVPR 2024 Highlight"},{"id":"http://arxiv.org/abs/2408.03633v3","updated":"2024-08-26T06:19:53Z","published":"2024-08-07T08:44:44Z","title":"CARE: A Clue-guided Assistant for CSRs to Read User Manuals","summary":"  It is time-saving to build a reading assistant for customer service\nrepresentations (CSRs) when reading user manuals, especially information-rich\nones. Current solutions don't fit the online custom service scenarios well due\nto the lack of attention to user questions and possible responses. Hence, we\npropose to develop a time-saving and careful reading assistant for CSRs, named\nCARE. It can help the CSRs quickly find proper responses from the user manuals\nvia explicit clue chains. Specifically, each of the clue chains is formed by\ninferring over the user manuals, starting from the question clue aligned with\nthe user question and ending at a possible response. To overcome the shortage\nof supervised data, we adopt the self-supervised strategy for model learning.\nThe offline experiment shows that CARE is efficient in automatically inferring\naccurate responses from the user manual. The online experiment further\ndemonstrates the superiority of CARE to reduce CSRs' reading burden and keep\nhigh service quality, in particular with >35% decrease in time spent and\nkeeping a >0.75 ICC score.\n","authors":["Weihong Du","Jia Liu","Zujie Wen","Dingnan Jin","Hongru Liang","Wenqiang Lei"],"pdf_url":"https://arxiv.org/pdf/2408.03633v3.pdf","comment":"Accepted to The 62nd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2024)"},{"id":"http://arxiv.org/abs/2408.14028v1","updated":"2024-08-26T05:38:27Z","published":"2024-08-26T05:38:27Z","title":"SurGen: Text-Guided Diffusion Model for Surgical Video Generation","summary":"  Diffusion-based video generation models have made significant strides,\nproducing outputs with improved visual fidelity, temporal coherence, and user\ncontrol. These advancements hold great promise for improving surgical education\nby enabling more realistic, diverse, and interactive simulation environments.\nIn this study, we introduce SurGen, a text-guided diffusion model tailored for\nsurgical video synthesis, producing the highest resolution and longest duration\nvideos among existing surgical video generation models. We validate the visual\nand temporal quality of the outputs using standard image and video generation\nmetrics. Additionally, we assess their alignment to the corresponding text\nprompts through a deep learning classifier trained on surgical data. Our\nresults demonstrate the potential of diffusion models to serve as valuable\neducational tools for surgical trainees.\n","authors":["Joseph Cho","Samuel Schmidgall","Cyril Zakka","Mrudang Mathur","Rohan Shad","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2408.14028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14026v1","updated":"2024-08-26T05:36:35Z","published":"2024-08-26T05:36:35Z","title":"Empowering Low-Resource Language ASR via Large-Scale Pseudo Labeling","summary":"  In this study, we tackle the challenge of limited labeled data for\nlow-resource languages in ASR, focusing on Hindi. Specifically, we explore\npseudo-labeling, by proposing a generic framework combining multiple ideas from\nexisting works. Our framework integrates multiple base models for transcription\nand evaluators for assessing audio-transcript pairs, resulting in robust\npseudo-labeling for low resource languages. We validate our approach with a new\nbenchmark, IndicYT, comprising diverse YouTube audio files from multiple\ncontent categories. Our findings show that augmenting pseudo labeled data from\nYouTube with existing training data leads to significant performance\nimprovements on IndicYT, without affecting performance on out-of-domain\nbenchmarks, demonstrating the efficacy of pseudo-labeled data in enhancing ASR\ncapabilities for low-resource languages. The benchmark, code and models\ndeveloped as a part of this work will be made publicly available.\n","authors":["Kaushal Santosh Bhogale","Deovrat Mehendale","Niharika Parasa","Sathish Kumar Reddy G","Tahir Javed","Pratyush Kumar","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2408.14026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05561v5","updated":"2024-08-26T05:31:38Z","published":"2024-01-10T22:07:21Z","title":"TrustLLM: Trustworthiness in Large Language Models","summary":"  Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.\n","authors":["Yue Huang","Lichao Sun","Haoran Wang","Siyuan Wu","Qihui Zhang","Yuan Li","Chujie Gao","Yixin Huang","Wenhan Lyu","Yixuan Zhang","Xiner Li","Zhengliang Liu","Yixin Liu","Yijue Wang","Zhikun Zhang","Bertie Vidgen","Bhavya Kailkhura","Caiming Xiong","Chaowei Xiao","Chunyuan Li","Eric Xing","Furong Huang","Hao Liu","Heng Ji","Hongyi Wang","Huan Zhang","Huaxiu Yao","Manolis Kellis","Marinka Zitnik","Meng Jiang","Mohit Bansal","James Zou","Jian Pei","Jian Liu","Jianfeng Gao","Jiawei Han","Jieyu Zhao","Jiliang Tang","Jindong Wang","Joaquin Vanschoren","John Mitchell","Kai Shu","Kaidi Xu","Kai-Wei Chang","Lifang He","Lifu Huang","Michael Backes","Neil Zhenqiang Gong","Philip S. Yu","Pin-Yu Chen","Quanquan Gu","Ran Xu","Rex Ying","Shuiwang Ji","Suman Jana","Tianlong Chen","Tianming Liu","Tianyi Zhou","William Wang","Xiang Li","Xiangliang Zhang","Xiao Wang","Xing Xie","Xun Chen","Xuyu Wang","Yan Liu","Yanfang Ye","Yinzhi Cao","Yong Chen","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.05561v5.pdf","comment":"This work is still under work and we welcome your contribution"},{"id":"http://arxiv.org/abs/2405.14213v2","updated":"2024-08-26T04:59:05Z","published":"2024-05-23T06:17:23Z","title":"From Text to Pixel: Advancing Long-Context Understanding in MLLMs","summary":"  The rapid progress in Multimodal Large Language Models (MLLMs) has\nsignificantly advanced their ability to process and understand complex visual\nand textual information. However, the integration of multiple images and\nextensive textual contexts remains a challenge due to the inherent limitation\nof the models' capacity to handle long input sequences efficiently. In this\npaper, we introduce SEEKER, a multimodal large language model designed to\ntackle this issue. SEEKER aims to optimize the compact encoding of long text by\ncompressing the text sequence into the visual pixel space via images, enabling\nthe model to handle long text within a fixed token-length budget efficiently.\nOur empirical experiments on six long-context multimodal tasks demonstrate that\nSEEKER can leverage fewer image tokens to convey the same amount of textual\ninformation compared with the OCR-based approach, and is more efficient in\nunderstanding long-form multimodal input and generating long-form textual\noutput, outperforming all existing proprietary and open-source MLLMs by large\nmargins.\n","authors":["Yujie Lu","Xiujun Li","Tsu-Jui Fu","Miguel Eckstein","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2405.14213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12529v2","updated":"2024-08-26T04:28:41Z","published":"2024-07-17T13:11:28Z","title":"Crafting the Path: Robust Query Rewriting for Information Retrieval","summary":"  Query rewriting aims to generate a new query that can complement the original\nquery to improve the information retrieval system. Recent studies on query\nrewriting, such as query2doc, query2expand and querey2cot, rely on the internal\nknowledge of Large Language Models (LLMs) to generate a relevant passage to add\ninformation to the query. Nevertheless, the efficacy of these methodologies may\nmarkedly decline in instances where the requisite knowledge is not encapsulated\nwithin the model's intrinsic parameters. In this paper, we propose a novel\nstructured query rewriting method called Crafting the Path tailored for\nretrieval systems. Crafting the Path involves a three-step process that crafts\nquery-related information necessary for finding the passages to be searched in\neach step. Specifically, the Crafting the Path begins with Query Concept\nComprehension, proceeds to Query Type Identification, and finally conducts\nExpected Answer Extraction. Experimental results show that our method\noutperforms previous rewriting methods, especially in less familiar domains for\nLLMs. We demonstrate that our method is less dependent on the internal\nparameter knowledge of the model and generates queries with fewer factual\ninaccuracies. Furthermore, we observe that \\name{} demonstrates superior\nperformance in the retrieval-augmented generation scenarios.\n","authors":["Ingeol Baek","Jimin Lee","Joonho Yang","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2407.12529v2.pdf","comment":"3 figures, 13 tables"},{"id":"http://arxiv.org/abs/2408.12321v2","updated":"2024-08-26T04:27:54Z","published":"2024-08-22T11:57:16Z","title":"MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework\n  for Multimodal Large Language Model","summary":"  This paper presents MaVEn, an innovative Multi-granularity Visual Encoding\nframework designed to enhance the capabilities of Multimodal Large Language\nModels (MLLMs) in multi-image reasoning. Current MLLMs primarily focus on\nsingle-image visual understanding, limiting their ability to interpret and\nintegrate information across multiple images. MaVEn addresses this limitation\nby combining discrete visual symbol sequences, which abstract coarse-grained\nsemantic concepts, with traditional continuous representation sequences that\nmodel fine-grained features. This dual approach bridges the semantic gap\nbetween visual and textual data, thereby improving the model's ability to\nprocess and interpret information from multiple images effectively.\nAdditionally, we design a dynamic reduction mechanism by for long-sequence\ncontinuous features to enhance multi-image processing efficiency. Experimental\nresults demonstrate that MaVEn significantly enhances MLLMs' understanding in\ncomplex multi-image scenarios, while also improving performance in single-image\ncontexts.\n","authors":["Chaoya Jiang","Jia Hongrui","Haiyang Xu","Wei Ye","Mengfan Dong","Ming Yan","Ji Zhang","Fei Huang","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.12321v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13282v1","updated":"2024-08-26T02:53:55Z","published":"2024-08-26T02:53:55Z","title":"Question answering system of bridge design specification based on large\n  language model","summary":"  This paper constructs question answering system for bridge design\nspecification based on large language model. Three implementation schemes are\ntried: full fine-tuning of the Bert pretrained model, parameter-efficient\nfine-tuning of the Bert pretrained model, and self-built language model from\nscratch. Through the self-built question and answer task dataset, based on the\ntensorflow and keras deep learning platform framework, the model is constructed\nand trained to predict the start position and end position of the answer in the\nbridge design specification given by the user. The experimental results show\nthat full fine-tuning of the Bert pretrained model achieves 100% accuracy in\nthe training-dataset, validation-dataset and test-dataset, and the system can\nextract the answers from the bridge design specification given by the user to\nanswer various questions of the user; While parameter-efficient fine-tuning of\nthe Bert pretrained model and self-built language model from scratch perform\nwell in the training-dataset, their generalization ability in the test-dataset\nneeds to be improved. The research of this paper provides a useful reference\nfor the development of question answering system in professional field.\n","authors":["Leye Zhang","Xiangxiang Tian","Hongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.13282v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.13987v1","updated":"2024-08-26T02:53:24Z","published":"2024-08-26T02:53:24Z","title":"Focused Large Language Models are Stable Many-Shot Learners","summary":"  In-Context Learning (ICL) enables large language models (LLMs) to achieve\nrapid task adaptation by learning from demonstrations. With the increase in\navailable context length of LLMs, recent experiments have shown that the\nperformance of ICL does not necessarily scale well in many-shot (demonstration)\nsettings. We theoretically and experimentally confirm that the reason lies in\nmore demonstrations dispersing the model attention from the query, hindering\nits understanding of key content. Inspired by how humans learn from examples,\nwe propose a training-free method FocusICL, which conducts triviality filtering\nto avoid attention being diverted by unimportant contents at token-level and\noperates hierarchical attention to further ensure sufficient attention towards\ncurrent query at demonstration-level. We also design an efficient\nhyperparameter searching strategy for FocusICL based on model perplexity of\ndemonstrations. Comprehensive experiments validate that FocusICL achieves an\naverage performance improvement of 5.2% over vanilla ICL and scales well with\nmany-shot demonstrations.\n","authors":["Peiwen Yuan","Shaoxiong Feng","Yiwei Li","Xinglin Wang","Yueqi Zhang","Chuyi Tan","Boyuan Pan","Heda Wang","Yao Hu","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2408.13987v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2305.07895v7","updated":"2024-08-26T02:37:14Z","published":"2023-05-13T11:28:37Z","title":"OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models","summary":"  Large models have recently played a dominant role in natural language\nprocessing and multimodal vision-language learning. However, their\neffectiveness in text-related visual tasks remains relatively unexplored. In\nthis paper, we conducted a comprehensive evaluation of Large Multimodal Models,\nsuch as GPT4V and Gemini, in various text-related visual tasks including Text\nRecognition, Scene Text-Centric Visual Question Answering (VQA),\nDocument-Oriented VQA, Key Information Extraction (KIE), and Handwritten\nMathematical Expression Recognition (HMER). To facilitate the assessment of\nOptical Character Recognition (OCR) capabilities in Large Multimodal Models, we\npropose OCRBench, a comprehensive evaluation benchmark. OCRBench contains 29\ndatasets, making it the most comprehensive OCR evaluation benchmark available.\nFurthermore, our study reveals both the strengths and weaknesses of these\nmodels, particularly in handling multilingual text, handwritten text,\nnon-semantic text, and mathematical expression recognition. Most importantly,\nthe baseline results presented in this study could provide a foundational\nframework for the conception and assessment of innovative strategies targeted\nat enhancing zero-shot multimodal techniques. The evaluation pipeline and\nbenchmark are available at https://github.com/Yuliang-Liu/MultimodalOCR.\n","authors":["Yuliang Liu","Zhang Li","Mingxin Huang","Biao Yang","Wenwen Yu","Chunyuan Li","Xucheng Yin","Cheng-lin Liu","Lianwen Jin","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2305.07895v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13986v1","updated":"2024-08-26T02:36:55Z","published":"2024-08-26T02:36:55Z","title":"AgentMove: Predicting Human Mobility Anywhere Using Large Language Model\n  based Agentic Framework","summary":"  Human mobility prediction plays a crucial role in various real-world\napplications. Although deep learning based models have shown promising results\nover the past decade, their reliance on extensive private mobility data for\ntraining and their inability to perform zero-shot predictions, have hindered\nfurther advancements. Recently, attempts have been made to apply large language\nmodels (LLMs) to mobility prediction task. However, their performance has been\nconstrained by the absence of a systematic design of workflow. They directly\ngenerate the final output using LLMs, which limits the potential of LLMs to\nuncover complex mobility patterns and underestimates their extensive reserve of\nglobal geospatial knowledge. In this paper, we introduce AgentMove, a\nsystematic agentic prediction framework to achieve generalized mobility\nprediction for any cities worldwide. In AgentMove, we first decompose the\nmobility prediction task into three sub-tasks and then design corresponding\nmodules to complete these subtasks, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments on mobility data from two sources in 12 cities\ndemonstrate that AgentMove outperforms the best baseline more than 8% in\nvarious metrics and it shows robust predictions with various LLMs as base and\nalso less geographical bias across cities. Codes and data can be found in\nhttps://github.com/tsinghua-fib-lab/AgentMove.\n","authors":["Jie Feng","Yuwei Du","Jie Zhao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2408.13986v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2408.13985v1","updated":"2024-08-26T02:35:37Z","published":"2024-08-26T02:35:37Z","title":"TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models","summary":"  With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies.\n","authors":["Zelin Li","Kehai Chen","Xuefeng Bai","Lemao Liu","Mingming Yang","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.13985v1.pdf","comment":"14 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2305.17440 by other authors"},{"id":"http://arxiv.org/abs/2408.12095v2","updated":"2024-08-26T02:26:31Z","published":"2024-08-22T03:08:49Z","title":"uMedSum: A Unified Framework for Advancing Medical Abstractive\n  Summarization","summary":"  Medical abstractive summarization faces the challenge of balancing\nfaithfulness and informativeness. Current methods often sacrifice key\ninformation for faithfulness or introduce confabulations when prioritizing\ninformativeness. While recent advancements in techniques like in-context\nlearning (ICL) and fine-tuning have improved medical summarization, they often\noverlook crucial aspects such as faithfulness and informativeness without\nconsidering advanced methods like model reasoning and self-improvement.\nMoreover, the field lacks a unified benchmark, hindering systematic evaluation\ndue to varied metrics and datasets. This paper addresses these gaps by\npresenting a comprehensive benchmark of six advanced abstractive summarization\nmethods across three diverse datasets using five standardized metrics. Building\non these findings, we propose uMedSum, a modular hybrid summarization framework\nthat introduces novel approaches for sequential confabulation removal followed\nby key missing information addition, ensuring both faithfulness and\ninformativeness. Our work improves upon previous GPT-4-based state-of-the-art\n(SOTA) medical summarization methods, significantly outperforming them in both\nquantitative metrics and qualitative domain expert evaluations. Notably, we\nachieve an average relative performance improvement of 11.8% in reference-free\nmetrics over the previous SOTA. Doctors prefer uMedSum's summaries 6 times more\nthan previous SOTA in difficult cases where there are chances of confabulations\nor missing information. These results highlight uMedSum's effectiveness and\ngeneralizability across various datasets and metrics, marking a significant\nadvancement in medical summarization.\n","authors":["Aishik Nagar","Yutong Liu","Andy T. Liu","Viktor Schlegel","Vijay Prakash Dwivedi","Arun-Kumar Kaliya-Perumal","Guna Pratheep Kalanchiam","Yili Tang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2408.12095v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2310.03328v3","updated":"2024-08-26T02:05:37Z","published":"2023-10-05T05:55:06Z","title":"Reformulating Domain Adaptation of Large Language Models as\n  Adapt-Retrieve-Revise: A Case Study on Chinese Legal Domain","summary":"  While large language models (LLMs) like GPT-4 have recently demonstrated\nastonishing zero-shot capabilities in general domain tasks, they often generate\ncontent with hallucinations in specific domains such as Chinese law, hindering\ntheir application in these areas. This is typically due to the absence of\ntraining data that encompasses such a specific domain, preventing GPT-4 from\nacquiring in-domain knowledge. A pressing challenge is that it's not plausible\nto continue training LLMs of such scale on in-domain data.\n  This paper introduces a simple and effective domain adaptation framework for\nGPT-4 by reformulating generation as an \\textbf{adapt-retrieve-revise} process.\nThe initial step is to \\textbf{adapt} an affordable 7B LLM to the target domain\nby continuing learning on in-domain data. When solving a task, we leverage the\nadapted LLM to generate a draft answer given a task query. Then, the draft\nanswer will be used to \\textbf{retrieve} supporting evidence candidates from an\nexternal in-domain knowledge base. Finally, the draft answer and retrieved\nevidence are concatenated into a whole prompt to let GPT-4 assess the evidence\nand \\textbf{revise} the draft answer to generate the final answer.\n  Our proposal combines the advantages of the efficiency of adapting a smaller\n7B model with the evidence-assessing capability of GPT-4 and effectively\nprevents GPT-4 from generating hallucinatory content. In the zero-shot setting\nof four Chinese legal tasks, our method improves accuracy by 33.3\\% compared to\nthe direct generation by GPT-4. When compared to two stronger retrieval-based\nbaselines, our method outperforms them by 15.4\\% and 23.9\\%. Our code will be\nreleased\n","authors":["Zhen wan","Yating Zhang","Yexiang Wang","Fei Cheng","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2310.03328v3.pdf","comment":"Accepted by ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2407.07275v2","updated":"2024-08-26T00:55:01Z","published":"2024-07-09T23:39:37Z","title":"Remastering Divide and Remaster: A Cinematic Audio Source Separation\n  Dataset with Multilingual Support","summary":"  Cinematic audio source separation (CASS), as a problem of extracting the\ndialogue, music, and effects stems from their mixture, is a relatively new\nsubtask of audio source separation. To date, only one publicly available\ndataset exists for CASS, that is, the Divide and Remaster (DnR) dataset, which\nis currently at version 2. While DnR v2 has been an incredibly useful resource\nfor CASS, several areas of improvement have been identified, particularly\nthrough its use in the 2023 Sound Demixing Challenge. In this work, we develop\nversion 3 of the DnR dataset, addressing issues relating to vocal content in\nnon-dialogue stems, loudness distributions, mastering process, and linguistic\ndiversity. In particular, the dialogue stem of DnR v3 includes speech content\nfrom more than 30 languages from multiple families including but not limited to\nthe Germanic, Romance, Indo-Aryan, Dravidian, Malayo-Polynesian, and Bantu\nfamilies. Benchmark results using the Bandit model indicated that training on\nmultilingual data yields significant generalizability to the model even in\nlanguages with low data availability. Even in languages with high data\navailability, the multilingual model often performs on par or better than\ndedicated models trained on monolingual CASS datasets. Dataset and model\nimplementation will be made available at\nhttps://github.com/kwatcharasupat/source-separation-landing.\n","authors":["Karn N. Watcharasupat","Chih-Wei Wu","Iroro Orife"],"pdf_url":"https://arxiv.org/pdf/2407.07275v2.pdf","comment":"Accepted to the 5th IEEE International Symposium on the Internet of\n  Sounds. Camera-ready version"},{"id":"http://arxiv.org/abs/2408.13966v1","updated":"2024-08-26T00:23:56Z","published":"2024-08-26T00:23:56Z","title":"Reducing the Cost: Cross-Prompt Pre-Finetuning for Short Answer Scoring","summary":"  Automated Short Answer Scoring (SAS) is the task of automatically scoring a\ngiven input to a prompt based on rubrics and reference answers. Although SAS is\nuseful in real-world applications, both rubrics and reference answers differ\nbetween prompts, thus requiring a need to acquire new data and train a model\nfor each new prompt. Such requirements are costly, especially for schools and\nonline courses where resources are limited and only a few prompts are used. In\nthis work, we attempt to reduce this cost through a two-phase approach: train a\nmodel on existing rubrics and answers with gold score signals and finetune it\non a new prompt. Specifically, given that scoring rubrics and reference answers\ndiffer for each prompt, we utilize key phrases, or representative expressions\nthat the answer should contain to increase scores, and train a SAS model to\nlearn the relationship between key phrases and answers using already annotated\nprompts (i.e., cross-prompts). Our experimental results show that finetuning on\nexisting cross-prompt data with key phrases significantly improves scoring\naccuracy, especially when the training data is limited. Finally, our extensive\nanalysis shows that it is crucial to design the model so that it can learn the\ntask's general property.\n","authors":["Hiroaki Funayama","Yuya Asazuma","Yuichiroh Matsubayashi","Tomoya Mizumoto","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2408.13966v1.pdf","comment":"This is the draft submitted to AIED 2023. For the latest version,\n  please visit: https://link.springer.com/chapter/10.1007/978-3-031-36272-9_7"},{"id":"http://arxiv.org/abs/2408.14698v1","updated":"2024-08-26T23:52:27Z","published":"2024-08-26T23:52:27Z","title":"Smart Multi-Modal Search: Contextual Sparse and Dense Embedding\n  Integration in Adobe Express","summary":"  As user content and queries become increasingly multi-modal, the need for\neffective multi-modal search systems has grown. Traditional search systems\noften rely on textual and metadata annotations for indexed images, while\nmulti-modal embeddings like CLIP enable direct search using text and image\nembeddings. However, embedding-based approaches face challenges in integrating\ncontextual features such as user locale and recency. Building a scalable\nmulti-modal search system requires fine-tuning several components. This paper\npresents a multi-modal search architecture and a series of AB tests that\noptimize embeddings and multi-modal technologies in Adobe Express template\nsearch. We address considerations such as embedding model selection, the roles\nof embeddings in matching and ranking, and the balance between dense and sparse\nembeddings. Our iterative approach demonstrates how utilizing sparse, dense,\nand contextual features enhances short and long query search, significantly\nreduces null rates (over 70\\%), and increases click-through rates (CTR). Our\nfindings provide insights into developing robust multi-modal search systems,\nthereby enhancing relevance for complex queries.\n","authors":["Cherag Aroraa","Tracy Holloway King","Jayant Kumar","Yi Lu","Sanat Sharma","Arvind Srikantan","David Uvalle","Josep Valls-Vargas","Harsha Vardhan"],"pdf_url":"https://arxiv.org/pdf/2408.14698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14690v1","updated":"2024-08-26T23:30:15Z","published":"2024-08-26T23:30:15Z","title":"Training-Free Activation Sparsity in Large Language Models","summary":"  Activation sparsity can enable practical inference speedups in large language\nmodels (LLMs) by reducing the compute and memory-movement required for matrix\nmultiplications during the forward pass. However, existing methods face\nlimitations that inhibit widespread adoption. Some approaches are tailored\ntowards older models with ReLU-based sparsity, while others require extensive\ncontinued pre-training on up to hundreds of billions of tokens. This paper\ndescribes TEAL, a simple training-free method that applies magnitude-based\nactivation sparsity to hidden states throughout the entire model. TEAL achieves\n40-50% model-wide sparsity with minimal performance degradation across Llama-2,\nLlama-3, and Mistral families, with sizes varying from 7B to 70B. We improve\nexisting sparse kernels and demonstrate wall-clock decoding speed-ups of up to\n1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is\ncompatible with weight quantization, enabling further efficiency gains.\n","authors":["James Liu","Pragaash Ponnusamy","Tianle Cai","Han Guo","Yoon Kim","Ben Athiwaratkun"],"pdf_url":"https://arxiv.org/pdf/2408.14690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06264v2","updated":"2024-08-26T22:53:51Z","published":"2024-02-09T09:25:18Z","title":"LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to\n  Support Art Appreciation Education","summary":"  Art appreciation is vital in nurturing critical thinking and emotional\nintelligence among learners. However, traditional art appreciation education\nhas often been hindered by limited access to art resources, especially for\ndisadvantaged students, and an imbalanced emphasis on STEM subjects in\nmainstream education. In response to these challenges, recent technological\nadvancements have paved the way for innovative solutions. This study explores\nthe application of multi-modal large language models (MLLMs) in art\nappreciation education, focusing on developing LLaVA-Docent, a model that\nleverages these advancements. Our approach involved a comprehensive literature\nreview and consultations with experts in the field, leading to developing a\nrobust data framework. Utilizing this framework, we generated a virtual\ndialogue dataset that was leveraged by GPT-4. This dataset was instrumental in\ntraining the MLLM, named LLaVA-Docent. Six researchers conducted quantitative\nand qualitative evaluations of LLaVA-Docent to assess its effectiveness,\nbenchmarking it against the GPT-4 model in a few-shot setting. The evaluation\nprocess revealed distinct strengths and weaknesses of the LLaVA-Docent model.\nOur findings highlight the efficacy of LLaVA-Docent in enhancing the\naccessibility and engagement of art appreciation education. By harnessing the\npotential of MLLMs, this study makes a significant contribution to the field of\nart education, proposing a novel methodology that reimagines the way art\nappreciation is taught and experienced.\n","authors":["Unggi Lee","Minji Jeon","Yunseo Lee","Gyuri Byun","Yoorim Son","Jaeyoon Shin","Hongkyu Ko","Hyeoncheol Kim"],"pdf_url":"https://arxiv.org/pdf/2402.06264v2.pdf","comment":"37 pages, 4 figures, 10 tables"},{"id":"http://arxiv.org/abs/2406.10774v2","updated":"2024-08-26T21:01:02Z","published":"2024-06-16T01:33:02Z","title":"Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference","summary":"  As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .\n","authors":["Jiaming Tang","Yilong Zhao","Kan Zhu","Guangxuan Xiao","Baris Kasikci","Song Han"],"pdf_url":"https://arxiv.org/pdf/2406.10774v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.14636v1","updated":"2024-08-26T21:00:25Z","published":"2024-08-26T21:00:25Z","title":"Relationships are Complicated! An Analysis of Relationships Between\n  Datasets on the Web","summary":"  The Web today has millions of datasets, and the number of datasets continues\nto grow at a rapid pace. These datasets are not standalone entities; rather,\nthey are intricately connected through complex relationships. Semantic\nrelationships between datasets provide critical insights for research and\ndecision-making processes. In this paper, we study dataset relationships from\nthe perspective of users who discover, use, and share datasets on the Web: what\nrelationships are important for different tasks? What contextual information\nmight users want to know? We first present a comprehensive taxonomy of\nrelationships between datasets on the Web and map these relationships to user\ntasks performed during dataset discovery. We develop a series of methods to\nidentify these relationships and compare their performance on a large corpus of\ndatasets generated from Web pages with schema.org markup. We demonstrate that\nmachine-learning based methods that use dataset metadata achieve multi-class\nclassification accuracy of 90%. Finally, we highlight gaps in available\nsemantic markup for datasets and discuss how incorporating comprehensive\nsemantics can facilitate the identification of dataset relationships. By\nproviding a comprehensive overview of dataset relationships at scale, this\npaper sets a benchmark for future research.\n","authors":["Kate Lin","Tarfah Alrashed","Natasha Noy"],"pdf_url":"https://arxiv.org/pdf/2408.14636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03605v2","updated":"2024-08-26T20:48:19Z","published":"2024-04-04T17:25:30Z","title":"Mitigating the Impact of Outlier Channels for Language Model\n  Quantization with Activation Regularization","summary":"  We consider the problem of accurate quantization for language models, where\nboth the weights and activations are uniformly quantized to 4 bits per\nparameter, the lowest bitwidth format natively supported by GPU hardware. In\nthis context, the key challenge is activation quantization: it is known that\nlanguage models contain outlier channels whose values on average are orders of\nmagnitude higher than than other channels, which prevents accurate low-bitwidth\nquantization with known techniques. We systematically study this phenomena and\nfind that these outlier channels emerge early in training, and that they occur\nmore frequently in layers with residual streams. We then propose a simple\nstrategy which regularizes a layer's inputs via quantization-aware training\n(QAT) and its outputs via activation kurtosis regularization. We show that\nregularizing both the inputs and outputs is crucial for preventing a model's\n\"migrating\" the difficulty in input quantization to the weights, which makes\npost-training quantization (PTQ) of weights more difficult. When combined with\nweight PTQ, we show that our approach can obtain a W4A4 model that performs\ncompetitively to the standard-precision W16A16 baseline.\n","authors":["Aniruddha Nrusimha","Mayank Mishra","Naigang Wang","Dan Alistarh","Rameswar Panda","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2404.03605v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14623v1","updated":"2024-08-26T20:36:52Z","published":"2024-08-26T20:36:52Z","title":"MODOC: A Modular Interface for Flexible Interlinking of Text Retrieval\n  and Text Generation Functions","summary":"  Large Language Models (LLMs) produce eloquent texts but often the content\nthey generate needs to be verified. Traditional information retrieval systems\ncan assist with this task, but most systems have not been designed with\nLLM-generated queries in mind. As such, there is a compelling need for\nintegrated systems that provide both retrieval and generation functionality\nwithin a single user interface.\n  We present MODOC, a modular user interface that leverages the capabilities of\nLLMs and provides assistance with detecting their confabulations, promoting\nintegrity in scientific writing. MODOC represents a significant step forward in\nscientific writing assistance. Its modular architecture supports flexible\nfunctions for retrieving information and for writing and generating text in a\nsingle, user-friendly interface.\n","authors":["Yingqiang Gao","Jhony Prada","Nianlong Gu","Jessica Lam","Richard H. R. Hahnloser"],"pdf_url":"https://arxiv.org/pdf/2408.14623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14622v1","updated":"2024-08-26T20:35:42Z","published":"2024-08-26T20:35:42Z","title":"What Makes a Good Story and How Can We Measure It? A Comprehensive\n  Survey of Story Evaluation","summary":"  With the development of artificial intelligence, particularly the success of\nLarge Language Models (LLMs), the quantity and quality of automatically\ngenerated stories have significantly increased. This has led to the need for\nautomatic story evaluation to assess the generative capabilities of computing\nsystems and analyze the quality of both automatic-generated and human-written\nstories. Evaluating a story can be more challenging than other generation\nevaluation tasks. While tasks like machine translation primarily focus on\nassessing the aspects of fluency and accuracy, story evaluation demands complex\nadditional measures such as overall coherence, character development,\ninterestingness, etc. This requires a thorough review of relevant research. In\nthis survey, we first summarize existing storytelling tasks, including\ntext-to-text, visual-to-text, and text-to-visual. We highlight their evaluation\nchallenges, identify various human criteria to measure stories, and present\nexisting benchmark datasets. Then, we propose a taxonomy to organize evaluation\nmetrics that have been developed or can be adopted for story evaluation. We\nalso provide descriptions of these metrics, along with the discussion of their\nmerits and limitations. Later, we discuss the human-AI collaboration for story\nevaluation and generation. Finally, we suggest potential future research\ndirections, extending from story evaluation to general evaluations.\n","authors":["Dingyi Yang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2408.14622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01663v3","updated":"2024-08-26T20:30:40Z","published":"2024-04-02T06:07:35Z","title":"CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models","summary":"  Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs.\n","authors":["Xuechen Liang","Meiling Tao","Yinghui Xia","Tianyu Shi","Jun Wang","JingSong Yang"],"pdf_url":"https://arxiv.org/pdf/2404.01663v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06494v2","updated":"2024-08-26T20:10:52Z","published":"2024-08-12T21:04:16Z","title":"What Color Scheme is More Effective in Assisting Readers to Locate\n  Information in a Color-Coded Article?","summary":"  Color coding, a technique assigning specific colors to cluster information\ntypes, has proven advantages in aiding human cognitive activities, especially\nreading and comprehension. The rise of Large Language Models (LLMs) has\nstreamlined document coding, enabling simple automatic text labeling with\nvarious schemes. This has the potential to make color-coding more accessible\nand benefit more users. However, the impact of color choice on information\nseeking is understudied. We conducted a user study assessing various color\nschemes' effectiveness in LLM-coded text documents, standardizing contrast\nratios to approximately 5.55:1 across schemes. Participants performed timed\ninformation-seeking tasks in color-coded scholarly abstracts. Results showed\nnon-analogous and yellow-inclusive color schemes improved performance, with the\nlatter also being more preferred by participants. These findings can inform\nbetter color scheme choices for text annotation. As LLMs advance document\ncoding, we advocate for more research focusing on the \"color\" aspect of\ncolor-coding techniques.\n","authors":["Ho Yin Ng","Zeyu He","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2408.06494v2.pdf","comment":"This paper will appear at IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2405.11083v2","updated":"2024-08-26T20:02:16Z","published":"2024-05-17T20:30:49Z","title":"Prompt Exploration with Prompt Regression","summary":"  In the advent of democratized usage of large language models (LLMs), there is\na growing desire to systematize LLM prompt creation and selection processes\nbeyond iterative trial-and-error. Prior works majorly focus on searching the\nspace of prompts without accounting for relations between prompt variations.\nHere we propose a framework, Prompt Exploration with Prompt Regression (PEPR),\nto predict the effect of prompt combinations given results for individual\nprompt elements as well as a simple method to select an effective prompt for a\ngiven use-case. We evaluate our approach with open-source LLMs of different\nsizes on several different tasks.\n","authors":["Michael Feffer","Ronald Xu","Yuekai Sun","Mikhail Yurochkin"],"pdf_url":"https://arxiv.org/pdf/2405.11083v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2406.06484v2","updated":"2024-08-26T19:50:37Z","published":"2024-06-10T17:24:42Z","title":"Parallelizing Linear Transformers with the Delta Rule over Sequence\n  Length","summary":"  Transformers with linear attention (i.e., linear transformers) and\nstate-space models have recently been suggested as a viable linear-time\nalternative to transformers with softmax attention. However, these models still\nunderperform transformers especially on tasks that require in-context\nretrieval. While more expressive variants of linear transformers which replace\nthe additive outer-product update in linear transformers with the delta rule\nhave been found to be more effective at associative recall, existing algorithms\nfor training such models do not parallelize over sequence length and are thus\ninefficient to train on modern hardware. This work describes a\nhardware-efficient algorithm for training linear transformers with the delta\nrule, which exploits a memory-efficient representation for computing products\nof Householder matrices. This algorithm allows us to scale up DeltaNet to\nstandard language modeling settings. We train a 1.3B model for 100B tokens and\nfind that it outperforms recent linear-time baselines such as Mamba and GLA in\nterms of perplexity and zero-shot performance on downstream tasks (including on\ntasks that focus on recall). We also experiment with two hybrid models which\ncombine DeltaNet layers with (1) sliding-window attention layers every other\nlayer or (2) two global attention layers, and find that these hybrid models\noutperform strong transformer baselines.\n","authors":["Songlin Yang","Bailin Wang","Yu Zhang","Yikang Shen","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2406.06484v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2408.14595v1","updated":"2024-08-26T19:26:55Z","published":"2024-08-26T19:26:55Z","title":"Surprisingly Fragile: Assessing and Addressing Prompt Instability in\n  Multimodal Foundation Models","summary":"  Multimodal foundation models (MFMs) such as OFASys show the potential to\nunlock analysis of complex data such as images, videos, and audio data via text\nprompts alone. However, their performance may suffer in the face of text input\nthat differs even slightly from their training distribution, which is\nsurprising considering the use of modality-specific data to \"ground\" the text\ninput. This study demonstrates that prompt instability is a major concern for\nMFMs, leading to a consistent drop in performance across all modalities, but\nthat instability can be mitigated with additional training with augmented data.\nWe evaluate several methods for grounded prompt perturbation, where we generate\nperturbations and filter based on similarity to text and/or modality data.\nAfter re-training the models on the augmented data, we find improved accuracy\nand more stable performance on the perturbed test data regardless of\nperturbation condition, suggesting that the data augmentation strategy helps\nthe models handle domain shifts more effectively. In error analysis, we find\nconsistent patterns of performance improvement across domains, suggesting that\nretraining on prompt perturbations tends to help general reasoning capabilities\nin MFMs.\n","authors":["Ian Stewart","Sameera Horawalavithana","Brendan Kennedy","Sai Munikoti","Karl Pazdernik"],"pdf_url":"https://arxiv.org/pdf/2408.14595v1.pdf","comment":"in submission"},{"id":"http://arxiv.org/abs/2402.17700v2","updated":"2024-08-26T19:26:06Z","published":"2024-02-27T17:25:37Z","title":"RAVEL: Evaluating Interpretability Methods on Disentangling Language\n  Model Representations","summary":"  Individual neurons participate in the representation of multiple high-level\nconcepts. To what extent can different interpretability methods successfully\ndisentangle these roles? To help address this question, we introduce RAVEL\n(Resolving Attribute-Value Entanglements in Language Models), a dataset that\nenables tightly controlled, quantitative comparisons between a variety of\nexisting interpretability methods. We use the resulting conceptual framework to\ndefine the new method of Multi-task Distributed Alignment Search (MDAS), which\nallows us to find distributed representations satisfying multiple causal\ncriteria. With Llama2-7B as the target language model, MDAS achieves\nstate-of-the-art results on RAVEL, demonstrating the importance of going beyond\nneuron-level analyses to identify features distributed across activations. We\nrelease our benchmark at https://github.com/explanare/ravel.\n","authors":["Jing Huang","Zhengxuan Wu","Christopher Potts","Mor Geva","Atticus Geiger"],"pdf_url":"https://arxiv.org/pdf/2402.17700v2.pdf","comment":"Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2024)"},{"id":"http://arxiv.org/abs/2408.14572v1","updated":"2024-08-26T18:42:59Z","published":"2024-08-26T18:42:59Z","title":"CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting\n  Mitigation","summary":"  This paper introduces CURLoRA, a novel approach to fine-tuning large language\nmodels (LLMs) that leverages CUR matrix decomposition in the context of\nLow-Rank Adaptation (LoRA). Our method addresses two critical challenges in LLM\nfine-tuning: mitigating catastrophic forgetting during continual learning and\nreducing the number of trainable parameters. We propose a unique modification\nto the CUR decomposition process, utilizing inverted probabilities for column\nand row selection which acts as an implicit regularization, and initializing\nthe $U$ matrix as a zero matrix, and only fine-tuning it. We demonstrate\nthrough experiments on multiple datasets that CURLoRA outperforms standard LoRA\nin mitigating catastrophic forgetting. It maintains model stability and\nperformance across tasks while significantly reducing the number of trainable\nparameters. Our results show that CURLoRA achieves very good and stable task\naccuracy while maintaining base model's perplexity scores fixed compared to\nLoRA upon continual fine-tuning, particularly in scenarios with limited data.\n","authors":["Muhammad Fawi"],"pdf_url":"https://arxiv.org/pdf/2408.14572v1.pdf","comment":"Code available at https://github.com/MNoorFawi/curlora"},{"id":"http://arxiv.org/abs/2408.14568v1","updated":"2024-08-26T18:39:31Z","published":"2024-08-26T18:39:31Z","title":"Improving Clinical Note Generation from Complex Doctor-Patient\n  Conversation","summary":"  Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods.\n","authors":["Yizhan Li","Sifan Wu","Christopher Smith","Thomas Lo","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14547v1","updated":"2024-08-26T18:00:33Z","published":"2024-08-26T18:00:33Z","title":"Revisiting Image Captioning Training Paradigm via Direct CLIP-based\n  Optimization","summary":"  The conventional training approach for image captioning involves pre-training\na network using teacher forcing and subsequent fine-tuning with Self-Critical\nSequence Training to maximize hand-crafted captioning metrics. However, when\nattempting to optimize modern and higher-quality metrics like CLIP-Score and\nPAC-Score, this training method often encounters instability and fails to\nacquire the genuine descriptive capabilities needed to produce fluent and\ninformative captions. In this paper, we propose a new training paradigm termed\nDirect CLIP-Based Optimization (DiCO). Our approach jointly learns and\noptimizes a reward model that is distilled from a learnable captioning\nevaluator with high human correlation. This is done by solving a weighted\nclassification problem directly inside the captioner. At the same time, DiCO\nprevents divergence from the original model, ensuring that fluency is\nmaintained. DiCO not only exhibits improved stability and enhanced quality in\nthe generated captions but also aligns more closely with human preferences\ncompared to existing methods, especially in modern metrics. Additionally, it\nmaintains competitive performance in traditional metrics. Our source code and\ntrained models are publicly available at https://github.com/aimagelab/DiCO.\n","authors":["Nicholas Moratelli","Davide Caffagni","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2408.14547v1.pdf","comment":"BMVC 2024"},{"id":"http://arxiv.org/abs/2012.12311v4","updated":"2024-08-26T15:34:13Z","published":"2020-12-22T19:32:52Z","title":"Unboxing Engagement in YouTube Influencer Videos: An Attention-Based\n  Approach","summary":"  Influencer marketing videos have surged in popularity, yet significant gaps\nremain in understanding the relationship between video features and engagement.\nThis challenge is intensified by the complexities of interpreting unstructured\ndata. While deep learning models effectively leverage unstructured data to\npredict business outcomes, they often function as black boxes with limited\ninterpretability, particularly when human validation is hindered by the absence\nof a known ground truth. To address this issue, the authors develop an\n\"interpretable deep learning framework\" that not only makes good out-of-sample\npredictions using unstructured data but also provides insights into the\ncaptured relationships. Inspired by visual attention in print advertising, the\ninterpretation approach uses measures of model attention to video features,\neliminating spurious associations through a two-step process and shortlisting\nrelationships for formal causal testing. This method is applicable across\nwell-known attention mechanisms - additive attention, scaled dot-product\nattention, and gradient-based attention - when analyzing text, audio, or video\nimage data. Validated using simulations, this approach outperforms benchmark\nfeature selection methods. This framework is applied to YouTube influencer\nvideos, linking video features to measures of shallow and deep engagement\ndeveloped based on the dual-system framework of thinking. The findings guide\ninfluencers and brands in prioritizing video features associated with deep\nengagement.\n","authors":["Prashant Rajaram","Puneet Manchanda"],"pdf_url":"https://arxiv.org/pdf/2012.12311v4.pdf","comment":"50 pages, Online Appendix"},{"id":"http://arxiv.org/abs/2408.13985v1","updated":"2024-08-26T02:35:37Z","published":"2024-08-26T02:35:37Z","title":"TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models","summary":"  With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies.\n","authors":["Zelin Li","Kehai Chen","Xuefeng Bai","Lemao Liu","Mingming Yang","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.13985v1.pdf","comment":"14 pages, 6 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.14471v1","updated":"2024-08-26T17:59:01Z","published":"2024-08-26T17:59:01Z","title":"A Practitioner's Guide to Continual Multimodal Pretraining","summary":"  Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux.\n","authors":["Karsten Roth","Vishaal Udandarao","Sebastian Dziadzio","Ameya Prabhu","Mehdi Cherti","Oriol Vinyals","Olivier HÃ©naff","Samuel Albanie","Matthias Bethge","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2408.14471v1.pdf","comment":"Technical Report. 52 pages"},{"id":"http://arxiv.org/abs/2408.14469v1","updated":"2024-08-26T17:58:47Z","published":"2024-08-26T17:58:47Z","title":"Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos","summary":"  This paper considers the problem of Multi-Hop Video Question Answering\n(MH-VidQA) in long-form egocentric videos. This task not only requires to\nanswer visual questions, but also to localize multiple relevant time intervals\nwithin the video as visual evidences. We develop an automated pipeline to\ncreate multi-hop question-answering pairs with associated temporal evidence,\nenabling to construct a large-scale dataset for instruction-tuning. To monitor\nthe progress of this new task, we further curate a high-quality benchmark,\nMultiHop-EgoQA, with careful manual verification and refinement. Experimental\nresults reveal that existing multi-modal systems exhibit inadequate multi-hop\ngrounding and reasoning abilities, resulting in unsatisfactory performance. We\nthen propose a novel architecture, termed as Grounding Scattered Evidence with\nLarge Language Model (GeLM), that enhances multi-modal large language models\n(MLLMs) by incorporating a grounding module to retrieve temporal evidence from\nvideos using flexible grounding tokens. Trained on our visual instruction data,\nGeLM demonstrates improved multi-hop grounding and reasoning capabilities,\nsetting a new baseline for this challenging task. Furthermore, when trained on\nthird-person view videos, the same architecture also achieves state-of-the-art\nperformance on the single-hop VidQA benchmark, ActivityNet-RTL, demonstrating\nits effectiveness.\n","authors":["Qirui Chen","Shangzhe Di","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2408.14469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14457v1","updated":"2024-08-26T17:49:27Z","published":"2024-08-26T17:49:27Z","title":"Dense Center-Direction Regression for Object Counting and Localization\n  with Point Supervision","summary":"  Object counting and localization problems are commonly addressed with point\nsupervised learning, which allows the use of less labor-intensive point\nannotations. However, learning based on point annotations poses challenges due\nto the high imbalance between the sets of annotated and unannotated pixels,\nwhich is often treated with Gaussian smoothing of point annotations and focal\nloss. However, these approaches still focus on the pixels in the immediate\nvicinity of the point annotations and exploit the rest of the data only\nindirectly. In this work, we propose a novel approach termed CeDiRNet for\npoint-supervised learning that uses a dense regression of directions pointing\ntowards the nearest object centers, i.e. center-directions. This provides\ngreater support for each center point arising from many surrounding pixels\npointing towards the object center. We propose a formulation of\ncenter-directions that allows the problem to be split into the domain-specific\ndense regression of center-directions and the final localization task based on\na small, lightweight, and domain-agnostic localization network that can be\ntrained with synthetic data completely independent of the target domain. We\ndemonstrate the performance of the proposed method on six different datasets\nfor object counting and localization, and show that it outperforms the existing\nstate-of-the-art methods. The code is accessible on GitHub at\nhttps://github.com/vicoslab/CeDiRNet.git.\n","authors":["Domen Tabernik","Jon MuhoviÄ","Danijel SkoÄaj"],"pdf_url":"https://arxiv.org/pdf/2408.14457v1.pdf","comment":"Published in Pattern Recognition"},{"id":"http://arxiv.org/abs/2408.14456v1","updated":"2024-08-26T17:49:05Z","published":"2024-08-26T17:49:05Z","title":"Center Direction Network for Grasping Point Localization on Cloths","summary":"  Object grasping is a fundamental challenge in robotics and computer vision,\ncritical for advancing robotic manipulation capabilities. Deformable objects,\nlike fabrics and cloths, pose additional challenges due to their non-rigid\nnature. In this work, we introduce CeDiRNet-3DoF, a deep-learning model for\ngrasp point detection, with a particular focus on cloth objects. CeDiRNet-3DoF\nemploys center direction regression alongside a localization network, attaining\nfirst place in the perception task of ICRA 2023's Cloth Manipulation Challenge.\nRecognizing the lack of standardized benchmarks in the literature that hinder\neffective method comparison, we present the ViCoS Towel Dataset. This extensive\nbenchmark dataset comprises 8,000 real and 12,000 synthetic images, serving as\na robust resource for training and evaluating contemporary data-driven\ndeep-learning approaches. Extensive evaluation revealed CeDiRNet-3DoF's\nrobustness in real-world performance, outperforming state-of-the-art methods,\nincluding the latest transformer-based models. Our work bridges a crucial gap,\noffering a robust solution and benchmark for cloth grasping in computer vision\nand robotics. Code and dataset are available at:\nhttps://github.com/vicoslab/CeDiRNet-3DoF\n","authors":["Domen Tabernik","Jon MuhoviÄ","Matej Urbas","Danijel SkoÄaj"],"pdf_url":"https://arxiv.org/pdf/2408.14456v1.pdf","comment":"Accepted for publication in IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2408.14442v1","updated":"2024-08-26T17:35:01Z","published":"2024-08-26T17:35:01Z","title":"Model Parallel Training and Transfer Learning for Convolutional Neural\n  Networks by Domain Decomposition","summary":"  Deep convolutional neural networks (CNNs) have been shown to be very\nsuccessful in a wide range of image processing applications. However, due to\ntheir increasing number of model parameters and an increasing availability of\nlarge amounts of training data, parallelization strategies to efficiently train\ncomplex CNNs are necessary. In previous work by the authors, a novel model\nparallel CNN architecture was proposed which is loosely inspired by domain\ndecomposition. In particular, the novel network architecture is based on a\ndecomposition of the input data into smaller subimages. For each of these\nsubimages, local CNNs with a proportionally smaller number of parameters are\ntrained in parallel and the resulting local classifications are then aggregated\nin a second step by a dense feedforward neural network (DNN). In the present\nwork, we compare the resulting CNN-DNN architecture to less costly alternatives\nto combine the local classifications into a final, global decision.\nAdditionally, we investigate the performance of the CNN-DNN trained as one\ncoherent model as well as using a transfer learning strategy, where the\nparameters of the pre-trained local CNNs are used as initial values for a\nsubsequently trained global coherent CNN-DNN model.\n","authors":["Axel Klawonn","Martin Lanser","Janine Weber"],"pdf_url":"https://arxiv.org/pdf/2408.14442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14441v1","updated":"2024-08-26T17:33:47Z","published":"2024-08-26T17:33:47Z","title":"Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification","summary":"  Exploiting both audio and visual modalities for video classification is a\nchallenging task, as the existing methods require large model architectures,\nleading to high computational complexity and resource requirements. Smaller\narchitectures, on the other hand, struggle to achieve optimal performance. In\nthis paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that\nintroduces a compact model architecture specifically designed to capture\nintricate audio-visual relationships in video data. Through extensive\nexperiments on the challenging YouTube-8M dataset, we demonstrate that\nAttend-Fusion achieves an F1 score of 75.64\\% with only 72M parameters, which\nis comparable to the performance of larger baseline models such as\nFully-Connected Late Fusion (75.96\\% F1 score, 341M parameters). Attend-Fusion\nachieves similar performance to the larger baseline model while reducing the\nmodel size by nearly 80\\%, highlighting its efficiency in terms of model\ncomplexity. Our work demonstrates that the Attend-Fusion model effectively\ncombines audio and visual information for video classification, achieving\ncompetitive performance with significantly reduced model size. This approach\nopens new possibilities for deploying high-performance video understanding\nsystems in resource-constrained environments across various applications.\n","authors":["Mahrukh Awan","Asmar Nadeem","Muhammad Junaid Awan","Armin Mustafa","Syed Sameed Husain"],"pdf_url":"https://arxiv.org/pdf/2408.14441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14435v1","updated":"2024-08-26T17:21:54Z","published":"2024-08-26T17:21:54Z","title":"Social perception of faces in a vision-language model","summary":"  We explore social perception of human faces in CLIP, a widely used\nopen-source vision-language model. To this end, we compare the similarity in\nCLIP embeddings between different textual prompts and a set of face images. Our\ntextual prompts are constructed from well-validated social psychology terms\ndenoting social perception. The face images are synthetic and are\nsystematically and independently varied along six dimensions: the legally\nprotected attributes of age, gender, and race, as well as facial expression,\nlighting, and pose. Independently and systematically manipulating face\nattributes allows us to study the effect of each on social perception and\navoids confounds that can occur in wild-collected data due to uncontrolled\nsystematic correlations between attributes. Thus, our findings are experimental\nrather than observational. Our main findings are three. First, while CLIP is\ntrained on the widest variety of images and texts, it is able to make\nfine-grained human-like social judgments on face images. Second, age, gender,\nand race do systematically impact CLIP's social perception of faces, suggesting\nan undesirable bias in CLIP vis-a-vis legally protected attributes. Most\nstrikingly, we find a strong pattern of bias concerning the faces of Black\nwomen, where CLIP produces extreme values of social perception across different\nages and facial expressions. Third, facial expression impacts social perception\nmore than age and lighting as much as age. The last finding predicts that\nstudies that do not control for unprotected visual attributes may reach the\nwrong conclusions on bias. Our novel method of investigation, which is founded\non the social psychology literature and on the experiments involving the\nmanipulation of individual attributes, yields sharper and more reliable\nobservations than previous observational methods and may be applied to study\nbiases in any vision-language model.\n","authors":["Carina I. Hausladen","Manuel Knott","Colin F. Camerer","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2408.14435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14427v1","updated":"2024-08-26T17:15:37Z","published":"2024-08-26T17:15:37Z","title":"Few-Shot 3D Volumetric Segmentation with Multi-Surrogate Fusion","summary":"  Conventional 3D medical image segmentation methods typically require learning\nheavy 3D networks (e.g., 3D-UNet), as well as large amounts of in-domain data\nwith accurate pixel/voxel-level labels to avoid overfitting. These solutions\nare thus extremely time- and labor-expensive, but also may easily fail to\ngeneralize to unseen objects during training. To alleviate this issue, we\npresent MSFSeg, a novel few-shot 3D segmentation framework with a lightweight\nmulti-surrogate fusion (MSF). MSFSeg is able to automatically segment unseen 3D\nobjects/organs (during training) provided with one or a few annotated 2D slices\nor 3D sequence segments, via learning dense query-support organ/lesion anatomy\ncorrelations across patient populations. Our proposed MSF module mines\ncomprehensive and diversified morphology correlations between unlabeled and the\nfew labeled slices/sequences through multiple designated surrogates, making it\nable to generate accurate cross-domain 3D segmentation masks given annotated\nslices or sequences. We demonstrate the effectiveness of our proposed framework\nby showing superior performance on conventional few-shot segmentation\nbenchmarks compared to prior art, and remarkable cross-domain cross-volume\nsegmentation performance on proprietary 3D segmentation datasets for\nchallenging entities, i.e., tubular structures, with only limited 2D or 3D\nlabels.\n","authors":["Meng Zheng","Benjamin Planche","Zhongpai Gao","Terrence Chen","Richard J. Radke","Ziyan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14427v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.14421v1","updated":"2024-08-26T17:04:52Z","published":"2024-08-26T17:04:52Z","title":"Evaluating saliency scores in point clouds of natural environments by\n  learning surface anomalies","summary":"  In recent years, three-dimensional point clouds are used increasingly to\ndocument natural environments. Each dataset contains a diverse set of objects,\nat varying shapes and sizes, distributed throughout the data and intricately\nintertwined with the topography. Therefore, regions of interest are difficult\nto find and consequent analyses become a challenge. Inspired from visual\nperception principles, we propose to differentiate objects of interest from the\ncluttered environment by evaluating how much they stand out from their\nsurroundings, i.e., their geometric salience. Previous saliency detection\napproaches suggested mostly handcrafted attributes for the task. However, such\nmethods fail when the data are too noisy or have high levels of texture. Here\nwe propose a learning-based mechanism that accommodates noise and textured\nsurfaces. We assume that within the natural environment any change from the\nprevalent surface would suggest a salient object. Thus, we first learn the\nunderlying surface and then search for anomalies within it. Initially, a deep\nneural network is trained to reconstruct the surface. Regions where the\nreconstructed part deviates significantly from the original point cloud yield a\nsubstantial reconstruction error, signifying an anomaly, i.e., saliency. We\ndemonstrate the effectiveness of the proposed approach by searching for salient\nfeatures in various natural scenarios, which were acquired by different\nacquisition platforms. We show the strong correlation between the\nreconstruction error and salient objects.\n","authors":["Reuma Arav","Dennis Wittich","Franz Rottensteiner"],"pdf_url":"https://arxiv.org/pdf/2408.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14419v1","updated":"2024-08-26T17:04:23Z","published":"2024-08-26T17:04:23Z","title":"CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language\n  Models","summary":"  We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large\nlanguage models. CHARTOM consists of specially designed data visualizing\ncharts. Given a chart, a language model needs to not only correctly comprehend\nthe chart (the FACT question) but also judge if the chart will be misleading to\na human reader (the MIND question). Both questions have significant societal\nbenefits. We detail the construction of the CHARTOM benchmark including its\ncalibration on human performance.\n","authors":["Shubham Bharti","Shiyun Cheng","Jihyun Rho","Martina Rao","Xiaojin Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.14419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14415v1","updated":"2024-08-26T17:02:25Z","published":"2024-08-26T17:02:25Z","title":"LoG-VMamba: Local-Global Vision Mamba for Medical Image Segmentation","summary":"  Mamba, a State Space Model (SSM), has recently shown competitive performance\nto Convolutional Neural Networks (CNNs) and Transformers in Natural Language\nProcessing and general sequence modeling. Various attempts have been made to\nadapt Mamba to Computer Vision tasks, including medical image segmentation\n(MIS). Vision Mamba (VM)-based networks are particularly attractive due to\ntheir ability to achieve global receptive fields, similar to Vision\nTransformers, while also maintaining linear complexity in the number of tokens.\nHowever, the existing VM models still struggle to maintain both spatially local\nand global dependencies of tokens in high dimensional arrays due to their\nsequential nature. Employing multiple and/or complicated scanning strategies is\ncomputationally costly, which hinders applications of SSMs to high-dimensional\n2D and 3D images that are common in MIS problems. In this work, we propose\nLocal-Global Vision Mamba, LoG-VMamba, that explicitly enforces spatially\nadjacent tokens to remain nearby on the channel axis, and retains the global\ncontext in a compressed form. Our method allows the SSMs to access the local\nand global contexts even before reaching the last token while requiring only a\nsimple scanning strategy. Our segmentation models are computationally efficient\nand substantially outperform both CNN and Transformers-based baselines on a\ndiverse set of 2D and 3D MIS tasks. The implementation of LoG-VMamba is\navailable at \\url{https://github.com/Oulu-IMEDS/LoG-VMamba}.\n","authors":["Trung Dinh Quoc Dang","Huy Hoang Nguyen","Aleksei Tiulpin"],"pdf_url":"https://arxiv.org/pdf/2408.14415v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2310.05873v6","updated":"2024-08-26T16:55:02Z","published":"2023-10-09T17:13:10Z","title":"Implicit Concept Removal of Diffusion Models","summary":"  Text-to-image (T2I) diffusion models often inadvertently generate unwanted\nconcepts such as watermarks and unsafe images. These concepts, termed as the\n\"implicit concepts\", could be unintentionally learned during training and then\nbe generated uncontrollably during inference. Existing removal methods still\nstruggle to eliminate implicit concepts primarily due to their dependency on\nthe model's ability to recognize concepts it actually can not discern. To\naddress this, we utilize the intrinsic geometric characteristics of implicit\nconcepts and present the Geom-Erasing, a novel concept removal method based on\nthe geometric-driven control. Specifically, once an unwanted implicit concept\nis identified, we integrate the existence and geometric information of the\nconcept into the text prompts with the help of an accessible classifier or\ndetector model. Subsequently, the model is optimized to identify and\ndisentangle this information, which is then adopted as negative prompts during\ngeneration. Moreover, we introduce the Implicit Concept Dataset (ICD), a novel\nimage-text dataset imbued with three typical implicit concepts (i.e., QR codes,\nwatermarks, and text), reflecting real-life situations where implicit concepts\nare easily injected. Geom-Erasing effectively mitigates the generation of\nimplicit concepts, achieving the state-of-the-art results on the Inappropriate\nImage Prompts (I2P) and our challenging Implicit Concept Dataset (ICD)\nbenchmarks.\n","authors":["Zhili Liu","Kai Chen","Yifan Zhang","Jianhua Han","Lanqing Hong","Hang Xu","Zhenguo Li","Dit-Yan Yeung","James Kwok"],"pdf_url":"https://arxiv.org/pdf/2310.05873v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02060v3","updated":"2024-08-26T16:42:45Z","published":"2023-10-03T14:03:20Z","title":"Global Attractor for a Reaction-Diffusion Model Arising in Biological\n  Dynamic in 3D Soil Structure","summary":"  Partial Differential Equations (PDEs) play a crucial role as tools for\nmodeling and comprehending intricate natural processes, notably within the\ndomain of biology. This research explores the domain of microbial activity\nwithin the complex matrix of 3D soil structures, providing valuable\nunderstanding into both the existence and uniqueness of solutions and the\nasymptotic behavior of the corresponding PDE model. Our investigation results\nin the discovery of a global attractor, a fundamental feature with significant\nimplications for long-term system behavior. To enhance the clarity of our\nfindings, numerical simulations are employed to visually illustrate the\nattributes of this global attractor.\n","authors":["Mohamed Elghandouri","Khalil Ezzinbi","Mouad Klai","Olivier Monga"],"pdf_url":"https://arxiv.org/pdf/2310.02060v3.pdf","comment":"Preprint submitted to Mathematical Modeling in Natural Phenomena"},{"id":"http://arxiv.org/abs/2408.14400v1","updated":"2024-08-26T16:34:13Z","published":"2024-08-26T16:34:13Z","title":"Satellite Sunroof: High-res Digital Surface Models and Roof Segmentation\n  for Global Solar Mapping","summary":"  The transition to renewable energy, particularly solar, is key to mitigating\nclimate change. Google's Solar API aids this transition by estimating solar\npotential from aerial imagery, but its impact is constrained by geographical\ncoverage. This paper proposes expanding the API's reach using satellite\nimagery, enabling global solar potential assessment. We tackle challenges\ninvolved in building a Digital Surface Model (DSM) and roof instance\nsegmentation from lower resolution and single oblique views using deep learning\nmodels. Our models, trained on aligned satellite and aerial datasets, produce\n25cm DSMs and roof segments. With ~1m DSM MAE on buildings, ~5deg roof pitch\nerror and ~56% IOU on roof segmentation, they significantly enhance the Solar\nAPI's potential to promote solar adoption.\n","authors":["Vishal Batchu","Alex Wilson","Betty Peng","Carl Elkin","Umangi Jain","Christopher Van Arsdale","Ross Goroshin","Varun Gulshan"],"pdf_url":"https://arxiv.org/pdf/2408.14400v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.14397v1","updated":"2024-08-26T16:28:56Z","published":"2024-08-26T16:28:56Z","title":"Uncovering Knowledge Gaps in Radiology Report Generation Models through\n  Knowledge Graphs","summary":"  Recent advancements in artificial intelligence have significantly improved\nthe automatic generation of radiology reports. However, existing evaluation\nmethods fail to reveal the models' understanding of radiological images and\ntheir capacity to achieve human-level granularity in descriptions. To bridge\nthis gap, we introduce a system, named ReXKG, which extracts structured\ninformation from processed reports to construct a comprehensive radiology\nknowledge graph. We then propose three metrics to evaluate the similarity of\nnodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs\n(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative\nanalysis of AI-generated and human-written radiology reports, assessing the\nperformance of both specialist and generalist models. Our study provides a\ndeeper understanding of the capabilities and limitations of current AI models\nin radiology report generation, offering valuable insights for improving model\nperformance and clinical applicability.\n","authors":["Xiaoman Zhang","JuliÃ¡n N. Acosta","Hong-Yu Zhou","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2408.14397v1.pdf","comment":"Code is available at: https://github.com/rajpurkarlab/ReXKG"},{"id":"http://arxiv.org/abs/2402.00752v4","updated":"2024-08-26T16:27:42Z","published":"2024-02-01T16:43:58Z","title":"On the Error Analysis of 3D Gaussian Splatting and an Optimal Projection\n  Strategy","summary":"  3D Gaussian Splatting has garnered extensive attention and application in\nreal-time neural rendering. Concurrently, concerns have been raised about the\nlimitations of this technology in aspects such as point cloud storage,\nperformance, and robustness in sparse viewpoints, leading to various\nimprovements. However, there has been a notable lack of attention to the\nfundamental problem of projection errors introduced by the local affine\napproximation inherent in the splatting itself, and the consequential impact of\nthese errors on the quality of photo-realistic rendering. This paper addresses\nthe projection error function of 3D Gaussian Splatting, commencing with the\nresidual error from the first-order Taylor expansion of the projection\nfunction. The analysis establishes a correlation between the error and the\nGaussian mean position. Subsequently, leveraging function optimization theory,\nthis paper analyzes the function's minima to provide an optimal projection\nstrategy for Gaussian Splatting referred to Optimal Gaussian Splatting, which\ncan accommodate a variety of camera models. Experimental validation further\nconfirms that this projection methodology reduces artifacts, resulting in a\nmore convincingly realistic rendering.\n","authors":["Letian Huang","Jiayang Bai","Jie Guo","Yuanqi Li","Yanwen Guo"],"pdf_url":"https://arxiv.org/pdf/2402.00752v4.pdf","comment":"Accepted by ECCV2024; Project Page:\n  https://letianhuang.github.io/op43dgs/"},{"id":"http://arxiv.org/abs/2404.03507v3","updated":"2024-08-26T16:22:35Z","published":"2024-04-04T15:10:24Z","title":"DQ-DETR: DETR with Dynamic Query for Tiny Object Detection","summary":"  Despite previous DETR-like methods having performed successfully in generic\nobject detection, tiny object detection is still a challenging task for them\nsince the positional information of object queries is not customized for\ndetecting tiny objects, whose scale is extraordinarily smaller than general\nobjects. Also, DETR-like methods using a fixed number of queries make them\nunsuitable for aerial datasets, which only contain tiny objects, and the\nnumbers of instances are imbalanced between different images. Thus, we present\na simple yet effective model, named DQ-DETR, which consists of three different\ncomponents: categorical counting module, counting-guided feature enhancement,\nand dynamic query selection to solve the above-mentioned problems. DQ-DETR uses\nthe prediction and density maps from the categorical counting module to\ndynamically adjust the number of object queries and improve the positional\ninformation of queries. Our model DQ-DETR outperforms previous CNN-based and\nDETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2\ndataset, which mostly consists of tiny objects.\n","authors":["Yi-Xin Huang","Hou-I Liu","Hong-Han Shuai","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.03507v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03771v2","updated":"2024-08-26T16:15:57Z","published":"2024-07-04T09:32:12Z","title":"SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors","summary":"  3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance\nin 3D scene reconstruction. However, 3DGS heavily relies on the sharp images.\nFulfilling this requirement can be challenging in real-world scenarios\nespecially when the camera moves fast, which severely limits the application of\n3DGS. To address these challenges, we proposed Spike Gausian Splatting\n(SpikeGS), the first framework that integrates the spike streams into 3DGS\npipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With\naccumulation rasterization, interval supervision, and a specially designed\npipeline, SpikeGS extracts detailed geometry and texture from high temporal\nresolution but texture lacking spike stream, reconstructs 3D scenes captured in\n1 second. Extensive experiments on multiple synthetic and real-world datasets\ndemonstrate the superiority of SpikeGS compared with existing spike-based and\ndeblur 3D scene reconstruction methods. Codes and data will be released soon.\n","authors":["Yijia Guo","Liwen Hu","Lei Ma","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2407.03771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05180v2","updated":"2024-08-26T16:13:30Z","published":"2024-04-08T04:10:50Z","title":"GloSoFarID: Global multispectral dataset for Solar Farm IDentification\n  in satellite imagery","summary":"  Solar Photovoltaic (PV) technology is increasingly recognized as a pivotal\nsolution in the global pursuit of clean and renewable energy. This technology\naddresses the urgent need for sustainable energy alternatives by converting\nsolar power into electricity without greenhouse gas emissions. It not only\ncurtails global carbon emissions but also reduces reliance on finite,\nnon-renewable energy sources. In this context, monitoring solar panel farms\nbecomes essential for understanding and facilitating the worldwide shift toward\nclean energy. This study contributes to this effort by developing the first\ncomprehensive global dataset of multispectral satellite imagery of solar panel\nfarms. This dataset is intended to form the basis for training robust machine\nlearning models, which can accurately map and analyze the expansion and\ndistribution of solar panel farms globally. The insights gained from this\nendeavor will be instrumental in guiding informed decision-making for a\nsustainable energy future. https://github.com/yzyly1992/GloSoFarID\n","authors":["Zhiyuan Yang","Ryan Rad"],"pdf_url":"https://arxiv.org/pdf/2404.05180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14381v1","updated":"2024-08-26T16:04:13Z","published":"2024-08-26T16:04:13Z","title":"Learning Tree-Structured Composition of Data Augmentation","summary":"  Data augmentation is widely used for training a neural network given little\nlabeled data. A common practice of augmentation training is applying a\ncomposition of multiple transformations sequentially to the data. Existing\naugmentation methods such as RandAugment randomly sample from a list of\npre-selected transformations, while methods such as AutoAugment apply advanced\nsearch to optimize over an augmentation set of size $k^d$, which is the number\nof transformation sequences of length $d$, given a list of $k$ transformations.\n  In this paper, we design efficient algorithms whose running time complexity\nis much faster than the worst-case complexity of $O(k^d)$, provably. We propose\na new algorithm to search for a binary tree-structured composition of $k$\ntransformations, where each tree node corresponds to one transformation. The\nbinary tree generalizes sequential augmentations, such as the SimCLR\naugmentation scheme for contrastive learning. Using a top-down, recursive\nsearch procedure, our algorithm achieves a runtime complexity of $O(2^d k)$,\nwhich is much faster than $O(k^d)$ as $k$ increases above $2$. We apply our\nalgorithm to tackle data distributions with heterogeneous subpopulations by\nsearching for one tree in each subpopulation and then learning a weighted\ncombination, resulting in a forest of trees.\n  We validate our proposed algorithms on numerous graph and image datasets,\nincluding a multi-label graph classification dataset we collected. The dataset\nexhibits significant variations in the sizes of graphs and their average\ndegrees, making it ideal for studying data augmentation. We show that our\napproach can reduce the computation cost by 43% over existing search methods\nwhile improving performance by 4.3%. The tree structures can be used to\ninterpret the relative importance of each transformation, such as identifying\nthe important transformations on small vs. large graphs.\n","authors":["Dongyue Li","Kailai Chen","Predrag Radivojac","Hongyang R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.14381v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2408.14371v1","updated":"2024-08-26T15:53:50Z","published":"2024-08-26T15:53:50Z","title":"SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery","summary":"  In this paper, we address Generalized Category Discovery, aiming to\nsimultaneously uncover novel categories and accurately classify known ones.\nTraditional methods, which lean heavily on self-supervision and contrastive\nlearning, often fall short when distinguishing between fine-grained categories.\nTo address this, we introduce a novel concept called `self-expertise', which\nenhances the model's ability to recognize subtle differences and uncover\nunknown categories. Our approach combines unsupervised and supervised\nself-expertise strategies to refine the model's discernment and generalization.\nInitially, hierarchical pseudo-labeling is used to provide `soft supervision',\nimproving the effectiveness of self-expertise. Our supervised technique differs\nfrom traditional methods by utilizing more abstract positive and negative\nsamples, aiding in the formation of clusters that can generalize to novel\ncategories. Meanwhile, our unsupervised strategy encourages the model to\nsharpen its category distinctions by considering within-category examples as\n`hard' negatives. Supported by theoretical insights, our empirical results\nshowcase that our method outperforms existing state-of-the-art techniques in\nGeneralized Category Discovery across several fine-grained datasets. Our code\nis available at: https://github.com/SarahRastegar/SelEx.\n","authors":["Sarah Rastegar","Mohammadreza Salehi","Yuki M. Asano","Hazel Doughty","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2408.14371v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2405.03762v2","updated":"2024-08-26T15:40:18Z","published":"2024-05-06T18:01:13Z","title":"Swin transformers are robust to distribution and concept drift in\n  endoscopy-based longitudinal rectal cancer assessment","summary":"  Endoscopic images are used at various stages of rectal cancer treatment\nstarting from cancer screening, diagnosis, during treatment to assess response\nand toxicity from treatments such as colitis, and at follow up to detect new\ntumor or local regrowth (LR). However, subjective assessment is highly variable\nand can underestimate the degree of response in some patients, subjecting them\nto unnecessary surgery, or overestimate response that places patients at risk\nof disease spread. Advances in deep learning has shown the ability to produce\nconsistent and objective response assessment for endoscopic images. However,\nmethods for detecting cancers, regrowth, and monitoring response during the\nentire course of patient treatment and follow-up are lacking. This is because,\nautomated diagnosis and rectal cancer response assessment requires methods that\nare robust to inherent imaging illumination variations and confounding\nconditions (blood, scope, blurring) present in endoscopy images as well as\nchanges to the normal lumen and tumor during treatment. Hence, a hierarchical\nshifted window (Swin) transformer was trained to distinguish rectal cancer from\nnormal lumen using endoscopy images. Swin as well as two convolutional\n(ResNet-50, WideResNet-50), and vision transformer (ViT) models were trained\nand evaluated on follow-up longitudinal images to detect LR on private dataset\nas well as on out-of-distribution (OOD) public colonoscopy datasets to detect\npre/non-cancerous polyps. Color shifts were applied using optimal transport to\nsimulate distribution shifts. Swin and ResNet models were similarly accurate in\nthe in-distribution dataset. Swin was more accurate than other methods\n(follow-up: 0.84, OOD: 0.83) even when subject to color shifts (follow-up:\n0.83, OOD: 0.87), indicating capability to provide robust performance for\nlongitudinal cancer assessment.\n","authors":["Jorge Tapias Gomez","Aneesh Rangnekar","Hannah Williams","Hannah Thompson","Julio Garcia-Aguilar","Joshua Jesse Smith","Harini Veeraraghavan"],"pdf_url":"https://arxiv.org/pdf/2405.03762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14358v1","updated":"2024-08-26T15:32:31Z","published":"2024-08-26T15:32:31Z","title":"An Embedding is Worth a Thousand Noisy Labels","summary":"  The performance of deep neural networks scales with dataset size and label\nquality, rendering the efficient mitigation of low-quality data annotations\ncrucial for building robust and cost-effective systems. Existing strategies to\naddress label noise exhibit severe limitations due to computational complexity\nand application dependency. In this work, we propose WANN, a Weighted Adaptive\nNearest Neighbor approach that builds on self-supervised feature\nrepresentations obtained from foundation models. To guide the weighted voting\nscheme, we introduce a reliability score, which measures the likelihood of a\ndata label being correct. WANN outperforms reference methods, including a\nlinear layer trained with robust loss functions, on diverse datasets of varying\nsize and under various noise types and severities. WANN also exhibits superior\ngeneralization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed\nk-NNs. Furthermore, the proposed weighting scheme enhances supervised\ndimensionality reduction under noisy labels. This yields a significant boost in\nclassification performance with 10x and 100x smaller image embeddings,\nminimizing latency and storage requirements. Our approach, emphasizing\nefficiency and explainability, emerges as a simple, robust solution to overcome\nthe inherent limitations of deep neural network training. The code is available\nat https://github.com/francescodisalvo05/wann-noisy-labels .\n","authors":["Francesco Di Salvo","Sebastian Doerrich","Ines Rieger","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2408.14358v1.pdf","comment":"Preprint submitted to the International Journal of Computer Vision\n  (IJCV)"},{"id":"http://arxiv.org/abs/2408.14348v1","updated":"2024-08-26T15:26:27Z","published":"2024-08-26T15:26:27Z","title":"Deep learning-based ecological analysis of camera trap images is\n  impacted by training data quality and size","summary":"  Large wildlife image collections from camera traps are crucial for\nbiodiversity monitoring, offering insights into species richness, occupancy,\nand activity patterns. However, manual processing of these data is\ntime-consuming, hindering analytical processes. To address this, deep neural\nnetworks have been widely adopted to automate image analysis. Despite their\ngrowing use, the impact of model training decisions on downstream ecological\nmetrics remains unclear. Here, we analyse camera trap data from an African\nsavannah and an Asian sub-tropical dry forest to compare key ecological metrics\nderived from expert-generated species identifications with those generated from\ndeep neural networks. We assess the impact of model architecture, training data\nnoise, and dataset size on ecological metrics, including species richness,\noccupancy, and activity patterns. Our results show that while model\narchitecture has minimal impact, large amounts of noise and reduced dataset\nsize significantly affect these metrics. Nonetheless, estimated ecological\nmetrics are resilient to considerable noise, tolerating up to 10% error in\nspecies labels and a 50% reduction in training set size without changing\nsignificantly. We also highlight that conventional metrics like classification\nerror may not always be representative of a model's ability to accurately\nmeasure ecological metrics. We conclude that ecological metrics derived from\ndeep neural network predictions closely match those calculated from expert\nlabels and remain robust to variations in the factors explored. However,\ntraining decisions for deep neural networks can impact downstream ecological\nanalysis. Therefore, practitioners should prioritize creating large, clean\ntraining sets and evaluate deep neural network solutions based on their ability\nto measure the ecological metrics of interest.\n","authors":["Omiros Pantazis","Peggy Bevan","Holly Pringle","Guilherme Braga Ferreira","Daniel J. Ingram","Emily Madsen","Liam Thomas","Dol Raj Thanet","Thakur Silwal","Santosh Rayamajhi","Gabriel Brostow","Oisin Mac Aodha","Kate E. Jones"],"pdf_url":"https://arxiv.org/pdf/2408.14348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11126v2","updated":"2024-08-26T15:19:12Z","published":"2024-08-20T18:26:09Z","title":"Binocular Model: A deep learning solution for online melt pool\n  temperature analysis using dual-wavelength Imaging Pyrometry","summary":"  In metal Additive Manufacturing (AM), monitoring the temperature of the Melt\nPool (MP) is crucial for ensuring part quality, process stability, defect\nprevention, and overall process optimization. Traditional methods, are slow to\nconverge and require extensive manual effort to translate data into actionable\ninsights, rendering them impractical for real-time monitoring and control. To\naddress this challenge, we propose an Artificial Intelligence (AI)-based\nsolution aimed at reducing manual data processing reliance and improving the\nefficiency of transitioning from data to insight. In our study, we utilize a\ndataset comprising dual-wavelength real-time process monitoring data and\ncorresponding temperature maps. We introduce a deep learning model called the\n\"Binocular model,\" which exploits dual input observations to perform a precise\nanalysis of MP temperature in Laser Powder Bed Fusion (L-PBF). Through advanced\ndeep learning techniques, we seamlessly convert raw data into temperature maps,\nsignificantly streamlining the process and enabling batch processing at a rate\nof up to 750 frames per second, approximately 1000 times faster than\nconventional methods. Our Binocular model achieves high accuracy in temperature\nestimation, evidenced by a 0.95 R-squared score, while simultaneously enhancing\nprocessing efficiency by a factor of $\\sim1000x$ times. This model directly\naddresses the challenge of real-time MP temperature monitoring and offers\ninsights into the encountered constraints and the benefits of our Deep\nLearning-based approach. By combining efficiency and precision, our work\ncontributes to the advancement of temperature monitoring in L-PBF, thus driving\nprogress in the field of metal AM.\n","authors":["Javid Akhavan","Chaitanya Krishna Vallabh","Xiayun Zhao","Souran Manoochehri"],"pdf_url":"https://arxiv.org/pdf/2408.11126v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14343v1","updated":"2024-08-26T15:16:28Z","published":"2024-08-26T15:16:28Z","title":"A Brief Analysis of the Iterative Next Boundary Detection Network for\n  Tree Rings Delineation in Images of Pinus taeda","summary":"  This work presents the INBD network proposed by Gillert et al. in CVPR-2023\nand studies its application for delineating tree rings in RGB images of Pinus\ntaeda cross sections captured by a smartphone (UruDendro dataset), which are\nimages with different characteristics from the ones used to train the method.\nThe INBD network operates in two stages: first, it segments the background,\npith, and ring boundaries. In the second stage, the image is transformed into\npolar coordinates, and ring boundaries are iteratively segmented from the pith\nto the bark. Both stages are based on the U-Net architecture. The method\nachieves an F-Score of 77.5, a mAR of 0.540, and an ARAND of 0.205 on the\nevaluation set. The code for the experiments is available at\nhttps://github.com/hmarichal93/mlbrief_inbd.\n","authors":["Henry Marichal","Gregory Randall"],"pdf_url":"https://arxiv.org/pdf/2408.14343v1.pdf","comment":"Submitted to IPOL ad an MLBriefs paper"},{"id":"http://arxiv.org/abs/2408.14339v1","updated":"2024-08-26T15:08:12Z","published":"2024-08-26T15:08:12Z","title":"ConceptMix: A Compositional Image Generation Benchmark with Controllable\n  Difficulty","summary":"  Compositionality is a critical capability in Text-to-Image (T2I) models, as\nit reflects their ability to understand and combine multiple concepts from text\ndescriptions. Existing evaluations of compositional capability rely heavily on\nhuman-designed text prompts or fixed templates, limiting their diversity and\ncomplexity, and yielding low discriminative power. We propose ConceptMix, a\nscalable, controllable, and customizable benchmark which automatically\nevaluates compositional generation ability of T2I models. This is done in two\nstages. First, ConceptMix generates the text prompts: concretely, using\ncategories of visual concepts (e.g., objects, colors, shapes, spatial\nrelationships), it randomly samples an object and k-tuples of visual concepts,\nthen uses GPT4-o to generate text prompts for image generation based on these\nsampled concepts. Second, ConceptMix evaluates the images generated in response\nto these prompts: concretely, it checks how many of the k concepts actually\nappeared in the image by generating one question per visual concept and using a\nstrong VLM to answer them. Through administering ConceptMix to a diverse set of\nT2I models (proprietary as well as open ones) using increasing values of k, we\nshow that our ConceptMix has higher discrimination power than earlier\nbenchmarks. Specifically, ConceptMix reveals that the performance of several\nmodels, especially open models, drops dramatically with increased k.\nImportantly, it also provides insight into the lack of prompt diversity in\nwidely-used training datasets. Additionally, we conduct extensive human studies\nto validate the design of ConceptMix and compare our automatic grading with\nhuman judgement. We hope it will guide future T2I model development.\n","authors":["Xindi Wu","Dingli Yu","Yangsibo Huang","Olga Russakovsky","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2408.14339v1.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2408.14336v1","updated":"2024-08-26T15:07:01Z","published":"2024-08-26T15:07:01Z","title":"Equivariant Reinforcement Learning under Partial Observability","summary":"  Incorporating inductive biases is a promising approach for tackling\nchallenging robot learning domains with sample-efficient solutions. This paper\nidentifies partially observable domains where symmetries can be a useful\ninductive bias for efficient learning. Specifically, by encoding the\nequivariance regarding specific group symmetries into the neural networks, our\nactor-critic reinforcement learning agents can reuse solutions in the past for\nrelated scenarios. Consequently, our equivariant agents outperform\nnon-equivariant approaches significantly in terms of sample efficiency and\nfinal performance, demonstrated through experiments on a range of robotic tasks\nin simulation and real hardware.\n","authors":["Hai Nguyen","Andrea Baisero","David Klee","Dian Wang","Robert Platt","Christopher Amato"],"pdf_url":"https://arxiv.org/pdf/2408.14336v1.pdf","comment":"Conference on Robot Learning, 2023"},{"id":"http://arxiv.org/abs/2408.14329v1","updated":"2024-08-26T14:55:23Z","published":"2024-08-26T14:55:23Z","title":"PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection\n  Dataset","summary":"  PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection\ndataset. By removing pixel information and providing only de-identified human\nannotations, PHEVA safeguards personally identifiable information. The dataset\nincludes seven indoor/outdoor scenes, featuring one novel, context-specific\ncamera, and offers over 5x the pose-annotated frames compared to the largest\nprevious dataset. This study benchmarks state-of-the-art methods on PHEVA using\na comprehensive set of metrics, including the 10% Error Rate (10ER), a metric\nused for anomaly detection for the first time providing insights relevant to\nreal-world deployment. As the first of its kind, PHEVA bridges the gap between\nconventional training and real-world deployment by introducing continual\nlearning benchmarks, with models outperforming traditional methods in 82.14% of\ncases. The dataset is publicly available at\nhttps://github.com/TeCSAR-UNCC/PHEVA.git.\n","authors":["Ghazal Alinezhad Noghre","Shanle Yao","Armin Danesh Pazho","Babak Rahimi Ardabili","Vinit Katariya","Hamed Tabkhi"],"pdf_url":"https://arxiv.org/pdf/2408.14329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14326v1","updated":"2024-08-26T14:54:14Z","published":"2024-08-26T14:54:14Z","title":"Streamline tractography of the fetal brain in utero with machine\n  learning","summary":"  Diffusion-weighted magnetic resonance imaging (dMRI) is the only non-invasive\ntool for studying white matter tracts and structural connectivity of the brain.\nThese assessments rely heavily on tractography techniques, which reconstruct\nvirtual streamlines representing white matter fibers. Much effort has been\ndevoted to improving tractography methodology for adult brains, while\ntractography of the fetal brain has been largely neglected. Fetal tractography\nfaces unique difficulties due to low dMRI signal quality, immature and rapidly\ndeveloping brain structures, and paucity of reference data. This work presents\nthe first machine learning model for fetal tractography. The model input\nconsists of five sources of information: (1) Fiber orientation, inferred from a\ndiffusion tensor fit to the dMRI signal; (2) Directions of recent propagation\nsteps; (3) Global spatial information, encoded as distances to keypoints in the\nbrain cortex; (4) Tissue segmentation information; and (5) Prior information\nabout the expected local fiber orientations supplied with an atlas. In order to\nmitigate the local tensor estimation error, a large spatial context around the\ncurrent point in the diffusion tensor image is encoded using convolutional and\nattention neural network modules. Moreover, the diffusion tensor information at\na hypothetical next point is included in the model input. Filtering rules based\non anatomically constrained tractography are applied to prune implausible\nstreamlines. We trained the model on manually-refined whole-brain fetal\ntractograms and validated the trained model on an independent set of 11 test\nscans with gestational ages between 23 and 36 weeks. Results show that our\nproposed method achieves superior performance across all evaluated tracts. The\nnew method can significantly advance the capabilities of dMRI for studying\nnormal and abnormal brain development in utero.\n","authors":["Weide Liu","Camilo Calixto","Simon K. Warfield","Davood Karimi"],"pdf_url":"https://arxiv.org/pdf/2408.14326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15699v3","updated":"2024-08-26T14:39:33Z","published":"2023-05-25T04:14:49Z","title":"Cross-view Action Recognition Understanding From Exocentric to\n  Egocentric Perspective","summary":"  Understanding action recognition in egocentric videos has emerged as a vital\nresearch topic with numerous practical applications. With the limitation in the\nscale of egocentric data collection, learning robust deep learning-based action\nrecognition models remains difficult. Transferring knowledge learned from the\nlarge-scale exocentric data to the egocentric data is challenging due to the\ndifference in videos across views. Our work introduces a novel cross-view\nlearning approach to action recognition (CVAR) that effectively transfers\nknowledge from the exocentric to the selfish view. First, we present a novel\ngeometric-based constraint into the self-attention mechanism in Transformer\nbased on analyzing the camera positions between two views. Then, we propose a\nnew cross-view self-attention loss learned on unpaired cross-view data to\nenforce the self-attention mechanism learning to transfer knowledge across\nviews. Finally, to further improve the performance of our cross-view learning\napproach, we present the metrics to measure the correlations in videos and\nattention maps effectively. Experimental results on standard egocentric action\nrecognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, and\nEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-art\nperformance.\n","authors":["Thanh-Dat Truong","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2305.15699v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14284v1","updated":"2024-08-26T14:09:40Z","published":"2024-08-26T14:09:40Z","title":"May the Forgetting Be with You: Alternate Replay for Learning with Noisy\n  Labels","summary":"  Forgetting presents a significant challenge during incremental training,\nmaking it particularly demanding for contemporary AI systems to assimilate new\nknowledge in streaming data environments. To address this issue, most\napproaches in Continual Learning (CL) rely on the replay of a restricted buffer\nof past data. However, the presence of noise in real-world scenarios, where\nhuman annotation is constrained by time limitations or where data is\nautomatically gathered from the web, frequently renders these strategies\nvulnerable. In this study, we address the problem of CL under Noisy Labels\n(CLN) by introducing Alternate Experience Replay (AER), which takes advantage\nof forgetting to maintain a clear distinction between clean, complex, and noisy\nsamples in the memory buffer. The idea is that complex or mislabeled examples,\nwhich hardly fit the previously learned data distribution, are most likely to\nbe forgotten. To grasp the benefits of such a separation, we equip AER with\nAsymmetric Balanced Sampling (ABS): a new sample selection strategy that\nprioritizes purity on the current task while retaining relevant samples from\nthe past. Through extensive computational comparisons, we demonstrate the\neffectiveness of our approach in terms of both accuracy and purity of the\nobtained buffer, resulting in a remarkable average gain of 4.71% points in\naccuracy with respect to existing loss-based purification strategies. Code is\navailable at https://github.com/aimagelab/mammoth.\n","authors":["Monica Millunzi","Lorenzo Bonicelli","Angelo Porrello","Jacopo Credi","Petter N. Kolm","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2408.14284v1.pdf","comment":"25 pages, 5 figures. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2408.12615v2","updated":"2024-08-26T14:06:59Z","published":"2024-08-08T14:11:06Z","title":"Pediatric TSC-Related Epilepsy Classification from Clinical MR Images\n  Using Quantum Neural Network","summary":"  Tuberous sclerosis complex (TSC) manifests as a multisystem disorder with\nsignificant neurological implications. This study addresses the critical need\nfor robust classification models tailored to TSC in pediatric patients,\nintroducing QResNet,a novel deep learning model seamlessly integrating\nconventional convolutional neural networks with quantum neural networks. The\nmodel incorporates a two-layer quantum layer (QL), comprising ZZFeatureMap and\nAnsatz layers, strategically designed for processing classical data within a\nquantum framework. A comprehensive evaluation, demonstrates the superior\nperformance of QResNet in TSC MRI image classification compared to conventional\n3D-ResNet models. These compelling findings underscore the potential of quantum\ncomputing to revolutionize medical imaging and diagnostics.Remarkably, this\nmethod surpasses conventional CNNs in accuracy and Area Under the Curve (AUC)\nmetrics with the current dataset. Future research endeavors may focus on\nexploring the scalability and practical implementation of quantum algorithms in\nreal-world medical imaging scenarios.\n","authors":["Ling Lin","Yihang Zhou","Zhanqi Hu","Dian Jiang","Congcong Liu","Shuo Zhou","Yanjie Zhu","Jianxiang Liao","Dong Liang","Hairong Zheng","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.12615v2.pdf","comment":"5 pages,4 figures,2 tables,presented at ISBI 2024"},{"id":"http://arxiv.org/abs/2408.14281v1","updated":"2024-08-26T14:02:30Z","published":"2024-08-26T14:02:30Z","title":"Uncertainties of Latent Representations in Computer Vision","summary":"  Uncertainty quantification is a key pillar of trustworthy machine learning.\nIt enables safe reactions under unsafe inputs, like predicting only when the\nmachine learning model detects sufficient evidence, discarding anomalous data,\nor emitting warnings when an error is likely to be inbound. This is\nparticularly crucial in safety-critical areas like medical image classification\nor self-driving cars. Despite the plethora of proposed uncertainty\nquantification methods achieving increasingly higher scores on performance\nbenchmarks, uncertainty estimates are often shied away from in practice. Many\nmachine learning projects start from pretrained latent representations that\ncome without uncertainty estimates. Uncertainties would need to be trained by\npractitioners on their own, which is notoriously difficult and\nresource-intense.\n  This thesis makes uncertainty estimates easily accessible by adding them to\nthe latent representation vectors of pretrained computer vision models. Besides\nproposing approaches rooted in probability and decision theory, such as\nMonte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both\ntheoretical and empirical questions. We show that these unobservable\nuncertainties about unobservable latent representations are indeed provably\ncorrect. We also provide an uncertainty-aware representation learning (URL)\nbenchmark to compare these unobservables against observable ground-truths.\nFinally, we compile our findings to pretrain lightweight representation\nuncertainties on large-scale computer vision models that transfer to unseen\ndatasets in a zero-shot manner.\n  Our findings do not only advance the current theoretical understanding of\nuncertainties over latent variables, but also facilitate the access to\nuncertainty quantification for future researchers inside and outside the field,\nenabling straightforward but trustworthy machine learning.\n","authors":["Michael Kirchhof"],"pdf_url":"https://arxiv.org/pdf/2408.14281v1.pdf","comment":"Doctoral thesis"},{"id":"http://arxiv.org/abs/2403.05451v2","updated":"2024-08-26T13:58:16Z","published":"2024-03-08T16:57:47Z","title":"Attention-guided Feature Distillation for Semantic Segmentation","summary":"  In contrast to existing complex methodologies commonly employed for\ndistilling knowledge from a teacher to a student, this paper showcases the\nefficacy of a simple yet powerful method for utilizing refined feature maps to\ntransfer attention. The proposed method has proven to be effective in\ndistilling rich information, outperforming existing methods in semantic\nsegmentation as a dense prediction task. The proposed Attention-guided Feature\nDistillation (AttnFD) method, employs the Convolutional Block Attention Module\n(CBAM), which refines feature maps by taking into account both channel-specific\nand spatial information content. Simply using the Mean Squared Error (MSE) loss\nfunction between the refined feature maps of the teacher and the student,\nAttnFD demonstrates outstanding performance in semantic segmentation, achieving\nstate-of-the-art results in terms of improving the mean Intersection over Union\n(mIoU) of the student network on the PascalVoc 2012, Cityscapes, COCO, and\nCamVid datasets.\n","authors":["Amir M. Mansourian","Arya Jalali","Rozhan Ahmadi","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2403.05451v2.pdf","comment":"9 pages, 8 figures, and 6 tables"},{"id":"http://arxiv.org/abs/2408.09869v2","updated":"2024-08-26T13:55:59Z","published":"2024-08-19T10:20:06Z","title":"Docling Technical Report","summary":"  This technical report introduces Docling, an easy to use, self-contained,\nMIT-licensed open-source package for PDF document conversion. It is powered by\nstate-of-the-art specialized AI models for layout analysis (DocLayNet) and\ntable structure recognition (TableFormer), and runs efficiently on commodity\nhardware in a small resource budget. The code interface allows for easy\nextensibility and addition of new features and models.\n","authors":["Christoph Auer","Maksym Lysak","Ahmed Nassar","Michele Dolfi","Nikolaos Livathinos","Panos Vagenas","Cesar Berrospi Ramis","Matteo Omenetti","Fabian Lindlbauer","Kasper Dinkla","Valery Weber","Lucas Morin","Ingmar Meijer","Viktor Kuropiatnyk","Peter W. J. Staar"],"pdf_url":"https://arxiv.org/pdf/2408.09869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14279v1","updated":"2024-08-26T13:55:42Z","published":"2024-08-26T13:55:42Z","title":"Learning Local Pattern Modularization for Point Cloud Reconstruction\n  from Unseen Classes","summary":"  It is challenging to reconstruct 3D point clouds in unseen classes from\nsingle 2D images. Instead of object-centered coordinate system, current methods\ngeneralized global priors learned in seen classes to reconstruct 3D shapes from\nunseen classes in viewer-centered coordinate system. However, the\nreconstruction accuracy and interpretability are still eager to get improved.\nTo resolve this issue, we introduce to learn local pattern modularization for\nreconstructing 3D shapes in unseen classes, which achieves both good\ngeneralization ability and high reconstruction accuracy. Our insight is to\nlearn a local prior which is class-agnostic and easy to generalize in\nobject-centered coordinate system. Specifically, the local prior is learned via\na process of learning and customizing local pattern modularization in seen\nclasses. During this process, we first learn a set of patterns in local\nregions, which is the basis in the object-centered coordinate system to\nrepresent an arbitrary region on shapes across different classes. Then, we\nmodularize each region on an initially reconstructed shape using the learned\nlocal patterns. Based on that, we customize the local pattern modularization\nusing the input image by refining the reconstruction with more details. Our\nmethod enables to reconstruct high fidelity point clouds from unseen classes in\nobject-centered coordinate system without requiring a large number of patterns\nor any additional information, such as segmentation supervision or camera\nposes. Our experimental results under widely used benchmarks show that our\nmethod achieves the state-of-the-art reconstruction accuracy for shapes from\nunseen classes. The code is available at https://github.com/chenchao15/Unseen.\n","authors":["Chao Chen","Zhizhong Han","Yu-Shen Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14279v1.pdf","comment":"14pages, 11figures, accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2312.06726v3","updated":"2024-08-26T13:52:18Z","published":"2023-12-11T05:57:09Z","title":"Filter & Align: Curating Image-Text Data with Human Knowledge","summary":"  The increasing availability of image-text pairs has largely fueled the rapid\nadvancement in vision-language foundation models. However, the vast scale of\nthese datasets inevitably introduces significant variability in data quality,\nwhich can adversely affect the model performance. This highlights the critical\nrole of data filtering, not only to enhance training efficiency but also to\nimprove overall data quality. Existing methods typically rely on metrics such\nas CLIP Score and BLIP Score, which are derived from pre-trained models.\nHowever, these models are often trained on uncurated, noisy datasets, which can\nperpetuate errors and misalignments in the filtered dataset. We present a novel\nalgorithm that incorporates human knowledge on image-text alignment to guide\nfiltering vast corpus of web-crawled image-text datasets into a compact and\nhigh-quality form. To systemically capture human preferences on image-text\nalignments, we collect a diverse image-text dataset where each image is\nassociated with multiple captions from various sources, and establish a\ncomprehensive set of both subjective and objective criteria for critically\nguiding the alignment assessment from labelers. Additionally, we train a reward\nmodel on these human-preference annotations to internalize the nuanced human\nunderstanding of image-text alignment. The resulting reward model thus can act\nas a human-like referee to filter image-text pairs. Extensive experiments\ndemonstrate that we can maintain, sometimes even improve, model performance\nwhile compressing the image-text datasets up to ~90%. An impressive example is\nthat, by aggressively reducing the total training sample from 130M to only\n15.5M, our BLIP-B/16 models consistently show an average improvement of 2.9% on\nretrieval tasks and 11.5% on captioning tasks compared to full-size-dataset\ncounterparts.\n","authors":["Lei Zhang","Fangxun Shu","Tianyang Liu","Sucheng Ren","Hao Jiang","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2312.06726v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14270v1","updated":"2024-08-26T13:45:58Z","published":"2024-08-26T13:45:58Z","title":"Reliable Multi-modal Medical Image-to-image Translation Independent of\n  Pixel-wise Aligned Data","summary":"  The current mainstream multi-modal medical image-to-image translation methods\nface a contradiction. Supervised methods with outstanding performance rely on\npixel-wise aligned training data to constrain the model optimization. However,\nobtaining pixel-wise aligned multi-modal medical image datasets is challenging.\nUnsupervised methods can be trained without paired data, but their reliability\ncannot be guaranteed. At present, there is no ideal multi-modal medical\nimage-to-image translation method that can generate reliable translation\nresults without the need for pixel-wise aligned data. This work aims to develop\na novel medical image-to-image translation model that is independent of\npixel-wise aligned data (MITIA), enabling reliable multi-modal medical\nimage-to-image translation under the condition of misaligned training data. The\nproposed MITIA model utilizes a prior extraction network composed of a\nmulti-modal medical image registration module and a multi-modal misalignment\nerror detection module to extract pixel-level prior information from training\ndata with misalignment errors to the largest extent. The extracted prior\ninformation is then used to construct a regularization term to constrain the\noptimization of the unsupervised cycle-consistent GAN model, restricting its\nsolution space and thereby improving the performance and reliability of the\ngenerator. We trained the MITIA model using six datasets containing different\nmisalignment errors and two well-aligned datasets. Subsequently, we compared\nthe proposed method with six other state-of-the-art image-to-image translation\nmethods. The results of both quantitative analysis and qualitative visual\ninspection indicate that MITIA achieves superior performance compared to the\ncompeting state-of-the-art methods, both on misaligned data and aligned data.\n","authors":["Langrui Zhou","Guang Li"],"pdf_url":"https://arxiv.org/pdf/2408.14270v1.pdf","comment":"This paper has been accepted as a research article by Medical Physics"},{"id":"http://arxiv.org/abs/2210.07182v7","updated":"2024-08-26T13:43:46Z","published":"2022-10-13T17:03:36Z","title":"PDEBENCH: An Extensive Benchmark for Scientific Machine Learning","summary":"  Machine learning-based modeling of physical systems has experienced increased\ninterest in recent years. Despite some impressive progress, there is still a\nlack of benchmarks for Scientific ML that are easy to use but still challenging\nand representative of a wide range of problems. We introduce PDEBench, a\nbenchmark suite of time-dependent simulation tasks based on Partial\nDifferential Equations (PDEs). PDEBench comprises both code and data to\nbenchmark the performance of novel machine learning models against both\nclassical numerical simulations and machine learning baselines. Our proposed\nset of benchmark problems contribute the following unique features: (1) A much\nwider range of PDEs compared to existing benchmarks, ranging from relatively\ncommon examples to more realistic and difficult problems; (2) much larger\nready-to-use datasets compared to prior work, comprising multiple simulation\nruns across a larger number of initial and boundary conditions and PDE\nparameters; (3) more extensible source codes with user-friendly APIs for data\ngeneration and baseline results with popular machine learning models (FNO,\nU-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to\nextend the benchmark freely for their own purposes using a standardized API and\nto compare the performance of new models to existing baseline methods. We also\npropose new evaluation metrics with the aim to provide a more holistic\nunderstanding of learning methods in the context of Scientific ML. With those\nmetrics we identify tasks which are challenging for recent ML methods and\npropose these tasks as future challenges for the community. The code is\navailable at https://github.com/pdebench/PDEBench.\n","authors":["Makoto Takamoto","Timothy Praditia","Raphael Leiteritz","Dan MacKinlay","Francesco Alesiani","Dirk PflÃ¼ger","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2210.07182v7.pdf","comment":"16 pages (main body) + 34 pages (supplemental material), accepted for\n  publication in NeurIPS 2022 Track Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2408.14267v1","updated":"2024-08-26T13:42:43Z","published":"2024-08-26T13:42:43Z","title":"1-Bit FQT: Pushing the Limit of Fully Quantized Training to 1-bit","summary":"  Fully quantized training (FQT) accelerates the training of deep neural\nnetworks by quantizing the activations, weights, and gradients into lower\nprecision. To explore the ultimate limit of FQT (the lowest achievable\nprecision), we make a first attempt to 1-bit FQT. We provide a theoretical\nanalysis of FQT based on Adam and SGD, revealing that the gradient variance\ninfluences the convergence of FQT. Building on these theoretical results, we\nintroduce an Activation Gradient Pruning (AGP) strategy. The strategy leverages\nthe heterogeneity of gradients by pruning less informative gradients and\nenhancing the numerical precision of remaining gradients to mitigate gradient\nvariance. Additionally, we propose Sample Channel joint Quantization (SCQ),\nwhich utilizes different quantization strategies in the computation of weight\ngradients and activation gradients to ensure that the method is friendly to\nlow-bitwidth hardware. Finally, we present a framework to deploy our algorithm.\nFor fine-tuning VGGNet-16 and ResNet-18 on multiple datasets, our algorithm\nachieves an average accuracy improvement of approximately 6%, compared to\nper-sample quantization. Moreover, our training speedup can reach a maximum of\n5.13x compared to full precision training.\n","authors":["Chang Gao","Jianfei Chen","Kang Zhao","Jiaqi Wang","Liping Jing"],"pdf_url":"https://arxiv.org/pdf/2408.14267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09431v2","updated":"2024-08-26T13:41:14Z","published":"2024-04-15T03:12:12Z","title":"VFMM3D: Releasing the Potential of Image by Vision Foundation Model for\n  Monocular 3D Object Detection","summary":"  Due to its cost-effectiveness and widespread availability, monocular 3D\nobject detection, which relies solely on a single camera during inference,\nholds significant importance across various applications, including autonomous\ndriving and robotics. Nevertheless, directly predicting the coordinates of\nobjects in 3D space from monocular images poses challenges. Therefore, an\neffective solution involves transforming monocular images into LiDAR-like\nrepresentations and employing a LiDAR-based 3D object detector to predict the\n3D coordinates of objects. The key step in this method is accurately converting\nthe monocular image into a reliable point cloud form. In this paper, we present\nVFMM3D, an innovative framework that leverages the capabilities of Vision\nFoundation Models (VFMs) to accurately transform single-view images into LiDAR\npoint cloud representations. VFMM3D utilizes the Segment Anything Model (SAM)\nand Depth Anything Model (DAM) to generate high-quality pseudo-LiDAR data\nenriched with rich foreground information. Specifically, the Depth Anything\nModel (DAM) is employed to generate dense depth maps. Subsequently, the Segment\nAnything Model (SAM) is utilized to differentiate foreground and background\nregions by predicting instance masks. These predicted instance masks and depth\nmaps are then combined and projected into 3D space to generate pseudo-LiDAR\npoints. Finally, any object detectors based on point clouds can be utilized to\npredict the 3D coordinates of objects. Comprehensive experiments are conducted\non two challenging 3D object detection datasets, KITTI and Waymo. Our VFMM3D\nestablishes a new state-of-the-art performance on both datasets. Additionally,\nexperimental results demonstrate the generality of VFMM3D, showcasing its\nseamless integration into various LiDAR-based 3D object detectors.\n","authors":["Bonan Ding","Jin Xie","Jing Nie","Jiale Cao","Xuelong Li","Yanwei Pang"],"pdf_url":"https://arxiv.org/pdf/2404.09431v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.14253v1","updated":"2024-08-26T13:16:03Z","published":"2024-08-26T13:16:03Z","title":"Text3DAug -- Prompted Instance Augmentation for LiDAR Perception","summary":"  LiDAR data of urban scenarios poses unique challenges, such as heterogeneous\ncharacteristics and inherent class imbalance. Therefore, large-scale datasets\nare necessary to apply deep learning methods. Instance augmentation has emerged\nas an efficient method to increase dataset diversity. However, current methods\nrequire the time-consuming curation of 3D models or costly manual data\nannotation. To overcome these limitations, we propose Text3DAug, a novel\napproach leveraging generative models for instance augmentation. Text3DAug does\nnot depend on labeled data and is the first of its kind to generate instances\nand annotations from text. This allows for a fully automated pipeline,\neliminating the need for manual effort in practical applications. Additionally,\nText3DAug is sensor agnostic and can be applied regardless of the LiDAR sensor\nused. Comprehensive experimental analysis on LiDAR segmentation, detection and\nnovel class discovery demonstrates that Text3DAug is effective in supplementing\nexisting methods or as a standalone method, performing on par or better than\nestablished methods, however while overcoming their specific drawbacks. The\ncode is publicly available.\n","authors":["Laurenz Reichardt","Luca Uhr","Oliver WasenmÃ¼ller"],"pdf_url":"https://arxiv.org/pdf/2408.14253v1.pdf","comment":"Accepted at the 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2408.14249v1","updated":"2024-08-26T13:09:23Z","published":"2024-08-26T13:09:23Z","title":"Beyond Few-shot Object Detection: A Detailed Survey","summary":"  Object detection is a critical field in computer vision focusing on\naccurately identifying and locating specific objects in images or videos.\nTraditional methods for object detection rely on large labeled training\ndatasets for each object category, which can be time-consuming and expensive to\ncollect and annotate. To address this issue, researchers have introduced\nfew-shot object detection (FSOD) approaches that merge few-shot learning and\nobject detection principles. These approaches allow models to quickly adapt to\nnew object categories with only a few annotated samples. While traditional FSOD\nmethods have been studied before, this survey paper comprehensively reviews\nFSOD research with a specific focus on covering different FSOD settings such as\nstandard FSOD, generalized FSOD, incremental FSOD, open-set FSOD, and domain\nadaptive FSOD. These approaches play a vital role in reducing the reliance on\nextensive labeled datasets, particularly as the need for efficient machine\nlearning models continues to rise. This survey paper aims to provide a\ncomprehensive understanding of the above-mentioned few-shot settings and\nexplore the methodologies for each FSOD task. It thoroughly compares\nstate-of-the-art methods across different FSOD settings, analyzing them in\ndetail based on their evaluation protocols. Additionally, it offers insights\ninto their applications, challenges, and potential future directions in the\nevolving field of object detection with limited data.\n","authors":["Vishal Chudasama","Hiran Sarkar","Pankaj Wasnik","Vineeth N Balasubramanian","Jayateja Kalla"],"pdf_url":"https://arxiv.org/pdf/2408.14249v1.pdf","comment":"43 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.08282v3","updated":"2024-08-26T13:01:39Z","published":"2024-06-12T14:47:51Z","title":"Interpretable Representation Learning of Cardiac MRI via Attribute\n  Regularization","summary":"  Interpretability is essential in medical imaging to ensure that clinicians\ncan comprehend and trust artificial intelligence models. Several approaches\nhave been recently considered to encode attributes in the latent space to\nenhance its interpretability. Notably, attribute regularization aims to encode\na set of attributes along the dimensions of a latent representation. However,\nthis approach is based on Variational AutoEncoder and suffers from blurry\nreconstruction. In this paper, we propose an Attributed-regularized Soft\nIntrospective Variational Autoencoder that combines attribute regularization of\nthe latent space within the framework of an adversarially trained variational\nautoencoder. We demonstrate on short-axis cardiac Magnetic Resonance images of\nthe UK Biobank the ability of the proposed method to address blurry\nreconstruction issues of variational autoencoder methods while preserving the\nlatent space interpretability.\n","authors":["Maxime Di Folco","Cosmin I. Bercea","Emily Chan","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2406.08282v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2312.08915"},{"id":"http://arxiv.org/abs/2408.14244v1","updated":"2024-08-26T12:59:32Z","published":"2024-08-26T12:59:32Z","title":"Cascaded Temporal Updating Network for Efficient Video Super-Resolution","summary":"  Existing video super-resolution (VSR) methods generally adopt a recurrent\npropagation network to extract spatio-temporal information from the entire\nvideo sequences, exhibiting impressive performance. However, the key components\nin recurrent-based VSR networks significantly impact model efficiency, e.g.,\nthe alignment module occupies a substantial portion of model parameters, while\nthe bidirectional propagation mechanism significantly amplifies the inference\ntime. Consequently, developing a compact and efficient VSR method that can be\ndeployed on resource-constrained devices, e.g., smartphones, remains\nchallenging. To this end, we propose a cascaded temporal updating network\n(CTUN) for efficient VSR. We first develop an implicit cascaded alignment\nmodule to explore spatio-temporal correspondences from adjacent frames.\nMoreover, we propose a unidirectional propagation updating network to\nefficiently explore long-range temporal information, which is crucial for\nhigh-quality video reconstruction. Specifically, we develop a simple yet\neffective hidden updater that can leverage future information to update hidden\nfeatures during forward propagation, significantly reducing inference time\nwhile maintaining performance. Finally, we formulate all of these components\ninto an end-to-end trainable VSR network. Extensive experimental results show\nthat our CTUN achieves a favorable trade-off between efficiency and performance\ncompared to existing methods. Notably, compared with BasicVSR, our method\nobtains better results while employing only about 30% of the parameters and\nrunning time. The source code and pre-trained models will be available at\nhttps://github.com/House-Leo/CTUN.\n","authors":["Hao Li","Jiangxin Dong","Jinshan Pan"],"pdf_url":"https://arxiv.org/pdf/2408.14244v1.pdf","comment":"Project website: https://github.com/House-Leo/CTUN"},{"id":"http://arxiv.org/abs/2403.12848v2","updated":"2024-08-26T12:55:44Z","published":"2024-03-19T15:54:48Z","title":"Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit\n  regularization","summary":"  Compositional 3D scene synthesis has diverse applications across a spectrum\nof industries such as robotics, films, and video games, as it closely mirrors\nthe complexity of real-world multi-object environments. Conventional works\ntypically employ shape retrieval based frameworks which naturally suffer from\nlimited shape diversity. Recent progresses have been made in object shape\ngeneration with generative models such as diffusion models, which increases the\nshape fidelity. However, these approaches separately treat 3D shape generation\nand layout generation. The synthesized scenes are usually hampered by layout\ncollision, which suggests that the scene-level fidelity is still\nunder-explored. In this paper, we aim at generating realistic and reasonable 3D\nindoor scenes from scene graph. To enrich the priors of the given scene graph\ninputs, large language model is utilized to aggregate the global-wise features\nwith local node-wise and edge-wise features. With a unified graph encoder,\ngraph features are extracted to guide joint layout-shape generation. Additional\nregularization is introduced to explicitly constrain the produced 3D layouts.\nBenchmarked on the SG-FRONT dataset, our method achieves better 3D scene\nsynthesis, especially in terms of scene-level fidelity. The source code will be\nreleased after publication.\n","authors":["Yao Wei","Martin Renqiang Min","George Vosselman","Li Erran Li","Michael Ying Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12848v2.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2401.16712v2","updated":"2024-08-26T12:52:25Z","published":"2024-01-30T03:17:02Z","title":"LF Tracy: A Unified Single-Pipeline Approach for Salient Object\n  Detection in Light Field Cameras","summary":"  Leveraging rich information is crucial for dense prediction tasks. Light\nfield (LF) cameras are instrumental in this regard, as they allow data to be\nsampled from various perspectives. This capability provides valuable spatial,\ndepth, and angular information, enhancing scene-parsing tasks. However, we have\nidentified two overlooked issues for the LF salient object detection (SOD)\ntask. (1): Previous approaches predominantly employ a customized two-stream\ndesign to discover the spatial and depth features within light field images.\nThe network struggles to learn the implicit angular information between\ndifferent images due to a lack of intra-network data connectivity. (2): Little\nresearch has been directed towards the data augmentation strategy for LF SOD.\nResearch on inter-network data connectivity is scant. In this study, we propose\nan efficient paradigm (LF Tracy) to address those issues. This comprises a\nsingle-pipeline encoder paired with a highly efficient information aggregation\n(IA) module (around 8M parameters) to establish an intra-network connection.\nThen, a simple yet effective data augmentation strategy called MixLD is\ndesigned to bridge the inter-network connections. Owing to this innovative\nparadigm, our model surpasses the existing state-of-the-art method through\nextensive experiments. Especially, LF Tracy demonstrates a 23% improvement over\nprevious results on the latest large-scale PKU dataset. The source code is\npublicly available at: https://github.com/FeiBryantkit/LF-Tracy.\n","authors":["Fei Teng","Jiaming Zhang","Jiawei Liu","Kunyu Peng","Xina Cheng","Zhiyong Li","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2401.16712v2.pdf","comment":"Accepted to ICPR 2024. The source code is publicly available at:\n  https://github.com/FeiBryantkit/LF-Tracy"},{"id":"http://arxiv.org/abs/2408.14229v1","updated":"2024-08-26T12:44:17Z","published":"2024-08-26T12:44:17Z","title":"Gallery-Aware Uncertainty Estimation For Open-Set Face Recognition","summary":"  Accurately estimating image quality and model robustness improvement are\ncritical challenges in unconstrained face recognition, which can be addressed\nthrough uncertainty estimation via probabilistic face embeddings. Previous\nresearch mainly focused on uncertainty estimation in face verification, leaving\nthe open-set face recognition task underexplored. In open-set face recognition,\none seeks to classify an image, which could also be unknown. Here, the low\nvariance of probabilistic embedding does not imply a low error probability: an\nimage embedding could be close to several classes in a gallery, thus yielding\nhigh uncertainty. We propose a method aware of two sources of ambiguity in the\nopen-set recognition system: (1) the gallery uncertainty caused by overlapping\nclasses and (2) the uncertainty of the face embeddings. To detect both types,\nwe use a Bayesian probabilistic model of embedding distribution, which provides\na principled uncertainty estimate. Challenging open-set face recognition\ndatasets, such as IJB-C, serve as a testbed for our method. We also propose a\nnew open-set recognition protocol for whale and dolphin identification. The\nproposed approach better identifies recognition errors than uncertainty\nestimation methods based solely on image quality.\n","authors":["Leonid Erlygin","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2408.14229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14227v1","updated":"2024-08-26T12:43:48Z","published":"2024-08-26T12:43:48Z","title":"TC-PDM: Temporally Consistent Patch Diffusion Models for\n  Infrared-to-Visible Video Translation","summary":"  Infrared imaging offers resilience against changing lighting conditions by\ncapturing object temperatures. Yet, in few scenarios, its lack of visual\ndetails compared to daytime visible images, poses a significant challenge for\nhuman and machine interpretation. This paper proposes a novel diffusion method,\ndubbed Temporally Consistent Patch Diffusion Models (TC-DPM), for\ninfrared-to-visible video translation. Our method, extending the Patch\nDiffusion Model, consists of two key components. Firstly, we propose a\nsemantic-guided denoising, leveraging the strong representations of\nfoundational models. As such, our method faithfully preserves the semantic\nstructure of generated visible images. Secondly, we propose a novel temporal\nblending module to guide the denoising trajectory, ensuring the temporal\nconsistency between consecutive frames. Experiment shows that TC-PDM\noutperforms state-of-the-art methods by 35.3% in FVD for infrared-to-visible\nvideo translation and by 6.1% in AP50 for day-to-night object detection. Our\ncode is publicly available at https://github.com/dzungdoan6/tc-pdm\n","authors":["Anh-Dzung Doan","Vu Minh Hieu Phan","Surabhi Gupta","Markus Wagner","Tat-Jun Chin","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2408.14227v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2408.14211v1","updated":"2024-08-26T12:10:52Z","published":"2024-08-26T12:10:52Z","title":"MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware\n  Diffusion and Iterative Refinement","summary":"  Existing works in single-image human reconstruction suffer from weak\ngeneralizability due to insufficient training data or 3D inconsistencies for a\nlack of comprehensive multi-view knowledge. In this paper, we introduce\nMagicMan, a human-specific multi-view diffusion model designed to generate\nhigh-quality novel view images from a single reference image. As its core, we\nleverage a pre-trained 2D diffusion model as the generative prior for\ngeneralizability, with the parametric SMPL-X model as the 3D body prior to\npromote 3D awareness. To tackle the critical challenge of maintaining\nconsistency while achieving dense multi-view generation for improved 3D human\nreconstruction, we first introduce hybrid multi-view attention to facilitate\nboth efficient and thorough information interchange across different views.\nAdditionally, we present a geometry-aware dual branch to perform concurrent\ngeneration in both RGB and normal domains, further enhancing consistency via\ngeometry cues. Last but not least, to address ill-shaped issues arising from\ninaccurate SMPL-X estimation that conflicts with the reference image, we\npropose a novel iterative refinement strategy, which progressively optimizes\nSMPL-X accuracy while enhancing the quality and consistency of the generated\nmulti-views. Extensive experimental results demonstrate that our method\nsignificantly outperforms existing approaches in both novel view synthesis and\nsubsequent 3D human reconstruction tasks.\n","authors":["Xu He","Xiaoyu Li","Di Kang","Jiangnan Ye","Chaopeng Zhang","Liyang Chen","Xiangjun Gao","Han Zhang","Zhiyong Wu","Haolin Zhuang"],"pdf_url":"https://arxiv.org/pdf/2408.14211v1.pdf","comment":"Project Page: https://thuhcsi.github.io/MagicMan"},{"id":"http://arxiv.org/abs/2408.14197v1","updated":"2024-08-26T11:53:09Z","published":"2024-08-26T11:53:09Z","title":"Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting\n  and Planning via World Models for Autonomous Driving","summary":"  World models envision potential future states based on various ego actions.\nThey embed extensive knowledge about the driving environment, facilitating safe\nand scalable autonomous driving. Most existing methods primarily focus on\neither data generation or the pretraining paradigms of world models. Unlike the\naforementioned prior works, we propose Drive-OccWorld, which adapts a\nvision-centric 4D forecasting world model to end-to-end planning for autonomous\ndriving. Specifically, we first introduce a semantic and motion-conditional\nnormalization in the memory module, which accumulates semantic and dynamic\ninformation from historical BEV embeddings. These BEV features are then\nconveyed to the world decoder for future occupancy and flow forecasting,\nconsidering both geometry and spatiotemporal modeling. Additionally, we propose\ninjecting flexible action conditions, such as velocity, steering angle,\ntrajectory, and commands, into the world model to enable controllable\ngeneration and facilitate a broader range of downstream applications.\nFurthermore, we explore integrating the generative capabilities of the 4D world\nmodel with end-to-end planning, enabling continuous forecasting of future\nstates and the selection of optimal trajectories using an occupancy-based cost\nfunction. Extensive experiments on the nuScenes dataset demonstrate that our\nmethod can generate plausible and controllable 4D occupancy, opening new\navenues for driving world generation and end-to-end planning.\n","authors":["Yu Yang","Jianbiao Mei","Yukai Ma","Siliang Du","Wenqing Chen","Yijie Qian","Yuxiang Feng","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14197v1.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.14192v1","updated":"2024-08-26T11:36:38Z","published":"2024-08-26T11:36:38Z","title":"Feature Aligning Few shot Learning Method Using Local Descriptors\n  Weighted Rules","summary":"  Few-shot classification involves identifying new categories using a limited\nnumber of labeled samples. Current few-shot classification methods based on\nlocal descriptors primarily leverage underlying consistent features across\nvisible and invisible classes, facing challenges including redundant\nneighboring information, noisy representations, and limited interpretability.\nThis paper proposes a Feature Aligning Few-shot Learning Method Using Local\nDescriptors Weighted Rules (FAFD-LDWR). It innovatively introduces a\ncross-normalization method into few-shot image classification to preserve the\ndiscriminative information of local descriptors as much as possible; and\nenhances classification performance by aligning key local descriptors of\nsupport and query sets to remove background noise. FAFD-LDWR performs\nexcellently on three benchmark datasets , outperforming state-of-the-art\nmethods in both 1-shot and 5-shot settings. The designed visualization\nexperiments also demonstrate FAFD-LDWR's improvement in prediction\ninterpretability.\n","authors":["Bingchen Yan"],"pdf_url":"https://arxiv.org/pdf/2408.14192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14189v1","updated":"2024-08-26T11:26:27Z","published":"2024-08-26T11:26:27Z","title":"EMDFNet: Efficient Multi-scale and Diverse Feature Network for Traffic\n  Sign Detection","summary":"  The detection of small objects, particularly traffic signs, is a critical\nsubtask within object detection and autonomous driving. Despite the notable\nachievements in previous research, two primary challenges persist. Firstly, the\nmain issue is the singleness of feature extraction. Secondly, the detection\nprocess fails to effectively integrate with objects of varying sizes or scales.\nThese issues are also prevalent in generic object detection. Motivated by these\nchallenges, in this paper, we propose a novel object detection network named\nEfficient Multi-scale and Diverse Feature Network (EMDFNet) for traffic sign\ndetection that integrates an Augmented Shortcut Module and an Efficient Hybrid\nEncoder to address the aforementioned issues simultaneously. Specifically, the\nAugmented Shortcut Module utilizes multiple branches to integrate various\nspatial semantic information and channel semantic information, thereby\nenhancing feature diversity. The Efficient Hybrid Encoder utilizes global\nfeature fusion and local feature interaction based on various features to\ngenerate distinctive classification features by integrating feature information\nin an adaptable manner. Extensive experiments on the Tsinghua-Tencent 100K\n(TT100K) benchmark and the German Traffic Sign Detection Benchmark (GTSDB)\ndemonstrate that our EMDFNet outperforms other state-of-the-art detectors in\nperformance while retaining the real-time processing capabilities of\nsingle-stage models. This substantiates the effectiveness of EMDFNet in\ndetecting small traffic signs.\n","authors":["Pengyu Li","Chenhe Liu","Tengfei Li","Xinyu Wang","Shihui Zhang","Dongyang Yu"],"pdf_url":"https://arxiv.org/pdf/2408.14189v1.pdf","comment":"15 pages,5 figures,accepted to ICANN"},{"id":"http://arxiv.org/abs/2408.14187v1","updated":"2024-08-26T11:24:13Z","published":"2024-08-26T11:24:13Z","title":"Ensemble Predicate Decoding for Unbiased Scene Graph Generation","summary":"  Scene Graph Generation (SGG) aims to generate a comprehensive graphical\nrepresentation that accurately captures the semantic information of a given\nscenario. However, the SGG model's performance in predicting more fine-grained\npredicates is hindered by a significant predicate bias. According to existing\nworks, the long-tail distribution of predicates in training data results in the\nbiased scene graph. However, the semantic overlap between predicate categories\nmakes predicate prediction difficult, and there is a significant difference in\nthe sample size of semantically similar predicates, making the predicate\nprediction more difficult. Therefore, higher requirements are placed on the\ndiscriminative ability of the model. In order to address this problem, this\npaper proposes Ensemble Predicate Decoding (EPD), which employs multiple\ndecoders to attain unbiased scene graph generation. Two auxiliary decoders\ntrained on lower-frequency predicates are used to improve the discriminative\nability of the model. Extensive experiments are conducted on the VG, and the\nexperiment results show that EPD enhances the model's representation capability\nfor predicates. In addition, we find that our approach ensures a relatively\nsuperior predictive capability for more frequent predicates compared to\nprevious unbiased SGG methods.\n","authors":["Jiasong Feng","Lichun Wang","Hongbo Xu","Kai Xu","Baocai Yin"],"pdf_url":"https://arxiv.org/pdf/2408.14187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14186v1","updated":"2024-08-26T11:22:52Z","published":"2024-08-26T11:22:52Z","title":"Affine steerers for structured keypoint description","summary":"  We propose a way to train deep learning based keypoint descriptors that makes\nthem approximately equivariant for locally affine transformations of the image\nplane. The main idea is to use the representation theory of GL(2) to generalize\nthe recently introduced concept of steerers from rotations to affine\ntransformations. Affine steerers give high control over how keypoint\ndescriptions transform under image transformations. We demonstrate the\npotential of using this control for image matching. Finally, we propose a way\nto finetune keypoint descriptors with a set of steerers on upright images and\nobtain state-of-the-art results on several standard benchmarks. Code will be\npublished at github.com/georg-bn/affine-steerers.\n","authors":["Georg BÃ¶kman","Johan Edstedt","Michael Felsberg","Fredrik Kahl"],"pdf_url":"https://arxiv.org/pdf/2408.14186v1.pdf","comment":"To be presented at ECCV 2024"},{"id":"http://arxiv.org/abs/2408.14180v1","updated":"2024-08-26T11:08:44Z","published":"2024-08-26T11:08:44Z","title":"I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing","summary":"  Significant progress has been made in the field of Instruction-based Image\nEditing (IIE). However, evaluating these models poses a significant challenge.\nA crucial requirement in this field is the establishment of a comprehensive\nevaluation benchmark for accurately assessing editing results and providing\nvaluable insights for its further development. In response to this need, we\npropose I2EBench, a comprehensive benchmark designed to automatically evaluate\nthe quality of edited images produced by IIE models from multiple dimensions.\nI2EBench consists of 2,000+ images for editing, along with 4,000+ corresponding\noriginal and diverse instructions. It offers three distinctive characteristics:\n1) Comprehensive Evaluation Dimensions: I2EBench comprises 16 evaluation\ndimensions that cover both high-level and low-level aspects, providing a\ncomprehensive assessment of each IIE model. 2) Human Perception Alignment: To\nensure the alignment of our benchmark with human perception, we conducted an\nextensive user study for each evaluation dimension. 3) Valuable Research\nInsights: By analyzing the advantages and disadvantages of existing IIE models\nacross the 16 dimensions, we offer valuable research insights to guide future\ndevelopment in the field. We will open-source I2EBench, including all\ninstructions, input images, human annotations, edited images from all evaluated\nmethods, and a simple script for evaluating the results from new IIE models.\nThe code, dataset and generated images from all IIE models are provided in\ngithub: https://github.com/cocoshe/I2EBench.\n","authors":["Yiwei Ma","Jiayi Ji","Ke Ye","Weihuang Lin","Zhibin Wang","Yonghan Zheng","Qiang Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.14180v1.pdf","comment":"Tech report, 39 pages, 41 figures"},{"id":"http://arxiv.org/abs/2408.13149v2","updated":"2024-08-26T11:05:58Z","published":"2024-08-23T15:16:01Z","title":"Focus on Neighbors and Know the Whole: Towards Consistent Dense\n  Multiview Text-to-Image Generator for 3D Creation","summary":"  Generating dense multiview images from text prompts is crucial for creating\nhigh-fidelity 3D assets. Nevertheless, existing methods struggle with\nspace-view correspondences, resulting in sparse and low-quality outputs. In\nthis paper, we introduce CoSER, a novel consistent dense Multiview\nText-to-Image Generator for Text-to-3D, achieving both efficiency and quality\nby meticulously learning neighbor-view coherence and further alleviating\nambiguity through the swift traversal of all views. For achieving neighbor-view\nconsistency, each viewpoint densely interacts with adjacent viewpoints to\nperceive the global spatial structure, and aggregates information along motion\npaths explicitly defined by physical principles to refine details. To further\nenhance cross-view consistency and alleviate content drift, CoSER rapidly scan\nall views in spiral bidirectional manner to aware holistic information and then\nscores each point based on semantic material. Subsequently, we conduct weighted\ndown-sampling along the spatial dimension based on scores, thereby facilitating\nprominent information fusion across all views with lightweight computation.\nTechnically, the core module is built by integrating the attention mechanism\nwith a selective state space model, exploiting the robust learning capabilities\nof the former and the low overhead of the latter. Extensive evaluation shows\nthat CoSER is capable of producing dense, high-fidelity, content-consistent\nmultiview images that can be flexibly integrated into various 3D generation\nmodels.\n","authors":["Bonan Li","Zicheng Zhang","Xingyi Yang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.13149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02474v3","updated":"2024-08-26T11:04:45Z","published":"2024-02-04T13:09:13Z","title":"Deep Spectral Improvement for Unsupervised Image Instance Segmentation","summary":"  Deep spectral methods reframe the image decomposition process as a graph\npartitioning task by extracting features using self-supervised learning and\nutilizing the Laplacian of the affinity matrix to obtain eigensegments.\nHowever, instance segmentation has received less attention compared to other\ntasks within the context of deep spectral methods. This paper addresses the\nfact that not all channels of the feature map extracted from a self-supervised\nbackbone contain sufficient information for instance segmentation purposes. In\nfact, Some channels are noisy and hinder the accuracy of the task. To overcome\nthis issue, this paper proposes two channel reduction modules: Noise Channel\nReduction (NCR) and Deviation-based Channel Reduction (DCR). The NCR retains\nchannels with lower entropy, as they are less likely to be noisy, while DCR\nprunes channels with low standard deviation, as they lack sufficient\ninformation for effective instance segmentation. Furthermore, the paper\ndemonstrates that the dot product, commonly used in deep spectral methods, is\nnot suitable for instance segmentation due to its sensitivity to feature map\nvalues, potentially leading to incorrect instance segments. A new similarity\nmetric called Bray-Curtis over Chebyshev (BoC) is proposed to address this\nissue. It takes into account the distribution of features in addition to their\nvalues, providing a more robust similarity measure for instance segmentation.\nQuantitative and qualitative results on the Youtube-VIS2019 dataset highlight\nthe improvements achieved by the proposed channel reduction methods and the use\nof BoC instead of the conventional dot product for creating the affinity\nmatrix. These improvements are observed in terms of mean Intersection over\nUnion and extracted instance segments, demonstrating enhanced instance\nsegmentation performance. The code is available on:\nhttps://github.com/farnooshar/SpecUnIIS\n","authors":["Farnoosh Arefi","Amir M. Mansourian","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2402.02474v3.pdf","comment":"11 pages, 13 figures and 5 tables"},{"id":"http://arxiv.org/abs/2408.04249v2","updated":"2024-08-26T10:57:15Z","published":"2024-08-08T06:29:32Z","title":"InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian\n  Splatting","summary":"  We present InstantStyleGaussian, an innovative 3D style transfer method based\non the 3D Gaussian Splatting (3DGS) scene representation. By inputting a\ntarget-style image, it quickly generates new 3D GS scenes. Our method operates\non pre-reconstructed GS scenes, combining diffusion models with an improved\niterative dataset update strategy. It utilizes diffusion models to generate\ntarget style images, adds these new images to the training dataset, and uses\nthis dataset to iteratively update and optimize the GS scenes, significantly\naccelerating the style editing process while ensuring the quality of the\ngenerated scenes. Extensive experimental results demonstrate that our method\nensures high-quality stylized scenes while offering significant advantages in\nstyle transfer speed and consistency.\n","authors":["Xin-Yi Yu","Jun-Xin Yu","Li-Bo Zhou","Yan Wei","Lin-Lin Ou"],"pdf_url":"https://arxiv.org/pdf/2408.04249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14177v1","updated":"2024-08-26T10:50:14Z","published":"2024-08-26T10:50:14Z","title":"NimbleD: Enhancing Self-supervised Monocular Depth Estimation with\n  Pseudo-labels and Large-scale Video Pre-training","summary":"  We introduce NimbleD, an efficient self-supervised monocular depth estimation\nlearning framework that incorporates supervision from pseudo-labels generated\nby a large vision model. This framework does not require camera intrinsics,\nenabling large-scale pre-training on publicly available videos. Our\nstraightforward yet effective learning strategy significantly enhances the\nperformance of fast and lightweight models without introducing any overhead,\nallowing them to achieve performance comparable to state-of-the-art\nself-supervised monocular depth estimation models. This advancement is\nparticularly beneficial for virtual and augmented reality applications\nrequiring low latency inference. The source code, model weights, and\nacknowledgments are available at https://github.com/xapaxca/nimbled .\n","authors":["Albert Luginov","Muhammad Shahzad"],"pdf_url":"https://arxiv.org/pdf/2408.14177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17428v2","updated":"2024-08-26T10:49:29Z","published":"2023-11-29T08:09:01Z","title":"SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Human Action\n  Segmentation","summary":"  Multi-modal human action segmentation is a critical and challenging task with\na wide range of applications. Nowadays, the majority of approaches concentrate\non the fusion of dense signals (i.e., RGB, optical flow, and depth maps).\nHowever, the potential contributions of sparse IoT sensor signals, which can be\ncrucial for achieving accurate recognition, have not been fully explored. To\nmake up for this, we introduce a Sparse signalguided Transformer (SigFormer) to\ncombine both dense and sparse signals. We employ mask attention to fuse\nlocalized features by constraining cross-attention within the regions where\nsparse signals are valid. However, since sparse signals are discrete, they lack\nsufficient information about the temporal action boundaries. Therefore, in\nSigFormer, we propose to emphasize the boundary information at two stages to\nalleviate this problem. In the first feature extraction stage, we introduce an\nintermediate bottleneck module to jointly learn both category and boundary\nfeatures of each dense modality through the inner loss functions. After the\nfusion of dense modalities and sparse signals, we then devise a two-branch\narchitecture that explicitly models the interrelationship between action\ncategory and temporal boundary. Experimental results demonstrate that SigFormer\noutperforms the state-of-the-art approaches on a multi-modal action\nsegmentation dataset from real industrial environments, reaching an outstanding\nF1 score of 0.958. The codes and pre-trained models have been available at\nhttps://github.com/LIUQI-creat/SigFormer.\n","authors":["Qi Liu","Xinchen Liu","Kun Liu","Xiaoyan Gu","Wu Liu"],"pdf_url":"https://arxiv.org/pdf/2311.17428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14176v1","updated":"2024-08-26T10:42:53Z","published":"2024-08-26T10:42:53Z","title":"SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its\n  Teacher","summary":"  In this paper, we aim to enhance the performance of SwiftBrush, a prominent\none-step text-to-image diffusion model, to be competitive with its multi-step\nStable Diffusion counterpart. Initially, we explore the quality-diversity\ntrade-off between SwiftBrush and SD Turbo: the former excels in image\ndiversity, while the latter excels in image quality. This observation motivates\nour proposed modifications in the training methodology, including better weight\ninitialization and efficient LoRA training. Moreover, our introduction of a\nnovel clamped CLIP loss enhances image-text alignment and results in improved\nimage quality. Remarkably, by combining the weights of models trained with\nefficient LoRA and full training, we achieve a new state-of-the-art one-step\ndiffusion model, achieving an FID of 8.14 and surpassing all GAN-based and\nmulti-step Stable Diffusion models. The evaluation code is available at:\nhttps://github.com/vinairesearch/swiftbrushv2.\n","authors":["Trung Dao","Thuan Hoang Nguyen","Thanh Le","Duc Vu","Khoi Nguyen","Cuong Pham","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2408.14176v1.pdf","comment":"Accepted to ECCV'24"},{"id":"http://arxiv.org/abs/2402.11237v2","updated":"2024-08-26T10:39:22Z","published":"2024-02-17T10:02:22Z","title":"Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in\n  Deep Learning","summary":"  Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than\nlearning the intended task, they tend to draw inconclusive relationships\nbetween their inputs and outputs. Shortcut learning is ubiquitous among many\nfailure cases of neural networks, and traces of this phenomenon can be seen in\ntheir generalizability issues, domain shift, adversarial vulnerability, and\neven bias towards majority groups. In this paper, we argue that this\ncommonality in the cause of various DNN issues creates a significant\nopportunity that should be leveraged to find a unified solution for shortcut\nlearning. To this end, we outline the recent advances in topological data\nanalysis (TDA), and persistent homology (PH) in particular, to sketch a unified\nroadmap for detecting shortcuts in deep learning. We demonstrate our arguments\nby investigating the topological features of computational graphs in DNNs using\ntwo cases of unlearnable examples and bias in decision-making as our test\nstudies. Our analysis of these two failure cases of DNNs reveals that finding a\nunified solution for shortcut learning in DNNs is not out of reach, and TDA can\nplay a significant role in forming such a framework.\n","authors":["Hadi M. Dolatabadi","Sarah M. Erfani","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2402.11237v2.pdf","comment":"Accepted to the 2024 European Conference on Artificial Intelligence\n  (ECAI)"},{"id":"http://arxiv.org/abs/2408.14173v1","updated":"2024-08-26T10:39:01Z","published":"2024-08-26T10:39:01Z","title":"BackFlip: The Impact of Local and Global Data Augmentations on Artistic\n  Image Aesthetic Assessment","summary":"  Assessing the aesthetic quality of artistic images presents unique challenges\ndue to the subjective nature of aesthetics and the complex visual\ncharacteristics inherent to artworks. Basic data augmentation techniques\ncommonly applied to natural images in computer vision may not be suitable for\nart images in aesthetic evaluation tasks, as they can change the composition of\nthe art images. In this paper, we explore the impact of local and global data\naugmentation techniques on artistic image aesthetic assessment (IAA). We\nintroduce BackFlip, a local data augmentation technique designed specifically\nfor artistic IAA. We evaluate the performance of BackFlip across three artistic\nimage datasets and four neural network architectures, comparing it with the\ncommonly used data augmentation techniques. Then, we analyze the effects of\ncomponents within the BackFlip pipeline through an ablation study. Our findings\ndemonstrate that local augmentations, such as BackFlip, tend to outperform\nglobal augmentations on artistic IAA in most cases, probably because they do\nnot perturb the composition of the art images. These results emphasize the\nimportance of considering both local and global augmentations in future\ncomputational aesthetics research.\n","authors":["Ombretta Strafforello","Gonzalo Muradas Odriozola","Fatemeh Behrad","Li-Wei Chen","Anne-Sofie Maerten","Derya Soydaner","Johan Wagemans"],"pdf_url":"https://arxiv.org/pdf/2408.14173v1.pdf","comment":"Published at the VISART VII workshop at ECCV 2024. Ombretta\n  Strafforello, Gonzalo Muradas Odriozola, Fatemeh Behrad, Li-Wei Chen,\n  Anne-Sofie Maerten and Derya Soydaner contributed equally to this work"},{"id":"http://arxiv.org/abs/2408.01224v3","updated":"2024-08-26T09:59:55Z","published":"2024-08-02T12:27:15Z","title":"Multi-head Spatial-Spectral Mamba for Hyperspectral Image Classification","summary":"  Spatial-Spectral Mamba (SSM) improves computational efficiency and captures\nlong-range dependencies, addressing Transformer limitations. However,\ntraditional Mamba models overlook rich spectral information in HSIs and\nstruggle with high dimensionality and sequential data. To address these issues,\nwe propose the SSM with multi-head self-attention and token enhancement\n(MHSSMamba). This model integrates spectral and spatial information by\nenhancing spectral tokens and using multi-head attention to capture complex\nrelationships between spectral bands and spatial locations. It also manages\nlong-range dependencies and the sequential nature of HSI data, preserving\ncontextual information across spectral bands. MHSSMamba achieved remarkable\nclassification accuracies of 97.62\\% on Pavia University, 96.92\\% on the\nUniversity of Houston, 96.85\\% on Salinas, and 99.49\\% on Wuhan-longKou\ndatasets. The source code is available at\n\\href{https://github.com/MHassaanButt/MHA\\_SS\\_Mamba}{GitHub}.\n","authors":["Muhammad Ahmad","Muhammad Hassaan Farooq Butt","Muhammad Usama","Hamad Ahmed Altuwaijri","Manuel Mazzara","Salvatore Distefano"],"pdf_url":"https://arxiv.org/pdf/2408.01224v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14153v1","updated":"2024-08-26T09:55:34Z","published":"2024-08-26T09:55:34Z","title":"Explaining Vision-Language Similarities in Dual Encoders with\n  Feature-Pair Attributions","summary":"  Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and learn similarities between them. However, it is not\nunderstood how such models compare two inputs. Here, we address this research\ngap with two contributions. First, we derive a method to attribute predictions\nof any differentiable dual encoder onto feature-pair interactions between its\ninputs. Second, we apply our method to CLIP-type models and show that they\nlearn fine-grained correspondences between parts of captions and regions in\nimages. They match objects across input modes and also account for mismatches.\nHowever, this visual-linguistic grounding ability heavily varies between object\nclasses, depends on the training data distribution, and largely improves after\nin-domain training. Using our method we can identify knowledge gaps about\nspecific object classes in individual models and can monitor their improvement\nupon fine-tuning.\n","authors":["Lucas MÃ¶ller","Pascal Tilli","Ngoc Thang Vu","Sebastian PadÃ³"],"pdf_url":"https://arxiv.org/pdf/2408.14153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14152v1","updated":"2024-08-26T09:55:32Z","published":"2024-08-26T09:55:32Z","title":"Application of Disentanglement to Map Registration Problem","summary":"  Geospatial data come from various sources, such as satellites, aircraft, and\nLiDAR. The variability of the source is not limited to the types of data\nacquisition techniques, as we have maps from different time periods. To\nincorporate these data for a coherent analysis, it is essential to first align\ndifferent \"styles\" of geospatial data to its matching images that point to the\nsame location on the surface of the Earth. In this paper, we approach the image\nregistration as a two-step process of (1) extracting geospatial contents\ninvariant to visual (and any other non-content-related) information, and (2)\nmatching the data based on such (purely) geospatial contents. We hypothesize\nthat a combination of $\\beta$-VAE-like architecture [2] and adversarial\ntraining will achieve both the disentanglement of the geographic information\nand artistic styles and generation of new map tiles by composing the encoded\ngeographic information with any artistic style.\n","authors":["Hae Jin Song","Patrycja Krawczuk","Po-Hsuan Huang"],"pdf_url":"https://arxiv.org/pdf/2408.14152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14143v1","updated":"2024-08-26T09:41:40Z","published":"2024-08-26T09:41:40Z","title":"2D-Malafide: Adversarial Attacks Against Face Deepfake Detection Systems","summary":"  We introduce 2D-Malafide, a novel and lightweight adversarial attack designed\nto deceive face deepfake detection systems. Building upon the concept of 1D\nconvolutional perturbations explored in the speech domain, our method leverages\n2D convolutional filters to craft perturbations which significantly degrade the\nperformance of state-of-the-art face deepfake detectors. Unlike traditional\nadditive noise approaches, 2D-Malafide optimises a small number of filter\ncoefficients to generate robust adversarial perturbations which are\ntransferable across different face images. Experiments, conducted using the\nFaceForensics++ dataset, demonstrate that 2D-Malafide substantially degrades\ndetection performance in both white-box and black-box settings, with larger\nfilter sizes having the greatest impact. Additionally, we report an\nexplainability analysis using GradCAM which illustrates how 2D-Malafide\nmisleads detection systems by altering the image areas used most for\nclassification. Our findings highlight the vulnerability of current deepfake\ndetection systems to convolutional adversarial attacks as well as the need for\nfuture work to enhance detection robustness through improved image fidelity\nconstraints.\n","authors":["Chiara Galdi","Michele Panariello","Massimiliano Todisco","Nicholas Evans"],"pdf_url":"https://arxiv.org/pdf/2408.14143v1.pdf","comment":"Accepted at BIOSIG 2024"},{"id":"http://arxiv.org/abs/2408.14135v1","updated":"2024-08-26T09:32:16Z","published":"2024-08-26T09:32:16Z","title":"Foodfusion: A Novel Approach for Food Image Composition via Diffusion\n  Models","summary":"  Food image composition requires the use of existing dish images and\nbackground images to synthesize a natural new image, while diffusion models\nhave made significant advancements in image generation, enabling the\nconstruction of end-to-end architectures that yield promising results. However,\nexisting diffusion models face challenges in processing and fusing information\nfrom multiple images and lack access to high-quality publicly available\ndatasets, which prevents the application of diffusion models in food image\ncomposition. In this paper, we introduce a large-scale, high-quality food image\ncomposite dataset, FC22k, which comprises 22,000 foreground, background, and\nground truth ternary image pairs. Additionally, we propose a novel food image\ncomposition method, Foodfusion, which leverages the capabilities of the\npre-trained diffusion models and incorporates a Fusion Module for processing\nand integrating foreground and background information. This fused information\naligns the foreground features with the background structure by merging the\nglobal structural information at the cross-attention layer of the denoising\nUNet. To further enhance the content and structure of the background, we also\nintegrate a Content-Structure Control Module. Extensive experiments demonstrate\nthe effectiveness and scalability of our proposed method.\n","authors":["Chaohua Shi","Xuan Wang","Si Shi","Xule Wang","Mingrui Zhu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2408.14135v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.14131v1","updated":"2024-08-26T09:26:08Z","published":"2024-08-26T09:26:08Z","title":"GenFormer -- Generated Images are All You Need to Improve Robustness of\n  Transformers on Small Datasets","summary":"  Recent studies showcase the competitive accuracy of Vision Transformers\n(ViTs) in relation to Convolutional Neural Networks (CNNs), along with their\nremarkable robustness. However, ViTs demand a large amount of data to achieve\nadequate performance, which makes their application to small datasets\nchallenging, falling behind CNNs. To overcome this, we propose GenFormer, a\ndata augmentation strategy utilizing generated images, thereby improving\ntransformer accuracy and robustness on small-scale image classification tasks.\nIn our comprehensive evaluation we propose Tiny ImageNetV2, -R, and -A as new\ntest set variants of Tiny ImageNet by transferring established ImageNet\ngeneralization and robustness benchmarks to the small-scale data domain.\nSimilarly, we introduce MedMNIST-C and EuroSAT-C as corrupted test set variants\nof established fine-grained datasets in the medical and aerial domain. Through\na series of experiments conducted on small datasets of various domains,\nincluding Tiny ImageNet, CIFAR, EuroSAT and MedMNIST datasets, we demonstrate\nthe synergistic power of our method, in particular when combined with common\ntrain and test time augmentations, knowledge distillation, and architectural\ndesign choices. Additionally, we prove the effectiveness of our approach under\nchallenging conditions with limited training data, demonstrating significant\nimprovements in both accuracy and robustness, bridging the gap between CNNs and\nViTs in the small-scale dataset domain.\n","authors":["Sven Oehri","Nikolas Ebert","Ahmed Abdullah","Didier Stricker","Oliver WasenmÃ¼ller"],"pdf_url":"https://arxiv.org/pdf/2408.14131v1.pdf","comment":"This paper has been accepted at International Conference on Pattern\n  Recognition (ICPR), 2023"},{"id":"http://arxiv.org/abs/2406.02978v2","updated":"2024-08-26T09:23:44Z","published":"2024-06-05T06:21:54Z","title":"Self-Supervised Skeleton-Based Action Representation Learning: A\n  Benchmark and Beyond","summary":"  Self-supervised learning (SSL), which aims to learn meaningful prior\nrepresentations from unlabeled data, has been proven effective for\nskeleton-based action understanding. Different from the image domain, skeleton\ndata possesses sparser spatial structures and diverse representation forms,\nwith the absence of background clues and the additional temporal dimension,\npresenting new challenges for spatial-temporal motion pretext task design.\nRecently, many endeavors have been made for skeleton-based SSL, achieving\nremarkable progress. However, a systematic and thorough review is still\nlacking. In this paper, we conduct, for the first time, a comprehensive survey\non self-supervised skeleton-based action representation learning. Following the\ntaxonomy of context-based, generative learning, and contrastive learning\napproaches, we make a thorough review and benchmark of existing works and shed\nlight on the future possible directions. Remarkably, our investigation\ndemonstrates that most SSL works rely on the single paradigm, learning\nrepresentations of a single level, and are evaluated on the action recognition\ntask solely, which leaves the generalization power of skeleton SSL models\nunder-explored. To this end, a novel and effective SSL method for skeleton is\nfurther proposed, which integrates versatile representation learning objectives\nof different granularity, substantially boosting the generalization capacity\nfor multiple skeleton downstream tasks. Extensive experiments under three\nlarge-scale datasets demonstrate our method achieves superior generalization\nperformance on various downstream tasks, including recognition, retrieval,\ndetection, and few-shot learning.\n","authors":["Jiahang Zhang","Lilang Lin","Shuai Yang","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2406.02978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15318v2","updated":"2024-08-26T09:21:35Z","published":"2024-04-03T13:33:07Z","title":"VASARI-auto: equitable, efficient, and economical featurisation of\n  glioma MRI","summary":"  The VASARI MRI feature set is a quantitative system designed to standardise\nglioma imaging descriptions. Though effective, deriving VASARI is\ntime-consuming and seldom used in clinical practice. This is a problem that\nmachine learning could plausibly automate. Using glioma data from 1172\npatients, we developed VASARI-auto, an automated labelling software applied to\nboth open-source lesion masks and our openly available tumour segmentation\nmodel. In parallel, two consultant neuroradiologists independently quantified\nVASARI features in a subsample of 100 glioblastoma cases. We quantified: 1)\nagreement across neuroradiologists and VASARI-auto; 2) calibration of\nperformance equity; 3) an economic workforce analysis; and 4) fidelity in\npredicting patient survival. Tumour segmentation was compatible with the\ncurrent state of the art and equally performant regardless of age or sex. A\nmodest inter-rater variability between in-house neuroradiologists was\ncomparable to between neuroradiologists and VASARI-auto, with far higher\nagreement between VASARI-auto methods. The time taken for neuroradiologists to\nderive VASARI was substantially higher than VASARI-auto (mean time per case 317\nvs. 3 seconds). A UK hospital workforce analysis forecast that three years of\nVASARI featurisation would demand 29,777 consultant neuroradiologist workforce\nhours ({\\pounds}1,574,935), reducible to 332 hours of computing time (and\n{\\pounds}146 of power) with VASARI-auto. The best-performing survival model\nutilised VASARI-auto features as opposed to those derived by neuroradiologists.\nVASARI-auto is a highly efficient automated labelling system with equitable\nperformance across patient age or sex, a favourable economic profile if used as\na decision support tool, and with non-inferior fidelity in downstream patient\nsurvival prediction. Future work should iterate upon and integrate such tools\nto enhance patient care.\n","authors":["James K Ruffle","Samia Mohinta","Kelly Pegoretti Baruteau","Rebekah Rajiah","Faith Lee","Sebastian Brandner","Parashkev Nachev","Harpreet Hyare"],"pdf_url":"https://arxiv.org/pdf/2404.15318v2.pdf","comment":"36 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2401.07729v2","updated":"2024-08-26T09:16:57Z","published":"2024-01-15T14:43:40Z","title":"SSL-Interactions: Pretext Tasks for Interactive Trajectory Prediction","summary":"  This paper addresses motion forecasting in multi-agent environments, pivotal\nfor ensuring safety of autonomous vehicles. Traditional as well as recent\ndata-driven marginal trajectory prediction methods struggle to properly learn\nnon-linear agent-to-agent interactions. We present SSL-Interactions that\nproposes pretext tasks to enhance interaction modeling for trajectory\nprediction. We introduce four interaction-aware pretext tasks to encapsulate\nvarious aspects of agent interactions: range gap prediction, closest distance\nprediction, direction of movement prediction, and type of interaction\nprediction. We further propose an approach to curate interaction-heavy\nscenarios from datasets. This curated data has two advantages: it provides a\nstronger learning signal to the interaction model, and facilitates generation\nof pseudo-labels for interaction-centric pretext tasks. We also propose three\nnew metrics specifically designed to evaluate predictions in interactive\nscenes. Our empirical evaluations indicate SSL-Interactions outperforms\nstate-of-the-art motion forecasting methods quantitatively with up to 8%\nimprovement, and qualitatively, for interaction-heavy scenarios.\n","authors":["Prarthana Bhattacharyya","Chengjie Huang","Krzysztof Czarnecki"],"pdf_url":"https://arxiv.org/pdf/2401.07729v2.pdf","comment":"Accepted at IV-2024. 13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.05206v4","updated":"2024-08-26T09:15:11Z","published":"2024-07-06T23:16:41Z","title":"Helios: An extremely low power event-based gesture recognition for\n  always-on smart eyewear","summary":"  This paper introduces Helios, the first extremely low-power, real-time,\nevent-based hand gesture recognition system designed for all-day on smart\neyewear. As augmented reality (AR) evolves, current smart glasses like the Meta\nRay-Bans prioritize visual and wearable comfort at the expense of\nfunctionality. Existing human-machine interfaces (HMIs) in these devices, such\nas capacitive touch and voice controls, present limitations in ergonomics,\nprivacy and power consumption. Helios addresses these challenges by leveraging\nnatural hand interactions for a more intuitive and comfortable user experience.\nOur system utilizes a extremely low-power and compact 3mmx4mm/20mW event camera\nto perform natural hand-based gesture recognition for always-on smart eyewear.\nThe camera's output is processed by a convolutional neural network (CNN)\nrunning on a NXP Nano UltraLite compute platform, consuming less than 350mW.\nHelios can recognize seven classes of gestures, including subtle microgestures\nlike swipes and pinches, with 91% accuracy. We also demonstrate real-time\nperformance across 20 users at a remarkably low latency of 60ms. Our user\ntesting results align with the positive feedback we received during our recent\nsuccessful demo at AWE-USA-2024.\n","authors":["Prarthana Bhattacharyya","Joshua Mitton","Ryan Page","Owen Morgan","Ben Menzies","Gabriel Homewood","Kemi Jacobs","Paolo Baesso","David Trickett","Chris Mair","Taru Muhonen","Rory Clark","Louis Berridge","Richard Vigars","Iain Wallace"],"pdf_url":"https://arxiv.org/pdf/2407.05206v4.pdf","comment":"Accepted at ECCV-Integrating Computer Vision in Smart Eyewear, 2024.\n  18 pages, 10 figures. First three authors contributed equally to this paper"},{"id":"http://arxiv.org/abs/2211.07546v2","updated":"2024-08-26T09:01:23Z","published":"2022-11-14T17:11:15Z","title":"Vision meets algae: A novel way for microalgae recognization and health\n  monitor","summary":"  Marine microalgae are widespread in the ocean and play a crucial role in the\necosystem. Automatic identification and location of marine microalgae in\nmicroscopy images would help establish marine ecological environment monitoring\nand water quality evaluation system. We proposed a new dataset for the\ndetection of marine microalgae and a range of detection methods, the dataset\nincluding images of different genus of algae and the same genus in different\nstates. We set the number of unbalanced classes in the data set and added\nimages of mixed water samples in the test set to simulate the actual situation\nin the field. Then we trained, validated and tested the, TOOD, YOLOv5, YOLOv8\nand variants of RCNN algorithms on this dataset. The results showed both\none-stage and two-stage object detection models can achieve high mean average\nprecision, which proves the ability of computer vision in multi-object\ndetection of microalgae, and provides basic data and models for real-time\ndetection of microalgal cells.\n","authors":["Shizheng Zhou","Juntao Jiang","Xiaohan Hong","Yan Hong","Pengcheng Fu"],"pdf_url":"https://arxiv.org/pdf/2211.07546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14114v1","updated":"2024-08-26T08:59:22Z","published":"2024-08-26T08:59:22Z","title":"ShapeMamba-EM: Fine-Tuning Foundation Model with Local Shape Descriptors\n  and Mamba Blocks for 3D EM Image Segmentation","summary":"  Electron microscopy (EM) imaging offers unparalleled resolution for analyzing\nneural tissues, crucial for uncovering the intricacies of synaptic connections\nand neural processes fundamental to understanding behavioral mechanisms.\nRecently, the foundation models have demonstrated impressive performance across\nnumerous natural and medical image segmentation tasks. However, applying these\nfoundation models to EM segmentation faces significant challenges due to domain\ndisparities. This paper presents ShapeMamba-EM, a specialized fine-tuning\nmethod for 3D EM segmentation, which employs adapters for long-range dependency\nmodeling and an encoder for local shape description within the original\nfoundation model. This approach effectively addresses the unique volumetric and\nmorphological complexities of EM data. Tested over a wide range of EM images,\ncovering five segmentation tasks and 10 datasets, ShapeMamba-EM outperforms\nexisting methods, establishing a new standard in EM image segmentation and\nenhancing the understanding of neural tissue architecture.\n","authors":["Ruohua Shi","Qiufan Pang","Lei Ma","Lingyu Duan","Tiejun Huang","Tingting Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.14114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14111v1","updated":"2024-08-26T08:55:16Z","published":"2024-08-26T08:55:16Z","title":"Bengali Sign Language Recognition through Hand Pose Estimation using\n  Multi-Branch Spatial-Temporal Attention Model","summary":"  Hand gesture-based sign language recognition (SLR) is one of the most\nadvanced applications of machine learning, and computer vision uses hand\ngestures. Although, in the past few years, many researchers have widely\nexplored and studied how to address BSL problems, specific unaddressed issues\nremain, such as skeleton and transformer-based BSL recognition. In addition,\nthe lack of evaluation of the BSL model in various concealed environmental\nconditions can prove the generalized property of the existing model by facing\ndaily life signs. As a consequence, existing BSL recognition systems provide a\nlimited perspective of their generalisation ability as they are tested on\ndatasets containing few BSL alphabets that have a wide disparity in gestures\nand are easy to differentiate. To overcome these limitations, we propose a\nspatial-temporal attention-based BSL recognition model considering hand joint\nskeletons extracted from the sequence of images. The main aim of utilising hand\nskeleton-based BSL data is to ensure the privacy and low-resolution sequence of\nimages, which need minimum computational cost and low hardware configurations.\nOur model captures discriminative structural displacements and short-range\ndependency based on unified joint features projected onto high-dimensional\nfeature space. Specifically, the use of Separable TCN combined with a powerful\nmulti-head spatial-temporal attention architecture generated high-performance\naccuracy. The extensive experiments with a proposed dataset and two benchmark\nBSL datasets with a wide range of evaluations, such as intra- and inter-dataset\nevaluation settings, demonstrated that our proposed models achieve competitive\nperformance with extremely low computational complexity and run faster than\nexisting models.\n","authors":["Abu Saleh Musa Miah","Md. Al Mehedi Hasan","Md Hadiuzzaman","Muhammad Nazrul Islam","Jungpil Shin"],"pdf_url":"https://arxiv.org/pdf/2408.14111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11311v2","updated":"2024-08-26T08:47:00Z","published":"2024-06-17T08:18:41Z","title":"Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object\n  Detection","summary":"  The use of synthetic data in indoor 3D object detection offers the potential\nof greatly reducing the manual labor involved in 3D annotations and training\neffective zero-shot detectors. However, the complicated domain shifts across\nsyn-to-real indoor datasets remains underexplored. In this paper, we propose a\nnovel Object-wise Hierarchical Domain Alignment (OHDA) framework for\nsyn-to-real unsupervised domain adaptation in indoor 3D object detection. Our\napproach includes an object-aware augmentation strategy to effectively\ndiversify the source domain data, and we introduce a two-branch adaptation\nframework consisting of an adversarial training branch and a pseudo labeling\nbranch, in order to simultaneously reach holistic-level and class-level domain\nalignment. The pseudo labeling is further refined through two proposed schemes\nspecifically designed for indoor UDA. Our adaptation results from synthetic\ndataset 3D-FRONT to real-world datasets ScanNetV2 and SUN RGB-D demonstrate\nremarkable mAP25 improvements of 9.7% and 9.1% over Source-Only baselines,\nrespectively, and consistently outperform the methods adapted from 2D and 3D\noutdoor scenarios. The code will be publicly available upon paper acceptance.\n","authors":["Yunsong Wang","Na Zhao","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2406.11311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14087v1","updated":"2024-08-26T08:16:58Z","published":"2024-08-26T08:16:58Z","title":"LSM-YOLO: A Compact and Effective ROI Detector for Medical Detection","summary":"  In existing medical Region of Interest (ROI) detection, there lacks an\nalgorithm that can simultaneously satisfy both real-time performance and\naccuracy, not meeting the growing demand for automatic detection in medicine.\nAlthough the basic YOLO framework ensures real-time detection due to its fast\nspeed, it still faces challenges in maintaining precision concurrently. To\nalleviate the above problems, we propose a novel model named Lightweight Shunt\nMatching-YOLO (LSM-YOLO), with Lightweight Adaptive Extraction (LAE) and\nMultipath Shunt Feature Matching (MSFM). Firstly, by using LAE to refine\nfeature extraction, the model can obtain more contextual information and\nhigh-resolution details from multiscale feature maps, thereby extracting\ndetailed features of ROI in medical images while reducing the influence of\nnoise. Secondly, MSFM is utilized to further refine the fusion of high-level\nsemantic features and low-level visual features, enabling better fusion between\nROI features and neighboring features, thereby improving the detection rate for\nbetter diagnostic assistance. Experimental results demonstrate that LSM-YOLO\nachieves 48.6% AP on a private dataset of pancreatic tumors, 65.1% AP on the\nBCCD blood cell detection public dataset, and 73.0% AP on the Br35h brain tumor\ndetection public dataset. Our model achieves state-of-the-art performance with\nminimal parameter cost on the above three datasets. The source codes are at:\nhttps://github.com/VincentYuuuuuu/LSM-YOLO.\n","authors":["Zhongwen Yu","Qiu Guan","Jianmin Yang","Zhiqiang Yang","Qianwei Zhou","Yang Chen","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.14087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14084v1","updated":"2024-08-26T08:11:35Z","published":"2024-08-26T08:11:35Z","title":"HABD: a houma alliance book ancient handwritten character recognition\n  database","summary":"  The Houma Alliance Book, one of history's earliest calligraphic examples, was\nunearthed in the 1970s. These artifacts were meticulously organized,\nreproduced, and copied by the Shanxi Provincial Institute of Cultural Relics.\nHowever, because of their ancient origins and severe ink erosion, identifying\ncharacters in the Houma Alliance Book is challenging, necessitating the use of\ndigital technology. In this paper, we propose a new ancient handwritten\ncharacter recognition database for the Houma alliance book, along with a novel\nbenchmark based on deep learning architectures. More specifically, a collection\nof 26,732 characters samples from the Houma Alliance Book were gathered,\nencompassing 327 different types of ancient characters through iterative\nannotation. Furthermore, benchmark algorithms were proposed by combining four\ndeep neural network classifiers with two data augmentation methods. This\nresearch provides valuable resources and technical support for further studies\non the Houma Alliance Book and other ancient characters. This contributes to\nour understanding of ancient culture and history, as well as the preservation\nand inheritance of humanity's cultural heritage.\n","authors":["Xiaoyu Yuan","Xiaohua Huang","Zibo Zhang","Yabo Sun"],"pdf_url":"https://arxiv.org/pdf/2408.14084v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.14080v1","updated":"2024-08-26T08:02:57Z","published":"2024-08-26T08:02:57Z","title":"SONICS: Synthetic Or Not -- Identifying Counterfeit Songs","summary":"  The recent surge in AI-generated songs presents exciting possibilities and\nchallenges. While these tools democratize music creation, they also necessitate\nthe ability to distinguish between human-composed and AI-generated songs for\nsafeguarding artistic integrity and content curation. Existing research and\ndatasets in fake song detection only focus on singing voice deepfake detection\n(SVDD), where the vocals are AI-generated but the instrumental music is sourced\nfrom real songs. However, this approach is inadequate for contemporary\nend-to-end AI-generated songs where all components (vocals, lyrics, music, and\nstyle) could be AI-generated. Additionally, existing datasets lack lyrics-music\ndiversity, long-duration songs, and open fake songs. To address these gaps, we\nintroduce SONICS, a novel dataset for end-to-end Synthetic Song Detection\n(SSD), comprising over 97k songs with over 49k synthetic songs from popular\nplatforms like Suno and Udio. Furthermore, we highlight the importance of\nmodeling long-range temporal dependencies in songs for effective authenticity\ndetection, an aspect overlooked in existing methods. To capture these patterns,\nwe propose a novel model, SpecTTTra, that is up to 3 times faster and 6 times\nmore memory efficient compared to popular CNN and Transformer-based models\nwhile maintaining competitive performance. Finally, we offer both AI-based and\nHuman evaluation benchmarks, addressing another deficiency in current research.\n","authors":["Md Awsafur Rahman","Zaber Ibn Abdul Hakim","Najibul Haque Sarker","Bishmoy Paul","Shaikh Anowarul Fattah"],"pdf_url":"https://arxiv.org/pdf/2408.14080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14060v1","updated":"2024-08-26T07:37:59Z","published":"2024-08-26T07:37:59Z","title":"Evaluating the Visual Similarity of Southwest China's Ethnic Minority\n  Brocade Based on Deep Learning","summary":"  This paper employs deep learning methods to investigate the visual similarity\nof ethnic minority patterns in Southwest China. A customized SResNet-18 network\nwas developed, achieving an accuracy of 98.7% on the test set, outperforming\nResNet-18, VGGNet-16, and AlexNet. The extracted feature vectors from\nSResNet-18 were evaluated using three metrics: cosine similarity, Euclidean\ndistance, and Manhattan distance. The analysis results were visually\nrepresented on an ethnic thematic map, highlighting the connections between\nethnic patterns and their regional distributions.\n","authors":["Shichen Liu","Huaxing Lu"],"pdf_url":"https://arxiv.org/pdf/2408.14060v1.pdf","comment":"8 pages,2tables,5 figures"},{"id":"http://arxiv.org/abs/2405.17137v3","updated":"2024-08-26T07:36:03Z","published":"2024-05-27T12:54:09Z","title":"Jump-teaching: Ultra Efficient and Robust Learning with Noisy Label","summary":"  Sample selection is the most straightforward technique to combat label noise,\naiming to distinguish mislabeled samples during training and avoid the\ndegradation of the robustness of the model. In the workflow, $\\textit{selecting\npossibly clean data}$ and $\\textit{model update}$ are iterative. However, their\ninterplay and intrinsic characteristics hinder the robustness and efficiency of\nlearning with noisy labels: 1) The model chooses clean data with selection\nbias, leading to the accumulated error in the model update. 2) Most selection\nstrategies leverage partner networks or supplementary information to mitigate\nlabel corruption, albeit with increased computation resources and lower\nthroughput speed. Therefore, we employ only one network with the jump manner\nupdate to decouple the interplay and mine more semantic information from the\nloss for a more precise selection. Specifically, the selection of clean data\nfor each model update is based on one of the prior models, excluding the last\niteration. The strategy of model update exhibits a jump behavior in the form.\nMoreover, we map the outputs of the network and labels into the same semantic\nfeature space, respectively. In this space, a detailed and simple loss\ndistribution is generated to distinguish clean samples more effectively. Our\nproposed approach achieves almost up to $2.53\\times$ speedup, $0.46\\times$ peak\nmemory footprint, and superior robustness over state-of-the-art works with\nvarious noise settings.\n","authors":["Kangye Ji","Fei Cheng","Zeqing Wang","Bohu Huang"],"pdf_url":"https://arxiv.org/pdf/2405.17137v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14051v1","updated":"2024-08-26T07:17:05Z","published":"2024-08-26T07:17:05Z","title":"Let Video Teaches You More: Video-to-Image Knowledge Distillation using\n  DEtection TRansformer for Medical Video Lesion Detection","summary":"  AI-assisted lesion detection models play a crucial role in the early\nscreening of cancer. However, previous image-based models ignore the\ninter-frame contextual information present in videos. On the other hand,\nvideo-based models capture the inter-frame context but are computationally\nexpensive. To mitigate this contradiction, we delve into Video-to-Image\nknowledge distillation leveraging DEtection TRansformer (V2I-DETR) for the task\nof medical video lesion detection. V2I-DETR adopts a teacher-student network\nparadigm. The teacher network aims at extracting temporal contexts from\nmultiple frames and transferring them to the student network, and the student\nnetwork is an image-based model dedicated to fast prediction in inference. By\ndistilling multi-frame contexts into a single frame, the proposed V2I-DETR\ncombines the advantages of utilizing temporal contexts from video-based models\nand the inference speed of image-based models. Through extensive experiments,\nV2I-DETR outperforms previous state-of-the-art methods by a large margin while\nachieving the real-time inference speed (30 FPS) as the image-based model.\n","authors":["Yuncheng Jiang","Zixun Zhang","Jun Wei","Chun-Mei Feng","Guanbin Li","Xiang Wan","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2408.14051v1.pdf","comment":"BIBM2024"},{"id":"http://arxiv.org/abs/2305.12236v2","updated":"2024-08-26T07:09:52Z","published":"2023-05-20T17:01:52Z","title":"Searching a Compact Architecture for Robust Multi-Exposure Image Fusion","summary":"  In recent years, learning-based methods have achieved significant\nadvancements in multi-exposure image fusion. However, two major stumbling\nblocks hinder the development, including pixel misalignment and inefficient\ninference. Reliance on aligned image pairs in existing methods causes\nsusceptibility to artifacts due to device motion. Additionally, existing\ntechniques often rely on handcrafted architectures with huge network\nengineering, resulting in redundant parameters, adversely impacting inference\nefficiency and flexibility. To mitigate these limitations, this study\nintroduces an architecture search-based paradigm incorporating self-alignment\nand detail repletion modules for robust multi-exposure image fusion.\n  Specifically, targeting the extreme discrepancy of exposure, we propose the\nself-alignment module, leveraging scene relighting to constrain the\nillumination degree for following alignment and feature extraction. Detail\nrepletion is proposed to enhance the texture details of scenes. Additionally,\nincorporating a hardware-sensitive constraint, we present the fusion-oriented\narchitecture search to explore compact and efficient networks for fusion. The\nproposed method outperforms various competitive schemes, achieving a noteworthy\n3.19\\% improvement in PSNR for general scenarios and an impressive 23.5\\%\nenhancement in misaligned scenarios. Moreover, it significantly reduces\ninference time by 69.1\\%. The code will be available at\nhttps://github.com/LiuZhu-CV/CRMEF.\n","authors":["Zhu Liu","Jinyuan Liu","Guanyao Wu","Zihang Chen","Xin Fan","Risheng Liu"],"pdf_url":"https://arxiv.org/pdf/2305.12236v2.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.14047v1","updated":"2024-08-26T07:02:17Z","published":"2024-08-26T07:02:17Z","title":"Alleviating Class Imbalance in Semi-supervised Multi-organ Segmentation\n  via Balanced Subclass Regularization","summary":"  Semi-supervised learning (SSL) has shown notable potential in relieving the\nheavy demand of dense prediction tasks on large-scale well-annotated datasets,\nespecially for the challenging multi-organ segmentation (MoS). However, the\nprevailing class-imbalance problem in MoS, caused by the substantial variations\nin organ size, exacerbates the learning difficulty of the SSL network. To\nalleviate this issue, we present a two-phase semi-supervised network (BSR-Net)\nwith balanced subclass regularization for MoS. Concretely, in Phase I, we\nintroduce a class-balanced subclass generation strategy based on balanced\nclustering to effectively generate multiple balanced subclasses from original\nbiased ones according to their pixel proportions. Then, in Phase II, we design\nan auxiliary subclass segmentation (SCS) task within the multi-task framework\nof the main MoS task. The SCS task contributes a balanced subclass\nregularization to the main MoS task and transfers unbiased knowledge to the MoS\nnetwork, thus alleviating the influence of the class-imbalance problem.\nExtensive experiments conducted on two publicly available datasets, i.e., the\nMICCAI FLARE 2022 dataset and the WORD dataset, verify the superior performance\nof our method compared with other methods.\n","authors":["Zhenghao Feng","Lu Wen","Binyu Yan","Jiaqi Cui","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06607v4","updated":"2024-08-26T06:57:51Z","published":"2023-11-11T16:37:41Z","title":"Monkey: Image Resolution and Text Label Are Important Things for Large\n  Multi-modal Models","summary":"  Large Multimodal Models (LMMs) have shown promise in vision-language tasks\nbut struggle with high-resolution input and detailed scene understanding.\nAddressing these challenges, we introduce Monkey to enhance LMM capabilities.\nFirstly, Monkey processes input images by dividing them into uniform patches,\neach matching the size (e.g., 448x448) used in the original training of the\nwell-trained vision encoder. Equipped with individual adapter for each patch,\nMonkey can handle higher resolutions up to 1344x896 pixels, enabling the\ndetailed capture of complex visual information. Secondly, it employs a\nmulti-level description generation method, enriching the context for\nscene-object associations. This two-part strategy ensures more effective\nlearning from generated data: the higher resolution allows for a more detailed\ncapture of visuals, which in turn enhances the effectiveness of comprehensive\ndescriptions. Extensive ablative results validate the effectiveness of our\ndesigns. Additionally, experiments on 18 datasets further demonstrate that\nMonkey surpasses existing LMMs in many tasks like Image Captioning and various\nVisual Question Answering formats. Specially, in qualitative tests focused on\ndense text question answering, Monkey has exhibited encouraging results\ncompared with GPT4V. Code is available at\nhttps://github.com/Yuliang-Liu/Monkey.\n","authors":["Zhang Li","Biao Yang","Qiang Liu","Zhiyin Ma","Shuo Zhang","Jingxu Yang","Yabo Sun","Yuliang Liu","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2311.06607v4.pdf","comment":"CVPR 2024 Highlight"},{"id":"http://arxiv.org/abs/2406.10737v2","updated":"2024-08-26T06:37:24Z","published":"2024-06-15T20:47:38Z","title":"Dynamic Domains, Dynamic Solutions: DPCore for Continual Test-Time\n  Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) seeks to adapt a source pre-trained\nmodel to continually changing, unlabeled target domains. Existing TTA methods\nare typically designed for environments where domain changes occur sequentially\nand can struggle in more dynamic scenarios, as illustrated in Figure\n\\ref{fig:settings}. Inspired by the principles of online K-Means, we introduce\na novel approach to CTTA through visual prompting. We propose a \\emph{Dynamic\nPrompt Coreset} that not only preserves knowledge from previously visited\ndomains but also accommodates learning from new potential domains. This is\ncomplemented by a distance-based \\emph{Weight Updating Mechanism} that ensures\nthe coreset remains current and relevant. Our approach employs a fixed model\narchitecture alongside the coreset and an innovative updating system to\neffectively mitigate challenges such as catastrophic forgetting and error\naccumulation. Extensive testing on four widely-used benchmarks demonstrates\nthat our method consistently outperforms state-of-the-art alternatives in both\nclassification and segmentation CTTA tasks across the structured and dynamic\nCTTA settings, with $99\\%$ fewer trainable parameters.\n","authors":["Yunbei Zhang","Akshay Mehra","Jihun Hamm"],"pdf_url":"https://arxiv.org/pdf/2406.10737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.08158v4","updated":"2024-08-26T06:30:52Z","published":"2021-08-18T14:04:52Z","title":"Practical X-ray Gastric Cancer Screening Using Refined Stochastic Data\n  Augmentation and Hard Boundary Box Training","summary":"  Endoscopy is widely used to diagnose gastric cancer and has a high diagnostic\nperformance, but it must be performed by a physician, which limits the number\nof people who can be diagnosed. In contrast, gastric X-rays can be performed by\ntechnicians and screen a much larger number of patients, but accurate diagnosis\nrequires experience. We propose an unprecedented and practical gastric cancer\ndiagnosis support system for gastric X-ray images, enabling more people to be\nscreened. The system is based on a general deep learning-based object detection\nmodel and incorporates two novel techniques: refined probabilistic stomach\nimage augmentation (R-sGAIA) and hard boundary box training (HBBT). R-sGAIA\nenhances the probabilistic gastric fold region, providing more learning\npatterns for cancer detection models. HBBT is an efficient training method that\nimproves model performance by allowing the use of unannotated negative (i.e.,\nhealthy control) samples, which are typically unusable in conventional\ndetection models. The proposed system achieves a sensitivity (SE) for gastric\ncancer of 90.2%, higher than that of an expert (85.5%). Additionally, two out\nof five detected candidate boxes are cancerous, maintaining high precision\nwhile processing images at a speed of 0.51 seconds per image. The system also\noutperforms methods using the same object detection model and state-of-the-art\ndata augmentation, showing a 5.9-point improvement in the F1 score. In summary,\nthis system efficiently identifies areas for radiologists to examine within a\npractical timeframe, significantly reducing their workload.\n","authors":["Hideaki Okamoto","Quan Huu Cap","Takakiyo Nomura","Kazuhito Nabeshima","Jun Hashimoto","Hitoshi Iyatomi"],"pdf_url":"https://arxiv.org/pdf/2108.08158v4.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.14039v1","updated":"2024-08-26T06:22:54Z","published":"2024-08-26T06:22:54Z","title":"Collaborative Perception in Multi-Robot Systems: Case Studies in\n  Household Cleaning and Warehouse Operations","summary":"  This paper explores the paradigm of Collaborative Perception (CP), where\nmultiple robots and sensors in the environment share and integrate sensor data\nto construct a comprehensive representation of the surroundings. By aggregating\ndata from various sensors and utilizing advanced algorithms, the collaborative\nperception framework improves task efficiency, coverage, and safety. Two case\nstudies are presented to showcase the benefits of collaborative perception in\nmulti-robot systems. The first case study illustrates the benefits and\nadvantages of using CP for the task of household cleaning with a team of\ncleaning robots. The second case study performs a comparative analysis of the\nperformance of CP versus Standalone Perception (SP) for Autonomous Mobile\nRobots operating in a warehouse environment. The case studies validate the\neffectiveness of CP in enhancing multi-robot coordination, task completion, and\noverall system performance and its potential to impact operations in other\napplications as well. Future investigations will focus on optimizing the\nframework and validating its performance through empirical testing.\n","authors":["Bharath Rajiv Nair"],"pdf_url":"https://arxiv.org/pdf/2408.14039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02462v2","updated":"2024-08-26T06:12:05Z","published":"2024-04-03T05:04:55Z","title":"A Unified Membership Inference Method for Visual Self-supervised Encoder\n  via Part-aware Capability","summary":"  Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we aim to perform membership inference on visual self-supervised\nmodels in a more realistic setting: self-supervised training method and details\nare unknown for an adversary when attacking as he usually faces a black-box\nsystem in practice. In this setting, considering that self-supervised model\ncould be trained by completely different self-supervised paradigms, e.g.,\nmasked image modeling and contrastive learning, with complex training details,\nwe propose a unified membership inference method called PartCrop. It is\nmotivated by the shared part-aware capability among models and stronger part\nresponse on the training data. Specifically, PartCrop crops parts of objects in\nan image to query responses with the image in representation space. We conduct\nextensive attacks on self-supervised models with different training protocols\nand structures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Our code is\navailable at https://github.com/JiePKU/PartCrop.\n","authors":["Jie Zhu","Jirong Zha","Ding Li","Leye Wang"],"pdf_url":"https://arxiv.org/pdf/2404.02462v2.pdf","comment":"Accepted by ACM CCS2024, Full version"},{"id":"http://arxiv.org/abs/2408.14035v1","updated":"2024-08-26T06:01:54Z","published":"2024-08-26T06:01:54Z","title":"FAST-LIVO2: Fast, Direct LiDAR-Inertial-Visual Odometry","summary":"  This paper proposes FAST-LIVO2: a fast, direct LiDAR-inertial-visual odometry\nframework to achieve accurate and robust state estimation in SLAM tasks and\nprovide great potential in real-time, onboard robotic applications. FAST-LIVO2\nfuses the IMU, LiDAR and image measurements efficiently through an ESIKF. To\naddress the dimension mismatch between the heterogeneous LiDAR and image\nmeasurements, we use a sequential update strategy in the Kalman filter. To\nenhance the efficiency, we use direct methods for both the visual and LiDAR\nfusion, where the LiDAR module registers raw points without extracting edge or\nplane features and the visual module minimizes direct photometric errors\nwithout extracting ORB or FAST corner features. The fusion of both visual and\nLiDAR measurements is based on a single unified voxel map where the LiDAR\nmodule constructs the geometric structure for registering new LiDAR scans and\nthe visual module attaches image patches to the LiDAR points. To enhance the\naccuracy of image alignment, we use plane priors from the LiDAR points in the\nvoxel map (and even refine the plane prior) and update the reference patch\ndynamically after new images are aligned. Furthermore, to enhance the\nrobustness of image alignment, FAST-LIVO2 employs an on-demanding raycast\noperation and estimates the image exposure time in real time. Lastly, we detail\nthree applications of FAST-LIVO2: UAV onboard navigation demonstrating the\nsystem's computation efficiency for real-time onboard navigation, airborne\nmapping showcasing the system's mapping accuracy, and 3D model rendering\n(mesh-based and NeRF-based) underscoring the suitability of our reconstructed\ndense map for subsequent rendering tasks. We open source our code, dataset and\napplication on GitHub to benefit the robotics community.\n","authors":["Chunran Zheng","Wei Xu","Zuhao Zou","Tong Hua","Chongjian Yuan","Dongjiao He","Bingyang Zhou","Zheng Liu","Jiarong Lin","Fangcheng Zhu","Yunfan Ren","Rong Wang","Fanle Meng","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.14035v1.pdf","comment":"30 pages, 31 figures, due to the limitation that 'The abstract field\n  cannot exceed 1,920 characters', the abstract presented here is shorter than\n  the one in the PDF file"},{"id":"http://arxiv.org/abs/2403.18878v2","updated":"2024-08-26T05:54:21Z","published":"2024-03-27T10:46:24Z","title":"Teaching AI the Anatomy Behind the Scan: Addressing Anatomical Flaws in\n  Medical Image Segmentation with Learnable Prior","summary":"  Imposing key anatomical features, such as the number of organs, their shapes\nand relative positions, is crucial for building a robust multi-organ\nsegmentation model. Current attempts to incorporate anatomical features include\nbroadening the effective receptive field (ERF) size with data-intensive\nmodules, or introducing anatomical constraints that scales poorly to\nmulti-organ segmentation. We introduce a novel architecture called the\nAnatomy-Informed Cascaded Segmentation Network (AIC-Net). AIC-Net incorporates\na learnable input termed \"Anatomical Prior\", which can be adapted to\npatient-specific anatomy using a differentiable spatial deformation. The\ndeformed prior later guides decoder layers towards more anatomy-informed\npredictions. We repeat this process at a local patch level to enhance the\nrepresentation of intricate objects, resulting in a cascaded network structure.\nAIC-Net is a general method that enhances any existing segmentation models to\nbe more anatomy-aware. We have validated the performance of AIC-Net, with\nvarious backbones, on two multi-organ segmentation tasks: abdominal organs and\nvertebrae. For each respective task, our benchmarks demonstrate improved dice\nscore and Hausdorff distance.\n","authors":["Young Seok Jeon","Hongfei Yang","Huazhu Fu","Mengling Feng"],"pdf_url":"https://arxiv.org/pdf/2403.18878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14032v1","updated":"2024-08-26T05:52:35Z","published":"2024-08-26T05:52:35Z","title":"More Pictures Say More: Visual Intersection Network for Open Set Object\n  Detection","summary":"  Open Set Object Detection has seen rapid development recently, but it\ncontinues to pose significant challenges. Language-based methods, grappling\nwith the substantial modal disparity between textual and visual modalities,\nrequire extensive computational resources to bridge this gap. Although\nintegrating visual prompts into these frameworks shows promise for enhancing\nperformance, it always comes with constraints related to textual semantics. In\ncontrast, viusal-only methods suffer from the low-quality fusion of multiple\nvisual prompts. In response, we introduce a strong DETR-based model, Visual\nIntersection Network for Open Set Object Detection (VINO), which constructs a\nmulti-image visual bank to preserve the semantic intersections of each category\nacross all time steps. Our innovative multi-image visual updating mechanism\nlearns to identify the semantic intersections from various visual prompts,\nenabling the flexible incorporation of new information and continuous\noptimization of feature representations. Our approach guarantees a more precise\nalignment between target category semantics and region semantics, while\nsignificantly reducing pre-training time and resource demands compared to\nlanguage-based methods. Furthermore, the integration of a segmentation head\nillustrates the broad applicability of visual intersection in various visual\ntasks. VINO, which requires only 7 RTX4090 GPU days to complete one epoch on\nthe Objects365v1 dataset, achieves competitive performance on par with\nvision-language models on benchmarks such as LVIS and ODinW35.\n","authors":["Bingcheng Dong","Yuning Ding","Jinrong Zhang","Sifan Zhang","Shenglan Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14032v1.pdf","comment":"7pages"},{"id":"http://arxiv.org/abs/2408.14028v1","updated":"2024-08-26T05:38:27Z","published":"2024-08-26T05:38:27Z","title":"SurGen: Text-Guided Diffusion Model for Surgical Video Generation","summary":"  Diffusion-based video generation models have made significant strides,\nproducing outputs with improved visual fidelity, temporal coherence, and user\ncontrol. These advancements hold great promise for improving surgical education\nby enabling more realistic, diverse, and interactive simulation environments.\nIn this study, we introduce SurGen, a text-guided diffusion model tailored for\nsurgical video synthesis, producing the highest resolution and longest duration\nvideos among existing surgical video generation models. We validate the visual\nand temporal quality of the outputs using standard image and video generation\nmetrics. Additionally, we assess their alignment to the corresponding text\nprompts through a deep learning classifier trained on surgical data. Our\nresults demonstrate the potential of diffusion models to serve as valuable\neducational tools for surgical trainees.\n","authors":["Joseph Cho","Samuel Schmidgall","Cyril Zakka","Mrudang Mathur","Rohan Shad","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2408.14028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14023v1","updated":"2024-08-26T05:27:14Z","published":"2024-08-26T05:27:14Z","title":"Video-CCAM: Enhancing Video-Language Understanding with Causal\n  Cross-Attention Masks for Short and Long Videos","summary":"  Multi-modal large language models (MLLMs) have demonstrated considerable\npotential across various downstream tasks that require cross-domain knowledge.\nMLLMs capable of processing videos, known as Video-MLLMs, have attracted broad\ninterest in video-language understanding. However, videos, especially long\nvideos, contain more visual tokens than images, making them difficult for LLMs\nto process. Existing works either downsample visual features or extend the LLM\ncontext size, risking the loss of high-resolution information or slowing down\ninference speed. To address these limitations, we apply cross-attention layers\nin the intermediate projector between the visual encoder and the large language\nmodel (LLM). As the naive cross-attention mechanism is insensitive to temporal\norder, we further introduce causal cross-attention masks (CCAMs) within the\ncross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a\nstraightforward two-stage fashion: feature alignment and visual instruction\ntuning. We develop several Video-CCAM models based on LLMs of different sizes\n(4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows\noutstanding performance from short videos to long ones. Among standard video\nbenchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding\nperformances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA,\nMSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos,\nVideo-CCAM models can be directly adapted to long video understanding and still\nachieve exceptional scores despite being trained solely with images and\n16-frame videos. Using 96 frames (6$\\times$ the training number of frames),\nVideo-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among\nall open-source Video-MLLMs, respectively. The code is publicly available in\n\\url{https://github.com/QQ-MM/Video-CCAM}.\n","authors":["Jiajun Fei","Dian Li","Zhidong Deng","Zekun Wang","Gang Liu","Hui Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14023v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.11545v2","updated":"2024-08-26T05:21:35Z","published":"2024-08-21T11:53:53Z","title":"UNetMamba: An Efficient UNet-Like Mamba for Semantic Segmentation of\n  High-Resolution Remote Sensing Images","summary":"  Semantic segmentation of high-resolution remote sensing images is vital in\ndownstream applications such as land-cover mapping, urban planning and disaster\nassessment.Existing Transformer-based methods suffer from the constraint\nbetween accuracy and efficiency, while the recently proposed Mamba is renowned\nfor being efficient. Therefore, to overcome the dilemma, we propose UNetMamba,\na UNet-like semantic segmentation model based on Mamba. It incorporates a mamba\nsegmentation decoder (MSD) that can efficiently decode the complex information\nwithin high-resolution images, and a local supervision module (LSM), which is\ntrain-only but can significantly enhance the perception of local contents.\nExtensive experiments demonstrate that UNetMamba outperforms the\nstate-of-the-art methods with mIoU increased by 0.87% on LoveDA and 0.36% on\nISPRS Vaihingen, while achieving high efficiency through the lightweight\ndesign, less memory footprint and reduced computational cost. The source code\nis available at https://github.com/EnzeZhu2001/UNetMamba.\n","authors":["Enze Zhu","Zhan Chen","Dingkai Wang","Hanru Shi","Xiaoxuan Liu","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.11545v2.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2405.14213v2","updated":"2024-08-26T04:59:05Z","published":"2024-05-23T06:17:23Z","title":"From Text to Pixel: Advancing Long-Context Understanding in MLLMs","summary":"  The rapid progress in Multimodal Large Language Models (MLLMs) has\nsignificantly advanced their ability to process and understand complex visual\nand textual information. However, the integration of multiple images and\nextensive textual contexts remains a challenge due to the inherent limitation\nof the models' capacity to handle long input sequences efficiently. In this\npaper, we introduce SEEKER, a multimodal large language model designed to\ntackle this issue. SEEKER aims to optimize the compact encoding of long text by\ncompressing the text sequence into the visual pixel space via images, enabling\nthe model to handle long text within a fixed token-length budget efficiently.\nOur empirical experiments on six long-context multimodal tasks demonstrate that\nSEEKER can leverage fewer image tokens to convey the same amount of textual\ninformation compared with the OCR-based approach, and is more efficient in\nunderstanding long-form multimodal input and generating long-form textual\noutput, outperforming all existing proprietary and open-source MLLMs by large\nmargins.\n","authors":["Yujie Lu","Xiujun Li","Tsu-Jui Fu","Miguel Eckstein","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2405.14213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14016v1","updated":"2024-08-26T04:56:41Z","published":"2024-08-26T04:56:41Z","title":"Pixel-Aligned Multi-View Generation with Depth Guided Decoder","summary":"  The task of image-to-multi-view generation refers to generating novel views\nof an instance from a single image. Recent methods achieve this by extending\ntext-to-image latent diffusion models to multi-view version, which contains an\nVAE image encoder and a U-Net diffusion model. Specifically, these generation\nmethods usually fix VAE and finetune the U-Net only. However, the significant\ndownscaling of the latent vectors computed from the input images and\nindependent decoding leads to notable pixel-level misalignment across multiple\nviews. To address this, we propose a novel method for pixel-level\nimage-to-multi-view generation. Unlike prior work, we incorporate attention\nlayers across multi-view images in the VAE decoder of a latent video diffusion\nmodel. Specifically, we introduce a depth-truncated epipolar attention,\nenabling the model to focus on spatially adjacent regions while remaining\nmemory efficient. Applying depth-truncated attn is challenging during inference\nas the ground-truth depth is usually difficult to obtain and pre-trained depth\nestimation models is hard to provide accurate depth. Thus, to enhance the\ngeneralization to inaccurate depth when ground truth depth is missing, we\nperturb depth inputs during training. During inference, we employ a rapid\nmulti-view to 3D reconstruction approach, NeuS, to obtain coarse depth for the\ndepth-truncated epipolar attention. Our model enables better pixel alignment\nacross multi-view images. Moreover, we demonstrate the efficacy of our approach\nin improving downstream multi-view to 3D reconstruction tasks.\n","authors":["Zhenggang Tang","Peiye Zhuang","Chaoyang Wang","Aliaksandr Siarohin","Yash Kant","Alexander Schwing","Sergey Tulyakov","Hsin-Ying Lee"],"pdf_url":"https://arxiv.org/pdf/2408.14016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14013v1","updated":"2024-08-26T04:36:10Z","published":"2024-08-26T04:36:10Z","title":"A Multiscale Gradient Fusion Method for Edge Detection in Color Images\n  Utilizing the CBM3D Filter","summary":"  In this paper, a color edge detection strategy based on collaborative\nfiltering combined with multiscale gradient fusion is proposed. The\nblock-matching and 3D (BM3D) filter are used to enhance the sparse\nrepresentation in the transform domain and achieve the effect of denoising,\nwhereas the multiscale gradient fusion makes up for the defect of loss of\ndetails in single-scale edge detection and improves the edge detection\nresolution and quality. First, the RGB images in the dataset are converted to\nXYZ color space images through mathematical operations. Second, the colored\nblock-matching and 3D (CBM3D) filter are used on the sparse images and to\nremove noise interference. Then, the vector gradients of the color image and\nthe anisotropic Gaussian directional derivative of the two scale parameters are\ncalculated and averaged pixel-by-pixel to obtain a new edge strength map.\nFinally, the edge features are enhanced by image normalization and non-maximum\nsuppression technology, and on that basis, the edge contour is obtained by\ndouble threshold selection and a new morphological refinement method. Through\nan experimental analysis of the edge detection dataset, the method proposed has\ngood noise robustness and high edge quality, which is better than the Color\nSobel, Color Canny, SE and Color AGDD as shown by the PR curve, AUC, PSNR, MSE,\nand FOM indicators.\n","authors":["Zhuoyue Wang","Yiyi Tao","Danqing Ma"],"pdf_url":"https://arxiv.org/pdf/2408.14013v1.pdf","comment":"1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2408.14008v1","updated":"2024-08-26T04:29:52Z","published":"2024-08-26T04:29:52Z","title":"LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models","summary":"  The explosive growth of videos on streaming media platforms has underscored\nthe urgent need for effective video quality assessment (VQA) algorithms to\nmonitor and perceptually optimize the quality of streaming videos. However, VQA\nremains an extremely challenging task due to the diverse video content and the\ncomplex spatial and temporal distortions, thus necessitating more advanced\nmethods to address these issues. Nowadays, large multimodal models (LMMs), such\nas GPT-4V, have exhibited strong capabilities for various visual understanding\ntasks, motivating us to leverage the powerful multimodal representation ability\nof LMMs to solve the VQA task. Therefore, we propose the first Large\nMulti-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel\nspatiotemporal visual modeling strategy for quality-aware feature extraction.\nSpecifically, we first reformulate the quality regression problem into a\nquestion and answering (Q&A) task and construct Q&A prompts for VQA instruction\ntuning. Then, we design a spatiotemporal vision encoder to extract spatial and\ntemporal features to represent the quality characteristics of videos, which are\nsubsequently mapped into the language space by the spatiotemporal projector for\nmodality alignment. Finally, the aligned visual tokens and the quality-inquired\ntext tokens are aggregated as inputs for the large language model (LLM) to\ngenerate the quality score and level. Extensive experiments demonstrate that\nLMM-VQA achieves state-of-the-art performance across five VQA benchmarks,\nexhibiting an average improvement of $5\\%$ in generalization ability over\nexisting methods. Furthermore, due to the advanced design of the spatiotemporal\nencoder and projector, LMM-VQA also performs exceptionally well on general\nvideo understanding tasks, further validating its effectiveness. Our code will\nbe released at https://github.com/Sueqk/LMM-VQA.\n","authors":["Qihang Ge","Wei Sun","Yu Zhang","Yunhao Li","Zhongpeng Ji","Fengyu Sun","Shangling Jui","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2408.14008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12321v2","updated":"2024-08-26T04:27:54Z","published":"2024-08-22T11:57:16Z","title":"MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework\n  for Multimodal Large Language Model","summary":"  This paper presents MaVEn, an innovative Multi-granularity Visual Encoding\nframework designed to enhance the capabilities of Multimodal Large Language\nModels (MLLMs) in multi-image reasoning. Current MLLMs primarily focus on\nsingle-image visual understanding, limiting their ability to interpret and\nintegrate information across multiple images. MaVEn addresses this limitation\nby combining discrete visual symbol sequences, which abstract coarse-grained\nsemantic concepts, with traditional continuous representation sequences that\nmodel fine-grained features. This dual approach bridges the semantic gap\nbetween visual and textual data, thereby improving the model's ability to\nprocess and interpret information from multiple images effectively.\nAdditionally, we design a dynamic reduction mechanism by for long-sequence\ncontinuous features to enhance multi-image processing efficiency. Experimental\nresults demonstrate that MaVEn significantly enhances MLLMs' understanding in\ncomplex multi-image scenarios, while also improving performance in single-image\ncontexts.\n","authors":["Chaoya Jiang","Jia Hongrui","Haiyang Xu","Wei Ye","Mengfan Dong","Ming Yan","Ji Zhang","Fei Huang","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.12321v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08549v2","updated":"2024-08-26T03:59:17Z","published":"2024-04-12T15:45:26Z","title":"Practical Guidelines for Cell Segmentation Models Under Optical\n  Aberrations in Microscopy","summary":"  Cell segmentation is essential in biomedical research for analyzing cellular\nmorphology and behavior. Deep learning methods, particularly convolutional\nneural networks (CNNs), have revolutionized cell segmentation by extracting\nintricate features from images. However, the robustness of these methods under\nmicroscope optical aberrations remains a critical challenge. This study\nevaluates cell image segmentation models under optical aberrations from\nfluorescence and bright field microscopy. By simulating different types of\naberrations, including astigmatism, coma, spherical aberration, trefoil, and\nmixed aberrations, we conduct a thorough evaluation of various cell instance\nsegmentation models using the DynamicNuclearNet (DNN) and LIVECell datasets,\nrepresenting fluorescence and bright field microscopy cell datasets,\nrespectively. We train and test several segmentation models, including the Otsu\nthreshold method and Mask R-CNN with different network heads (FPN, C3) and\nbackbones (ResNet, VGG, Swin Transformer), under aberrated conditions.\nAdditionally, we provide usage recommendations for the Cellpose 2.0 Toolbox on\ncomplex cell degradation images. The results indicate that the combination of\nFPN and SwinS demonstrates superior robustness in handling simple cell images\naffected by minor aberrations. In contrast, Cellpose 2.0 proves effective for\ncomplex cell images under similar conditions. Furthermore, we innovatively\npropose the Point Spread Function Image Label Classification Model (PLCM). This\nmodel can quickly and accurately identify aberration types and amplitudes from\nPSF images, assisting researchers without optical training. Through PLCM,\nresearchers can better apply our proposed cell segmentation guidelines.\n","authors":["Boyuan Peng","Jiaju Chen","P. Bilha Githinji","Ijaz Gul","Qihui Ye","Minjiang Chen","Peiwu Qin","Xingru Huang","Chenggang Yan","Dongmei Yu","Jiansong Ji","Zhenglin Chen"],"pdf_url":"https://arxiv.org/pdf/2404.08549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12616v2","updated":"2024-08-26T03:47:06Z","published":"2024-08-08T16:46:14Z","title":"Semantic Communication based on Large Language Model for Underwater\n  Image Transmission","summary":"  Underwater communication is essential for environmental monitoring, marine\nbiology research, and underwater exploration. Traditional underwater\ncommunication faces limitations like low bandwidth, high latency, and\nsusceptibility to noise, while semantic communication (SC) offers a promising\nsolution by focusing on the exchange of semantics rather than symbols or bits.\nHowever, SC encounters challenges in underwater environments, including\nsemantic information mismatch and difficulties in accurately identifying and\ntransmitting critical information that aligns with the diverse requirements of\nunderwater applications. To address these challenges, we propose a novel\nSemantic Communication (SC) framework based on Large Language Models (LLMs).\nOur framework leverages visual LLMs to perform semantic compression and\nprioritization of underwater image data according to the query from users. By\nidentifying and encoding key semantic elements within the images, the system\nselectively transmits high-priority information while applying higher\ncompression rates to less critical regions. On the receiver side, an LLM-based\nrecovery mechanism, along with Global Vision ControlNet and Key Region\nControlNet networks, aids in reconstructing the images, thereby enhancing\ncommunication efficiency and robustness. Our framework reduces the overall data\nsize to 0.8\\% of the original. Experimental results demonstrate that our method\nsignificantly outperforms existing approaches, ensuring high-quality,\nsemantically accurate image reconstruction.\n","authors":["Weilong Chen","Wenxuan Xu","Haoran Chen","Xinran Zhang","Zhijin Qin","Yanru Zhang","Zhu Han"],"pdf_url":"https://arxiv.org/pdf/2408.12616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13995v1","updated":"2024-08-26T03:35:13Z","published":"2024-08-26T03:35:13Z","title":"Avatar Concept Slider: Manipulate Concepts In Your Human Avatar With\n  Fine-grained Control","summary":"  Language based editing of 3D human avatars to precisely match user\nrequirements is challenging due to the inherent ambiguity and limited\nexpressiveness of natural language. To overcome this, we propose the Avatar\nConcept Slider (ACS), a 3D avatar editing method that allows precise\nmanipulation of semantic concepts in human avatars towards a specified\nintermediate point between two extremes of concepts, akin to moving a knob\nalong a slider track. To achieve this, our ACS has three designs. 1) A Concept\nSliding Loss based on Linear Discriminant Analysis to pinpoint the\nconcept-specific axis for precise editing. 2) An Attribute Preserving Loss\nbased on Principal Component Analysis for improved preservation of avatar\nidentity during editing. 3) A 3D Gaussian Splatting primitive selection\nmechanism based on concept-sensitivity, which updates only the primitives that\nare the most sensitive to our target concept, to improve efficiency. Results\ndemonstrate that our ACS enables fine-grained 3D avatar editing with efficient\nfeedback, without harming the avatar quality or compromising the avatar's\nidentifying attributes.\n","authors":["Yixuan He","Lin Geng Foo","Ajmal Saeed Mian","Hossein Rahmani","Jun Jiu"],"pdf_url":"https://arxiv.org/pdf/2408.13995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06886v7","updated":"2024-08-26T03:25:12Z","published":"2024-07-09T14:14:47Z","title":"Aligning Cyber Space with Physical World: A Comprehensive Survey on\n  Embodied AI","summary":"  Embodied Artificial Intelligence (Embodied AI) is crucial for achieving\nArtificial General Intelligence (AGI) and serves as a foundation for various\napplications that bridge cyberspace and the physical world. Recently, the\nemergence of Multi-modal Large Models (MLMs) and World Models (WMs) have\nattracted significant attention due to their remarkable perception,\ninteraction, and reasoning capabilities, making them a promising architecture\nfor the brain of embodied agents. However, there is no comprehensive survey for\nEmbodied AI in the era of MLMs. In this survey, we give a comprehensive\nexploration of the latest advancements in Embodied AI. Our analysis firstly\nnavigates through the forefront of representative works of embodied robots and\nsimulators, to fully understand the research focuses and their limitations.\nThen, we analyze four main research targets: 1) embodied perception, 2)\nembodied interaction, 3) embodied agent, and 4) sim-to-real adaptation,\ncovering the state-of-the-art methods, essential paradigms, and comprehensive\ndatasets. Additionally, we explore the complexities of MLMs in virtual and real\nembodied agents, highlighting their significance in facilitating interactions\nin dynamic digital and physical environments. Finally, we summarize the\nchallenges and limitations of embodied AI and discuss their potential future\ndirections. We hope this survey will serve as a foundational reference for the\nresearch community and inspire continued innovation. The associated project can\nbe found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.\n","authors":["Yang Liu","Weixing Chen","Yongjie Bai","Xiaodan Liang","Guanbin Li","Wen Gao","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.06886v7.pdf","comment":"The first comprehensive review of Embodied AI in the era of MLMs, 39\n  pages. We also provide the paper list for Embodied AI:\n  https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List"},{"id":"http://arxiv.org/abs/2303.17117v2","updated":"2024-08-26T03:22:08Z","published":"2023-03-30T03:09:25Z","title":"Reliable Representations Learning for Incomplete Multi-View Partial\n  Multi-Label Classification","summary":"  As a cross-topic of multi-view learning and multi-label classification,\nmulti-view multi-label classification has gradually gained traction in recent\nyears. The application of multi-view contrastive learning has further\nfacilitated this process, however, the existing multi-view contrastive learning\nmethods crudely separate the so-called negative pair, which largely results in\nthe separation of samples belonging to the same category or similar ones.\nBesides, plenty of multi-view multi-label learning methods ignore the possible\nabsence of views and labels. To address these issues, in this paper, we propose\nan incomplete multi-view partial multi-label classification network named RANK.\nIn this network, a label-driven multi-view contrastive learning strategy is\nproposed to leverage supervised information to preserve the structure within\nview and perform consistent alignment across views. Furthermore, we break\nthrough the view-level weights inherent in existing methods and propose a\nquality-aware sub-network to dynamically assign quality scores to each view of\neach sample. The label correlation information is fully utilized in the final\nmulti-label cross-entropy classification loss, effectively improving the\ndiscriminative power. Last but not least, our model is not only able to handle\ncomplete multi-view multi-label datasets, but also works on datasets with\nmissing instances and labels. Extensive experiments confirm that our RANK\noutperforms existing state-of-the-art methods.\n","authors":["Chengliang Liu","Jie Wen","Yong Xu","Bob Zhang","Liqiang Nie","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.17117v2.pdf","comment":"Please contact me if you have any questions: liucl1996@163.com"},{"id":"http://arxiv.org/abs/2408.10848v2","updated":"2024-08-26T03:19:45Z","published":"2024-08-20T13:40:25Z","title":"Perception-guided Jailbreak against Text-to-Image Models","summary":"  In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.\n","authors":["Yihao Huang","Le Liang","Tianlin Li","Xiaojun Jia","Run Wang","Weikai Miao","Geguang Pu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.10848v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2408.13988v1","updated":"2024-08-26T03:02:41Z","published":"2024-08-26T03:02:41Z","title":"Automatic Medical Report Generation: Methods and Applications","summary":"  The increasing demand for medical imaging has surpassed the capacity of\navailable radiologists, leading to diagnostic delays and potential\nmisdiagnoses. Artificial intelligence (AI) techniques, particularly in\nautomatic medical report generation (AMRG), offer a promising solution to this\ndilemma. This review comprehensively examines AMRG methods from 2021 to 2024.\nIt (i) presents solutions to primary challenges in this field, (ii) explores\nAMRG applications across various imaging modalities, (iii) introduces publicly\navailable datasets, (iv) outlines evaluation metrics, (v) identifies techniques\nthat significantly enhance model performance, and (vi) discusses unresolved\nissues and potential future research directions. This paper aims to provide a\ncomprehensive understanding of the existing literature and inspire valuable\nfuture research.\n","authors":["Li Guo","Anas M. Tahir","Dong Zhang","Z. Jane Wang","Rabab K. Ward"],"pdf_url":"https://arxiv.org/pdf/2408.13988v1.pdf","comment":"42 pages and 9 figures"},{"id":"http://arxiv.org/abs/2401.12452v2","updated":"2024-08-26T02:50:28Z","published":"2024-01-23T02:41:06Z","title":"Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural\n  Calibration","summary":"  This paper introduces a novel self-supervised learning framework for\nenhancing 3D perception in autonomous driving scenes. Specifically, our\napproach, namely NCLR, focuses on 2D-3D neural calibration, a novel pretext\ntask that estimates the rigid pose aligning camera and LiDAR coordinate\nsystems. First, we propose the learnable transformation alignment to bridge the\ndomain gap between image and point cloud data, converting features into a\nunified representation space for effective comparison and matching. Second, we\nidentify the overlapping area between the image and point cloud with the fused\nfeatures. Third, we establish dense 2D-3D correspondences to estimate the rigid\npose. The framework not only learns fine-grained matching from points to pixels\nbut also achieves alignment of the image and point cloud at a holistic level,\nunderstanding their relative pose. We demonstrate the efficacy of NCLR by\napplying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D\nsemantic segmentation, object detection, and panoptic segmentation.\nComprehensive experiments on various datasets illustrate the superiority of\nNCLR over existing self-supervised methods. The results confirm that joint\nlearning from different modalities significantly enhances the network's\nunderstanding abilities and effectiveness of learned representation. The code\nis publicly available at https://github.com/Eaphan/NCLR.\n","authors":["Yifan Zhang","Siyu Ren","Junhui Hou","Jinjian Wu","Yixuan Yuan","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2401.12452v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2305.07895v7","updated":"2024-08-26T02:37:14Z","published":"2023-05-13T11:28:37Z","title":"OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models","summary":"  Large models have recently played a dominant role in natural language\nprocessing and multimodal vision-language learning. However, their\neffectiveness in text-related visual tasks remains relatively unexplored. In\nthis paper, we conducted a comprehensive evaluation of Large Multimodal Models,\nsuch as GPT4V and Gemini, in various text-related visual tasks including Text\nRecognition, Scene Text-Centric Visual Question Answering (VQA),\nDocument-Oriented VQA, Key Information Extraction (KIE), and Handwritten\nMathematical Expression Recognition (HMER). To facilitate the assessment of\nOptical Character Recognition (OCR) capabilities in Large Multimodal Models, we\npropose OCRBench, a comprehensive evaluation benchmark. OCRBench contains 29\ndatasets, making it the most comprehensive OCR evaluation benchmark available.\nFurthermore, our study reveals both the strengths and weaknesses of these\nmodels, particularly in handling multilingual text, handwritten text,\nnon-semantic text, and mathematical expression recognition. Most importantly,\nthe baseline results presented in this study could provide a foundational\nframework for the conception and assessment of innovative strategies targeted\nat enhancing zero-shot multimodal techniques. The evaluation pipeline and\nbenchmark are available at https://github.com/Yuliang-Liu/MultimodalOCR.\n","authors":["Yuliang Liu","Zhang Li","Mingxin Huang","Biao Yang","Wenwen Yu","Chunyuan Li","Xucheng Yin","Cheng-lin Liu","Lianwen Jin","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2305.07895v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13983v1","updated":"2024-08-26T02:33:47Z","published":"2024-08-26T02:33:47Z","title":"Dual-Path Adversarial Lifting for Domain Shift Correction in Online\n  Test-time Adaptation","summary":"  Transformer-based methods have achieved remarkable success in various machine\nlearning tasks. How to design efficient test-time adaptation methods for\ntransformer models becomes an important research task. In this work, motivated\nby the dual-subband wavelet lifting scheme developed in multi-scale signal\nprocessing which is able to efficiently separate the input signals into\nprincipal components and noise components, we introduce a dual-path token\nlifting for domain shift correction in test time adaptation. Specifically, we\nintroduce an extra token, referred to as \\textit{domain shift token}, at each\nlayer of the transformer network. We then perform dual-path lifting with\ninterleaved token prediction and update between the path of domain shift tokens\nand the path of class tokens at all network layers. The prediction and update\nnetworks are learned in an adversarial manner. Specifically, the task of the\nprediction network is to learn the residual noise of domain shift which should\nbe largely invariant across all classes and all samples in the target domain.\nIn other words, the predicted domain shift noise should be indistinguishable\nbetween all sample classes. On the other hand, the task of the update network\nis to update the class tokens by removing the domain shift from the input image\nsamples so that input samples become more discriminative between different\nclasses in the feature space. To effectively learn the prediction and update\nnetworks with two adversarial tasks, both theoretically and practically, we\ndemonstrate that it is necessary to use smooth optimization for the update\nnetwork but non-smooth optimization for the prediction network. Experimental\nresults on the benchmark datasets demonstrate that our proposed method\nsignificantly improves the online fully test-time domain adaptation\nperformance. Code is available at \\url{https://github.com/yushuntang/DPAL}.\n","authors":["Yushun Tang","Shuoshuo Chen","Zhihe Lu","Xinchao Wang","Zhihai He"],"pdf_url":"https://arxiv.org/pdf/2408.13983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13981v1","updated":"2024-08-26T02:26:09Z","published":"2024-08-26T02:26:09Z","title":"ARANet: Attention-based Residual Adversarial Network with Deep\n  Supervision for Radiotherapy Dose Prediction of Cervical Cancer","summary":"  Radiation therapy is the mainstay treatment for cervical cancer, and its\nultimate goal is to ensure the planning target volume (PTV) reaches the\nprescribed dose while reducing dose deposition of organs-at-risk (OARs) as much\nas possible. To achieve these clinical requirements, the medical physicist\nneeds to manually tweak the radiotherapy plan repeatedly in a trial-anderror\nmanner until finding the optimal one in the clinic. However, such\ntrial-and-error processes are quite time-consuming, and the quality of plans\nhighly depends on the experience of the medical physicist. In this paper, we\npropose an end-to-end Attentionbased Residual Adversarial Network with deep\nsupervision, namely ARANet, to automatically predict the 3D dose distribution\nof cervical cancer. Specifically, given the computer tomography (CT) images and\ntheir corresponding segmentation masks of PTV and OARs, ARANet employs a\nprediction network to generate the dose maps. We also utilize a multi-scale\nresidual attention module and deep supervision mechanism to enforce the\nprediction network to extract more valuable dose features while suppressing\nirrelevant information. Our proposed method is validated on an in-house dataset\nincluding 54 cervical cancer patients, and experimental results have\ndemonstrated its obvious superiority compared to other state-of-the-art\nmethods.\n","authors":["Lu Wen","Wenxia Yin","Zhenghao Feng","Xi Wu","Deng Xiong","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.13981v1.pdf","comment":"Accepted by 2024 IEEE International Conference on Cybernetics and\n  Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and\n  Mechatronics (RAM)"},{"id":"http://arxiv.org/abs/2408.13980v1","updated":"2024-08-26T02:20:55Z","published":"2024-08-26T02:20:55Z","title":"FusionSAM: Latent Space driven Segment Anything Model for Multimodal\n  Fusion and Segmentation","summary":"  Multimodal image fusion and segmentation enhance scene understanding in\nautonomous driving by integrating data from various sensors. However, current\nmodels struggle to efficiently segment densely packed elements in such scenes,\ndue to the absence of comprehensive fusion features that can guide mid-process\nfine-tuning and focus attention on relevant areas. The Segment Anything Model\n(SAM) has emerged as a transformative segmentation method. It provides more\neffective prompts through its flexible prompt encoder, compared to transformers\nlacking fine-tuned control. Nevertheless, SAM has not been extensively studied\nin the domain of multimodal fusion for natural images. In this paper, we\nintroduce SAM into multimodal image segmentation for the first time, proposing\na novel framework that combines Latent Space Token Generation (LSTG) and Fusion\nMask Prompting (FMP) modules to enhance SAM's multimodal fusion and\nsegmentation capabilities. Specifically, we first obtain latent space features\nof the two modalities through vector quantization and embed them into a\ncross-attention-based inter-domain fusion module to establish long-range\ndependencies between modalities. Then, we use these comprehensive fusion\nfeatures as prompts to guide precise pixel-level segmentation. Extensive\nexperiments on several public datasets demonstrate that the proposed method\nsignificantly outperforms SAM and SAM2 in multimodal autonomous driving\nscenarios, achieving at least 3.9$\\%$ higher segmentation mIoU than the\nstate-of-the-art approaches.\n","authors":["Daixun Li","Weiying Xie","Mingxiang Cao","Yunke Wang","Jiaqing Zhang","Yunsong Li","Leyuan Fang","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2408.13980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10650v2","updated":"2024-08-26T02:19:11Z","published":"2024-03-15T19:35:10Z","title":"PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time\n  Adaptation","summary":"  Real-world vision models in dynamic environments face rapid shifts in domain\ndistributions, leading to decreased recognition performance. Using unlabeled\ntest data, continual test-time adaptation (CTTA) directly adjusts a pre-trained\nsource discriminative model to these changing domains. A highly effective CTTA\nmethod involves applying layer-wise adaptive learning rates for selectively\nadapting pre-trained layers. However, it suffers from the poor estimation of\ndomain shift and the inaccuracies arising from the pseudo-labels. This work\naims to overcome these limitations by identifying layers for adaptation via\nquantifying model prediction uncertainty without relying on pseudo-labels. We\nutilize the magnitude of gradients as a metric, calculated by backpropagating\nthe KL divergence between the softmax output and a uniform distribution, to\nselect layers for further adaptation. Subsequently, for the parameters\nexclusively belonging to these selected layers, with the remaining ones frozen,\nwe evaluate their sensitivity to approximate the domain shift and adjust their\nlearning rates accordingly. We conduct extensive image classification\nexperiments on CIFAR-10C, CIFAR-100C, and ImageNet-C, demonstrating the\nsuperior efficacy of our method compared to prior approaches.\n","authors":["Sarthak Kumar Maharana","Baoming Zhang","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2403.10650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04560v3","updated":"2024-08-26T02:11:58Z","published":"2024-01-09T13:56:37Z","title":"Phase-shifted remote photoplethysmography for estimating heart rate and\n  blood pressure from facial video","summary":"  Human health can be critically affected by cardiovascular diseases, such as\nhypertension, arrhythmias, and stroke. Heart rate and blood pressure are\nimportant biometric information for the monitoring of cardiovascular system and\nearly diagnosis of cardiovascular diseases. Existing methods for estimating the\nheart rate are based on electrocardiography and photoplethyomography, which\nrequire contacting the sensor to the skin surface. Moreover, catheter and\ncuff-based methods for measuring blood pressure cause inconvenience and have\nlimited applicability. Therefore, in this thesis, we propose a vision-based\nmethod for estimating the heart rate and blood pressure. This thesis proposes a\n2-stage deep learning framework consisting of a dual remote\nphotoplethysmography network (DRP-Net) and bounded blood pressure network\n(BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography\n(rPPG) signals for the acral and facial regions, and these phase-shifted rPPG\nsignals are utilized to estimate the heart rate. In the second stage, BBP-Net\nintegrates temporal features and analyzes phase discrepancy between the acral\nand facial rPPG signals to estimate SBP and DBP values. To improve the accuracy\nof estimating the heart rate, we employed a data augmentation method based on a\nframe interpolation model. Moreover, we designed BBP-Net to infer blood\npressure within a predefined range by incorporating a scaled sigmoid function.\nOur method resulted in estimating the heart rate with the mean absolute error\n(MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method,\non the MMSE-HR dataset. The MAE for estimating the systolic blood pressure\n(SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the\nV4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64\nmmHg, and 9.4 mmHg, respectively.\n","authors":["Gyutae Hwang","Sang Jun Lee"],"pdf_url":"https://arxiv.org/pdf/2401.04560v3.pdf","comment":"33 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.13979v1","updated":"2024-08-26T02:09:05Z","published":"2024-08-26T02:09:05Z","title":"Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models","summary":"  With the prevalence of large-scale pretrained vision-language models (VLMs),\nsuch as CLIP, soft-prompt tuning has become a popular method for adapting these\nmodels to various downstream tasks. However, few works delve into the inherent\nproperties of learnable soft-prompt vectors, specifically the impact of their\nnorms to the performance of VLMs. This motivates us to pose an unexplored\nresearch question: ``Do we need to normalize the soft prompts in VLMs?'' To\nfill this research gap, we first uncover a phenomenon, called the\n\\textbf{Low-Norm Effect} by performing extensive corruption experiments,\nsuggesting that reducing the norms of certain learned prompts occasionally\nenhances the performance of VLMs, while increasing them often degrades it. To\nharness this effect, we propose a novel method named \\textbf{N}ormalizing\nth\\textbf{e} soft-pro\\textbf{m}pt v\\textbf{e}ctors of vi\\textbf{si}on-language\nmodel\\textbf{s} (\\textbf{Nemesis}) to normalize soft-prompt vectors in VLMs. To\nthe best of our knowledge, our work is the first to systematically investigate\nthe role of norms of soft-prompt vector in VLMs, offering valuable insights for\nfuture research in soft-prompt tuning. The code is available at\n\\texttt{\\href{https://github.com/ShyFoo/Nemesis}{https://github.com/ShyFoo/Nemesis}}.\n","authors":["Shuai Fu","Xiequn Wang","Qiushi Huang","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.13979v1.pdf","comment":"Accepted at ICLR 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2408.13978v1","updated":"2024-08-26T01:54:37Z","published":"2024-08-26T01:54:37Z","title":"Histology Virtual Staining with Mask-Guided Adversarial Transfer\n  Learning for Tertiary Lymphoid Structure Detection","summary":"  Histological Tertiary Lymphoid Structures (TLSs) are increasingly recognized\nfor their correlation with the efficacy of immunotherapy in various solid\ntumors. Traditionally, the identification and characterization of TLSs rely on\nimmunohistochemistry (IHC) staining techniques, utilizing markers such as CD20\nfor B cells. Despite the specificity of IHC, Hematoxylin-Eosin (H&E) staining\noffers a more accessible and cost-effective choice. Capitalizing on the\nprevalence of H&E staining slides, we introduce a novel Mask-Guided Adversarial\nTransfer Learning method designed for virtual pathological staining. This\nmethod adeptly captures the nuanced color variations across diverse tissue\ntypes under various staining conditions, such as nucleus, red blood cells,\npositive reaction regions, without explicit label information, and adeptly\nsynthesizes realistic IHC-like virtual staining patches, even replicating the\npositive reaction. Further, we propose the Virtual IHC Pathology Analysis\nNetwork (VIPA-Net), an integrated framework encompassing a Mask-Guided Transfer\nModule and an H&E-Based Virtual Staining TLS Detection Module. VIPA-Net\nsynergistically harnesses both H\\&E staining slides and the synthesized virtual\nIHC patches to enhance the detection of TLSs within H&E Whole Slide Images\n(WSIs). We evaluate the network with a comprehensive dataset comprising 1019\nannotated slides from The Cancer Genome Atlas (TCGA). Experimental results\ncompellingly illustrate that the VIPA-Net substantially elevates TLS detection\naccuracy, effectively circumventing the need for actual CD20 staining across\nthe public dataset.\n","authors":["Qiuli Wang","Yongxu Liu","Li Ma","Xianqi Wang","Wei Chen","Xiaohong Yao"],"pdf_url":"https://arxiv.org/pdf/2408.13978v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.13972v1","updated":"2024-08-26T01:36:46Z","published":"2024-08-26T01:36:46Z","title":"DynaSurfGS: Dynamic Surface Reconstruction with Planar-based Gaussian\n  Splatting","summary":"  Dynamic scene reconstruction has garnered significant attention in recent\nyears due to its capabilities in high-quality and real-time rendering. Among\nvarious methodologies, constructing a 4D spatial-temporal representation, such\nas 4D-GS, has gained popularity for its high-quality rendered images. However,\nthese methods often produce suboptimal surfaces, as the discrete 3D Gaussian\npoint clouds fail to align with the object's surface precisely. To address this\nproblem, we propose DynaSurfGS to achieve both photorealistic rendering and\nhigh-fidelity surface reconstruction of dynamic scenarios. Specifically, the\nDynaSurfGS framework first incorporates Gaussian features from 4D neural voxels\nwith the planar-based Gaussian Splatting to facilitate precise surface\nreconstruction. It leverages normal regularization to enforce the smoothness of\nthe surface of dynamic objects. It also incorporates the as-rigid-as-possible\n(ARAP) constraint to maintain the approximate rigidity of local neighborhoods\nof 3D Gaussians between timesteps and ensure that adjacent 3D Gaussians remain\nclosely aligned throughout. Extensive experiments demonstrate that DynaSurfGS\nsurpasses state-of-the-art methods in both high-fidelity surface reconstruction\nand photorealistic rendering.\n","authors":["Weiwei Cai","Weicai Ye","Peng Ye","Tong He","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2408.13972v1.pdf","comment":"homepage: https://open3dvlab.github.io/DynaSurfGS/, code:\n  https://github.com/Open3DVLab/DynaSurfGS"},{"id":"http://arxiv.org/abs/2408.09403v2","updated":"2024-08-26T01:26:35Z","published":"2024-08-18T08:23:51Z","title":"Obtaining Optimal Spiking Neural Network in Sequence Learning via\n  CRNN-SNN Conversion","summary":"  Spiking neural networks (SNNs) are becoming a promising alternative to\nconventional artificial neural networks (ANNs) due to their rich neural\ndynamics and the implementation of energy-efficient neuromorphic chips.\nHowever, the non-differential binary communication mechanism makes SNN hard to\nconverge to an ANN-level accuracy. When SNN encounters sequence learning, the\nsituation becomes worse due to the difficulties in modeling long-range\ndependencies. To overcome these difficulties, researchers developed variants of\nLIF neurons and different surrogate gradients but still failed to obtain good\nresults when the sequence became longer (e.g., $>$500). Unlike them, we obtain\nan optimal SNN in sequence learning by directly mapping parameters from a\nquantized CRNN. We design two sub-pipelines to support the end-to-end\nconversion of different structures in neural networks, which is called\nCNN-Morph (CNN $\\rightarrow$ QCNN $\\rightarrow$ BIFSNN) and RNN-Morph (RNN\n$\\rightarrow$ QRNN $\\rightarrow$ RBIFSNN). Using conversion pipelines and the\ns-analog encoding method, the conversion error of our framework is zero.\nFurthermore, we give the theoretical and experimental demonstration of the\nlossless CRNN-SNN conversion. Our results show the effectiveness of our method\nover short and long timescales tasks compared with the state-of-the-art\nlearning- and conversion-based methods. We reach the highest accuracy of 99.16%\n(0.46 $\\uparrow$) on S-MNIST, 94.95% (3.95 $\\uparrow$) on PS-MNIST (sequence\nlength of 784) respectively, and the lowest loss of 0.057 (0.013 $\\downarrow$)\nwithin 8 time-steps in collision avoidance dataset.\n","authors":["Jiahao Su","Kang You","Zekai Xu","Weizhi Xu","Zhezhi He"],"pdf_url":"https://arxiv.org/pdf/2408.09403v2.pdf","comment":"Accepted by 33rd International Conference on Artificial Neural\n  Networks"},{"id":"http://arxiv.org/abs/2408.12677v2","updated":"2024-08-26T01:08:36Z","published":"2024-08-22T18:32:50Z","title":"GSFusion: Online RGB-D Mapping Where Gaussian Splatting Meets TSDF\n  Fusion","summary":"  Traditional volumetric fusion algorithms preserve the spatial structure of 3D\nscenes, which is beneficial for many tasks in computer vision and robotics.\nHowever, they often lack realism in terms of visualization. Emerging 3D\nGaussian splatting bridges this gap, but existing Gaussian-based reconstruction\nmethods often suffer from artifacts and inconsistencies with the underlying 3D\nstructure, and struggle with real-time optimization, unable to provide users\nwith immediate feedback in high quality. One of the bottlenecks arises from the\nmassive amount of Gaussian parameters that need to be updated during\noptimization. Instead of using 3D Gaussian as a standalone map representation,\nwe incorporate it into a volumetric mapping system to take advantage of\ngeometric information and propose to use a quadtree data structure on images to\ndrastically reduce the number of splats initialized. In this way, we\nsimultaneously generate a compact 3D Gaussian map with fewer artifacts and a\nvolumetric map on the fly. Our method, GSFusion, significantly enhances\ncomputational efficiency without sacrificing rendering quality, as demonstrated\non both synthetic and real datasets. Code will be available at\nhttps://github.com/goldoak/GSFusion.\n","authors":["Jiaxin Wei","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2408.12677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14698v1","updated":"2024-08-26T23:52:27Z","published":"2024-08-26T23:52:27Z","title":"Smart Multi-Modal Search: Contextual Sparse and Dense Embedding\n  Integration in Adobe Express","summary":"  As user content and queries become increasingly multi-modal, the need for\neffective multi-modal search systems has grown. Traditional search systems\noften rely on textual and metadata annotations for indexed images, while\nmulti-modal embeddings like CLIP enable direct search using text and image\nembeddings. However, embedding-based approaches face challenges in integrating\ncontextual features such as user locale and recency. Building a scalable\nmulti-modal search system requires fine-tuning several components. This paper\npresents a multi-modal search architecture and a series of AB tests that\noptimize embeddings and multi-modal technologies in Adobe Express template\nsearch. We address considerations such as embedding model selection, the roles\nof embeddings in matching and ranking, and the balance between dense and sparse\nembeddings. Our iterative approach demonstrates how utilizing sparse, dense,\nand contextual features enhances short and long query search, significantly\nreduces null rates (over 70\\%), and increases click-through rates (CTR). Our\nfindings provide insights into developing robust multi-modal search systems,\nthereby enhancing relevance for complex queries.\n","authors":["Cherag Aroraa","Tracy Holloway King","Jayant Kumar","Yi Lu","Sanat Sharma","Arvind Srikantan","David Uvalle","Josep Valls-Vargas","Harsha Vardhan"],"pdf_url":"https://arxiv.org/pdf/2408.14698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14681v1","updated":"2024-08-26T23:10:42Z","published":"2024-08-26T23:10:42Z","title":"Enhancing Neural Network Interpretability Through Conductance-Based\n  Information Plane Analysis","summary":"  The Information Plane is a conceptual framework used to analyze the flow of\ninformation in neural networks, but traditional methods based on activations\nmay not fully capture the dynamics of information processing. This paper\nintroduces a new approach that uses layer conductance, a measure of sensitivity\nto input features, to enhance the Information Plane analysis. By incorporating\ngradient-based contributions, we provide a more precise characterization of\ninformation dynamics within the network. The proposed conductance-based\nInformation Plane and a new Information Transformation Efficiency (ITE) metric\nare evaluated on pretrained ResNet50 and VGG16 models using the ImageNet\ndataset. Our results demonstrate the ability to identify critical hidden layers\nthat contribute significantly to model performance and interpretability, giving\ninsights into information compression, preservation, and utilization across\nlayers. The conductance-based approach offers a granular perspective on feature\nattribution, enhancing our understanding of the decision-making processes\nwithin neural networks. Furthermore, our empirical findings challenge certain\ntheoretical predictions of the Information Bottleneck theory, highlighting the\ncomplexities of information dynamics in real-world data scenarios. The proposed\nmethod not only advances our understanding of information dynamics in neural\nnetworks but also has the potential to significantly impact the broader field\nof Artificial Intelligence by enabling the development of more interpretable,\nefficient, and robust models.\n","authors":["Jaouad Dabounou","Amine Baazzouz"],"pdf_url":"https://arxiv.org/pdf/2408.14681v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.02534v2","updated":"2024-08-26T22:56:28Z","published":"2024-07-01T16:58:55Z","title":"Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything","summary":"  Large Visual Language Model\\textbfs (VLMs) such as GPT-4V have achieved\nremarkable success in generating comprehensive and nuanced responses.\nResearchers have proposed various benchmarks for evaluating the capabilities of\nVLMs. With the integration of visual and text inputs in VLMs, new security\nissues emerge, as malicious attackers can exploit multiple modalities to\nachieve their objectives. This has led to increasing attention on the\nvulnerabilities of VLMs to jailbreak. Most existing research focuses on\ngenerating adversarial images or nonsensical image to jailbreak these models.\nHowever, no researchers evaluate whether logic understanding capabilities of\nVLMs in flowchart can influence jailbreak. Therefore, to fill this gap, this\npaper first introduces a novel dataset Flow-JD specifically designed to\nevaluate the logic-based flowchart jailbreak capabilities of VLMs. We conduct\nan extensive evaluation on GPT-4o, GPT-4V, other 5 SOTA open source VLMs and\nthe jailbreak rate is up to 92.8%. Our research reveals significant\nvulnerabilities in current VLMs concerning image-to-text jailbreak and these\nfindings underscore the the urgency for the development of robust and effective\nfuture defenses.\n","authors":["Xiaotian Zou","Ke Li","Yongkang Chen"],"pdf_url":"https://arxiv.org/pdf/2407.02534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14674v1","updated":"2024-08-26T22:50:59Z","published":"2024-08-26T22:50:59Z","title":"gWaveNet: Classification of Gravity Waves from Noisy Satellite Data\n  using Custom Kernel Integrated Deep Learning Method","summary":"  Atmospheric gravity waves occur in the Earths atmosphere caused by an\ninterplay between gravity and buoyancy forces. These waves have profound\nimpacts on various aspects of the atmosphere, including the patterns of\nprecipitation, cloud formation, ozone distribution, aerosols, and pollutant\ndispersion. Therefore, understanding gravity waves is essential to comprehend\nand monitor changes in a wide range of atmospheric behaviors. Limited studies\nhave been conducted to identify gravity waves from satellite data using machine\nlearning techniques. Particularly, without applying noise removal techniques,\nit remains an underexplored area of research. This study presents a novel\nkernel design aimed at identifying gravity waves within satellite images. The\nproposed kernel is seamlessly integrated into a deep convolutional neural\nnetwork, denoted as gWaveNet. Our proposed model exhibits impressive\nproficiency in detecting images containing gravity waves from noisy satellite\ndata without any feature engineering. The empirical results show our model\noutperforms related approaches by achieving over 98% training accuracy and over\n94% test accuracy which is known to be the best result for gravity waves\ndetection up to the time of this work. We open sourced our code at\nhttps://rb.gy/qn68ku.\n","authors":["Seraj Al Mahmud Mostafa","Omar Faruque","Chenxi Wang","Jia Yue","Sanjay Purushotham","Jianwu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14674v1.pdf","comment":"This paper has been accepted at the 27th International Conference on\n  Pattern Recognition (ICPR) 2024"},{"id":"http://arxiv.org/abs/2408.14672v1","updated":"2024-08-26T22:39:08Z","published":"2024-08-26T22:39:08Z","title":"Physically Feasible Semantic Segmentation","summary":"  State-of-the-art semantic segmentation models are typically optimized in a\ndata-driven fashion, minimizing solely per-pixel classification objectives on\ntheir training data. This purely data-driven paradigm often leads to absurd\nsegmentations, especially when the domain of input images is shifted from the\none encountered during training. For instance, state-of-the-art models may\nassign the label ``road'' to a segment which is located above a segment that is\nrespectively labeled as ``sky'', although our knowledge of the physical world\ndictates that such a configuration is not feasible for images captured by\nforward-facing upright cameras. Our method, Physically Feasible Semantic\nSegmentation (PhyFea), extracts explicit physical constraints that govern\nspatial class relations from the training sets of semantic segmentation\ndatasets and enforces a differentiable loss function that penalizes violations\nof these constraints to promote prediction feasibility. PhyFea yields\nsignificant performance improvements in mIoU over each state-of-the-art network\nwe use as baseline across ADE20K, Cityscapes and ACDC, notably a $1.5\\%$\nimprovement on ADE20K and a $2.1\\%$ improvement on ACDC.\n","authors":["Shamik Basu","Christos Sakaridis","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2408.14672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14659v1","updated":"2024-08-26T21:55:06Z","published":"2024-08-26T21:55:06Z","title":"Comparative Analysis: Violence Recognition from Videos using Transfer\n  Learning","summary":"  Action recognition has become a hot topic in computer vision. However, the\nmain applications of computer vision in video processing have focused on\ndetection of relatively simple actions while complex events such as violence\ndetection have been comparatively less investigated. This study focuses on the\nbenchmarking of various deep learning techniques on a complex dataset. Next, a\nlarger dataset is utilized to test the uplift from increasing volume of data.\nThe dataset size increase from 500 to 1,600 videos resulted in a notable\naverage accuracy improvement of 6% across four models.\n","authors":["Dursun Dashdamirov"],"pdf_url":"https://arxiv.org/pdf/2408.14659v1.pdf","comment":"6 pages, 5 figures, The paper will be published in IEEE AICT 2024\n  Conference"},{"id":"http://arxiv.org/abs/2308.13651v5","updated":"2024-08-26T21:11:26Z","published":"2023-08-25T19:40:56Z","title":"PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained\n  Image Classification Accuracy for AIs and Humans","summary":"  Nearest neighbors (NN) are traditionally used to compute final decisions,\ne.g., in Support Vector Machines or k-NN classifiers, and to provide users with\nexplanations for the model's decision. In this paper, we show a novel utility\nof nearest neighbors: To improve predictions of a frozen, pretrained image\nclassifier C. We leverage an image comparator S that (1) compares the input\nimage with NN images from the top-K most probable classes given by C; and (2)\nuses scores from S to weight the confidence scores of C to refine predictions.\nOur method consistently improves fine-grained image classification accuracy on\nCUB-200, Cars-196, and Dogs-120. Also, a human study finds that showing users\nour probable-class nearest neighbors (PCNN) reduces over-reliance on AI, thus\nimproving their decision accuracy over prior work which only shows only the\nmost-probable (top-1) class examples.\n","authors":[" Giang"," Nguyen","Valerie Chen","Mohammad Reza Taesiri","Anh Totti Nguyen"],"pdf_url":"https://arxiv.org/pdf/2308.13651v5.pdf","comment":"Accepted to Transaction on Machine Learning Research 2024; 50 pages,\n  35 Figures & 17 Tables"},{"id":"http://arxiv.org/abs/2408.11836v3","updated":"2024-08-26T20:31:08Z","published":"2024-08-06T22:09:50Z","title":"Analysis of Unstructured High-Density Crowded Scenes for Crowd\n  Monitoring","summary":"  We are interested in developing an automated system for detection of\norganized movements in human crowds. Computer vision algorithms can extract\ninformation from videos of crowded scenes and automatically detect and track\ngroups of individuals undergoing organized motion that represents an anomalous\nbehavior in the context of conflict aversion. Our system can detect organized\ncohorts against the background of randomly moving objects and we can estimate\nthe number of participants in an organized cohort, the speed and direction of\nmotion in real time, within three to four video frames, which is less than one\nsecond from the onset of motion captured on a CCTV. We have performed\npreliminary analysis in this context in biological cell data containing up to\nfour thousand objects per frame and will extend this numerically to a\nhundred-fold for public safety applications.\n  We envisage using the existing infrastructure of video cameras for acquiring\nimage datasets on-the-fly and deploying an easy-to-use data-driven software\nsystem for parsing of significant events by analyzing image sequences taken\ninside and outside of sports stadiums or other public venues. Other prospective\nusers are organizers of political rallies, civic and wildlife organizations,\nsecurity firms, and the military. We will optimize the performance of the\nsoftware by implementing a classification method able to distinguish between\nactivities posing a threat and those not posing a threat.\n","authors":["Alexandre Matov"],"pdf_url":"https://arxiv.org/pdf/2408.11836v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06494v2","updated":"2024-08-26T20:10:52Z","published":"2024-08-12T21:04:16Z","title":"What Color Scheme is More Effective in Assisting Readers to Locate\n  Information in a Color-Coded Article?","summary":"  Color coding, a technique assigning specific colors to cluster information\ntypes, has proven advantages in aiding human cognitive activities, especially\nreading and comprehension. The rise of Large Language Models (LLMs) has\nstreamlined document coding, enabling simple automatic text labeling with\nvarious schemes. This has the potential to make color-coding more accessible\nand benefit more users. However, the impact of color choice on information\nseeking is understudied. We conducted a user study assessing various color\nschemes' effectiveness in LLM-coded text documents, standardizing contrast\nratios to approximately 5.55:1 across schemes. Participants performed timed\ninformation-seeking tasks in color-coded scholarly abstracts. Results showed\nnon-analogous and yellow-inclusive color schemes improved performance, with the\nlatter also being more preferred by participants. These findings can inform\nbetter color scheme choices for text annotation. As LLMs advance document\ncoding, we advocate for more research focusing on the \"color\" aspect of\ncolor-coding techniques.\n","authors":["Ho Yin Ng","Zeyu He","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2408.06494v2.pdf","comment":"This paper will appear at IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2408.14606v1","updated":"2024-08-26T19:59:20Z","published":"2024-08-26T19:59:20Z","title":"BreakNet: Discontinuity-Resilient Multi-Scale Transformer Segmentation\n  of Retinal Layers","summary":"  Visible light optical coherence tomography (vis-OCT) is gaining traction for\nretinal imaging due to its high resolution and functional capabilities.\nHowever, the significant absorption of hemoglobin in the visible light range\nleads to pronounced shadow artifacts from retinal blood vessels, posing\nchallenges for accurate layer segmentation. In this study, we present BreakNet,\na multi-scale Transformer-based segmentation model designed to address boundary\ndiscontinuities caused by these shadow artifacts. BreakNet utilizes\nhierarchical Transformer and convolutional blocks to extract multi-scale global\nand local feature maps, capturing essential contextual, textural, and edge\ncharacteristics. The model incorporates decoder blocks that expand pathwaproys\nto enhance the extraction of fine details and semantic information, ensuring\nprecise segmentation. Evaluated on rodent retinal images acquired with\nprototype vis-OCT, BreakNet demonstrated superior performance over\nstate-of-the-art segmentation models, such as TCCT-BP and U-Net, even when\nfaced with limited-quality ground truth data. Our findings indicate that\nBreakNet has the potential to significantly improve retinal quantification and\nanalysis.\n","authors":["Razieh Ganjee","Bingjie Wang","Lingyun Wang","Chengcheng Zhao","JosÃ©-Alain Sahel","Shaohua Pi"],"pdf_url":"https://arxiv.org/pdf/2408.14606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14601v1","updated":"2024-08-26T19:44:18Z","published":"2024-08-26T19:44:18Z","title":"3D Point Cloud Network Pruning: When Some Weights Do not Matter","summary":"  A point cloud is a crucial geometric data structure utilized in numerous\napplications. The adoption of deep neural networks referred to as Point Cloud\nNeural Networks (PC- NNs), for processing 3D point clouds, has significantly\nadvanced fields that rely on 3D geometric data to enhance the efficiency of\ntasks. Expanding the size of both neural network models and 3D point clouds\nintroduces significant challenges in minimizing computational and memory\nrequirements. This is essential for meeting the demanding requirements of\nreal-world applications, which prioritize minimal energy consumption and low\nlatency. Therefore, investigating redundancy in PCNNs is crucial yet\nchallenging due to their sensitivity to parameters. Additionally, traditional\npruning methods face difficulties as these networks rely heavily on weights and\npoints. Nonetheless, our research reveals a promising phenomenon that could\nrefine standard PCNN pruning techniques. Our findings suggest that preserving\nonly the top p% of the highest magnitude weights is crucial for accuracy\npreservation. For example, pruning 99% of the weights from the PointNet model\nstill results in accuracy close to the base level. Specifically, in the\nModelNet40 dataset, where the base accuracy with the PointNet model was 87. 5%,\npreserving only 1% of the weights still achieves an accuracy of 86.8%. Codes\nare available in: https://github.com/apurba-nsu-rnd-lab/PCNN_Pruning\n","authors":["Amrijit Biswas","Md. Ismail Hossain","M M Lutfe Elahi","Ali Cheraghian","Fuad Rahman","Nabeel Mohammed","Shafin Rahman"],"pdf_url":"https://arxiv.org/pdf/2408.14601v1.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2408.14600v1","updated":"2024-08-26T19:43:01Z","published":"2024-08-26T19:43:01Z","title":"PVAFN: Point-Voxel Attention Fusion Network with Multi-Pooling Enhancing\n  for 3D Object Detection","summary":"  The integration of point and voxel representations is becoming more common in\nLiDAR-based 3D object detection. However, this combination often struggles with\ncapturing semantic information effectively. Moreover, relying solely on point\nfeatures within regions of interest can lead to information loss and\nlimitations in local feature representation. To tackle these challenges, we\npropose a novel two-stage 3D object detector, called Point-Voxel Attention\nFusion Network (PVAFN). PVAFN leverages an attention mechanism to improve\nmulti-modal feature fusion during the feature extraction phase. In the\nrefinement stage, it utilizes a multi-pooling strategy to integrate both\nmulti-scale and region-specific information effectively. The point-voxel\nattention mechanism adaptively combines point cloud and voxel-based\nBird's-Eye-View (BEV) features, resulting in richer object representations that\nhelp to reduce false detections. Additionally, a multi-pooling enhancement\nmodule is introduced to boost the model's perception capabilities. This module\nemploys cluster pooling and pyramid pooling techniques to efficiently capture\nkey geometric details and fine-grained shape structures, thereby enhancing the\nintegration of local and global features. Extensive experiments on the KITTI\nand Waymo datasets demonstrate that the proposed PVAFN achieves competitive\nperformance. The code and models will be available.\n","authors":["Yidi Li","Jiahao Wen","Bin Ren","Wenhao Li","Zhenhuan Xu","Hao Guo","Hong Liu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2408.14600v1.pdf","comment":"3D Object Detection"},{"id":"http://arxiv.org/abs/2404.00491v2","updated":"2024-08-26T19:39:19Z","published":"2024-03-30T23:19:40Z","title":"Denoising Monte Carlo Renders with Diffusion Models","summary":"  Physically-based renderings contain Monte-Carlo noise, with variance that\nincreases as the number of rays per pixel decreases. This noise, while\nzero-mean for good modern renderers, can have heavy tails (most notably, for\nscenes containing specular or refractive objects). Learned methods for\nrestoring low fidelity renders are highly developed, because suppressing render\nnoise means one can save compute and use fast renders with few rays per pixel.\nWe demonstrate that a diffusion model can denoise low fidelity renders\nsuccessfully. Furthermore, our method can be conditioned on a variety of\nnatural render information, and this conditioning helps performance.\nQuantitative experiments show that our method is competitive with SOTA across a\nrange of sampling rates. Qualitative examination of the reconstructions\nsuggests that the image prior applied by a diffusion method strongly favors\nreconstructions that are like real images -- so have straight shadow\nboundaries, curved specularities and no fireflies.\n","authors":["Vaibhav Vavilala","Rahul Vasanth","David Forsyth"],"pdf_url":"https://arxiv.org/pdf/2404.00491v2.pdf","comment":"25 pages, 18 figures, 2 tables"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.14432v1","updated":"2024-08-26T17:20:34Z","published":"2024-08-26T17:20:34Z","title":"Contextual Bandit with Herding Effects: Algorithms and Recommendation\n  Applications","summary":"  Contextual bandits serve as a fundamental algorithmic framework for\noptimizing recommendation decisions online. Though extensive attention has been\npaid to tailoring contextual bandits for recommendation applications, the\n\"herding effects\" in user feedback have been ignored. These herding effects\nbias user feedback toward historical ratings, breaking down the assumption of\nunbiased feedback inherent in contextual bandits. This paper develops a novel\nvariant of the contextual bandit that is tailored to address the feedback bias\ncaused by the herding effects. A user feedback model is formulated to capture\nthis feedback bias. We design the TS-Conf (Thompson Sampling under Conformity)\nalgorithm, which employs posterior sampling to balance the exploration and\nexploitation tradeoff. We prove an upper bound for the regret of the algorithm,\nrevealing the impact of herding effects on learning speed. Extensive\nexperiments on datasets demonstrate that TS-Conf outperforms four benchmark\nalgorithms. Analysis reveals that TS-Conf effectively mitigates the negative\nimpact of herding effects, resulting in faster learning and improved\nrecommendation accuracy.\n","authors":["Luyue Xu","Liming Wang","Hong Xie","Mingqiang Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.14432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14393v1","updated":"2024-08-26T16:21:50Z","published":"2024-08-26T16:21:50Z","title":"CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper\n  Influence","summary":"  With increasing privacy concerns in artificial intelligence, regulations have\nmandated the right to be forgotten, granting individuals the right to withdraw\ntheir data from models. Machine unlearning has emerged as a potential solution\nto enable selective forgetting in models, particularly in recommender systems\nwhere historical data contains sensitive user information. Despite recent\nadvances in recommendation unlearning, evaluating unlearning methods\ncomprehensively remains challenging due to the absence of a unified evaluation\nframework and overlooked aspects of deeper influence, e.g., fairness. To\naddress these gaps, we propose CURE4Rec, the first comprehensive benchmark for\nrecommendation unlearning evaluation. CURE4Rec covers four aspects, i.e.,\nunlearning Completeness, recommendation Utility, unleaRning efficiency, and\nrecommendation fairnEss, under three data selection strategies, i.e., core\ndata, edge data, and random data. Specifically, we consider the deeper\ninfluence of unlearning on recommendation fairness and robustness towards data\nwith varying impact levels. We construct multiple datasets with CURE4Rec\nevaluation and conduct extensive experiments on existing recommendation\nunlearning methods. Our code is released at\nhttps://github.com/xiye7lai/CURE4Rec.\n","authors":["Chaochao Chen","Jiaming Zhang","Yizhao Zhang","Li Zhang","Lingjuan Lyu","Yuyuan Li","Biao Gong","Chenggang Yan"],"pdf_url":"https://arxiv.org/pdf/2408.14393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14238v1","updated":"2024-08-26T12:52:02Z","published":"2024-08-26T12:52:02Z","title":"Are LLM-based Recommenders Already the Best? Simple Scaled Cross-entropy\n  Unleashes the Potential of Traditional Sequential Recommenders","summary":"  Large language models (LLMs) have been garnering increasing attention in the\nrecommendation community. Some studies have observed that LLMs, when fine-tuned\nby the cross-entropy (CE) loss with a full softmax, could achieve\n`state-of-the-art' performance in sequential recommendation. However, most of\nthe baselines used for comparison are trained using a pointwise/pairwise loss\nfunction. This inconsistent experimental setting leads to the underestimation\nof traditional methods and further fosters over-confidence in the ranking\ncapability of LLMs.\n  In this study, we provide theoretical justification for the superiority of\nthe cross-entropy loss by demonstrating its two desirable properties: tightness\nand coverage. Furthermore, this study sheds light on additional novel insights:\n1) Taking into account only the recommendation performance, CE is not yet\noptimal as it is not a quite tight bound in terms of some ranking metrics. 2)\nIn scenarios that full softmax cannot be performed, an effective alternative is\nto scale up the sampled normalizing term. These findings then help unleash the\npotential of traditional recommendation models, allowing them to surpass\nLLM-based counterparts. Given the substantial computational burden, existing\nLLM-based methods are not as effective as claimed for sequential\nrecommendation. We hope that these theoretical understandings in conjunction\nwith the empirical results will facilitate an objective evaluation of LLM-based\nrecommendation in the future.\n","authors":["Cong Xu","Zhangchi Zhu","Mo Yu","Jun Wang","Jianyong Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.14238v1.pdf","comment":"18 pages. arXiv admin note: substantial text overlap with\n  arXiv:2402.06216"},{"id":"http://arxiv.org/abs/2408.05141v2","updated":"2024-08-26T10:53:28Z","published":"2024-08-09T15:53:55Z","title":"A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning","summary":"  Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.\n","authors":["Ye Yuan","Chengwu Liu","Jingyang Yuan","Gongbo Sun","Siqi Li","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.05141v2.pdf","comment":"Technical report for 3rd prize in Task 1 of Meta CRAG KDD Cup 2024"},{"id":"http://arxiv.org/abs/2408.14118v1","updated":"2024-08-26T09:06:35Z","published":"2024-08-26T09:06:35Z","title":"Towards Lifelong Learning Embeddings: An Algorithmic Approach to\n  Dynamically Extend Embeddings","summary":"  The rapid evolution of technology has transformed business operations and\ncustomer interactions worldwide, with personalization emerging as a key\nopportunity for e-commerce companies to engage customers more effectively. The\napplication of machine learning, particularly that of deep learning models, has\ngained significant traction due to its ability to rapidly recognize patterns in\nlarge datasets, thereby offering numerous possibilities for personalization.\nThese models use embeddings to map discrete information, such as product IDs,\ninto a latent vector space, a method increasingly popular in recent years.\nHowever, e-commerce's dynamic nature, characterized by frequent new product\nintroductions, poses challenges for these embeddings, which typically require\nfixed dimensions and inputs, leading to the need for periodic retraining from\nscratch. This paper introduces a modular algorithm that extends embedding input\nsize while preserving learned knowledge, addressing the challenges posed by\ne-commerce's dynamism. The proposed algorithm also incorporates strategies to\nmitigate the cold start problem associated with new products. The results of\ninitial experiments suggest that this method outperforms traditional\nembeddings.\n","authors":["Miguel Alves Gomes","Philipp Meisen","Tobias Meisen"],"pdf_url":"https://arxiv.org/pdf/2408.14118v1.pdf","comment":"Accepted Extended Abstract for 3rd Workshop on End-End Customer\n  Journey Optimization at KDD2024, Barcelona, Spain"},{"id":"http://arxiv.org/abs/2408.08713v2","updated":"2024-08-26T03:03:47Z","published":"2024-08-16T12:51:52Z","title":"Beyond KAN: Introducing KarSein for Adaptive High-Order Feature\n  Interaction Modeling in CTR Prediction","summary":"  Modeling feature interactions is crucial for click-through rate (CTR)\nprediction, particularly when it comes to high-order explicit interactions.\nTraditional methods struggle with this task because they often predefine a\nmaximum interaction order, which relies heavily on prior knowledge and can\nlimit the model's effectiveness. Additionally, modeling high-order interactions\ntypically leads to increased computational costs. Therefore, the challenge lies\nin adaptively modeling high-order feature interactions while maintaining\nefficiency. To address this issue, we introduce Kolmogorov-Arnold Represented\nSparse Efficient Interaction Network (KarSein), designed to optimize both\npredictive accuracy and computational efficiency. We firstly identify\nlimitations of directly applying Kolmogorov-Arnold Networks (KAN) to CTR and\nthen introduce KarSein to overcome these issues. It features a novel\narchitecture that reduces the computational costs of KAN and supports embedding\nvectors as feature inputs. Additionally, KarSein employs guided symbolic\nregression to address the challenge of KAN in spontaneously learning\nmultiplicative relationships. Extensive experiments demonstrate KarSein's\nsuperior performance, achieving significant predictive accuracy with minimal\ncomputational overhead. Furthermore, KarSein maintains strong global\nexplainability while enabling the removal of redundant features, resulting in a\nsparse network structure. These advantages also position KarSein as a promising\nmethod for efficient inference.\n","authors":["Yunxiao Shi","Wujiang Xu","Mingyu Jin","Haimin Zhang","Qiang Wu","Yongfeng Zhang","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2408.08713v2.pdf","comment":"KarSein for CTR"},{"id":"http://arxiv.org/abs/2408.13986v1","updated":"2024-08-26T02:36:55Z","published":"2024-08-26T02:36:55Z","title":"AgentMove: Predicting Human Mobility Anywhere Using Large Language Model\n  based Agentic Framework","summary":"  Human mobility prediction plays a crucial role in various real-world\napplications. Although deep learning based models have shown promising results\nover the past decade, their reliance on extensive private mobility data for\ntraining and their inability to perform zero-shot predictions, have hindered\nfurther advancements. Recently, attempts have been made to apply large language\nmodels (LLMs) to mobility prediction task. However, their performance has been\nconstrained by the absence of a systematic design of workflow. They directly\ngenerate the final output using LLMs, which limits the potential of LLMs to\nuncover complex mobility patterns and underestimates their extensive reserve of\nglobal geospatial knowledge. In this paper, we introduce AgentMove, a\nsystematic agentic prediction framework to achieve generalized mobility\nprediction for any cities worldwide. In AgentMove, we first decompose the\nmobility prediction task into three sub-tasks and then design corresponding\nmodules to complete these subtasks, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments on mobility data from two sources in 12 cities\ndemonstrate that AgentMove outperforms the best baseline more than 8% in\nvarious metrics and it shows robust predictions with various LLMs as base and\nalso less geographical bias across cities. Codes and data can be found in\nhttps://github.com/tsinghua-fib-lab/AgentMove.\n","authors":["Jie Feng","Yuwei Du","Jie Zhao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2408.13986v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2406.18747v2","updated":"2024-08-26T01:07:11Z","published":"2024-06-26T20:25:53Z","title":"A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond\n  Four Stems","summary":"  Despite significant recent progress across multiple subtasks of audio source\nseparation, few music source separation systems support separation beyond the\nfour-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current\nsystems that support source separation beyond this setup, most continue to rely\non an inflexible decoder setup that can only support a fixed pre-defined set of\nstems. Increasing stem support in these inflexible systems correspondingly\nrequires increasing computational complexity, rendering extensions of these\nsystems computationally infeasible for long-tail instruments. In this work, we\npropose Banquet, a system that allows source separation of multiple stems using\njust one decoder. A bandsplit source separation model is extended to work in a\nquery-based setup in tandem with a music instrument recognition PaSST model. On\nthe MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached\nthe performance level of the significantly more complex 6-stem Hybrid\nTransformer Demucs on VDBO stems and outperformed it on guitar and piano. The\nquery-based setup allows for the separation of narrow instrument classes such\nas clean acoustic guitars, and can be successfully applied to the extraction of\nless common stems such as reeds and organs. Implementation is available at\nhttps://github.com/kwatcharasupat/query-bandit.\n","authors":["Karn N. Watcharasupat","Alexander Lerch"],"pdf_url":"https://arxiv.org/pdf/2406.18747v2.pdf","comment":"Accepted to the 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024). Camera-ready version"},{"id":"http://arxiv.org/abs/2408.14698v1","updated":"2024-08-26T23:52:27Z","published":"2024-08-26T23:52:27Z","title":"Smart Multi-Modal Search: Contextual Sparse and Dense Embedding\n  Integration in Adobe Express","summary":"  As user content and queries become increasingly multi-modal, the need for\neffective multi-modal search systems has grown. Traditional search systems\noften rely on textual and metadata annotations for indexed images, while\nmulti-modal embeddings like CLIP enable direct search using text and image\nembeddings. However, embedding-based approaches face challenges in integrating\ncontextual features such as user locale and recency. Building a scalable\nmulti-modal search system requires fine-tuning several components. This paper\npresents a multi-modal search architecture and a series of AB tests that\noptimize embeddings and multi-modal technologies in Adobe Express template\nsearch. We address considerations such as embedding model selection, the roles\nof embeddings in matching and ranking, and the balance between dense and sparse\nembeddings. Our iterative approach demonstrates how utilizing sparse, dense,\nand contextual features enhances short and long query search, significantly\nreduces null rates (over 70\\%), and increases click-through rates (CTR). Our\nfindings provide insights into developing robust multi-modal search systems,\nthereby enhancing relevance for complex queries.\n","authors":["Cherag Aroraa","Tracy Holloway King","Jayant Kumar","Yi Lu","Sanat Sharma","Arvind Srikantan","David Uvalle","Josep Valls-Vargas","Harsha Vardhan"],"pdf_url":"https://arxiv.org/pdf/2408.14698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14689v1","updated":"2024-08-26T23:29:03Z","published":"2024-08-26T23:29:03Z","title":"Federated User Preference Modeling for Privacy-Preserving Cross-Domain\n  Recommendation","summary":"  Cross-domain recommendation (CDR) aims to address the data-sparsity problem\nby transferring knowledge across domains. Existing CDR methods generally assume\nthat the user-item interaction data is shareable between domains, which leads\nto privacy leakage. Recently, some privacy-preserving CDR (PPCDR) models have\nbeen proposed to solve this problem. However, they primarily transfer simple\nrepresentations learned only from user-item interaction histories, overlooking\nother useful side information, leading to inaccurate user preferences.\nAdditionally, they transfer differentially private user-item interaction\nmatrices or embeddings across domains to protect privacy. However, these\nmethods offer limited privacy protection, as attackers may exploit external\ninformation to infer the original data. To address these challenges, we propose\na novel Federated User Preference Modeling (FUPM) framework. In FUPM, first, a\nnovel comprehensive preference exploration module is proposed to learn users'\ncomprehensive preferences from both interaction data and additional data\nincluding review texts and potentially positive items. Next, a private\npreference transfer module is designed to first learn differentially private\nlocal and global prototypes, and then privately transfer the global prototypes\nusing a federated learning strategy. These prototypes are generalized\nrepresentations of user groups, making it difficult for attackers to infer\nindividual information. Extensive experiments on four CDR tasks conducted on\nthe Amazon and Douban datasets validate the superiority of FUPM over SOTA\nbaselines. Code is available at https://github.com/Lili1013/FUPM.\n","authors":["Li Wang","Shoujin Wang","Quangui Zhang","Qiang Wu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2408.14689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12276v2","updated":"2024-08-26T23:05:42Z","published":"2024-02-19T16:40:38Z","title":"Explain then Rank: Scale Calibration of Neural Rankers Using Natural\n  Language Explanations from LLMs","summary":"  In search settings, calibrating the scores during the ranking process to\nquantities such as click-through rates or relevance levels enhances a system's\nusefulness and trustworthiness for downstream users. While previous research\nhas improved this notion of calibration for low complexity learning-to-rank\nmodels, the larger data demands and parameter count specific to modern neural\ntext rankers produce unique obstacles that hamper the efficacy of methods\nintended for the learning-to-rank setting.\n  This paper proposes exploiting large language models (LLMs) to provide\nrelevance and uncertainty signals for these neural text rankers to produce\nscale-calibrated scores through Monte Carlo sampling of natural language\nexplanations (NLEs). Our approach transforms the neural ranking task from\nranking textual query-document pairs to ranking corresponding synthesized NLEs.\nComprehensive experiments on two popular document ranking datasets show that\nthe NLE-based calibration approach consistently outperforms past calibration\nmethods and LLM-based methods for ranking, calibration, and query performance\nprediction tasks.\n","authors":["Puxuan Yu","Daniel Cohen","Hemank Lamba","Joel Tetreault","Alex Jaimes"],"pdf_url":"https://arxiv.org/pdf/2402.12276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14678v1","updated":"2024-08-26T23:01:48Z","published":"2024-08-26T23:01:48Z","title":"Bridging the Gap: Unpacking the Hidden Challenges in Knowledge\n  Distillation for Online Ranking Systems","summary":"  Knowledge Distillation (KD) is a powerful approach for compressing a large\nmodel into a smaller, more efficient model, particularly beneficial for\nlatency-sensitive applications like recommender systems. However, current KD\nresearch predominantly focuses on Computer Vision (CV) and NLP tasks,\noverlooking unique data characteristics and challenges inherent to recommender\nsystems. This paper addresses these overlooked challenges, specifically: (1)\nmitigating data distribution shifts between teacher and student models, (2)\nefficiently identifying optimal teacher configurations within time and\nbudgetary constraints, and (3) enabling computationally efficient and rapid\nsharing of teacher labels to support multiple students. We present a robust KD\nsystem developed and rigorously evaluated on multiple large-scale personalized\nvideo recommendation systems within Google. Our live experiment results\ndemonstrate significant improvements in student model performance while\nensuring consistent and reliable generation of high quality teacher labels from\na continuous data stream of data.\n","authors":["Nikhil Khani","Shuo Yang","Aniruddh Nath","Yang Liu","Pendo Abbo","Li Wei","Shawn Andrews","Maciej Kula","Jarrod Kahn","Zhe Zhao","Lichan Hong","Ed Chi"],"pdf_url":"https://arxiv.org/pdf/2408.14678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14658v1","updated":"2024-08-26T21:47:49Z","published":"2024-08-26T21:47:49Z","title":"KGPrune: a Web Application to Extract Subgraphs of Interest from\n  Wikidata with Analogical Pruning","summary":"  Knowledge graphs (KGs) have become ubiquitous publicly available knowledge\nsources, and are nowadays covering an ever increasing array of domains.\nHowever, not all knowledge represented is useful or pertaining when considering\na new application or specific task. Also, due to their increasing size,\nhandling large KGs in their entirety entails scalability issues. These two\naspects asks for efficient methods to extract subgraphs of interest from\nexisting KGs. To this aim, we introduce KGPrune, a Web Application that, given\nseed entities of interest and properties to traverse, extracts their\nneighboring subgraphs from Wikidata. To avoid topical drift, KGPrune relies on\na frugal pruning algorithm based on analogical reasoning to only keep relevant\nneighbors while pruning irrelevant ones. The interest of KGPrune is illustrated\nby two concrete applications, namely, bootstrapping an enterprise KG and\nextracting knowledge related to looted artworks.\n","authors":["Pierre Monnin","Cherif-Hassan Nousradine","Lucas Jarnac","Laurel Zuckerman","Miguel Couceiro"],"pdf_url":"https://arxiv.org/pdf/2408.14658v1.pdf","comment":"Accepted as a demo paper at ECAI 2024"},{"id":"http://arxiv.org/abs/2408.14636v1","updated":"2024-08-26T21:00:25Z","published":"2024-08-26T21:00:25Z","title":"Relationships are Complicated! An Analysis of Relationships Between\n  Datasets on the Web","summary":"  The Web today has millions of datasets, and the number of datasets continues\nto grow at a rapid pace. These datasets are not standalone entities; rather,\nthey are intricately connected through complex relationships. Semantic\nrelationships between datasets provide critical insights for research and\ndecision-making processes. In this paper, we study dataset relationships from\nthe perspective of users who discover, use, and share datasets on the Web: what\nrelationships are important for different tasks? What contextual information\nmight users want to know? We first present a comprehensive taxonomy of\nrelationships between datasets on the Web and map these relationships to user\ntasks performed during dataset discovery. We develop a series of methods to\nidentify these relationships and compare their performance on a large corpus of\ndatasets generated from Web pages with schema.org markup. We demonstrate that\nmachine-learning based methods that use dataset metadata achieve multi-class\nclassification accuracy of 90%. Finally, we highlight gaps in available\nsemantic markup for datasets and discuss how incorporating comprehensive\nsemantics can facilitate the identification of dataset relationships. By\nproviding a comprehensive overview of dataset relationships at scale, this\npaper sets a benchmark for future research.\n","authors":["Kate Lin","Tarfah Alrashed","Natasha Noy"],"pdf_url":"https://arxiv.org/pdf/2408.14636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14623v1","updated":"2024-08-26T20:36:52Z","published":"2024-08-26T20:36:52Z","title":"MODOC: A Modular Interface for Flexible Interlinking of Text Retrieval\n  and Text Generation Functions","summary":"  Large Language Models (LLMs) produce eloquent texts but often the content\nthey generate needs to be verified. Traditional information retrieval systems\ncan assist with this task, but most systems have not been designed with\nLLM-generated queries in mind. As such, there is a compelling need for\nintegrated systems that provide both retrieval and generation functionality\nwithin a single user interface.\n  We present MODOC, a modular user interface that leverages the capabilities of\nLLMs and provides assistance with detecting their confabulations, promoting\nintegrity in scientific writing. MODOC represents a significant step forward in\nscientific writing assistance. Its modular architecture supports flexible\nfunctions for retrieving information and for writing and generating text in a\nsingle, user-friendly interface.\n","authors":["Yingqiang Gao","Jhony Prada","Nianlong Gu","Jessica Lam","Richard H. R. Hahnloser"],"pdf_url":"https://arxiv.org/pdf/2408.14623v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.14471v1","updated":"2024-08-26T17:59:01Z","published":"2024-08-26T17:59:01Z","title":"A Practitioner's Guide to Continual Multimodal Pretraining","summary":"  Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux.\n","authors":["Karsten Roth","Vishaal Udandarao","Sebastian Dziadzio","Ameya Prabhu","Mehdi Cherti","Oriol Vinyals","Olivier HÃ©naff","Samuel Albanie","Matthias Bethge","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2408.14471v1.pdf","comment":"Technical Report. 52 pages"},{"id":"http://arxiv.org/abs/2408.14461v1","updated":"2024-08-26T17:50:47Z","published":"2024-08-26T17:50:47Z","title":"A domain decomposition-based autoregressive deep learning model for\n  unsteady and nonlinear partial differential equations","summary":"  In this paper, we propose a domain-decomposition-based deep learning (DL)\nframework, named transient-CoMLSim, for accurately modeling unsteady and\nnonlinear partial differential equations (PDEs). The framework consists of two\nkey components: (a) a convolutional neural network (CNN)-based autoencoder\narchitecture and (b) an autoregressive model composed of fully connected\nlayers. Unlike existing state-of-the-art methods that operate on the entire\ncomputational domain, our CNN-based autoencoder computes a lower-dimensional\nbasis for solution and condition fields represented on subdomains. Timestepping\nis performed entirely in the latent space, generating embeddings of the\nsolution variables from the time history of embeddings of solution and\ncondition variables. This approach not only reduces computational complexity\nbut also enhances scalability, making it well-suited for large-scale\nsimulations. Furthermore, to improve the stability of our rollouts, we employ a\ncurriculum learning (CL) approach during the training of the autoregressive\nmodel. The domain-decomposition strategy enables scaling to out-of-distribution\ndomain sizes while maintaining the accuracy of predictions -- a feature not\neasily integrated into popular DL-based approaches for physics simulations. We\nbenchmark our model against two widely-used DL architectures, Fourier Neural\nOperator (FNO) and U-Net, and demonstrate that our framework outperforms them\nin terms of accuracy, extrapolation to unseen timesteps, and stability for a\nwide range of use cases.\n","authors":["Sheel Nidhan","Haoliang Jiang","Lalit Ghule","Clancy Umphrey","Rishikesh Ranade","Jay Pathak"],"pdf_url":"https://arxiv.org/pdf/2408.14461v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2408.11796v2","updated":"2024-08-26T17:50:46Z","published":"2024-08-21T17:38:48Z","title":"LLM Pruning and Distillation in Practice: The Minitron Approach","summary":"  We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.\n","authors":["Sharath Turuvekere Sreenivas","Saurav Muralidharan","Raviraj Joshi","Marcin Chochowski","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2408.11796v2.pdf","comment":"v2: Added missing references. Cleaned up runtime performance section"},{"id":"http://arxiv.org/abs/2408.14453v1","updated":"2024-08-26T17:48:42Z","published":"2024-08-26T17:48:42Z","title":"Reconstructing physiological signals from fMRI across the adult lifespan","summary":"  Interactions between the brain and body are of fundamental importance for\nhuman behavior and health. Functional magnetic resonance imaging (fMRI)\ncaptures whole-brain activity noninvasively, and modeling how fMRI signals\ninteract with physiological dynamics of the body can provide new insight into\nbrain function and offer potential biomarkers of disease. However,\nphysiological recordings are not always possible to acquire since they require\nextra equipment and setup, and even when they are, the recorded physiological\nsignals may contain substantial artifacts. To overcome this limitation, machine\nlearning models have been proposed to directly extract features of respiratory\nand cardiac activity from resting-state fMRI signals. To date, such work has\nbeen carried out only in healthy young adults and in a pediatric population,\nleaving open questions about the efficacy of these approaches on older adults.\nHere, we propose a novel framework that leverages Transformer-based\narchitectures for reconstructing two key physiological signals - low-frequency\nrespiratory volume (RV) and heart rate (HR) fluctuations - from fMRI data, and\ntest these models on a dataset of individuals aged 36-89 years old. Our\nframework outperforms previously proposed approaches (attaining median\ncorrelations between predicted and measured signals of r ~ .698 for RV and r ~\n.618 for HR), indicating the potential of leveraging attention mechanisms to\nmodel fMRI-physiological signal relationships. We also evaluate several model\ntraining and fine-tuning strategies, and find that incorporating young-adult\ndata during training improves the performance when predicting physiological\nsignals in the aging cohort. Overall, our approach successfully infers key\nphysiological variables directly from fMRI data from individuals across a wide\nrange of the adult lifespan.\n","authors":["Shiyu Wang","Ziyuan Xu","Yamin Li","Mara Mather","Roza G. Bayrak","Catie Chang"],"pdf_url":"https://arxiv.org/pdf/2408.14453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14445v1","updated":"2024-08-26T17:36:51Z","published":"2024-08-26T17:36:51Z","title":"Symmetry & Critical Points","summary":"  Critical points of an invariant function may or may not be symmetric. We\nprove, however, that if a symmetric critical point exists, those adjacent to it\nare generically symmetry breaking. This mathematical mechanism is shown to\ncarry important implications for our ability to efficiently minimize invariant\nnonconvex functions, in particular those associated with neural networks.\n","authors":["Yossi Arjevani"],"pdf_url":"https://arxiv.org/pdf/2408.14445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14442v1","updated":"2024-08-26T17:35:01Z","published":"2024-08-26T17:35:01Z","title":"Model Parallel Training and Transfer Learning for Convolutional Neural\n  Networks by Domain Decomposition","summary":"  Deep convolutional neural networks (CNNs) have been shown to be very\nsuccessful in a wide range of image processing applications. However, due to\ntheir increasing number of model parameters and an increasing availability of\nlarge amounts of training data, parallelization strategies to efficiently train\ncomplex CNNs are necessary. In previous work by the authors, a novel model\nparallel CNN architecture was proposed which is loosely inspired by domain\ndecomposition. In particular, the novel network architecture is based on a\ndecomposition of the input data into smaller subimages. For each of these\nsubimages, local CNNs with a proportionally smaller number of parameters are\ntrained in parallel and the resulting local classifications are then aggregated\nin a second step by a dense feedforward neural network (DNN). In the present\nwork, we compare the resulting CNN-DNN architecture to less costly alternatives\nto combine the local classifications into a final, global decision.\nAdditionally, we investigate the performance of the CNN-DNN trained as one\ncoherent model as well as using a transfer learning strategy, where the\nparameters of the pre-trained local CNNs are used as initial values for a\nsubsequently trained global coherent CNN-DNN model.\n","authors":["Axel Klawonn","Martin Lanser","Janine Weber"],"pdf_url":"https://arxiv.org/pdf/2408.14442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.13840v3","updated":"2024-08-26T17:34:44Z","published":"2023-06-24T02:25:56Z","title":"Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data","summary":"  Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.\n","authors":["Brando Miranda","Alycia Lee","Sudharsan Sundar","Allison Casasola","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2306.13840v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10844v2","updated":"2024-08-26T17:31:16Z","published":"2024-07-15T15:59:39Z","title":"Improved Uncertainty Estimation of Graph Neural Network Potentials Using\n  Engineered Latent Space Distances","summary":"  Graph neural networks (GNNs) have been shown to be astonishingly capable\nmodels for molecular property prediction, particularly as surrogates for\nexpensive density functional theory calculations of relaxed energy for novel\nmaterial discovery. However, one limitation of GNNs in this context is the lack\nof useful uncertainty prediction methods, as this is critical to the material\ndiscovery pipeline. In this work, we show that uncertainty quantification for\nrelaxed energy calculations is more complex than uncertainty quantification for\nother kinds of molecular property prediction, due to the effect that structure\noptimizations have on the error distribution. We propose that distribution-free\ntechniques are more useful tools for assessing calibration, recalibrating, and\ndeveloping uncertainty prediction methods for GNNs performing relaxed energy\ncalculations. We also develop a relaxed energy task for evaluating uncertainty\nmethods for equivariant GNNs, based on distribution-free recalibration and\nusing the Open Catalyst Project dataset. We benchmark a set of popular\nuncertainty prediction methods on this task, and show that latent distance\nmethods, with our novel improvements, are the most well-calibrated and\neconomical approach for relaxed energy calculations. Finally, we demonstrate\nthat our latent space distance method produces results which align with our\nexpectations on a clustering example, and on specific equation of state and\nadsorbate coverage examples from outside the training dataset.\n","authors":["Joseph Musielewicz","Janice Lan","Matt Uyttendaele","John R. Kitchin"],"pdf_url":"https://arxiv.org/pdf/2407.10844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10468v3","updated":"2024-08-26T17:28:23Z","published":"2024-08-20T00:40:49Z","title":"Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions","summary":"  The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.\n","authors":["Jinxin Liu","Zao Yang"],"pdf_url":"https://arxiv.org/pdf/2408.10468v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14435v1","updated":"2024-08-26T17:21:54Z","published":"2024-08-26T17:21:54Z","title":"Social perception of faces in a vision-language model","summary":"  We explore social perception of human faces in CLIP, a widely used\nopen-source vision-language model. To this end, we compare the similarity in\nCLIP embeddings between different textual prompts and a set of face images. Our\ntextual prompts are constructed from well-validated social psychology terms\ndenoting social perception. The face images are synthetic and are\nsystematically and independently varied along six dimensions: the legally\nprotected attributes of age, gender, and race, as well as facial expression,\nlighting, and pose. Independently and systematically manipulating face\nattributes allows us to study the effect of each on social perception and\navoids confounds that can occur in wild-collected data due to uncontrolled\nsystematic correlations between attributes. Thus, our findings are experimental\nrather than observational. Our main findings are three. First, while CLIP is\ntrained on the widest variety of images and texts, it is able to make\nfine-grained human-like social judgments on face images. Second, age, gender,\nand race do systematically impact CLIP's social perception of faces, suggesting\nan undesirable bias in CLIP vis-a-vis legally protected attributes. Most\nstrikingly, we find a strong pattern of bias concerning the faces of Black\nwomen, where CLIP produces extreme values of social perception across different\nages and facial expressions. Third, facial expression impacts social perception\nmore than age and lighting as much as age. The last finding predicts that\nstudies that do not control for unprotected visual attributes may reach the\nwrong conclusions on bias. Our novel method of investigation, which is founded\non the social psychology literature and on the experiments involving the\nmanipulation of individual attributes, yields sharper and more reliable\nobservations than previous observational methods and may be applied to study\nbiases in any vision-language model.\n","authors":["Carina I. Hausladen","Manuel Knott","Colin F. Camerer","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2408.14435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14434v1","updated":"2024-08-26T17:21:19Z","published":"2024-08-26T17:21:19Z","title":"Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena","summary":"  Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.\n","authors":["Logan Ward","J. Gregory Pauloski","Valerie Hayot-Sasson","Yadu Babuji","Alexander Brace","Ryan Chard","Kyle Chard","Rajeev Thakur","Ian Foster"],"pdf_url":"https://arxiv.org/pdf/2408.14434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14432v1","updated":"2024-08-26T17:20:34Z","published":"2024-08-26T17:20:34Z","title":"Contextual Bandit with Herding Effects: Algorithms and Recommendation\n  Applications","summary":"  Contextual bandits serve as a fundamental algorithmic framework for\noptimizing recommendation decisions online. Though extensive attention has been\npaid to tailoring contextual bandits for recommendation applications, the\n\"herding effects\" in user feedback have been ignored. These herding effects\nbias user feedback toward historical ratings, breaking down the assumption of\nunbiased feedback inherent in contextual bandits. This paper develops a novel\nvariant of the contextual bandit that is tailored to address the feedback bias\ncaused by the herding effects. A user feedback model is formulated to capture\nthis feedback bias. We design the TS-Conf (Thompson Sampling under Conformity)\nalgorithm, which employs posterior sampling to balance the exploration and\nexploitation tradeoff. We prove an upper bound for the regret of the algorithm,\nrevealing the impact of herding effects on learning speed. Extensive\nexperiments on datasets demonstrate that TS-Conf outperforms four benchmark\nalgorithms. Analysis reveals that TS-Conf effectively mitigates the negative\nimpact of herding effects, resulting in faster learning and improved\nrecommendation accuracy.\n","authors":["Luyue Xu","Liming Wang","Hong Xie","Mingqiang Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.14432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03341v4","updated":"2024-08-26T17:12:07Z","published":"2024-06-05T14:58:32Z","title":"Tackling GenAI Copyright Issues: Originality Estimation and\n  Genericization","summary":"  The rapid progress of generative AI technology has sparked significant\ncopyright concerns, leading to numerous lawsuits filed against AI developers.\nWhile various techniques for mitigating copyright issues have been studied,\nsignificant risks remain. Here, we propose a genericization method that\nmodifies the outputs of a generative model to make them more generic and less\nlikely to infringe copyright. To achieve this, we introduce a metric for\nquantifying the level of originality of data in a manner that is consistent\nwith the legal framework. This metric can be practically estimated by drawing\nsamples from a generative model, which is then used for the genericization\nprocess. As a practical implementation, we introduce PREGen, which combines our\ngenericization method with an existing mitigation technique. Experiments\ndemonstrate that our genericization method successfully modifies the output of\na text-to-image generative model so that it produces more generic,\ncopyright-compliant images. Compared to the existing method, PREGen reduces the\nlikelihood of generating copyrighted characters by more than half when the\nnames of copyrighted characters are used as the prompt, dramatically improving\nthe performance. Additionally, while generative models can produce copyrighted\ncharacters even when their names are not directly mentioned in the prompt,\nPREGen almost entirely prevents the generation of such characters in these\ncases.\n","authors":["Hiroaki Chiba-Okabe","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2406.03341v4.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2405.12295v3","updated":"2024-08-26T17:10:41Z","published":"2024-05-20T18:01:15Z","title":"Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) are recognized as potent tools for processing\nreal-world data organized in graph structures. Especially inductive GNNs, which\nallow for the processing of graph-structured data without relying on predefined\ngraph structures, are becoming increasingly important in a wide range of\napplications. As such these networks become attractive targets for\nmodel-stealing attacks where an adversary seeks to replicate the functionality\nof the targeted network. Significant efforts have been devoted to developing\nmodel-stealing attacks that extract models trained on images and texts.\nHowever, little attention has been given to stealing GNNs trained on graph\ndata. This paper identifies a new method of performing unsupervised\nmodel-stealing attacks against inductive GNNs, utilizing graph contrastive\nlearning and spectral graph augmentations to efficiently extract information\nfrom the targeted model. The new type of attack is thoroughly evaluated on six\ndatasets and the results show that our approach outperforms the current\nstate-of-the-art by Shen et al. (2021). In particular, our attack surpasses the\nbaseline across all benchmarks, attaining superior fidelity and downstream\naccuracy of the stolen model while necessitating fewer queries directed toward\nthe target model.\n","authors":["Marcin Podhajski","Jan DubiÅski","Franziska Boenisch","Adam Dziedzic","Agnieszka Pregowska And Tomasz Michalak"],"pdf_url":"https://arxiv.org/pdf/2405.12295v3.pdf","comment":"Accepted at ECAI - 27TH EUROPEAN CONFERENCE ON ARTIFICIAL\n  INTELLIGENCE"},{"id":"http://arxiv.org/abs/2408.14421v1","updated":"2024-08-26T17:04:52Z","published":"2024-08-26T17:04:52Z","title":"Evaluating saliency scores in point clouds of natural environments by\n  learning surface anomalies","summary":"  In recent years, three-dimensional point clouds are used increasingly to\ndocument natural environments. Each dataset contains a diverse set of objects,\nat varying shapes and sizes, distributed throughout the data and intricately\nintertwined with the topography. Therefore, regions of interest are difficult\nto find and consequent analyses become a challenge. Inspired from visual\nperception principles, we propose to differentiate objects of interest from the\ncluttered environment by evaluating how much they stand out from their\nsurroundings, i.e., their geometric salience. Previous saliency detection\napproaches suggested mostly handcrafted attributes for the task. However, such\nmethods fail when the data are too noisy or have high levels of texture. Here\nwe propose a learning-based mechanism that accommodates noise and textured\nsurfaces. We assume that within the natural environment any change from the\nprevalent surface would suggest a salient object. Thus, we first learn the\nunderlying surface and then search for anomalies within it. Initially, a deep\nneural network is trained to reconstruct the surface. Regions where the\nreconstructed part deviates significantly from the original point cloud yield a\nsubstantial reconstruction error, signifying an anomaly, i.e., saliency. We\ndemonstrate the effectiveness of the proposed approach by searching for salient\nfeatures in various natural scenarios, which were acquired by different\nacquisition platforms. We show the strong correlation between the\nreconstruction error and salient objects.\n","authors":["Reuma Arav","Dennis Wittich","Franz Rottensteiner"],"pdf_url":"https://arxiv.org/pdf/2408.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14416v1","updated":"2024-08-26T17:03:14Z","published":"2024-08-26T17:03:14Z","title":"Hyperdimensional Computing Empowered Federated Foundation Model over\n  Wireless Networks for Metaverse","summary":"  The Metaverse, a burgeoning collective virtual space merging augmented\nreality and persistent virtual worlds, necessitates advanced artificial\nintelligence (AI) and communication technologies to support immersive and\ninteractive experiences. Federated learning (FL) has emerged as a promising\ntechnique for collaboratively training AI models while preserving data privacy.\nHowever, FL faces challenges such as high communication overhead and\nsubstantial computational demands, particularly for neural network (NN) models.\nTo address these issues, we propose an integrated federated split learning and\nhyperdimensional computing (FSL-HDC) framework for emerging foundation models.\nThis novel approach reduces communication costs, computation load, and privacy\nrisks, making it particularly suitable for resource-constrained edge devices in\nthe Metaverse, ensuring real-time responsive interactions. Additionally, we\nintroduce an optimization algorithm that concurrently optimizes transmission\npower and bandwidth to minimize the maximum transmission time among all users\nto the server. The simulation results based on the MNIST dataset indicate that\nFSL-HDC achieves an accuracy rate of approximately 87.5%, which is slightly\nlower than that of FL-HDC. However, FSL-HDC exhibits a significantly faster\nconvergence speed, approximately 3.733x that of FSL-NN, and demonstrates\nrobustness to non-IID data distributions. Moreover, our proposed optimization\nalgorithm can reduce the maximum transmission time by up to 64% compared with\nthe baseline.\n","authors":["Yahao Ding","Wen Shang","Minrui Xu","Zhaohui Yang","Ye Hu","Dusit Niyato","Mohammad Shikh-Bahaei"],"pdf_url":"https://arxiv.org/pdf/2408.14416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14415v1","updated":"2024-08-26T17:02:25Z","published":"2024-08-26T17:02:25Z","title":"LoG-VMamba: Local-Global Vision Mamba for Medical Image Segmentation","summary":"  Mamba, a State Space Model (SSM), has recently shown competitive performance\nto Convolutional Neural Networks (CNNs) and Transformers in Natural Language\nProcessing and general sequence modeling. Various attempts have been made to\nadapt Mamba to Computer Vision tasks, including medical image segmentation\n(MIS). Vision Mamba (VM)-based networks are particularly attractive due to\ntheir ability to achieve global receptive fields, similar to Vision\nTransformers, while also maintaining linear complexity in the number of tokens.\nHowever, the existing VM models still struggle to maintain both spatially local\nand global dependencies of tokens in high dimensional arrays due to their\nsequential nature. Employing multiple and/or complicated scanning strategies is\ncomputationally costly, which hinders applications of SSMs to high-dimensional\n2D and 3D images that are common in MIS problems. In this work, we propose\nLocal-Global Vision Mamba, LoG-VMamba, that explicitly enforces spatially\nadjacent tokens to remain nearby on the channel axis, and retains the global\ncontext in a compressed form. Our method allows the SSMs to access the local\nand global contexts even before reaching the last token while requiring only a\nsimple scanning strategy. Our segmentation models are computationally efficient\nand substantially outperform both CNN and Transformers-based baselines on a\ndiverse set of 2D and 3D MIS tasks. The implementation of LoG-VMamba is\navailable at \\url{https://github.com/Oulu-IMEDS/LoG-VMamba}.\n","authors":["Trung Dinh Quoc Dang","Huy Hoang Nguyen","Aleksei Tiulpin"],"pdf_url":"https://arxiv.org/pdf/2408.14415v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2407.03194v5","updated":"2024-08-26T16:57:34Z","published":"2024-07-03T15:26:02Z","title":"Prediction Instability in Machine Learning Ensembles","summary":"  In machine learning ensembles predictions from multiple models are\naggregated. Despite widespread use and strong performance of ensembles in\napplied problems little is known about the mathematical properties of\naggregating models and associated consequences for safe, explainable use of\nsuch models. In this paper we prove a theorem that shows that any ensemble will\nexhibit at least one of the following forms of prediction instability. It will\neither ignore agreement among all underlying models, change its mind when none\nof the underlying models have done so, or be manipulable through inclusion or\nexclusion of options it would never actually predict. As a consequence,\nensemble aggregation procedures will always need to balance the benefits of\ninformation use against the risk of these prediction instabilities. This\nanalysis also sheds light on what specific forms of prediction instability to\nexpect from particular ensemble algorithms; for example popular tree ensembles\nlike random forest, or xgboost will violate basic, intuitive fairness\nproperties. Finally, we show that this can be ameliorated by using consistent\nmodels in asymptotic conditions.\n","authors":["Jeremy Kedziora"],"pdf_url":"https://arxiv.org/pdf/2407.03194v5.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2408.14407v1","updated":"2024-08-26T16:49:42Z","published":"2024-08-26T16:49:42Z","title":"Spectrally Informed Learning of Fluid Flows","summary":"  Accurate and efficient fluid flow models are essential for applications\nrelating to many physical phenomena including geophysical, aerodynamic, and\nbiological systems. While these flows may exhibit rich and multiscale dynamics,\nin many cases underlying low-rank structures exist which describe the bulk of\nthe motion. These structures tend to be spatially large and temporally slow,\nand may contain most of the energy in a given flow. The extraction and\nparsimonious representation of these low-rank dynamics from high-dimensional\ndata is a key challenge. Inspired by the success of physics-informed machine\nlearning methods, we propose a spectrally-informed approach to extract low-rank\nmodels of fluid flows by leveraging known spectral properties in the learning\nprocess. We incorporate this knowledge by imposing regularizations on the\nlearned dynamics, which bias the training process towards learning\nlow-frequency structures with corresponding higher power. We demonstrate the\neffectiveness of this method to improve prediction and produce learned models\nwhich better match the underlying spectral properties of prototypical fluid\nflows.\n","authors":["Benjamin D. Shaffer","Jeremy R. Vorenberg","M. Ani Hsieh"],"pdf_url":"https://arxiv.org/pdf/2408.14407v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.05720v2","updated":"2024-08-26T16:48:08Z","published":"2024-03-08T23:17:55Z","title":"A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models","summary":"  Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\n  Using clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation.\n","authors":["Asad Aali","Dave Van Veen","Yamin Ishraq Arefeen","Jason Hom","Christian Bluethgen","Eduardo Pontes Reis","Sergios Gatidis","Namuun Clifford","Joseph Daws","Arash S. Tehrani","Jangwon Kim","Akshay S. Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2403.05720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14404v1","updated":"2024-08-26T16:47:20Z","published":"2024-08-26T16:47:20Z","title":"Application of Neural Ordinary Differential Equations for ITER Burning\n  Plasma Dynamics","summary":"  The dynamics of burning plasmas in tokamaks are crucial for advancing\ncontrolled thermonuclear fusion. This study introduces the NeuralPlasmaODE, a\nmulti-region multi-timescale transport model to simulate the complex energy\ntransfer processes in ITER deuterium-tritium (D-T) plasmas. Our model captures\nthe interactions between energetic alpha particles, electrons, and ions, which\nare vital for understanding phenomena such as thermal runaway instability. We\nemploy neural ordinary differential equations (Neural ODEs) for the numerical\nderivation of diffusivity parameters, enabling precise modeling of energy\ninteractions between different plasma regions. By leveraging transfer learning,\nwe utilize model parameters derived from DIII-D experimental data, enhancing\nthe efficiency and accuracy of our simulations without training from scratch.\nApplying this model to ITER's inductive and non-inductive operational\nscenarios, our results demonstrate that radiation and transport processes\neffectively remove excess heat from the core plasma, preventing thermal runaway\ninstability. This study underscores the potential of machine learning in\nadvancing our understanding and control of burning plasma dynamics in fusion\nreactors.\n","authors":["Zefang Liu","Weston M. Stacey"],"pdf_url":"https://arxiv.org/pdf/2408.14404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14398v1","updated":"2024-08-26T16:29:13Z","published":"2024-08-26T16:29:13Z","title":"Language-specific Calibration for Pruning Multilingual Language Models","summary":"  Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.\n","authors":["Simon Kurz","Zhixue Zhao","Jian-Jia Chen","Lucie Flek"],"pdf_url":"https://arxiv.org/pdf/2408.14398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14393v1","updated":"2024-08-26T16:21:50Z","published":"2024-08-26T16:21:50Z","title":"CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper\n  Influence","summary":"  With increasing privacy concerns in artificial intelligence, regulations have\nmandated the right to be forgotten, granting individuals the right to withdraw\ntheir data from models. Machine unlearning has emerged as a potential solution\nto enable selective forgetting in models, particularly in recommender systems\nwhere historical data contains sensitive user information. Despite recent\nadvances in recommendation unlearning, evaluating unlearning methods\ncomprehensively remains challenging due to the absence of a unified evaluation\nframework and overlooked aspects of deeper influence, e.g., fairness. To\naddress these gaps, we propose CURE4Rec, the first comprehensive benchmark for\nrecommendation unlearning evaluation. CURE4Rec covers four aspects, i.e.,\nunlearning Completeness, recommendation Utility, unleaRning efficiency, and\nrecommendation fairnEss, under three data selection strategies, i.e., core\ndata, edge data, and random data. Specifically, we consider the deeper\ninfluence of unlearning on recommendation fairness and robustness towards data\nwith varying impact levels. We construct multiple datasets with CURE4Rec\nevaluation and conduct extensive experiments on existing recommendation\nunlearning methods. Our code is released at\nhttps://github.com/xiye7lai/CURE4Rec.\n","authors":["Chaochao Chen","Jiaming Zhang","Yizhao Zhang","Li Zhang","Lingjuan Lyu","Yuyuan Li","Biao Gong","Chenggang Yan"],"pdf_url":"https://arxiv.org/pdf/2408.14393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14387v1","updated":"2024-08-26T16:11:53Z","published":"2024-08-26T16:11:53Z","title":"Reprogramming Foundational Large Language Models(LLMs) for Enterprise\n  Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in\n  Copilot-Guided Cross-Modal Time Series Representation Learning","summary":"  Spatio-temporal forecasting plays a crucial role in various sectors such as\ntransportation systems, logistics, and supply chain management. However,\nexisting methods are limited by their ability to handle large, complex\ndatasets. To overcome this limitation, we introduce a hybrid approach that\ncombines the strengths of open-source large and small-scale language models\n(LLMs and LMs) with traditional forecasting methods. We augment traditional\nmethods with dynamic prompting and a grouped-query, multi-head attention\nmechanism to more effectively capture both intra-series and inter-series\ndependencies in evolving nonlinear time series data. In addition, we facilitate\non-premises customization by fine-tuning smaller open-source LMs for time\nseries trend analysis utilizing descriptions generated by open-source large LMs\non consumer-grade hardware using Low-Rank Adaptation with Activation Memory\nReduction (LoRA-AMR) technique to reduce computational overhead and activation\nstorage memory demands while preserving inference latency. We combine language\nmodel processing for time series trend analysis with traditional time series\nrepresentation learning method for cross-modal integration, achieving robust\nand accurate forecasts. The framework effectiveness is demonstrated through\nextensive experiments on various real-world datasets, outperforming existing\nmethods by significant margins in terms of forecast accuracy.\n","authors":["Sakhinana Sagar Srinivas","Chidaksh Ravuru","Geethan Sannidhi","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2408.14387v1.pdf","comment":"Paper published at the Deployable AI (DAI) workshop at AAAI-2024"},{"id":"http://arxiv.org/abs/2408.14381v1","updated":"2024-08-26T16:04:13Z","published":"2024-08-26T16:04:13Z","title":"Learning Tree-Structured Composition of Data Augmentation","summary":"  Data augmentation is widely used for training a neural network given little\nlabeled data. A common practice of augmentation training is applying a\ncomposition of multiple transformations sequentially to the data. Existing\naugmentation methods such as RandAugment randomly sample from a list of\npre-selected transformations, while methods such as AutoAugment apply advanced\nsearch to optimize over an augmentation set of size $k^d$, which is the number\nof transformation sequences of length $d$, given a list of $k$ transformations.\n  In this paper, we design efficient algorithms whose running time complexity\nis much faster than the worst-case complexity of $O(k^d)$, provably. We propose\na new algorithm to search for a binary tree-structured composition of $k$\ntransformations, where each tree node corresponds to one transformation. The\nbinary tree generalizes sequential augmentations, such as the SimCLR\naugmentation scheme for contrastive learning. Using a top-down, recursive\nsearch procedure, our algorithm achieves a runtime complexity of $O(2^d k)$,\nwhich is much faster than $O(k^d)$ as $k$ increases above $2$. We apply our\nalgorithm to tackle data distributions with heterogeneous subpopulations by\nsearching for one tree in each subpopulation and then learning a weighted\ncombination, resulting in a forest of trees.\n  We validate our proposed algorithms on numerous graph and image datasets,\nincluding a multi-label graph classification dataset we collected. The dataset\nexhibits significant variations in the sizes of graphs and their average\ndegrees, making it ideal for studying data augmentation. We show that our\napproach can reduce the computation cost by 43% over existing search methods\nwhile improving performance by 4.3%. The tree structures can be used to\ninterpret the relative importance of each transformation, such as identifying\nthe important transformations on small vs. large graphs.\n","authors":["Dongyue Li","Kailai Chen","Predrag Radivojac","Hongyang R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.14381v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2408.14371v1","updated":"2024-08-26T15:53:50Z","published":"2024-08-26T15:53:50Z","title":"SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery","summary":"  In this paper, we address Generalized Category Discovery, aiming to\nsimultaneously uncover novel categories and accurately classify known ones.\nTraditional methods, which lean heavily on self-supervision and contrastive\nlearning, often fall short when distinguishing between fine-grained categories.\nTo address this, we introduce a novel concept called `self-expertise', which\nenhances the model's ability to recognize subtle differences and uncover\nunknown categories. Our approach combines unsupervised and supervised\nself-expertise strategies to refine the model's discernment and generalization.\nInitially, hierarchical pseudo-labeling is used to provide `soft supervision',\nimproving the effectiveness of self-expertise. Our supervised technique differs\nfrom traditional methods by utilizing more abstract positive and negative\nsamples, aiding in the formation of clusters that can generalize to novel\ncategories. Meanwhile, our unsupervised strategy encourages the model to\nsharpen its category distinctions by considering within-category examples as\n`hard' negatives. Supported by theoretical insights, our empirical results\nshowcase that our method outperforms existing state-of-the-art techniques in\nGeneralized Category Discovery across several fine-grained datasets. Our code\nis available at: https://github.com/SarahRastegar/SelEx.\n","authors":["Sarah Rastegar","Mohammadreza Salehi","Yuki M. Asano","Hazel Doughty","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2408.14371v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2310.07979v2","updated":"2024-08-26T15:51:38Z","published":"2023-10-12T01:57:27Z","title":"Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks","summary":"  Machine learning (ML) approaches are increasingly being used to accelerate\ncombinatorial optimization (CO) problems. We investigate the Set Cover Problem\n(SCP) and propose Graph-SCP, a graph neural network method that augments\nexisting optimization solvers by learning to identify a much smaller\nsub-problem that contains the solution space. Graph-SCP uses both supervised\nlearning from prior solved instances and unsupervised learning aimed at\nminimizing the SCP objective. We evaluate the performance of Graph-SCP on\nsynthetically weighted and unweighted SCP instances with diverse problem\ncharacteristics and complexities, and on instances from the OR Library, a\ncanonical benchmark for SCP. We show that Graph-SCP reduces the problem size by\n60-80% and achieves runtime speedups of up to 10x on average when compared to\nGurobi (a state-of-the-art commercial solver), while maintaining solution\nquality. This is in contrast to fast greedy solutions that significantly\ncompromise solution quality to achieve guaranteed polynomial runtime. We\nshowcase Graph-SCP's ability to generalize to larger problem sizes, training on\nSCP instances with up to 3,000 subsets and testing on SCP instances with up to\n10,000 subsets.\n","authors":["Zohair Shafi","Benjamin A. Miller","Tina Eliassi-Rad","Rajmonda S. Caceres"],"pdf_url":"https://arxiv.org/pdf/2310.07979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14369v1","updated":"2024-08-26T15:49:31Z","published":"2024-08-26T15:49:31Z","title":"Exploiting Conjugate Label Information for Multi-Instance Partial-Label\n  Learning","summary":"  Multi-instance partial-label learning (MIPL) addresses scenarios where each\ntraining sample is represented as a multi-instance bag associated with a\ncandidate label set containing one true label and several false positives.\nExisting MIPL algorithms have primarily focused on mapping multi-instance bags\nto candidate label sets for disambiguation, disregarding the intrinsic\nproperties of the label space and the supervised information provided by\nnon-candidate label sets. In this paper, we propose an algorithm named ELIMIPL,\ni.e., Exploiting conjugate Label Information for Multi-Instance Partial-Label\nlearning, which exploits the conjugate label information to improve the\ndisambiguation performance. To achieve this, we extract the label information\nembedded in both candidate and non-candidate label sets, incorporating the\nintrinsic properties of the label space. Experimental results obtained from\nbenchmark and real-world datasets demonstrate the superiority of the proposed\nELIMIPL over existing MIPL algorithms and other well-established partial-label\nlearning algorithms.\n","authors":["Wei Tang","Weijia Zhang","Min-Ling Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.14369v1.pdf","comment":"Accepted at IJCAI 2024. The code can be found at\n  https://github.com/tangw-seu/ELIMIPL"},{"id":"http://arxiv.org/abs/2408.14358v1","updated":"2024-08-26T15:32:31Z","published":"2024-08-26T15:32:31Z","title":"An Embedding is Worth a Thousand Noisy Labels","summary":"  The performance of deep neural networks scales with dataset size and label\nquality, rendering the efficient mitigation of low-quality data annotations\ncrucial for building robust and cost-effective systems. Existing strategies to\naddress label noise exhibit severe limitations due to computational complexity\nand application dependency. In this work, we propose WANN, a Weighted Adaptive\nNearest Neighbor approach that builds on self-supervised feature\nrepresentations obtained from foundation models. To guide the weighted voting\nscheme, we introduce a reliability score, which measures the likelihood of a\ndata label being correct. WANN outperforms reference methods, including a\nlinear layer trained with robust loss functions, on diverse datasets of varying\nsize and under various noise types and severities. WANN also exhibits superior\ngeneralization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed\nk-NNs. Furthermore, the proposed weighting scheme enhances supervised\ndimensionality reduction under noisy labels. This yields a significant boost in\nclassification performance with 10x and 100x smaller image embeddings,\nminimizing latency and storage requirements. Our approach, emphasizing\nefficiency and explainability, emerges as a simple, robust solution to overcome\nthe inherent limitations of deep neural network training. The code is available\nat https://github.com/francescodisalvo05/wann-noisy-labels .\n","authors":["Francesco Di Salvo","Sebastian Doerrich","Ines Rieger","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2408.14358v1.pdf","comment":"Preprint submitted to the International Journal of Computer Vision\n  (IJCV)"},{"id":"http://arxiv.org/abs/2408.14352v1","updated":"2024-08-26T15:29:34Z","published":"2024-08-26T15:29:34Z","title":"Assessing Contamination in Large Language Models: Introducing the\n  LogProber method","summary":"  In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities.\n","authors":["Nicolas Yax","Pierre-Yves Oudeyer","Stefano Palminteri"],"pdf_url":"https://arxiv.org/pdf/2408.14352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11126v2","updated":"2024-08-26T15:19:12Z","published":"2024-08-20T18:26:09Z","title":"Binocular Model: A deep learning solution for online melt pool\n  temperature analysis using dual-wavelength Imaging Pyrometry","summary":"  In metal Additive Manufacturing (AM), monitoring the temperature of the Melt\nPool (MP) is crucial for ensuring part quality, process stability, defect\nprevention, and overall process optimization. Traditional methods, are slow to\nconverge and require extensive manual effort to translate data into actionable\ninsights, rendering them impractical for real-time monitoring and control. To\naddress this challenge, we propose an Artificial Intelligence (AI)-based\nsolution aimed at reducing manual data processing reliance and improving the\nefficiency of transitioning from data to insight. In our study, we utilize a\ndataset comprising dual-wavelength real-time process monitoring data and\ncorresponding temperature maps. We introduce a deep learning model called the\n\"Binocular model,\" which exploits dual input observations to perform a precise\nanalysis of MP temperature in Laser Powder Bed Fusion (L-PBF). Through advanced\ndeep learning techniques, we seamlessly convert raw data into temperature maps,\nsignificantly streamlining the process and enabling batch processing at a rate\nof up to 750 frames per second, approximately 1000 times faster than\nconventional methods. Our Binocular model achieves high accuracy in temperature\nestimation, evidenced by a 0.95 R-squared score, while simultaneously enhancing\nprocessing efficiency by a factor of $\\sim1000x$ times. This model directly\naddresses the challenge of real-time MP temperature monitoring and offers\ninsights into the encountered constraints and the benefits of our Deep\nLearning-based approach. By combining efficiency and precision, our work\ncontributes to the advancement of temperature monitoring in L-PBF, thus driving\nprogress in the field of metal AM.\n","authors":["Javid Akhavan","Chaitanya Krishna Vallabh","Xiayun Zhao","Souran Manoochehri"],"pdf_url":"https://arxiv.org/pdf/2408.11126v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04522v3","updated":"2024-08-26T15:13:22Z","published":"2024-07-05T14:07:15Z","title":"Graph Reinforcement Learning for Power Grids: A Comprehensive Survey","summary":"  The rise of renewable energy and distributed generation requires new\napproaches to overcome the limitations of traditional methods. In this context,\nGraph Neural Networks are promising due to their ability to learn from\ngraph-structured data. Combined with Reinforcement Learning, they can serve as\ncontrol approaches to determine remedial network actions. This review analyses\nhow Graph Reinforcement Learning (GRL) can improve representation learning and\ndecision making in power grid use cases. Although GRL has demonstrated\nadaptability to unpredictable events and noisy data, it is primarily at a\nproof-of-concept stage. We highlight open challenges and limitations with\nrespect to real-world applications.\n","authors":["Mohamed Hassouna","Clara HolzhÃ¼ter","Pawel Lytaev","Josephine Thomas","Bernhard Sick","Christoph Scholz"],"pdf_url":"https://arxiv.org/pdf/2407.04522v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14340v1","updated":"2024-08-26T15:13:14Z","published":"2024-08-26T15:13:14Z","title":"Foundation Models for Music: A Survey","summary":"  In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.\n","authors":["Yinghao Ma","Anders Ãland","Anton Ragni","Bleiz MacSen Del Sette","Charalampos Saitis","Chris Donahue","Chenghua Lin","Christos Plachouras","Emmanouil Benetos","Elio Quinton","Elona Shatri","Fabio Morreale","Ge Zhang","GyÃ¶rgy Fazekas","Gus Xia","Huan Zhang","Ilaria Manco","Jiawen Huang","Julien Guinot","Liwei Lin","Luca Marinelli","Max W. Y. Lam","Megha Sharma","Qiuqiang Kong","Roger B. Dannenberg","Ruibin Yuan","Shangda Wu","Shih-Lun Wu","Shuqi Dai","Shun Lei","Shiyin Kang","Simon Dixon","Wenhu Chen","Wehhao Huang","Xingjian Du","Xingwei Qu","Xu Tan","Yizhi Li","Zeyue Tian","Zhiyong Wu","Zhizheng Wu","Ziyang Ma","Ziyu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14338v1","updated":"2024-08-26T15:07:35Z","published":"2024-08-26T15:07:35Z","title":"Machine Learning for Quantifier Selection in cvc5","summary":"  In this work we considerably improve the state-of-the-art SMT solving on\nfirst-order quantified problems by efficient machine learning guidance of\nquantifier selection. Quantifiers represent a significant challenge for SMT and\nare technically a source of undecidability. In our approach, we train an\nefficient machine learning model that informs the solver which quantifiers\nshould be instantiated and which not. Each quantifier may be instantiated\nmultiple times and the set of the active quantifiers changes as the solving\nprogresses. Therefore, we invoke the ML predictor many times, during the whole\nrun of the solver. To make this efficient, we use fast ML models based on\ngradient boosting decision trees. We integrate our approach into the\nstate-of-the-art cvc5 SMT solver and show a considerable increase of the\nsystem's holdout-set performance after training it on a large set of\nfirst-order problems collected from the Mizar Mathematical Library.\n","authors":["Jan JakubÅ¯v","MikolÃ¡Å¡ Janota","Jelle Piepenbrock","Josef Urban"],"pdf_url":"https://arxiv.org/pdf/2408.14338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14332v1","updated":"2024-08-26T15:01:04Z","published":"2024-08-26T15:01:04Z","title":"One-layer transformers fail to solve the induction heads task","summary":"  A simple communication complexity argument proves that no one-layer\ntransformer can solve the induction heads task unless its size is exponentially\nlarger than the size sufficient for a two-layer transformer.\n","authors":["Clayton Sanford","Daniel Hsu","Matus Telgarsky"],"pdf_url":"https://arxiv.org/pdf/2408.14332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16528v2","updated":"2024-08-26T14:59:53Z","published":"2024-05-26T11:29:57Z","title":"LoQT: Low Rank Adapters for Quantized Training","summary":"  Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware.\n","authors":["Sebastian Loeschcke","Mads Toftrup","Michael J. Kastoryano","Serge Belongie","VÃ©steinn SnÃ¦bjarnarson"],"pdf_url":"https://arxiv.org/pdf/2405.16528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14331v1","updated":"2024-08-26T14:55:40Z","published":"2024-08-26T14:55:40Z","title":"Automated Machine Learning in Insurance","summary":"  Machine Learning (ML) has gained popularity in actuarial research and\ninsurance industrial applications. However, the performance of most ML tasks\nheavily depends on data preprocessing, model selection, and hyperparameter\noptimization, which are considered to be intensive in terms of domain\nknowledge, experience, and manual labor. Automated Machine Learning (AutoML)\naims to automatically complete the full life-cycle of ML tasks and provides\nstate-of-the-art ML models without human intervention or supervision. This\npaper introduces an AutoML workflow that allows users without domain knowledge\nor prior experience to achieve robust and effortless ML deployment by writing\nonly a few lines of code. This proposed AutoML is specifically tailored for the\ninsurance application, with features like the balancing step in data\npreprocessing, ensemble pipelines, and customized loss functions. These\nfeatures are designed to address the unique challenges of the insurance domain,\nincluding the imbalanced nature of common insurance datasets. The full code and\ndocumentation are available on the GitHub repository.\n(https://github.com/PanyiDong/InsurAutoML)\n","authors":["Panyi Dong","Zhiyu Quan"],"pdf_url":"https://arxiv.org/pdf/2408.14331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14326v1","updated":"2024-08-26T14:54:14Z","published":"2024-08-26T14:54:14Z","title":"Streamline tractography of the fetal brain in utero with machine\n  learning","summary":"  Diffusion-weighted magnetic resonance imaging (dMRI) is the only non-invasive\ntool for studying white matter tracts and structural connectivity of the brain.\nThese assessments rely heavily on tractography techniques, which reconstruct\nvirtual streamlines representing white matter fibers. Much effort has been\ndevoted to improving tractography methodology for adult brains, while\ntractography of the fetal brain has been largely neglected. Fetal tractography\nfaces unique difficulties due to low dMRI signal quality, immature and rapidly\ndeveloping brain structures, and paucity of reference data. This work presents\nthe first machine learning model for fetal tractography. The model input\nconsists of five sources of information: (1) Fiber orientation, inferred from a\ndiffusion tensor fit to the dMRI signal; (2) Directions of recent propagation\nsteps; (3) Global spatial information, encoded as distances to keypoints in the\nbrain cortex; (4) Tissue segmentation information; and (5) Prior information\nabout the expected local fiber orientations supplied with an atlas. In order to\nmitigate the local tensor estimation error, a large spatial context around the\ncurrent point in the diffusion tensor image is encoded using convolutional and\nattention neural network modules. Moreover, the diffusion tensor information at\na hypothetical next point is included in the model input. Filtering rules based\non anatomically constrained tractography are applied to prune implausible\nstreamlines. We trained the model on manually-refined whole-brain fetal\ntractograms and validated the trained model on an independent set of 11 test\nscans with gestational ages between 23 and 36 weeks. Results show that our\nproposed method achieves superior performance across all evaluated tracts. The\nnew method can significantly advance the capabilities of dMRI for studying\nnormal and abnormal brain development in utero.\n","authors":["Weide Liu","Camilo Calixto","Simon K. Warfield","Davood Karimi"],"pdf_url":"https://arxiv.org/pdf/2408.14326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14325v1","updated":"2024-08-26T14:54:13Z","published":"2024-08-26T14:54:13Z","title":"Function-Space MCMC for Bayesian Wide Neural Networks","summary":"  Bayesian Neural Networks represent a fascinating confluence of deep learning\nand probabilistic reasoning, offering a compelling framework for understanding\nuncertainty in complex predictive models. In this paper, we investigate the use\nof the preconditioned Crank-Nicolson algorithm and its Langevin version to\nsample from the reparametrised posterior distribution of the weights as the\nwidths of Bayesian Neural Networks grow larger. In addition to being robust in\nthe infinite-dimensional setting, we prove that the acceptance probabilities of\nthe proposed methods approach 1 as the width of the network increases,\nindependently of any stepsize tuning. Moreover, we examine and compare how the\nmixing speeds of the underdamped Langevin Monte Carlo, the preconditioned\nCrank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are\ninfluenced by changes in the network width in some real-world cases. Our\nfindings suggest that, in wide Bayesian Neural Networks configurations, the\npreconditioned Crank-Nicolson method allows for more efficient sampling of the\nreparametrised posterior distribution, as evidenced by a higher effective\nsample size and improved diagnostic results compared with the other analysed\nalgorithms.\n","authors":["Lucia Pezzetti","Stefano Favaro","Stefano Pelucchetti"],"pdf_url":"https://arxiv.org/pdf/2408.14325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14319v1","updated":"2024-08-26T14:51:26Z","published":"2024-08-26T14:51:26Z","title":"Rethinking Knowledge Transfer in Learning Using Privileged Information","summary":"  In supervised machine learning, privileged information (PI) is information\nthat is unavailable at inference, but is accessible during training time.\nResearch on learning using privileged information (LUPI) aims to transfer the\nknowledge captured in PI onto a model that can perform inference without PI. It\nseems that this extra bit of information ought to make the resulting model\nbetter. However, finding conclusive theoretical or empirical evidence that\nsupports the ability to transfer knowledge using PI has been challenging. In\nthis paper, we critically examine the assumptions underlying existing\ntheoretical analyses and argue that there is little theoretical justification\nfor when LUPI should work. We analyze LUPI methods and reveal that apparent\nimprovements in empirical risk of existing research may not directly result\nfrom PI. Instead, these improvements often stem from dataset anomalies or\nmodifications in model design misguidedly attributed to PI. Our experiments for\na wide variety of application domains further demonstrate that state-of-the-art\nLUPI approaches fail to effectively transfer knowledge from PI. Thus, we\nadvocate for practitioners to exercise caution when working with PI to avoid\nunintended inductive biases.\n","authors":["Danil Provodin","Bram van den Akker","Christina Katsimerou","Maurits Kaptein","Mykola Pechenizkiy"],"pdf_url":"https://arxiv.org/pdf/2408.14319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07437v3","updated":"2024-08-26T14:46:08Z","published":"2023-02-15T02:58:09Z","title":"Bridging the Usability Gap: Theoretical and Methodological Advances for\n  Spectral Learning of Hidden Markov Models","summary":"  The Baum-Welch (B-W) algorithm is the most widely accepted method for\ninferring hidden Markov models (HMM). However, it is prone to getting stuck in\nlocal optima, and can be too slow for many real-time applications. Spectral\nlearning of HMMs (SHMM), based on the method of moments (MOM) has been proposed\nin the literature to overcome these obstacles. Despite its promises, asymptotic\ntheory for SHMM has been elusive, and the long-run performance of SHMM can\ndegrade due to unchecked propagation of error. In this paper, we (1) provide an\nasymptotic distribution for the approximate error of the likelihood estimated\nby SHMM, (2) propose a novel algorithm called projected SHMM (PSHMM) that\nmitigates the problem of error propagation, and (3) develop online learning\nvariants of both SHMM and PSHMM that accommodate potential nonstationarity. We\ncompare the performance of SHMM with PSHMM and estimation through the B-W\nalgorithm on both simulated data and data from real world applications, and\nfind that PSHMM not only retains the computational advantages of SHMM, but also\nprovides more robust estimation and forecasting.\n","authors":["Xiaoyuan Ma","Jordan Rodu"],"pdf_url":"https://arxiv.org/pdf/2302.07437v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14307v1","updated":"2024-08-26T14:38:19Z","published":"2024-08-26T14:38:19Z","title":"LLM-3D Print: Large Language Models To Monitor and Control 3D Printing","summary":"  Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention.\n","authors":["Yayati Jadhav","Peter Pak","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2408.14307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14284v1","updated":"2024-08-26T14:09:40Z","published":"2024-08-26T14:09:40Z","title":"May the Forgetting Be with You: Alternate Replay for Learning with Noisy\n  Labels","summary":"  Forgetting presents a significant challenge during incremental training,\nmaking it particularly demanding for contemporary AI systems to assimilate new\nknowledge in streaming data environments. To address this issue, most\napproaches in Continual Learning (CL) rely on the replay of a restricted buffer\nof past data. However, the presence of noise in real-world scenarios, where\nhuman annotation is constrained by time limitations or where data is\nautomatically gathered from the web, frequently renders these strategies\nvulnerable. In this study, we address the problem of CL under Noisy Labels\n(CLN) by introducing Alternate Experience Replay (AER), which takes advantage\nof forgetting to maintain a clear distinction between clean, complex, and noisy\nsamples in the memory buffer. The idea is that complex or mislabeled examples,\nwhich hardly fit the previously learned data distribution, are most likely to\nbe forgotten. To grasp the benefits of such a separation, we equip AER with\nAsymmetric Balanced Sampling (ABS): a new sample selection strategy that\nprioritizes purity on the current task while retaining relevant samples from\nthe past. Through extensive computational comparisons, we demonstrate the\neffectiveness of our approach in terms of both accuracy and purity of the\nobtained buffer, resulting in a remarkable average gain of 4.71% points in\naccuracy with respect to existing loss-based purification strategies. Code is\navailable at https://github.com/aimagelab/mammoth.\n","authors":["Monica Millunzi","Lorenzo Bonicelli","Angelo Porrello","Jacopo Credi","Petter N. Kolm","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2408.14284v1.pdf","comment":"25 pages, 5 figures. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2305.07715v2","updated":"2024-08-26T14:09:37Z","published":"2023-05-12T18:14:21Z","title":"Field theory for optimal signal propagation in ResNets","summary":"  Residual networks have significantly better trainability and thus performance\nthan feed-forward networks at large depth. Introducing skip connections\nfacilitates signal propagation to deeper layers. In addition, previous works\nfound that adding a scaling parameter for the residual branch further improves\ngeneralization performance. While they empirically identified a particularly\nbeneficial range of values for this scaling parameter, the associated\nperformance improvement and its universality across network hyperparameters yet\nneed to be understood. For feed-forward networks, finite-size theories have led\nto important insights with regard to signal propagation and hyperparameter\ntuning. We here derive a systematic finite-size field theory for residual\nnetworks to study signal propagation and its dependence on the scaling for the\nresidual branch. We derive analytical expressions for the response function, a\nmeasure for the network's sensitivity to inputs, and show that for deep\nnetworks the empirically found values for the scaling parameter lie within the\nrange of maximal sensitivity. Furthermore, we obtain an analytical expression\nfor the optimal scaling parameter that depends only weakly on other network\nhyperparameters, such as the weight variance, thereby explaining its\nuniversality across hyperparameters. Overall, this work provides a theoretical\nframework to study ResNets at finite size.\n","authors":["Kirsten Fischer","David Dahmen","Moritz Helias"],"pdf_url":"https://arxiv.org/pdf/2305.07715v2.pdf","comment":"21 pages, 8 figures, under review"},{"id":"http://arxiv.org/abs/2408.12615v2","updated":"2024-08-26T14:06:59Z","published":"2024-08-08T14:11:06Z","title":"Pediatric TSC-Related Epilepsy Classification from Clinical MR Images\n  Using Quantum Neural Network","summary":"  Tuberous sclerosis complex (TSC) manifests as a multisystem disorder with\nsignificant neurological implications. This study addresses the critical need\nfor robust classification models tailored to TSC in pediatric patients,\nintroducing QResNet,a novel deep learning model seamlessly integrating\nconventional convolutional neural networks with quantum neural networks. The\nmodel incorporates a two-layer quantum layer (QL), comprising ZZFeatureMap and\nAnsatz layers, strategically designed for processing classical data within a\nquantum framework. A comprehensive evaluation, demonstrates the superior\nperformance of QResNet in TSC MRI image classification compared to conventional\n3D-ResNet models. These compelling findings underscore the potential of quantum\ncomputing to revolutionize medical imaging and diagnostics.Remarkably, this\nmethod surpasses conventional CNNs in accuracy and Area Under the Curve (AUC)\nmetrics with the current dataset. Future research endeavors may focus on\nexploring the scalability and practical implementation of quantum algorithms in\nreal-world medical imaging scenarios.\n","authors":["Ling Lin","Yihang Zhou","Zhanqi Hu","Dian Jiang","Congcong Liu","Shuo Zhou","Yanjie Zhu","Jianxiang Liao","Dong Liang","Hairong Zheng","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.12615v2.pdf","comment":"5 pages,4 figures,2 tables,presented at ISBI 2024"},{"id":"http://arxiv.org/abs/2408.14281v1","updated":"2024-08-26T14:02:30Z","published":"2024-08-26T14:02:30Z","title":"Uncertainties of Latent Representations in Computer Vision","summary":"  Uncertainty quantification is a key pillar of trustworthy machine learning.\nIt enables safe reactions under unsafe inputs, like predicting only when the\nmachine learning model detects sufficient evidence, discarding anomalous data,\nor emitting warnings when an error is likely to be inbound. This is\nparticularly crucial in safety-critical areas like medical image classification\nor self-driving cars. Despite the plethora of proposed uncertainty\nquantification methods achieving increasingly higher scores on performance\nbenchmarks, uncertainty estimates are often shied away from in practice. Many\nmachine learning projects start from pretrained latent representations that\ncome without uncertainty estimates. Uncertainties would need to be trained by\npractitioners on their own, which is notoriously difficult and\nresource-intense.\n  This thesis makes uncertainty estimates easily accessible by adding them to\nthe latent representation vectors of pretrained computer vision models. Besides\nproposing approaches rooted in probability and decision theory, such as\nMonte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both\ntheoretical and empirical questions. We show that these unobservable\nuncertainties about unobservable latent representations are indeed provably\ncorrect. We also provide an uncertainty-aware representation learning (URL)\nbenchmark to compare these unobservables against observable ground-truths.\nFinally, we compile our findings to pretrain lightweight representation\nuncertainties on large-scale computer vision models that transfer to unseen\ndatasets in a zero-shot manner.\n  Our findings do not only advance the current theoretical understanding of\nuncertainties over latent variables, but also facilitate the access to\nuncertainty quantification for future researchers inside and outside the field,\nenabling straightforward but trustworthy machine learning.\n","authors":["Michael Kirchhof"],"pdf_url":"https://arxiv.org/pdf/2408.14281v1.pdf","comment":"Doctoral thesis"},{"id":"http://arxiv.org/abs/2312.01210v4","updated":"2024-08-26T13:57:31Z","published":"2023-12-02T19:39:50Z","title":"When accurate prediction models yield harmful self-fulfilling prophecies","summary":"  Prediction models are popular in medical research and practice. By predicting\nan outcome of interest for specific patients, these models may help inform\ndifficult treatment decisions, and are often hailed as the poster children for\npersonalized, data-driven healthcare. We show however, that using prediction\nmodels for decision making can lead to harmful decisions, even when the\npredictions exhibit good discrimination after deployment. These models are\nharmful self-fulfilling prophecies: their deployment harms a group of patients\nbut the worse outcome of these patients does not invalidate the predictive\npower of the model. Our main result is a formal characterization of a set of\nsuch prediction models. Next we show that models that are well calibrated\nbefore and after deployment are useless for decision making as they made no\nchange in the data distribution. These results point to the need to revise\nstandard practices for validation, deployment and evaluation of prediction\nmodels that are used in medical decisions.\n","authors":["Wouter A. C. van Amsterdam","Nan van Geloven","Jesse H. Krijthe","Rajesh Ranganath","Giovanni CinÃ¡"],"pdf_url":"https://arxiv.org/pdf/2312.01210v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09131v2","updated":"2024-08-26T13:48:32Z","published":"2024-06-13T14:02:18Z","title":"OLGA: One-cLass Graph Autoencoder","summary":"  One-class learning (OCL) comprises a set of techniques applied when\nreal-world problems have a single class of interest. The usual procedure for\nOCL is learning a hypersphere that comprises instances of this class and,\nideally, repels unseen instances from any other classes. Besides, several OCL\nalgorithms for graphs have been proposed since graph representation learning\nhas succeeded in various fields. These methods may use a two-step strategy,\ninitially representing the graph and, in a second step, classifying its nodes.\nOn the other hand, end-to-end methods learn the node representations while\nclassifying the nodes in one learning process. We highlight three main gaps in\nthe literature on OCL for graphs: (i) non-customized representations for OCL;\n(ii) the lack of constraints on hypersphere parameters learning; and (iii) the\nmethods' lack of interpretability and visualization. We propose One-cLass Graph\nAutoencoder (OLGA). OLGA is end-to-end and learns the representations for the\ngraph nodes while encapsulating the interest instances by combining two loss\nfunctions. We propose a new hypersphere loss function to encapsulate the\ninterest instances. OLGA combines this new hypersphere loss with the graph\nautoencoder reconstruction loss to improve model learning. OLGA achieved\nstate-of-the-art results and outperformed six other methods with a\nstatistically significant difference from five methods. Moreover, OLGA learns\nlow-dimensional representations maintaining the classification performance with\nan interpretable model representation learning and results.\n","authors":["M. P. S. GÃ´lo","J. G. B. M. Junior","D. F. Silva","R. M. Marcacini"],"pdf_url":"https://arxiv.org/pdf/2406.09131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07182v7","updated":"2024-08-26T13:43:46Z","published":"2022-10-13T17:03:36Z","title":"PDEBENCH: An Extensive Benchmark for Scientific Machine Learning","summary":"  Machine learning-based modeling of physical systems has experienced increased\ninterest in recent years. Despite some impressive progress, there is still a\nlack of benchmarks for Scientific ML that are easy to use but still challenging\nand representative of a wide range of problems. We introduce PDEBench, a\nbenchmark suite of time-dependent simulation tasks based on Partial\nDifferential Equations (PDEs). PDEBench comprises both code and data to\nbenchmark the performance of novel machine learning models against both\nclassical numerical simulations and machine learning baselines. Our proposed\nset of benchmark problems contribute the following unique features: (1) A much\nwider range of PDEs compared to existing benchmarks, ranging from relatively\ncommon examples to more realistic and difficult problems; (2) much larger\nready-to-use datasets compared to prior work, comprising multiple simulation\nruns across a larger number of initial and boundary conditions and PDE\nparameters; (3) more extensible source codes with user-friendly APIs for data\ngeneration and baseline results with popular machine learning models (FNO,\nU-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to\nextend the benchmark freely for their own purposes using a standardized API and\nto compare the performance of new models to existing baseline methods. We also\npropose new evaluation metrics with the aim to provide a more holistic\nunderstanding of learning methods in the context of Scientific ML. With those\nmetrics we identify tasks which are challenging for recent ML methods and\npropose these tasks as future challenges for the community. The code is\navailable at https://github.com/pdebench/PDEBench.\n","authors":["Makoto Takamoto","Timothy Praditia","Raphael Leiteritz","Dan MacKinlay","Francesco Alesiani","Dirk PflÃ¼ger","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2210.07182v7.pdf","comment":"16 pages (main body) + 34 pages (supplemental material), accepted for\n  publication in NeurIPS 2022 Track Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2408.14267v1","updated":"2024-08-26T13:42:43Z","published":"2024-08-26T13:42:43Z","title":"1-Bit FQT: Pushing the Limit of Fully Quantized Training to 1-bit","summary":"  Fully quantized training (FQT) accelerates the training of deep neural\nnetworks by quantizing the activations, weights, and gradients into lower\nprecision. To explore the ultimate limit of FQT (the lowest achievable\nprecision), we make a first attempt to 1-bit FQT. We provide a theoretical\nanalysis of FQT based on Adam and SGD, revealing that the gradient variance\ninfluences the convergence of FQT. Building on these theoretical results, we\nintroduce an Activation Gradient Pruning (AGP) strategy. The strategy leverages\nthe heterogeneity of gradients by pruning less informative gradients and\nenhancing the numerical precision of remaining gradients to mitigate gradient\nvariance. Additionally, we propose Sample Channel joint Quantization (SCQ),\nwhich utilizes different quantization strategies in the computation of weight\ngradients and activation gradients to ensure that the method is friendly to\nlow-bitwidth hardware. Finally, we present a framework to deploy our algorithm.\nFor fine-tuning VGGNet-16 and ResNet-18 on multiple datasets, our algorithm\nachieves an average accuracy improvement of approximately 6%, compared to\nper-sample quantization. Moreover, our training speedup can reach a maximum of\n5.13x compared to full precision training.\n","authors":["Chang Gao","Jianfei Chen","Kang Zhao","Jiaqi Wang","Liping Jing"],"pdf_url":"https://arxiv.org/pdf/2408.14267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14266v1","updated":"2024-08-26T13:40:33Z","published":"2024-08-26T13:40:33Z","title":"HyperSBINN: A Hypernetwork-Enhanced Systems Biology-Informed Neural\n  Network for Efficient Drug Cardiosafety Assessment","summary":"  Mathematical modeling in systems toxicology enables a comprehensive\nunderstanding of the effects of pharmaceutical substances on cardiac health.\nHowever, the complexity of these models limits their widespread application in\nearly drug discovery. In this paper, we introduce a novel approach to solving\nparameterized models of cardiac action potentials by combining meta-learning\ntechniques with Systems Biology-Informed Neural Networks (SBINNs). The proposed\nmethod, HyperSBINN, effectively addresses the challenge of predicting the\neffects of various compounds at different concentrations on cardiac action\npotentials, outperforming traditional differential equation solvers in speed.\nOur model efficiently handles scenarios with limited data and complex\nparameterized differential equations. The HyperSBINN model demonstrates robust\nperformance in predicting APD90 values, indicating its potential as a reliable\ntool for modeling cardiac electrophysiology and aiding in preclinical drug\ndevelopment. This framework represents an advancement in computational\nmodeling, offering a scalable and efficient solution for simulating and\nunderstanding complex biological systems.\n","authors":["Inass Soukarieh","Gerhard Hessler","HervÃ© Minoux","Marcel Mohr","Friedemann Schmidt","Jan Wenzel","Pierre Barbillon","Hugo Gangloff","Pierre Gloaguen"],"pdf_url":"https://arxiv.org/pdf/2408.14266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14254v1","updated":"2024-08-26T13:16:42Z","published":"2024-08-26T13:16:42Z","title":"Integrated Brain Connectivity Analysis with fMRI, DTI, and sMRI Powered\n  by Interpretable Graph Neural Networks","summary":"  Multimodal neuroimaging modeling has becomes a widely used approach but\nconfronts considerable challenges due to heterogeneity, which encompasses\nvariability in data types, scales, and formats across modalities. This\nvariability necessitates the deployment of advanced computational methods to\nintegrate and interpret these diverse datasets within a cohesive analytical\nframework. In our research, we amalgamate functional magnetic resonance\nimaging, diffusion tensor imaging, and structural MRI into a cohesive\nframework. This integration capitalizes on the unique strengths of each\nmodality and their inherent interconnections, aiming for a comprehensive\nunderstanding of the brain's connectivity and anatomical characteristics.\nUtilizing the Glasser atlas for parcellation, we integrate imaging derived\nfeatures from various modalities: functional connectivity from fMRI, structural\nconnectivity from DTI, and anatomical features from sMRI within consistent\nregions. Our approach incorporates a masking strategy to differentially weight\nneural connections, thereby facilitating a holistic amalgamation of multimodal\nimaging data. This technique enhances interpretability at connectivity level,\ntranscending traditional analyses centered on singular regional attributes. The\nmodel is applied to the Human Connectome Project's Development study to\nelucidate the associations between multimodal imaging and cognitive functions\nthroughout youth. The analysis demonstrates improved predictive accuracy and\nuncovers crucial anatomical features and essential neural connections,\ndeepening our understanding of brain structure and function.\n","authors":["Gang Qu","Ziyu Zhou","Vince D. Calhoun","Aiying Zhang","Yu-Ping Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14252v1","updated":"2024-08-26T13:14:26Z","published":"2024-08-26T13:14:26Z","title":"An Evaluation of Explanation Methods for Black-Box Detectors of\n  Machine-Generated Text","summary":"  The increasing difficulty to distinguish language-model-generated from\nhuman-written text has led to the development of detectors of machine-generated\ntext (MGT). However, in many contexts, a black-box prediction is not\nsufficient, it is equally important to know on what grounds a detector made\nthat prediction. Explanation methods that estimate feature importance promise\nto provide indications of which parts of an input are used by classifiers for\nprediction. However, the quality of different explanation methods has not\npreviously been assessed for detectors of MGT. This study conducts the first\nsystematic evaluation of explanation quality for this task. The dimensions of\nfaithfulness and stability are assessed with five automated experiments, and\nusefulness is evaluated in a user study. We use a dataset of ChatGPT-generated\nand human-written documents, and pair predictions of three existing\nlanguage-model-based detectors with the corresponding SHAP, LIME, and Anchor\nexplanations. We find that SHAP performs best in terms of faithfulness,\nstability, and in helping users to predict the detector's behavior. In\ncontrast, LIME, perceived as most useful by users, scores the worst in terms of\nuser performance at predicting the detectors' behavior.\n","authors":["Loris Schoenegger","Yuxi Xia","Benjamin Roth"],"pdf_url":"https://arxiv.org/pdf/2408.14252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03816v2","updated":"2024-08-26T13:12:45Z","published":"2024-08-07T14:52:06Z","title":"Early Prediction of Causes (not Effects) in Healthcare by Long-Term\n  Clinical Time Series Forecasting","summary":"  Machine learning for early syndrome diagnosis aims to solve the intricate\ntask of predicting a ground truth label that most often is the outcome (effect)\nof a medical consensus definition applied to observed clinical measurements\n(causes), given clinical measurements observed several hours before. Instead of\nfocusing on the prediction of the future effect, we propose to directly predict\nthe causes via time series forecasting (TSF) of clinical variables and\ndetermine the effect by applying the gold standard consensus definition to the\nforecasted values. This method has the invaluable advantage of being\nstraightforwardly interpretable to clinical practitioners, and because model\ntraining does not rely on a particular label anymore, the forecasted data can\nbe used to predict any consensus-based label. We exemplify our method by means\nof long-term TSF with Transformer models, with a focus on accurate prediction\nof sparse clinical variables involved in the SOFA-based Sepsis-3 definition and\nthe new Simplified Acute Physiology Score (SAPS-II) definition. Our experiments\nare conducted on two datasets and show that contrary to recent proposals which\nadvocate set function encoders for time series and direct multi-step decoders,\nbest results are achieved by a combination of standard dense encoders with\niterative multi-step decoders. The key for success of iterative multi-step\ndecoding can be attributed to its ability to capture cross-variate dependencies\nand to a student forcing training strategy that teaches the model to rely on\nits own previous time step predictions for the next time step prediction.\n","authors":["Michael Staniek","Marius Fracarolli","Michael Hagmann","Stefan Riezler"],"pdf_url":"https://arxiv.org/pdf/2408.03816v2.pdf","comment":"Published at Machine Learning for Healthcare (MLHC), Toronto, 2024"},{"id":"http://arxiv.org/abs/2408.12658v2","updated":"2024-08-26T13:02:46Z","published":"2024-08-22T18:04:29Z","title":"Hierarchical Generative Modeling of Melodic Vocal Contours in Hindustani\n  Classical Music","summary":"  Hindustani music is a performance-driven oral tradition that exhibits the\nrendition of rich melodic patterns. In this paper, we focus on generative\nmodeling of singers' vocal melodies extracted from audio recordings, as the\nvoice is musically prominent within the tradition. Prior generative work in\nHindustani music models melodies as coarse discrete symbols which fails to\ncapture the rich expressive melodic intricacies of singing. Thus, we propose to\nuse a finely quantized pitch contour, as an intermediate representation for\nhierarchical audio modeling. We propose GaMaDHaNi, a modular two-level\nhierarchy, consisting of a generative model on pitch contours, and a pitch\ncontour to audio synthesis model. We compare our approach to non-hierarchical\naudio models and hierarchical models that use a self-supervised intermediate\nrepresentation, through a listening test and qualitative analysis. We also\nevaluate audio model's ability to faithfully represent the pitch contour input\nusing Pearson correlation coefficient. By using pitch contours as an\nintermediate representation, we show that our model may be better equipped to\nlisten and respond to musicians in a human-AI collaborative setting by\nhighlighting two potential interaction use cases (1) primed generation, and (2)\ncoarse pitch conditioning.\n","authors":["Nithya Shikarpur","Krishna Maneesha Dendukuri","Yusong Wu","Antoine Caillon","Cheng-Zhi Anna Huang"],"pdf_url":"https://arxiv.org/pdf/2408.12658v2.pdf","comment":"Accepted at International Society for Music Information Retrieval\n  (ISMIR) 2024"},{"id":"http://arxiv.org/abs/2408.14236v1","updated":"2024-08-26T12:50:27Z","published":"2024-08-26T12:50:27Z","title":"DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for\n  type classification","summary":"  We introduce semantic towers, an extrinsic knowledge representation method,\nand compare it to intrinsic knowledge in large language models for ontology\nlearning. Our experiments show a trade-off between performance and semantic\ngrounding for extrinsic knowledge compared to a fine-tuned model intrinsic\nknowledge. We report our findings on the Large Language Models for Ontology\nLearning (LLMs4OL) 2024 challenge.\n","authors":["Hanna Abi Akl"],"pdf_url":"https://arxiv.org/pdf/2408.14236v1.pdf","comment":"8 pages, 4 figures, accepted for the LLMs4OL challenge at the\n  International Semantic Web Conference (ISWC) 2024"},{"id":"http://arxiv.org/abs/2408.14234v1","updated":"2024-08-26T12:49:41Z","published":"2024-08-26T12:49:41Z","title":"FSDEM: Feature Selection Dynamic Evaluation Metric","summary":"  Expressive evaluation metrics are indispensable for informative experiments\nin all areas, and while several metrics are established in some areas, in\nothers, such as feature selection, only indirect or otherwise limited\nevaluation metrics are found. In this paper, we propose a novel evaluation\nmetric to address several problems of its predecessors and allow for flexible\nand reliable evaluation of feature selection algorithms. The proposed metric is\na dynamic metric with two properties that can be used to evaluate both the\nperformance and the stability of a feature selection algorithm. We conduct\nseveral empirical experiments to illustrate the use of the proposed metric in\nthe successful evaluation of feature selection algorithms. We also provide a\ncomparison and analysis to show the different aspects involved in the\nevaluation of the feature selection algorithms. The results indicate that the\nproposed metric is successful in carrying out the evaluation task for feature\nselection algorithms.\n  This paper is an extended version of a paper accepted at SISAP 2024.\n","authors":["Muhammad Rajabinasab","Anton D. Lautrup","Tobias Hyrup","Arthur Zimek"],"pdf_url":"https://arxiv.org/pdf/2408.14234v1.pdf","comment":"Short version of this paper is accepted at 17th International\n  Conference on Similarity Search and Applications, SISAP 2024"},{"id":"http://arxiv.org/abs/2408.14229v1","updated":"2024-08-26T12:44:17Z","published":"2024-08-26T12:44:17Z","title":"Gallery-Aware Uncertainty Estimation For Open-Set Face Recognition","summary":"  Accurately estimating image quality and model robustness improvement are\ncritical challenges in unconstrained face recognition, which can be addressed\nthrough uncertainty estimation via probabilistic face embeddings. Previous\nresearch mainly focused on uncertainty estimation in face verification, leaving\nthe open-set face recognition task underexplored. In open-set face recognition,\none seeks to classify an image, which could also be unknown. Here, the low\nvariance of probabilistic embedding does not imply a low error probability: an\nimage embedding could be close to several classes in a gallery, thus yielding\nhigh uncertainty. We propose a method aware of two sources of ambiguity in the\nopen-set recognition system: (1) the gallery uncertainty caused by overlapping\nclasses and (2) the uncertainty of the face embeddings. To detect both types,\nwe use a Bayesian probabilistic model of embedding distribution, which provides\na principled uncertainty estimate. Challenging open-set face recognition\ndatasets, such as IJB-C, serve as a testbed for our method. We also propose a\nnew open-set recognition protocol for whale and dolphin identification. The\nproposed approach better identifies recognition errors than uncertainty\nestimation methods based solely on image quality.\n","authors":["Leonid Erlygin","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2408.14229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14225v1","updated":"2024-08-26T12:41:41Z","published":"2024-08-26T12:41:41Z","title":"Provable Imbalanced Point Clustering","summary":"  We suggest efficient and provable methods to compute an approximation for\nimbalanced point clustering, that is, fitting $k$-centers to a set of points in\n$\\mathbb{R}^d$, for any $d,k\\geq 1$. To this end, we utilize \\emph{coresets},\nwhich, in the context of the paper, are essentially weighted sets of points in\n$\\mathbb{R}^d$ that approximate the fitting loss for every model in a given\nset, up to a multiplicative factor of $1\\pm\\varepsilon$. We provide [Section 3\nand Section E in the appendix] experiments that show the empirical contribution\nof our suggested methods for real images (novel and reference), synthetic data,\nand real-world data. We also propose choice clustering, which by combining\nclustering algorithms yields better performance than each one separately.\n","authors":["David Denisov","Dan Feldman","Shlomi Dolev","Michael Segal"],"pdf_url":"https://arxiv.org/pdf/2408.14225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02378v2","updated":"2024-08-26T12:36:51Z","published":"2023-07-05T15:45:53Z","title":"Continuum Limits of Ollivier's Ricci Curvature on data clouds: pointwise\n  consistency and global lower bounds","summary":"  Let $M$ denote a low-dimensional manifold embedded in Euclidean space and let\n${X}= \\{ x_1, \\dots, x_n \\}$ be a collection of points uniformly sampled from\nit. We study the relationship between the curvature of a random geometric graph\nbuilt from ${X}$ and the curvature of the manifold $M$ via continuum limits of\nOllivier's discrete Ricci curvature. We prove pointwise, non-asymptotic\nconsistency results and also show that if $M$ has Ricci curvature bounded from\nbelow by a positive constant, then the random geometric graph will inherit this\nglobal structural property with high probability. We discuss applications of\nthe global discrete curvature bounds to contraction properties of heat kernels\non graphs, as well as implications for manifold learning from data clouds. In\nparticular, we show that our consistency results allow for estimating the\nintrinsic curvature of a manifold by first estimating concrete extrinsic\nquantities.\n","authors":["Nicolas Garcia Trillos","Melanie Weber"],"pdf_url":"https://arxiv.org/pdf/2307.02378v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11341v2","updated":"2024-08-26T12:14:31Z","published":"2024-04-17T13:00:52Z","title":"The Causal Chambers: Real Physical Systems as a Testbed for AI\n  Methodology","summary":"  In some fields of AI, machine learning and statistics, the validation of new\nmethods and algorithms is often hindered by the scarcity of suitable real-world\ndatasets. Researchers must often turn to simulated data, which yields limited\ninformation about the applicability of the proposed methods to real problems.\nAs a step forward, we have constructed two devices that allow us to quickly and\ninexpensively produce large datasets from non-trivial but well-understood\nphysical systems. The devices, which we call causal chambers, are\ncomputer-controlled laboratories that allow us to manipulate and measure an\narray of variables from these physical systems, providing a rich testbed for\nalgorithms from a variety of fields. We illustrate potential applications\nthrough a series of case studies in fields such as causal discovery,\nout-of-distribution generalization, change point detection, independent\ncomponent analysis, and symbolic regression. For applications to causal\ninference, the chambers allow us to carefully perform interventions. We also\nprovide and empirically validate a causal model of each chamber, which can be\nused as ground truth for different tasks. All hardware and software is made\nopen source, and the datasets are publicly available at causalchamber.org or\nthrough the Python package causalchamber.\n","authors":["Juan L. Gamella","Jonas Peters","Peter BÃ¼hlmann"],"pdf_url":"https://arxiv.org/pdf/2404.11341v2.pdf","comment":"40 pages, 20 figures"},{"id":"http://arxiv.org/abs/2408.14206v1","updated":"2024-08-26T12:09:38Z","published":"2024-08-26T12:09:38Z","title":"Lemon and Orange Disease Classification using CNN-Extracted Features and\n  Machine Learning Classifier","summary":"  Lemons and oranges, both are the most economically significant citrus fruits\nglobally. The production of lemons and oranges is severely affected due to\ndiseases in its growth stages. Fruit quality has degraded due to the presence\nof flaws. Thus, it is necessary to diagnose the disease accurately so that we\ncan avoid major loss of lemons and oranges. To improve citrus farming, we\nproposed a disease classification approach for lemons and oranges. This\napproach would enable early disease detection and intervention, reduce yield\nlosses, and optimize resource allocation. For the initial modeling of disease\nclassification, the research uses innovative deep learning architectures such\nas VGG16, VGG19 and ResNet50. In addition, for achieving better accuracy, the\nbasic machine learning algorithms used for classification problems include\nRandom Forest, Naive Bayes, K-Nearest Neighbors (KNN) and Logistic Regression.\nThe lemon and orange fruits diseases are classified more accurately (95.0% for\nlemon and 99.69% for orange) by the model. The model's base features were\nextracted from the ResNet50 pre-trained model and the diseases are classified\nby the Logistic Regression which beats the performance given by VGG16 and VGG19\nfor other classifiers. Experimental outcomes show that the proposed model also\noutperforms existing models in which most of them classified the diseases using\nthe Softmax classifier without using any individual classifiers.\n","authors":["Khandoker Nosiba Arifin","Sayma Akter Rupa","Md Musfique Anwar","Israt Jahan"],"pdf_url":"https://arxiv.org/pdf/2408.14206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03846v2","updated":"2024-08-26T11:58:22Z","published":"2024-02-06T09:48:33Z","title":"Efficient Generation of Hidden Outliers for Improved Outlier Detection","summary":"  Outlier generation is a popular technique used for solving important outlier\ndetection tasks. Generating outliers with realistic behavior is challenging.\nPopular existing methods tend to disregard the 'multiple views' property of\noutliers in high-dimensional spaces. The only existing method accounting for\nthis property falls short in efficiency and effectiveness. We propose BISECT, a\nnew outlier generation method that creates realistic outliers mimicking said\nproperty. To do so, BISECT employs a novel proposition introduced in this\narticle stating how to efficiently generate said realistic outliers. Our method\nhas better guarantees and complexity than the current methodology for\nrecreating 'multiple views'. We use the synthetic outliers generated by BISECT\nto effectively enhance outlier detection in diverse datasets, for multiple use\ncases. For instance, oversampling with BISECT reduced the error by up to 3\ntimes when compared with the baselines.\n","authors":["Jose Cribeiro-Ramallo","Vadim Arzamasov","Klemens BÃ¶hm"],"pdf_url":"https://arxiv.org/pdf/2402.03846v2.pdf","comment":"Preprint. Full paper is scheduled to appear in TKDD; Updated results\n  in table 4"},{"id":"http://arxiv.org/abs/2408.14195v1","updated":"2024-08-26T11:47:52Z","published":"2024-08-26T11:47:52Z","title":"Representative Arm Identification: A fixed confidence approach to\n  identify cluster representatives","summary":"  We study the representative arm identification (RAI) problem in the\nmulti-armed bandits (MAB) framework, wherein we have a collection of arms, each\nassociated with an unknown reward distribution. An underlying instance is\ndefined by a partitioning of the arms into clusters of predefined sizes, such\nthat for any $j > i$, all arms in cluster $i$ have a larger mean reward than\nthose in cluster $j$. The goal in RAI is to reliably identify a certain\nprespecified number of arms from each cluster, while using as few arm pulls as\npossible. The RAI problem covers as special cases several well-studied MAB\nproblems such as identifying the best arm or any $M$ out of the top $K$, as\nwell as both full and coarse ranking. We start by providing an\ninstance-dependent lower bound on the sample complexity of any feasible\nalgorithm for this setting. We then propose two algorithms, based on the idea\nof confidence intervals, and provide high probability upper bounds on their\nsample complexity, which orderwise match the lower bound. Finally, we do an\nempirical comparison of both algorithms along with an LUCB-type alternative on\nboth synthetic and real-world datasets, and demonstrate the superior\nperformance of our proposed schemes in most cases.\n","authors":["Sarvesh Gharat","Aniket Yadav","Nikhil Karamchandani","Jayakrishnan Nair"],"pdf_url":"https://arxiv.org/pdf/2408.14195v1.pdf","comment":"We analyse a clustered multi-armed bandit formulation, where the\n  learning objective is to identify representative arms from each cluster, in a\n  fixed confidence setting"},{"id":"http://arxiv.org/abs/2408.05920v3","updated":"2024-08-26T11:41:28Z","published":"2024-08-12T05:00:23Z","title":"Urban Region Pre-training and Prompting: A Graph-based Approach","summary":"  Urban region representation is crucial for various urban downstream tasks.\nHowever, despite the proliferation of methods and their success, acquiring\ngeneral urban region knowledge and adapting to different tasks remains\nchallenging. Previous work often neglects the spatial structures and functional\nlayouts between entities, limiting their ability to capture transferable\nknowledge across regions. Further, these methods struggle to adapt effectively\nto specific downstream tasks, as they do not adequately address the unique\nfeatures and relationships required for different downstream tasks. In this\npaper, we propose a $\\textbf{G}$raph-based $\\textbf{U}$rban $\\textbf{R}$egion\n$\\textbf{P}$re-training and $\\textbf{P}$rompting framework ($\\textbf{GURPP}$)\nfor region representation learning. Specifically, we first construct an urban\nregion graph that integrates detailed spatial entity data for more effective\nurban region representation. Then, we develop a subgraph-centric urban region\npre-training model to capture the heterogeneous and transferable patterns of\ninteractions among entities. To further enhance the adaptability of these\nembeddings to different tasks, we design two graph-based prompting methods to\nincorporate explicit/hidden task knowledge. Extensive experiments on various\nurban region prediction tasks and different cities demonstrate the superior\nperformance of our GURPP framework.\n","authors":["Jiahui Jin","Yifan Song","Dong Kan","Haojia Zhu","Xiangguo Sun","Zhicheng Li","Xigang Sun","Jinghui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.05920v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02595v3","updated":"2024-08-26T11:35:52Z","published":"2023-04-02T02:19:15Z","title":"Bayesian neural networks via MCMC: a Python-based tutorial","summary":"  Bayesian inference provides a methodology for parameter estimation and\nuncertainty quantification in machine learning and deep learning methods.\nVariational inference and Markov Chain Monte-Carlo (MCMC) sampling methods are\nused to implement Bayesian inference. In the past three decades, MCMC sampling\nmethods have faced some challenges in being adapted to larger models (such as\nin deep learning) and big data problems. Advanced proposal distributions that\nincorporate gradients, such as a Langevin proposal distribution, provide a\nmeans to address some of the limitations of MCMC sampling for Bayesian neural\nnetworks. Furthermore, MCMC methods have typically been constrained to\nstatisticians and currently not well-known among deep learning researchers. We\npresent a tutorial for MCMC methods that covers simple Bayesian linear and\nlogistic models, and Bayesian neural networks. The aim of this tutorial is to\nbridge the gap between theory and implementation via coding, given a general\nsparsity of libraries and tutorials to this end. This tutorial provides code in\nPython with data and instructions that enable their use and extension. We\nprovide results for some benchmark problems showing the strengths and\nweaknesses of implementing the respective Bayesian models via MCMC. We\nhighlight the challenges in sampling multi-modal posterior distributions for\nthe case of Bayesian neural networks and the need for further improvement of\nconvergence diagnosis methods.\n","authors":["Rohitash Chandra","Joshua Simmons"],"pdf_url":"https://arxiv.org/pdf/2304.02595v3.pdf","comment":"IEEE Access (2024)"},{"id":"http://arxiv.org/abs/2406.12284v2","updated":"2024-08-26T11:33:13Z","published":"2024-06-18T05:23:29Z","title":"Demystifying the Recency Heuristic in Temporal-Difference Learning","summary":"  The recency heuristic in reinforcement learning is the assumption that\nstimuli that occurred closer in time to an acquired reward should be more\nheavily reinforced. The recency heuristic is one of the key assumptions made by\nTD($\\lambda$), which reinforces recent experiences according to an\nexponentially decaying weighting. In fact, all other widely used return\nestimators for TD learning, such as $n$-step returns, satisfy a weaker (i.e.,\nnon-monotonic) recency heuristic. Why is the recency heuristic effective for\ntemporal credit assignment? What happens when credit is assigned in a way that\nviolates this heuristic? In this paper, we analyze the specific mathematical\nimplications of adopting the recency heuristic in TD learning. We prove that\nany return estimator satisfying this heuristic: 1) is guaranteed to converge to\nthe correct value function, 2) has a relatively fast contraction rate, and 3)\nhas a long window of effective credit assignment, yet bounded worst-case\nvariance. We also give a counterexample where on-policy, tabular TD methods\nviolating the recency heuristic diverge. Our results offer some of the first\ntheoretical evidence that credit assignment based on the recency heuristic\nfacilitates learning.\n","authors":["Brett Daley","Marlos C. Machado","Martha White"],"pdf_url":"https://arxiv.org/pdf/2406.12284v2.pdf","comment":"RLC 2024. 18 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2408.14183v1","updated":"2024-08-26T11:16:03Z","published":"2024-08-26T11:16:03Z","title":"Robot Navigation with Entity-Based Collision Avoidance using Deep\n  Reinforcement Learning","summary":"  Efficient navigation in dynamic environments is crucial for autonomous robots\ninteracting with various environmental entities, including both moving agents\nand static obstacles. In this study, we present a novel methodology that\nenhances the robot's interaction with different types of agents and obstacles\nbased on specific safety requirements. This approach uses information about the\nentity types, improving collision avoidance and ensuring safer navigation. We\nintroduce a new reward function that penalizes the robot for collisions with\ndifferent entities such as adults, bicyclists, children, and static obstacles,\nand additionally encourages the robot's proximity to the goal. It also\npenalizes the robot for being close to entities, and the safe distance also\ndepends on the entity type. Additionally, we propose an optimized algorithm for\ntraining and testing, which significantly accelerates train, validation, and\ntest steps and enables training in complex environments. Comprehensive\nexperiments conducted using simulation demonstrate that our approach\nconsistently outperforms conventional navigation and collision avoidance\nmethods, including state-of-the-art techniques. To sum up, this work\ncontributes to enhancing the safety and efficiency of navigation systems for\nautonomous robots in dynamic, crowded environments.\n","authors":["Yury Kolomeytsev","Dmitry Golembiovsky"],"pdf_url":"https://arxiv.org/pdf/2408.14183v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.11237v2","updated":"2024-08-26T10:39:22Z","published":"2024-02-17T10:02:22Z","title":"Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in\n  Deep Learning","summary":"  Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than\nlearning the intended task, they tend to draw inconclusive relationships\nbetween their inputs and outputs. Shortcut learning is ubiquitous among many\nfailure cases of neural networks, and traces of this phenomenon can be seen in\ntheir generalizability issues, domain shift, adversarial vulnerability, and\neven bias towards majority groups. In this paper, we argue that this\ncommonality in the cause of various DNN issues creates a significant\nopportunity that should be leveraged to find a unified solution for shortcut\nlearning. To this end, we outline the recent advances in topological data\nanalysis (TDA), and persistent homology (PH) in particular, to sketch a unified\nroadmap for detecting shortcuts in deep learning. We demonstrate our arguments\nby investigating the topological features of computational graphs in DNNs using\ntwo cases of unlearnable examples and bias in decision-making as our test\nstudies. Our analysis of these two failure cases of DNNs reveals that finding a\nunified solution for shortcut learning in DNNs is not out of reach, and TDA can\nplay a significant role in forming such a framework.\n","authors":["Hadi M. Dolatabadi","Sarah M. Erfani","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2402.11237v2.pdf","comment":"Accepted to the 2024 European Conference on Artificial Intelligence\n  (ECAI)"},{"id":"http://arxiv.org/abs/2306.08270v2","updated":"2024-08-26T10:26:28Z","published":"2023-06-14T06:13:50Z","title":"Solar Active Regions Detection Via 2D Circular Kernel Time Series\n  Transformation, Entropy and Machine Learning Approach","summary":"  This study proposes an enhancement to the existing method for detecting Solar\nActive Regions (ARs). Our technique tracks ARs using images from the\nAtmospheric Imaging Assembly (AIA) of NASA's Solar Dynamics Observatory (SDO).\nIt involves a 2D circular kernel time series transformation, combined with\nStatistical and Entropy measures, and a Machine Learning (ML) approach. The\ntechnique transforms the circular area around pixels in the SDO AIA images into\none-dimensional time series (1-DTS). Statistical measures (Median Value, Xmed;\n95th Percentile, X95) and Entropy measures (Distribution Entropy, DisEn; Fuzzy\nEntropy, FuzzyEn) are used as feature selection methods (FSM 1), alongside a\nmethod applying 1-DTS elements directly as features (FSM 2). The ML algorithm\nclassifies these series into three categories: no Active Region (nARs type 1,\nclass 1), non-flaring Regions outside active regions with brightness (nARs type\n2, class 2), and flaring Active Regions (ARs, class 3). The ML model achieves a\nclassification accuracy of 0.900 and 0.914 for Entropy and Statistical\nmeasures, respectively. Notably, Fuzzy Entropy shows the highest classification\naccuracy (AKF=0.895), surpassing DisEn (AKF=0.738), X95 (AKF=0.873), and Xmed\n(AKF=0.840). This indicates the high effectiveness of Entropy and Statistical\nmeasures for AR detection in SDO AIA images. FSM 2 captures a similar\ndistribution of flaring AR activities as FSM 1. Additionally, we introduce a\ngeneralizing characteristic of AR activities (GSA), finding a direct agreement\nbetween increased AR activities and higher GSA values. The Python code\nimplementation of the proposed method is available in supplementary material.\n","authors":["Irewola Aaron Oludehinwa","Andrei Velichko","Maksim Belyaev","Olasunkanmi I. Olusola"],"pdf_url":"https://arxiv.org/pdf/2306.08270v2.pdf","comment":"30 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2404.03309v2","updated":"2024-08-26T10:21:00Z","published":"2024-04-04T09:08:04Z","title":"Optimistic Online Non-stochastic Control via FTRL","summary":"  This paper brings the concept of ``optimism\" to the new and promising\nframework of online Non-stochastic Control (NSC). Namely, we study how NSC can\nbenefit from a prediction oracle of unknown quality responsible for forecasting\nfuture costs. The posed problem is first reduced to an optimistic learning with\ndelayed feedback problem, which is handled through the Optimistic Follow the\nRegularized Leader (OFTRL) algorithmic family. This reduction enables the\ndesign of \\texttt{OptFTRL-C}, the first Disturbance Action Controller (DAC)\nwith optimistic policy regret bounds. These new bounds are commensurate with\nthe oracle's accuracy, ranging from $\\mathcal{O}(1)$ for perfect predictions to\nthe order-optimal $\\mathcal{O}(\\sqrt{T})$ even when all predictions fail. By\naddressing the challenge of incorporating untrusted predictions into online\ncontrol, this work contributes to the advancement of the NSC framework and\npaves the way toward effective and robust learning-based controllers.\n","authors":["Naram Mhaisen","George Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2404.03309v2.pdf","comment":"to appear in the proceedings of IEEE CDC 2024"},{"id":"http://arxiv.org/abs/2312.01878v8","updated":"2024-08-26T10:13:43Z","published":"2023-12-04T13:20:15Z","title":"HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot\n  Prompt Learning","summary":"  Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)\nare prominent techniques for homogeneous and heterogeneous graph representation\nlearning, yet their performance in an end-to-end supervised framework greatly\ndepends on the availability of task-specific supervision. To reduce the\nlabeling cost, pre-training on self-supervised pretext tasks has become a\npopular paradigm,but there is often a gap between the pre-trained model and\ndownstream tasks, stemming from the divergence in their objectives. To bridge\nthe gap, prompt learning has risen as a promising direction especially in\nfew-shot settings, without the need to fully fine-tune the pre-trained model.\nWhile there has been some early exploration of prompt-based learning on graphs,\nthey primarily deal with homogeneous graphs, ignoring the heterogeneous graphs\nthat are prevalent in downstream applications. In this paper, we propose\nHGPROMPT, a novel pre-training and prompting framework to unify not only\npre-training and downstream tasks but also homogeneous and heterogeneous graphs\nvia a dual-template design. Moreover, we propose dual-prompt in HGPROMPT to\nassist a downstream task in locating the most relevant prior to bridge the gaps\ncaused by not only feature variations but also heterogeneity differences across\ntasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensive\nexperiments on three public datasets.\n","authors":["Xingtong Yu","Yuan Fang","Zemin Liu","Xinming Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.01878v8.pdf","comment":"AAAI2024 main track"},{"id":"http://arxiv.org/abs/2311.15317v5","updated":"2024-08-26T10:12:45Z","published":"2023-11-26T14:35:28Z","title":"Generalized Graph Prompt: Toward a Unification of Pre-Training and\n  Downstream Tasks on Graphs","summary":"  Graph neural networks have emerged as a powerful tool for graph\nrepresentation learning, but their performance heavily relies on abundant\ntask-specific supervision. To reduce labeling requirement, the \"pre-train,\nprompt\" paradigms have become increasingly common. However, existing study of\nprompting on graphs is limited, lacking a universal treatment to appeal to\ndifferent downstream tasks. In this paper, we propose GraphPrompt, a novel\npre-training and prompting framework on graphs. GraphPrompt not only unifies\npre-training and downstream tasks into a common task template but also employs\na learnable prompt to assist a downstream task in locating the most relevant\nknowledge from the pre-trained model in a task-specific manner. To further\nenhance GraphPrompt in these two stages, we extend it into GraphPrompt+ with\ntwo major enhancements. First, we generalize several popular graph pre-training\ntasks beyond simple link prediction to broaden the compatibility with our task\ntemplate. Second, we propose a more generalized prompt design that incorporates\na series of prompt vectors within every layer of the pre-trained graph encoder,\nin order to capitalize on the hierarchical information across different layers\nbeyond just the readout layer. Finally, we conduct extensive experiments on\nfive public datasets to evaluate and analyze GraphPrompt and GraphPrompt+.\n","authors":["Xingtong Yu","Zhenghao Liu","Yuan Fang","Zemin Liu","Sihong Chen","Xinming Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.15317v5.pdf","comment":"Accepted by IEEE TKDE. Extension of \"GraphPrompt: Unifying\n  Pre-Training and Downstream Tasks for Graph Neural Networks\". arXiv admin\n  note: substantial text overlap with arXiv:2302.08043"},{"id":"http://arxiv.org/abs/2312.03731v7","updated":"2024-08-26T10:11:45Z","published":"2023-11-28T02:36:53Z","title":"MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs","summary":"  Graphs can inherently model interconnected objects on the Web, thereby\nfacilitating a series of Web applications, such as web analyzing and content\nrecommendation. Recently, Graph Neural Networks (GNNs) have emerged as a\nmainstream technique for graph representation learning. However, their efficacy\nwithin an end-to-end supervised framework is significantly tied to the\navailabilityof task-specific labels. To mitigate labeling costs and enhance\nrobustness in few-shot settings, pre-training on self-supervised tasks has\nemerged as a promising method, while prompting has been proposed to further\nnarrow the objective gap between pretext and downstream tasks. Although there\nhas been some initial exploration of prompt-based learning on graphs, they\nprimarily leverage a single pretext task, resulting in a limited subset of\ngeneral knowledge that could be learned from the pre-training data. Hence, in\nthis paper, we propose MultiGPrompt, a novel multi-task pre-training and\nprompting framework to exploit multiple pretext tasks for more comprehensive\npre-trained knowledge. First, in pre-training, we design a set of pretext\ntokens to synergize multiple pretext tasks. Second, we propose a dual-prompt\nmechanism consisting of composed and open prompts to leverage task-specific and\nglobal pre-training knowledge, to guide downstream tasks in few-shot settings.\nFinally, we conduct extensive experiments on six public datasets to evaluate\nand analyze MultiGPrompt.\n","authors":["Xingtong Yu","Chang Zhou","Yuan Fang","Xinming Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.03731v7.pdf","comment":"WWW2024 research track"},{"id":"http://arxiv.org/abs/2402.03903v3","updated":"2024-08-26T09:59:24Z","published":"2024-02-06T11:13:57Z","title":"Averaging $n$-step Returns Reduces Variance in Reinforcement Learning","summary":"  Multistep returns, such as $n$-step returns and $\\lambda$-returns, are\ncommonly used to improve the sample efficiency of reinforcement learning (RL)\nmethods. The variance of the multistep returns becomes the limiting factor in\ntheir length; looking too far into the future increases variance and reverses\nthe benefits of multistep learning. In our work, we demonstrate the ability of\ncompound returns -- weighted averages of $n$-step returns -- to reduce\nvariance. We prove for the first time that any compound return with the same\ncontraction modulus as a given $n$-step return has strictly lower variance. We\nadditionally prove that this variance-reduction property improves the\nfinite-sample complexity of temporal-difference learning under linear function\napproximation. Because general compound returns can be expensive to implement,\nwe introduce two-bootstrap returns which reduce variance while remaining\nefficient, even when using minibatched experience replay. We conduct\nexperiments showing that compound returns often increase the sample efficiency\nof $n$-step deep RL agents like DQN and PPO.\n","authors":["Brett Daley","Martha White","Marlos C. Machado"],"pdf_url":"https://arxiv.org/pdf/2402.03903v3.pdf","comment":"ICML 2024. 27 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.13431v2","updated":"2024-08-26T09:58:04Z","published":"2024-07-18T12:00:32Z","title":"Improving Out-of-Distribution Generalization of Trajectory Prediction\n  for Autonomous Driving via Polynomial Representations","summary":"  Robustness against Out-of-Distribution (OoD) samples is a key performance\nindicator of a trajectory prediction model. However, the development and\nranking of state-of-the-art (SotA) models are driven by their In-Distribution\n(ID) performance on individual competition datasets. We present an OoD testing\nprotocol that homogenizes datasets and prediction tasks across two large-scale\nmotion datasets. We introduce a novel prediction algorithm based on polynomial\nrepresentations for agent trajectory and road geometry on both the input and\noutput sides of the model. With a much smaller model size, training effort, and\ninference time, we reach near SotA performance for ID testing and significantly\nimprove robustness in OoD testing. Within our OoD testing protocol, we further\nstudy two augmentation strategies of SotA models and their effects on model\ngeneralization. Highlighting the contrast between ID and OoD performance, we\nsuggest adding OoD testing to the evaluation criteria of trajectory prediction\nmodels.\n","authors":["Yue Yao","Shengchao Yan","Daniel Goehring","Wolfram Burgard","Joerg Reichardt"],"pdf_url":"https://arxiv.org/pdf/2407.13431v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14152v1","updated":"2024-08-26T09:55:32Z","published":"2024-08-26T09:55:32Z","title":"Application of Disentanglement to Map Registration Problem","summary":"  Geospatial data come from various sources, such as satellites, aircraft, and\nLiDAR. The variability of the source is not limited to the types of data\nacquisition techniques, as we have maps from different time periods. To\nincorporate these data for a coherent analysis, it is essential to first align\ndifferent \"styles\" of geospatial data to its matching images that point to the\nsame location on the surface of the Earth. In this paper, we approach the image\nregistration as a two-step process of (1) extracting geospatial contents\ninvariant to visual (and any other non-content-related) information, and (2)\nmatching the data based on such (purely) geospatial contents. We hypothesize\nthat a combination of $\\beta$-VAE-like architecture [2] and adversarial\ntraining will achieve both the disentanglement of the geographic information\nand artistic styles and generation of new map tiles by composing the encoded\ngeographic information with any artistic style.\n","authors":["Hae Jin Song","Patrycja Krawczuk","Po-Hsuan Huang"],"pdf_url":"https://arxiv.org/pdf/2408.14152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14146v1","updated":"2024-08-26T09:44:21Z","published":"2024-08-26T09:44:21Z","title":"TSAK: Two-Stage Semantic-Aware Knowledge Distillation for Efficient\n  Wearable Modality and Model Optimization in Manufacturing Lines","summary":"  Smaller machine learning models, with less complex architectures and sensor\ninputs, can benefit wearable sensor-based human activity recognition (HAR)\nsystems in many ways, from complexity and cost to battery life. In the specific\ncase of smart factories, optimizing human-robot collaboration hinges on the\nimplementation of cutting-edge, human-centric AI systems. To this end, workers'\nactivity recognition enables accurate quantification of performance metrics,\nimproving efficiency holistically. We present a two-stage semantic-aware\nknowledge distillation (KD) approach, TSAK, for efficient, privacy-aware, and\nwearable HAR in manufacturing lines, which reduces the input sensor modalities\nas well as the machine learning model size, while reaching similar recognition\nperformance as a larger multi-modal and multi-positional teacher model. The\nfirst stage incorporates a teacher classifier model encoding attention, causal,\nand combined representations. The second stage encompasses a semantic\nclassifier merging the three representations from the first stage. To evaluate\nTSAK, we recorded a multi-modal dataset at a smart factory testbed with\nwearable and privacy-aware sensors (IMU and capacitive) located on both\nworkers' hands. In addition, we evaluated our approach on OpenPack, the only\navailable open dataset mimicking the wearable sensor placements on both hands\nin the manufacturing HAR scenario. We compared several KD strategies with\ndifferent representations to regulate the training process of a smaller student\nmodel. Compared to the larger teacher model, the student model takes fewer\nsensor channels from a single hand, has 79% fewer parameters, runs 8.88 times\nfaster, and requires 96.6% less computing power (FLOPS).\n","authors":["Hymalai Bello","Daniel GeiÃler","Sungho Suh","Bo Zhou","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2408.14146v1.pdf","comment":"Accepted in 27th International Conference on Pattern Recognition\n  (ICPR)"},{"id":"http://arxiv.org/abs/2407.02112v2","updated":"2024-08-26T09:43:12Z","published":"2024-07-02T09:54:39Z","title":"A Data-Centric Perspective on Evaluating Machine Learning Models for\n  Tabular Data","summary":"  Tabular data is prevalent in real-world machine learning applications, and\nnew models for supervised learning of tabular data are frequently proposed.\nComparative studies assessing the performance of models typically consist of\nmodel-centric evaluation setups with overly standardized data preprocessing.\nThis paper demonstrates that such model-centric evaluations are biased, as\nreal-world modeling pipelines often require dataset-specific preprocessing and\nfeature engineering. Therefore, we propose a data-centric evaluation framework.\nWe select 10 relevant datasets from Kaggle competitions and implement\nexpert-level preprocessing pipelines for each dataset. We conduct experiments\nwith different preprocessing pipelines and hyperparameter optimization (HPO)\nregimes to quantify the impact of model selection, HPO, feature engineering,\nand test-time adaptation. Our main findings are: 1. After dataset-specific\nfeature engineering, model rankings change considerably, performance\ndifferences decrease, and the importance of model selection reduces. 2. Recent\nmodels, despite their measurable progress, still significantly benefit from\nmanual feature engineering. This holds true for both tree-based models and\nneural networks. 3. While tabular data is typically considered static, samples\nare often collected over time, and adapting to distribution shifts can be\nimportant even in supposedly static data. These insights suggest that research\nefforts should be directed toward a data-centric perspective, acknowledging\nthat tabular data requires feature engineering and often exhibits temporal\ncharacteristics. Our framework is available under:\nhttps://github.com/atschalz/dc_tabeval.\n","authors":["Andrej Tschalzev","Sascha Marton","Stefan LÃ¼dtke","Christian Bartelt","Heiner Stuckenschmidt"],"pdf_url":"https://arxiv.org/pdf/2407.02112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14144v1","updated":"2024-08-26T09:42:18Z","published":"2024-08-26T09:42:18Z","title":"Neighborhood and Global Perturbations Supported SAM in Federated\n  Learning: From Local Tweaks To Global Awareness","summary":"  Federated Learning (FL) can be coordinated under the orchestration of a\ncentral server to collaboratively build a privacy-preserving model without the\nneed for data exchange. However, participant data heterogeneity leads to local\noptima divergence, subsequently affecting convergence outcomes. Recent research\nhas focused on global sharpness-aware minimization (SAM) and dynamic\nregularization techniques to enhance consistency between global and local\ngeneralization and optimization objectives. Nonetheless, the estimation of\nglobal SAM introduces additional computational and memory overhead, while\ndynamic regularization suffers from bias in the local and global dual variables\ndue to training isolation. In this paper, we propose a novel FL algorithm,\nFedTOGA, designed to consider optimization and generalization objectives while\nmaintaining minimal uplink communication overhead. By linking local\nperturbations to global updates, global generalization consistency is improved.\nAdditionally, global updates are used to correct local dynamic regularizers,\nreducing dual variables bias and enhancing optimization consistency. Global\nupdates are passively received by clients, reducing overhead. We also propose\nneighborhood perturbation to approximate local perturbation, analyzing its\nstrengths and limitations. Theoretical analysis shows FedTOGA achieves faster\nconvergence $O(1/T)$ under non-convex functions. Empirical studies demonstrate\nthat FedTOGA outperforms state-of-the-art algorithms, with a 1\\% accuracy\nincrease and 30\\% faster convergence, achieving state-of-the-art.\n","authors":["Boyuan Li","Zihao Peng","Yafei Li","Mingliang Xu","Shengbo Chen","Baofeng Ji","Cong Shen"],"pdf_url":"https://arxiv.org/pdf/2408.14144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14143v1","updated":"2024-08-26T09:41:40Z","published":"2024-08-26T09:41:40Z","title":"2D-Malafide: Adversarial Attacks Against Face Deepfake Detection Systems","summary":"  We introduce 2D-Malafide, a novel and lightweight adversarial attack designed\nto deceive face deepfake detection systems. Building upon the concept of 1D\nconvolutional perturbations explored in the speech domain, our method leverages\n2D convolutional filters to craft perturbations which significantly degrade the\nperformance of state-of-the-art face deepfake detectors. Unlike traditional\nadditive noise approaches, 2D-Malafide optimises a small number of filter\ncoefficients to generate robust adversarial perturbations which are\ntransferable across different face images. Experiments, conducted using the\nFaceForensics++ dataset, demonstrate that 2D-Malafide substantially degrades\ndetection performance in both white-box and black-box settings, with larger\nfilter sizes having the greatest impact. Additionally, we report an\nexplainability analysis using GradCAM which illustrates how 2D-Malafide\nmisleads detection systems by altering the image areas used most for\nclassification. Our findings highlight the vulnerability of current deepfake\ndetection systems to convolutional adversarial attacks as well as the need for\nfuture work to enhance detection robustness through improved image fidelity\nconstraints.\n","authors":["Chiara Galdi","Michele Panariello","Massimiliano Todisco","Nicholas Evans"],"pdf_url":"https://arxiv.org/pdf/2408.14143v1.pdf","comment":"Accepted at BIOSIG 2024"},{"id":"http://arxiv.org/abs/2405.18194v3","updated":"2024-08-26T09:35:54Z","published":"2024-05-28T14:04:09Z","title":"Delving into Differentially Private Transformer","summary":"  Deep learning with differential privacy (DP) has garnered significant\nattention over the past years, leading to the development of numerous methods\naimed at enhancing model accuracy and training efficiency. This paper delves\ninto the problem of training Transformer models with differential privacy. Our\ntreatment is modular: the logic is to `reduce' the problem of training DP\nTransformer to the more basic problem of training DP vanilla neural nets. The\nlatter is better understood and amenable to many model-agnostic methods. Such\n`reduction' is done by first identifying the hardness unique to DP Transformer\ntraining: the attention distraction phenomenon and a lack of compatibility with\nexisting techniques for efficient gradient clipping. To deal with these two\nissues, we propose the Re-Attention Mechanism and Phantom Clipping,\nrespectively. We believe that our work not only casts new light on training DP\nTransformers but also promotes a modular treatment to advance research in the\nfield of differentially private deep learning.\n","authors":["Youlong Ding","Xueyang Wu","Yining Meng","Yonggang Luo","Hao Wang","Weike Pan"],"pdf_url":"https://arxiv.org/pdf/2405.18194v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.14134v1","updated":"2024-08-26T09:29:56Z","published":"2024-08-26T09:29:56Z","title":"Exploring the Potential of Large Language Models for Heterophilic Graphs","summary":"  Graph Neural Networks (GNNs) are essential for various graph-based learning\ntasks. Notably, classical GNN architectures operate under the assumption of\nhomophily, which posits that connected nodes are likely to share similar\nfeatures. However, this assumption limits the effectiveness of GNNs in handling\nheterophilic graphs where connected nodes often exhibit dissimilar\ncharacteristics. Existing approaches for homophily graphs such as non-local\nneighbor extension and architectural refinement overlook the rich textual data\nassociated with nodes, which could unlock deeper insights into these\nheterophilic contexts. With advancements in Large Language Models (LLMs), there\nis significant promise to enhance GNNs by leveraging the extensive open-world\nknowledge within LLMs to more effectively interpret and utilize textual data\nfor characterizing heterophilic graphs. In this work, we explore the potential\nof LLMs for modeling heterophilic graphs and propose a novel two-stage\nframework: LLM-enhanced edge discriminator and LLM-guided edge reweighting.\nSpecifically, in the first stage, we fine-tune the LLM to better identify\nhomophilic and heterophilic edges based on the textual information of their\nnodes. In the second stage, we adaptively manage message propagation in GNNs\nfor different edge types based on node features, structures, and heterophilic\nor homophilic characteristics. To cope with the computational demands when\ndeploying LLMs in practical scenarios, we further explore model distillation\ntechniques to fine-tune smaller, more efficient models that maintain\ncompetitive performance. Extensive experiments validate the effectiveness of\nour framework, demonstrating the feasibility of using LLMs to enhance GNNs for\nnode classification on heterophilic graphs.\n","authors":["Yuxia Wu","Shujie Li","Yuan Fang","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.14134v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2408.14130v1","updated":"2024-08-26T09:24:36Z","published":"2024-08-26T09:24:36Z","title":"Theoretical Proportion Label Perturbation for Learning from Label\n  Proportions in Large Bags","summary":"  Learning from label proportions (LLP) is a kind of weakly supervised learning\nthat trains an instance-level classifier from label proportions of bags, which\nconsist of sets of instances without using instance labels. A challenge in LLP\narises when the number of instances in a bag (bag size) is numerous, making the\ntraditional LLP methods difficult due to GPU memory limitations. This study\naims to develop an LLP method capable of learning from bags with large sizes.\nIn our method, smaller bags (mini-bags) are generated by sampling instances\nfrom large-sized bags (original bags), and these mini-bags are used in place of\nthe original bags. However, the proportion of a mini-bag is unknown and differs\nfrom that of the original bag, leading to overfitting. To address this issue,\nwe propose a perturbation method for the proportion labels of sampled mini-bags\nto mitigate overfitting to noisy label proportions. This perturbation is added\nbased on the multivariate hypergeometric distribution, which is statistically\nmodeled. Additionally, loss weighting is implemented to reduce the negative\nimpact of proportions sampled from the tail of the distribution. Experimental\nresults demonstrate that the proportion label perturbation and loss weighting\nachieve classification accuracy comparable to that obtained without sampling.\nOur codes are available at https://github.com/stainlessnight/LLP-LargeBags.\n","authors":["Shunsuke Kubo","Shinnosuke Matsuo","Daiki Suehiro","Kazuhiro Terada","Hiroaki Ito","Akihiko Yoshizawa","Ryoma Bise"],"pdf_url":"https://arxiv.org/pdf/2408.14130v1.pdf","comment":"Accepted at ECAI2024"},{"id":"http://arxiv.org/abs/2408.14126v1","updated":"2024-08-26T09:19:58Z","published":"2024-08-26T09:19:58Z","title":"Enhancing Fairness through Reweighting: A Path to Attain the Sufficiency\n  Rule","summary":"  We introduce an innovative approach to enhancing the empirical risk\nminimization (ERM) process in model training through a refined reweighting\nscheme of the training data to enhance fairness. This scheme aims to uphold the\nsufficiency rule in fairness by ensuring that optimal predictors maintain\nconsistency across diverse sub-groups. We employ a bilevel formulation to\naddress this challenge, wherein we explore sample reweighting strategies.\nUnlike conventional methods that hinge on model size, our formulation bases\ngeneralization complexity on the space of sample weights. We discretize the\nweights to improve training speed. Empirical validation of our method showcases\nits effectiveness and robustness, revealing a consistent improvement in the\nbalance between prediction performance and fairness metrics across various\nexperiments.\n","authors":["Xuan Zhao","Klaus Broelemann","Salvatore Ruggieri","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2408.14126v1.pdf","comment":"accepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2407.05206v4","updated":"2024-08-26T09:15:11Z","published":"2024-07-06T23:16:41Z","title":"Helios: An extremely low power event-based gesture recognition for\n  always-on smart eyewear","summary":"  This paper introduces Helios, the first extremely low-power, real-time,\nevent-based hand gesture recognition system designed for all-day on smart\neyewear. As augmented reality (AR) evolves, current smart glasses like the Meta\nRay-Bans prioritize visual and wearable comfort at the expense of\nfunctionality. Existing human-machine interfaces (HMIs) in these devices, such\nas capacitive touch and voice controls, present limitations in ergonomics,\nprivacy and power consumption. Helios addresses these challenges by leveraging\nnatural hand interactions for a more intuitive and comfortable user experience.\nOur system utilizes a extremely low-power and compact 3mmx4mm/20mW event camera\nto perform natural hand-based gesture recognition for always-on smart eyewear.\nThe camera's output is processed by a convolutional neural network (CNN)\nrunning on a NXP Nano UltraLite compute platform, consuming less than 350mW.\nHelios can recognize seven classes of gestures, including subtle microgestures\nlike swipes and pinches, with 91% accuracy. We also demonstrate real-time\nperformance across 20 users at a remarkably low latency of 60ms. Our user\ntesting results align with the positive feedback we received during our recent\nsuccessful demo at AWE-USA-2024.\n","authors":["Prarthana Bhattacharyya","Joshua Mitton","Ryan Page","Owen Morgan","Ben Menzies","Gabriel Homewood","Kemi Jacobs","Paolo Baesso","David Trickett","Chris Mair","Taru Muhonen","Rory Clark","Louis Berridge","Richard Vigars","Iain Wallace"],"pdf_url":"https://arxiv.org/pdf/2407.05206v4.pdf","comment":"Accepted at ECCV-Integrating Computer Vision in Smart Eyewear, 2024.\n  18 pages, 10 figures. First three authors contributed equally to this paper"},{"id":"http://arxiv.org/abs/2408.14118v1","updated":"2024-08-26T09:06:35Z","published":"2024-08-26T09:06:35Z","title":"Towards Lifelong Learning Embeddings: An Algorithmic Approach to\n  Dynamically Extend Embeddings","summary":"  The rapid evolution of technology has transformed business operations and\ncustomer interactions worldwide, with personalization emerging as a key\nopportunity for e-commerce companies to engage customers more effectively. The\napplication of machine learning, particularly that of deep learning models, has\ngained significant traction due to its ability to rapidly recognize patterns in\nlarge datasets, thereby offering numerous possibilities for personalization.\nThese models use embeddings to map discrete information, such as product IDs,\ninto a latent vector space, a method increasingly popular in recent years.\nHowever, e-commerce's dynamic nature, characterized by frequent new product\nintroductions, poses challenges for these embeddings, which typically require\nfixed dimensions and inputs, leading to the need for periodic retraining from\nscratch. This paper introduces a modular algorithm that extends embedding input\nsize while preserving learned knowledge, addressing the challenges posed by\ne-commerce's dynamism. The proposed algorithm also incorporates strategies to\nmitigate the cold start problem associated with new products. The results of\ninitial experiments suggest that this method outperforms traditional\nembeddings.\n","authors":["Miguel Alves Gomes","Philipp Meisen","Tobias Meisen"],"pdf_url":"https://arxiv.org/pdf/2408.14118v1.pdf","comment":"Accepted Extended Abstract for 3rd Workshop on End-End Customer\n  Journey Optimization at KDD2024, Barcelona, Spain"},{"id":"http://arxiv.org/abs/2408.14116v1","updated":"2024-08-26T09:05:43Z","published":"2024-08-26T09:05:43Z","title":"Hierarchical Learning and Computing over Space-Ground Integrated\n  Networks","summary":"  Space-ground integrated networks hold great promise for providing global\nconnectivity, particularly in remote areas where large amounts of valuable data\nare generated by Internet of Things (IoT) devices, but lacking terrestrial\ncommunication infrastructure. The massive data is conventionally transferred to\nthe cloud server for centralized artificial intelligence (AI) models training,\nraising huge communication overhead and privacy concerns. To address this, we\npropose a hierarchical learning and computing framework, which leverages the\nlowlatency characteristic of low-earth-orbit (LEO) satellites and the global\ncoverage of geostationary-earth-orbit (GEO) satellites, to provide global\naggregation services for locally trained models on ground IoT devices. Due to\nthe time-varying nature of satellite network topology and the energy\nconstraints of LEO satellites, efficiently aggregating the received local\nmodels from ground devices on LEO satellites is highly challenging. By\nleveraging the predictability of inter-satellite connectivity, modeling the\nspace network as a directed graph, we formulate a network energy minimization\nproblem for model aggregation, which turns out to be a Directed Steiner Tree\n(DST) problem. We propose a topologyaware energy-efficient routing (TAEER)\nalgorithm to solve the DST problem by finding a minimum spanning arborescence\non a substitute directed graph. Extensive simulations under realworld\nspace-ground integrated network settings demonstrate that the proposed TAEER\nalgorithm significantly reduces energy consumption and outperforms benchmarks.\n","authors":["Jingyang Zhu","Yuanming Shi","Yong Zhou","Chunxiao Jiang","Linling Kuang"],"pdf_url":"https://arxiv.org/pdf/2408.14116v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.08525v2","updated":"2024-08-26T08:49:48Z","published":"2024-03-13T13:33:35Z","title":"From Weak to Strong Sound Event Labels using Adaptive Change-Point\n  Detection and Active Learning","summary":"  We propose an adaptive change point detection method (A-CPD) for machine\nguided weak label annotation of audio recording segments. The goal is to\nmaximize the amount of information gained about the temporal activations of the\ntarget sounds. For each unlabeled audio recording, we use a prediction model to\nderive a probability curve used to guide annotation. The prediction model is\ninitially pre-trained on available annotated sound event data with classes that\nare disjoint from the classes in the unlabeled dataset. The prediction model\nthen gradually adapts to the annotations provided by the annotator in an active\nlearning loop. We derive query segments to guide the weak label annotator\ntowards strong labels, using change point detection on these probabilities. We\nshow that it is possible to derive strong labels of high quality with a limited\nannotation budget, and show favorable results for A-CPD when compared to two\nbaseline query segment strategies.\n","authors":["John Martinsson","Olof Mogren","Maria Sandsten","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2403.08525v2.pdf","comment":"Accepted at EUSIPCO 2024 (nominated best student paper)"},{"id":"http://arxiv.org/abs/2405.08334v2","updated":"2024-08-26T08:24:14Z","published":"2024-05-14T06:09:08Z","title":"Could Chemical LLMs benefit from Message Passing","summary":"  Pretrained language models (LMs) showcase significant capabilities in\nprocessing molecular text, while concurrently, message passing neural networks\n(MPNNs) demonstrate resilience and versatility in the domain of molecular\nscience. Despite these advancements, we find there are limited studies\ninvestigating the bidirectional interactions between molecular structures and\ntheir corresponding textual representations. Therefore, in this paper, we\npropose two strategies to evaluate whether an information integration can\nenhance the performance: contrast learning, which involves utilizing an MPNN to\nsupervise the training of the LM, and fusion, which exploits information from\nboth models. Our empirical analysis reveals that the integration approaches\nexhibit superior performance compared to baselines when applied to smaller\nmolecular graphs, while these integration approaches do not yield performance\nenhancements on large scale graphs.\n","authors":["Jiaqing Xie","Ziheng Chi"],"pdf_url":"https://arxiv.org/pdf/2405.08334v2.pdf","comment":"Accepted at ACL @ Languages and Molecules 2024. In Proceedings of ACL\n  2024"},{"id":"http://arxiv.org/abs/2408.14086v1","updated":"2024-08-26T08:12:26Z","published":"2024-08-26T08:12:26Z","title":"ReLExS: Reinforcement Learning Explanations for Stackelberg No-Regret\n  Learners","summary":"  With the constraint of a no regret follower, will the players in a two-player\nStackelberg game still reach Stackelberg equilibrium? We first show when the\nfollower strategy is either reward-average or transform-reward-average, the two\nplayers can always get the Stackelberg Equilibrium. Then, we extend that the\nplayers can achieve the Stackelberg equilibrium in the two-player game under\nthe no regret constraint. Also, we show a strict upper bound of the follower's\nutility difference between with and without no regret constraint. Moreover, in\nconstant-sum two-player Stackelberg games with non-regret action sequences, we\nensure the total optimal utility of the game remains also bounded.\n","authors":["Xiangge Huang","Jingyuan Li","Jiaqing Xie"],"pdf_url":"https://arxiv.org/pdf/2408.14086v1.pdf","comment":"10 pages, 3 figures. Technical Report"},{"id":"http://arxiv.org/abs/2407.20003v2","updated":"2024-08-26T08:10:56Z","published":"2024-07-29T13:34:34Z","title":"On the Effects of Irrelevant Variables in Treatment Effect Estimation\n  with Deep Disentanglement","summary":"  Estimating treatment effects from observational data is paramount in\nhealthcare, education, and economics, but current deep disentanglement-based\nmethods to address selection bias are insufficiently handling irrelevant\nvariables. We demonstrate in experiments that this leads to prediction errors.\nWe disentangle pre-treatment variables with a deep embedding method and\nexplicitly identify and represent irrelevant variables, additionally to\ninstrumental, confounding and adjustment latent factors. To this end, we\nintroduce a reconstruction objective and create an embedding space for\nirrelevant variables using an attached autoencoder. Instead of relying on\nserendipitous suppression of irrelevant variables as in previous deep\ndisentanglement approaches, we explicitly force irrelevant variables into this\nembedding space and employ orthogonalization to prevent irrelevant information\nfrom leaking into the latent space representations of the other factors. Our\nexperiments with synthetic and real-world benchmark datasets show that we can\nbetter identify irrelevant variables and more precisely predict treatment\neffects than previous methods, while prediction quality degrades less when\nadditional irrelevant variables are introduced.\n","authors":["Ahmad Saeed Khan","Erik Schaffernicht","Johannes Andreas Stork"],"pdf_url":"https://arxiv.org/pdf/2407.20003v2.pdf","comment":"Paper is accepted at ECAI-2024"},{"id":"http://arxiv.org/abs/2408.14080v1","updated":"2024-08-26T08:02:57Z","published":"2024-08-26T08:02:57Z","title":"SONICS: Synthetic Or Not -- Identifying Counterfeit Songs","summary":"  The recent surge in AI-generated songs presents exciting possibilities and\nchallenges. While these tools democratize music creation, they also necessitate\nthe ability to distinguish between human-composed and AI-generated songs for\nsafeguarding artistic integrity and content curation. Existing research and\ndatasets in fake song detection only focus on singing voice deepfake detection\n(SVDD), where the vocals are AI-generated but the instrumental music is sourced\nfrom real songs. However, this approach is inadequate for contemporary\nend-to-end AI-generated songs where all components (vocals, lyrics, music, and\nstyle) could be AI-generated. Additionally, existing datasets lack lyrics-music\ndiversity, long-duration songs, and open fake songs. To address these gaps, we\nintroduce SONICS, a novel dataset for end-to-end Synthetic Song Detection\n(SSD), comprising over 97k songs with over 49k synthetic songs from popular\nplatforms like Suno and Udio. Furthermore, we highlight the importance of\nmodeling long-range temporal dependencies in songs for effective authenticity\ndetection, an aspect overlooked in existing methods. To capture these patterns,\nwe propose a novel model, SpecTTTra, that is up to 3 times faster and 6 times\nmore memory efficient compared to popular CNN and Transformer-based models\nwhile maintaining competitive performance. Finally, we offer both AI-based and\nHuman evaluation benchmarks, addressing another deficiency in current research.\n","authors":["Md Awsafur Rahman","Zaber Ibn Abdul Hakim","Najibul Haque Sarker","Bishmoy Paul","Shaikh Anowarul Fattah"],"pdf_url":"https://arxiv.org/pdf/2408.14080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14073v1","updated":"2024-08-26T07:56:17Z","published":"2024-08-26T07:56:17Z","title":"Score-based change point detection via tracking the best of infinitely\n  many experts","summary":"  We suggest a novel algorithm for online change point detection based on\nsequential score function estimation and tracking the best expert approach. The\ncore of the procedure is a version of the fixed share forecaster for the case\nof infinite number of experts and quadratic loss functions. The algorithm shows\na promising performance in numerical experiments on artificial and real-world\ndata sets. We also derive new upper bounds on the dynamic regret of the fixed\nshare forecaster with varying parameter, which are of independent interest.\n","authors":["Anna Markovich","Nikita Puchkin"],"pdf_url":"https://arxiv.org/pdf/2408.14073v1.pdf","comment":"43 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.02971v3","updated":"2024-08-26T07:46:53Z","published":"2023-11-06T09:17:18Z","title":"TabRepo: A Large Scale Repository of Tabular Model Evaluations and its\n  AutoML Applications","summary":"  We introduce TabRepo, a new dataset of tabular model evaluations and\npredictions. TabRepo contains the predictions and metrics of 1310 models\nevaluated on 200 classification and regression datasets. We illustrate the\nbenefit of our dataset in multiple ways. First, we show that it allows to\nperform analysis such as comparing Hyperparameter Optimization against current\nAutoML systems while also considering ensembling at marginal cost by using\nprecomputed model predictions. Second, we show that our dataset can be readily\nleveraged to perform transfer-learning. In particular, we show that applying\nstandard transfer-learning techniques allows to outperform current\nstate-of-the-art tabular systems in accuracy, runtime and latency.\n","authors":["David Salinas","Nick Erickson"],"pdf_url":"https://arxiv.org/pdf/2311.02971v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14063v1","updated":"2024-08-26T07:44:53Z","published":"2024-08-26T07:44:53Z","title":"Bridging the gap between Learning-to-plan, Motion Primitives and Safe\n  Reinforcement Learning","summary":"  Trajectory planning under kinodynamic constraints is fundamental for advanced\nrobotics applications that require dexterous, reactive, and rapid skills in\ncomplex environments. These constraints, which may represent task, safety, or\nactuator limitations, are essential for ensuring the proper functioning of\nrobotic platforms and preventing unexpected behaviors. Recent advances in\nkinodynamic planning demonstrate that learning-to-plan techniques can generate\ncomplex and reactive motions under intricate constraints. However, these\ntechniques necessitate the analytical modeling of both the robot and the entire\ntask, a limiting assumption when systems are extremely complex or when\nconstructing accurate task models is prohibitive. This paper addresses this\nlimitation by combining learning-to-plan methods with reinforcement learning,\nresulting in a novel integration of black-box learning of motion primitives and\noptimization. We evaluate our approach against state-of-the-art safe\nreinforcement learning methods, showing that our technique, particularly when\nexploiting task structure, outperforms baseline methods in challenging\nscenarios such as planning to hit in robot air hockey. This work demonstrates\nthe potential of our integrated approach to enhance the performance and safety\nof robots operating under complex kinodynamic constraints.\n","authors":["Piotr Kicki","Davide Tateo","Puze Liu","Jonas Guenster","Jan Peters","Krzysztof Walas"],"pdf_url":"https://arxiv.org/pdf/2408.14063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12961v2","updated":"2024-08-26T07:43:03Z","published":"2024-08-23T10:12:08Z","title":"Symplectic Bregman divergences","summary":"  We present a generalization of Bregman divergences in symplectic vector\nspaces that we term symplectic Bregman divergences. Symplectic Bregman\ndivergences are derived from a symplectic generalization of the Fenchel-Young\ninequality which relies on the notion of symplectic subdifferentials. The\nsymplectic Fenchel-Young inequality is obtained using the symplectic Fenchel\ntransform which is defined with respect to a linear symplectic form. When the\nsymplectic form is built from an inner product, we show that the corresponding\nsymplectic Bregman divergences amount to ordinary Bregman divergences with\nrespect to composite inner products. Some potential applications of symplectic\ndivergences in geometric mechanics, information geometry, and learning dynamics\nin machine learning are touched upon.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2408.12961v2.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2404.10635v4","updated":"2024-08-26T07:40:52Z","published":"2024-03-26T15:36:47Z","title":"Compressed Federated Reinforcement Learning with a Generative Model","summary":"  Reinforcement learning has recently gained unprecedented popularity, yet it\nstill grapples with sample inefficiency. Addressing this challenge, federated\nreinforcement learning (FedRL) has emerged, wherein agents collaboratively\nlearn a single policy by aggregating local estimations. However, this\naggregation step incurs significant communication costs. In this paper, we\npropose CompFedRL, a communication-efficient FedRL approach incorporating both\n\\textit{periodic aggregation} and (direct/error-feedback) compression\nmechanisms. Specifically, we consider compressed federated $Q$-learning with a\ngenerative model setup, where a central server learns an optimal $Q$-function\nby periodically aggregating compressed $Q$-estimates from local agents. For the\nfirst time, we characterize the impact of these two mechanisms (which have\nremained elusive) by providing a finite-time analysis of our algorithm,\ndemonstrating strong convergence behaviors when utilizing either direct or\nerror-feedback compression. Our bounds indicate improved solution accuracy\nconcerning the number of agents and other federated hyperparameters while\nsimultaneously reducing communication costs. To corroborate our theory, we also\nconduct in-depth numerical experiments to verify our findings, considering\nTop-$K$ and Sparsified-$K$ sparsification operators.\n","authors":["Ali Beikmohammadi","Sarit Khirirat","Sindri MagnÃºsson"],"pdf_url":"https://arxiv.org/pdf/2404.10635v4.pdf","comment":"European Conference on Machine Learning and Principles and Practice\n  of Knowledge Discovery in Databases (ECML-PKDD 2024)"},{"id":"http://arxiv.org/abs/2408.10174v2","updated":"2024-08-26T07:34:46Z","published":"2024-08-19T17:32:15Z","title":"SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From\n  Pre-Trained Foundation Models","summary":"  Deep model training on extensive datasets is increasingly becoming\ncost-prohibitive, prompting the widespread adoption of deep model fusion\ntechniques to leverage knowledge from pre-existing models. From simple weight\naveraging to more sophisticated methods like AdaMerging, model fusion\neffectively improves model performance and accelerates the development of new\nmodels. However, potential interference between parameters of individual models\nand the lack of interpretability in the fusion progress remain significant\nchallenges. Existing methods often try to resolve the parameter interference\nissue by evaluating attributes of parameters, such as their magnitude or sign,\nor by parameter pruning. In this study, we begin by examining the fine-tuning\nof linear layers through the lens of subspace analysis and explicitly define\nparameter interference as an optimization problem to shed light on this\nsubject. Subsequently, we introduce an innovative approach to model fusion\ncalled zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, which\nallows for the upscaling of source models into an MoE model without extra data\nor further training. Our approach relies on the observation that fine-tuning\nmostly keeps the important parts from the pre-training, but it uses less\nsignificant or unused areas to adapt to new tasks. Also, the issue of parameter\ninterference, which is intrinsically intractable in the original parameter\nspace, can be managed by expanding the dimensions. We conduct extensive\nexperiments across diverse scenarios, such as image classification and text\ngeneration tasks, using full fine-tuning and LoRA fine-tuning, and we apply our\nmethod to large language models (CLIP models, Flan-T5 models, and Mistral-7B\nmodels), highlighting the adaptability and scalability of SMILE. Code is\navailable at https://github.com/tanganke/fusion_bench\n","authors":["Anke Tang","Li Shen","Yong Luo","Shuai Xie","Han Hu","Lefei Zhang","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2408.10174v2.pdf","comment":"Code is available at https://github.com/tanganke/fusion_bench"},{"id":"http://arxiv.org/abs/2301.12778v3","updated":"2024-08-26T07:19:33Z","published":"2023-01-30T10:48:10Z","title":"Investigating Feature and Model Importance in Android Malware Detection:\n  An Implemented Survey and Experimental Comparison of ML-Based Methods","summary":"  The popularity of Android means it is a common target for malware. Over the\nyears, various studies have found that machine learning models can effectively\ndiscriminate malware from benign applications. However, as the operating system\nevolves, so does malware, bringing into question the findings of these previous\nstudies, many of which report very high accuracies using small, outdated, and\noften imbalanced datasets. In this paper, we reimplement 18 representative past\nworks and reevaluate them using a balanced, relevant, and up-to-date dataset\ncomprising 124,000 applications. We also carry out new experiments designed to\nfill holes in existing knowledge, and use our findings to identify the most\neffective features and models to use for Android malware detection within a\ncontemporary environment. We show that high detection accuracies (up to 96.8%)\ncan be achieved using features extracted through static analysis alone,\nyielding a modest benefit (1%) from using far more expensive dynamic analysis.\nAPI calls and opcodes are the most productive static and TCP network traffic\nprovide the most predictive dynamic features. Random forests are generally the\nmost effective model, outperforming more complex deep learning approaches.\nWhilst directly combining static and dynamic features is generally ineffective,\nensembling models separately leads to performances comparable to the best\nmodels but using less brittle features.\n","authors":["Ali Muzaffar","Hani Ragab Hassen","Hind Zantout","Michael A Lones"],"pdf_url":"https://arxiv.org/pdf/2301.12778v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13089v2","updated":"2024-08-26T06:40:07Z","published":"2024-08-23T14:16:10Z","title":"On the good reliability of an interval-based metric to validate\n  prediction uncertainty for machine learning regression tasks","summary":"  This short study presents an opportunistic approach to a (more) reliable\nvalidation method for prediction uncertainty average calibration. Considering\nthat variance-based calibration metrics (ZMS, NLL, RCE...) are quite sensitive\nto the presence of heavy tails in the uncertainty and error distributions, a\nshift is proposed to an interval-based metric, the Prediction Interval Coverage\nProbability (PICP). It is shown on a large ensemble of molecular properties\ndatasets that (1) sets of z-scores are well represented by Student's-$t(\\nu)$\ndistributions, $\\nu$ being the number of degrees of freedom; (2) accurate\nestimation of 95 $\\%$ prediction intervals can be obtained by the simple\n$2\\sigma$ rule for $\\nu>3$; and (3) the resulting PICPs are more quickly and\nreliably tested than variance-based calibration metrics. Overall, this method\nenables to test 20 $\\%$ more datasets than ZMS testing. Conditional calibration\nis also assessed using the PICP approach.\n","authors":["Pascal Pernot"],"pdf_url":"https://arxiv.org/pdf/2408.13089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14042v1","updated":"2024-08-26T06:39:49Z","published":"2024-08-26T06:39:49Z","title":"PAGE: Parametric Generative Explainer for Graph Neural Network","summary":"  This article introduces PAGE, a parameterized generative interpretive\nframework. PAGE is capable of providing faithful explanations for any graph\nneural network without necessitating prior knowledge or internal details.\nSpecifically, we train the auto-encoder to generate explanatory substructures\nby designing appropriate training strategy. Due to the dimensionality reduction\nof features in the latent space of the auto-encoder, it becomes easier to\nextract causal features leading to the model's output, which can be easily\nemployed to generate explanations. To accomplish this, we introduce an\nadditional discriminator to capture the causality between latent causal\nfeatures and the model's output. By designing appropriate optimization\nobjectives, the well-trained discriminator can be employed to constrain the\nencoder in generating enhanced causal features. Finally, these features are\nmapped to substructures of the input graph through the decoder to serve as\nexplanations. Compared to existing methods, PAGE operates at the sample scale\nrather than nodes or edges, eliminating the need for perturbation or encoding\nprocesses as seen in previous methods. Experimental results on both\nartificially synthesized and real-world datasets demonstrate that our approach\nnot only exhibits the highest faithfulness and accuracy but also significantly\noutperforms baseline models in terms of efficiency.\n","authors":["Yang Qiu","Wei Liu","Jun Wang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.14042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10737v2","updated":"2024-08-26T06:37:24Z","published":"2024-06-15T20:47:38Z","title":"Dynamic Domains, Dynamic Solutions: DPCore for Continual Test-Time\n  Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) seeks to adapt a source pre-trained\nmodel to continually changing, unlabeled target domains. Existing TTA methods\nare typically designed for environments where domain changes occur sequentially\nand can struggle in more dynamic scenarios, as illustrated in Figure\n\\ref{fig:settings}. Inspired by the principles of online K-Means, we introduce\na novel approach to CTTA through visual prompting. We propose a \\emph{Dynamic\nPrompt Coreset} that not only preserves knowledge from previously visited\ndomains but also accommodates learning from new potential domains. This is\ncomplemented by a distance-based \\emph{Weight Updating Mechanism} that ensures\nthe coreset remains current and relevant. Our approach employs a fixed model\narchitecture alongside the coreset and an innovative updating system to\neffectively mitigate challenges such as catastrophic forgetting and error\naccumulation. Extensive testing on four widely-used benchmarks demonstrates\nthat our method consistently outperforms state-of-the-art alternatives in both\nclassification and segmentation CTTA tasks across the structured and dynamic\nCTTA settings, with $99\\%$ fewer trainable parameters.\n","authors":["Yunbei Zhang","Akshay Mehra","Jihun Hamm"],"pdf_url":"https://arxiv.org/pdf/2406.10737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14037v1","updated":"2024-08-26T06:14:25Z","published":"2024-08-26T06:14:25Z","title":"Re-Mix: Optimizing Data Mixtures for Large Scale Imitation Learning","summary":"  Increasingly large imitation learning datasets are being collected with the\ngoal of training foundation models for robotics. However, despite the fact that\ndata selection has been of utmost importance in vision and natural language\nprocessing, little work in robotics has questioned what data such models should\nactually be trained on. In this work we investigate how to weigh different\nsubsets or ``domains'' of robotics datasets for robot foundation model\npre-training. Concrete, we use distributionally robust optimization (DRO) to\nmaximize worst-case performance across all possible downstream domains. Our\nmethod, Re-Mix, addresses the wide range of challenges that arise when applying\nDRO to robotics datasets including variability in action spaces and dynamics\nacross different datasets. Re-Mix employs early stopping, action normalization,\nand discretization to counteract these issues. Through extensive\nexperimentation on the largest open-source robot manipulation dataset, the Open\nX-Embodiment dataset, we demonstrate that data curation can have an outsized\nimpact on downstream performance. Specifically, domain weights learned by\nRe-Mix outperform uniform weights by 38\\% on average and outperform\nhuman-selected weights by 32\\% on datasets used to train existing generalist\nrobot policies, specifically the RT-X models.\n","authors":["Joey Hejna","Chethan Bhateja","Yichen Jian","Karl Pertsch","Dorsa Sadigh"],"pdf_url":"https://arxiv.org/pdf/2408.14037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19757v3","updated":"2024-08-26T05:54:22Z","published":"2024-05-30T07:06:02Z","title":"Improving SMOTE via Fusing Conditional VAE for Data-adaptive Noise\n  Filtering","summary":"  Recent advances in a generative neural network model extend the development\nof data augmentation methods. However, the augmentation methods based on the\nmodern generative models fail to achieve notable performance for class\nimbalance data compared to the conventional model, Synthetic Minority\nOversampling Technique (SMOTE). We investigate the problem of the generative\nmodel for imbalanced classification and introduce a framework to enhance the\nSMOTE algorithm using Variational Autoencoders (VAE). Our approach\nsystematically quantifies the density of data points in a low-dimensional\nlatent space using the VAE, simultaneously incorporating information on class\nlabels and classification difficulty. Then, the data points potentially\ndegrading the augmentation are systematically excluded, and the neighboring\nobservations are directly augmented on the data space. Empirical studies on\nseveral imbalanced datasets represent that this simple process innovatively\nimproves the conventional SMOTE algorithm over the deep learning models.\nConsequently, we conclude that the selection of minority data and the\ninterpolation in the data space are beneficial for imbalanced classification\nproblems with a relatively small number of data points.\n","authors":["Sungchul Hong","Seunghwan An","Jong-June Jeon"],"pdf_url":"https://arxiv.org/pdf/2405.19757v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18878v2","updated":"2024-08-26T05:54:21Z","published":"2024-03-27T10:46:24Z","title":"Teaching AI the Anatomy Behind the Scan: Addressing Anatomical Flaws in\n  Medical Image Segmentation with Learnable Prior","summary":"  Imposing key anatomical features, such as the number of organs, their shapes\nand relative positions, is crucial for building a robust multi-organ\nsegmentation model. Current attempts to incorporate anatomical features include\nbroadening the effective receptive field (ERF) size with data-intensive\nmodules, or introducing anatomical constraints that scales poorly to\nmulti-organ segmentation. We introduce a novel architecture called the\nAnatomy-Informed Cascaded Segmentation Network (AIC-Net). AIC-Net incorporates\na learnable input termed \"Anatomical Prior\", which can be adapted to\npatient-specific anatomy using a differentiable spatial deformation. The\ndeformed prior later guides decoder layers towards more anatomy-informed\npredictions. We repeat this process at a local patch level to enhance the\nrepresentation of intricate objects, resulting in a cascaded network structure.\nAIC-Net is a general method that enhances any existing segmentation models to\nbe more anatomy-aware. We have validated the performance of AIC-Net, with\nvarious backbones, on two multi-organ segmentation tasks: abdominal organs and\nvertebrae. For each respective task, our benchmarks demonstrate improved dice\nscore and Hausdorff distance.\n","authors":["Young Seok Jeon","Hongfei Yang","Huazhu Fu","Mengling Feng"],"pdf_url":"https://arxiv.org/pdf/2403.18878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14028v1","updated":"2024-08-26T05:38:27Z","published":"2024-08-26T05:38:27Z","title":"SurGen: Text-Guided Diffusion Model for Surgical Video Generation","summary":"  Diffusion-based video generation models have made significant strides,\nproducing outputs with improved visual fidelity, temporal coherence, and user\ncontrol. These advancements hold great promise for improving surgical education\nby enabling more realistic, diverse, and interactive simulation environments.\nIn this study, we introduce SurGen, a text-guided diffusion model tailored for\nsurgical video synthesis, producing the highest resolution and longest duration\nvideos among existing surgical video generation models. We validate the visual\nand temporal quality of the outputs using standard image and video generation\nmetrics. Additionally, we assess their alignment to the corresponding text\nprompts through a deep learning classifier trained on surgical data. Our\nresults demonstrate the potential of diffusion models to serve as valuable\neducational tools for surgical trainees.\n","authors":["Joseph Cho","Samuel Schmidgall","Cyril Zakka","Mrudang Mathur","Rohan Shad","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2408.14028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14025v1","updated":"2024-08-26T05:31:46Z","published":"2024-08-26T05:31:46Z","title":"An Item Response Theory-based R Module for Algorithm Portfolio Analysis","summary":"  Experimental evaluation is crucial in AI research, especially for assessing\nalgorithms across diverse tasks. Many studies often evaluate a limited set of\nalgorithms, failing to fully understand their strengths and weaknesses within a\ncomprehensive portfolio. This paper introduces an Item Response Theory (IRT)\nbased analysis tool for algorithm portfolio evaluation called AIRT-Module.\nTraditionally used in educational psychometrics, IRT models test question\ndifficulty and student ability using responses to test questions. Adapting IRT\nto algorithm evaluation, the AIRT-Module contains a Shiny web application and\nthe R package airt. AIRT-Module uses algorithm performance measures to compute\nanomalousness, consistency, and difficulty limits for an algorithm and the\ndifficulty of test instances. The strengths and weaknesses of algorithms are\nvisualised using the difficulty spectrum of the test instances. AIRT-Module\noffers a detailed understanding of algorithm capabilities across varied test\ninstances, thus enhancing comprehensive AI method assessment. It is available\nat https://sevvandi.shinyapps.io/AIRT/ .\n","authors":["Brodie Oldfield","Sevvandi Kandanaarachchi","Ziqi Xu","Mario AndrÃ©s MuÃ±oz"],"pdf_url":"https://arxiv.org/pdf/2408.14025v1.pdf","comment":"10 Pages, 6 Figures. Submitted to SoftwareX"},{"id":"http://arxiv.org/abs/2408.10566v2","updated":"2024-08-26T05:08:29Z","published":"2024-08-20T06:05:52Z","title":"SparseGrow: Addressing Growth-Induced Forgetting in Task-Agnostic\n  Continual Learning","summary":"  In continual learning (CL), model growth enhances adaptability over new data,\nimproving knowledge retention for more tasks. However, improper model growth\ncan lead to severe degradation of previously learned knowledge, an issue we\nname as growth-induced forgetting (GIFt), especially in task-agnostic CL using\nentire grown model for inference. Existing works, despite adopting model growth\nand random initialization for better adaptability, often fail to recognize the\npresence of GIFt caused by improper model growth. This oversight limits\ncomprehensive control of forgetting and hinders full utilization of model\ngrowth. We are the first in CL to identify this issue and conduct an in-depth\nstudy on root cause of GIFt, where layer expansion stands out among model\ngrowth strategies, widening layers without affecting model functionality. Yet,\ndirect adoption of layer expansion presents challenges. It lacks data-driven\ncontrol and initialization of expanded parameters to balance adaptability and\nknowledge retention. This paper presents a novel SparseGrow approach to\novercome the issue of GIFt while enhancing adaptability over new data.\nSparseGrow employs data-driven sparse layer expansion to control efficient\nparameter usage during growth, reducing GIFt from excessive growth and\nfunctionality changes. It also combines sparse growth with on-data\ninitialization at training late-stage to create partially 0-valued expansions\nthat fit learned distribution, enhancing retention and adaptability. To further\nminimize forgetting, freezing is applied by calculating the sparse mask,\nallowing data-driven preservation of important parameters. Through experiments\nacross datasets with various settings, cases and task numbers, we demonstrate\nthe necessity of layer expansion and showcase the effectiveness of SparseGrow\nin overcoming GIFt, highlighting its adaptability and knowledge retention for\nincremental tasks.\n","authors":["Yuqing Zhao","Divya Saxena","Jiannong Cao","Xiaoyun Liu","Changlin Song"],"pdf_url":"https://arxiv.org/pdf/2408.10566v2.pdf","comment":"This paper has been submitted to the AAAI conference. If accepted,\n  the final version will be updated to reflect the conference proceedings"},{"id":"http://arxiv.org/abs/2407.10784v3","updated":"2024-08-26T04:58:15Z","published":"2024-07-15T15:02:53Z","title":"AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware\n  Uncertainty Calibrator and Label Distribution Handler","summary":"  In real-world scenarios, tabular data often suffer from distribution shifts\nthat threaten the performance of machine learning models. Despite its\nprevalence and importance, handling distribution shifts in the tabular domain\nremains underexplored due to the inherent challenges within the tabular data\nitself. In this sense, test-time adaptation (TTA) offers a promising solution\nby adapting models to target data without accessing source data, crucial for\nprivacy-sensitive tabular domains. However, existing TTA methods either 1)\noverlook the nature of tabular distribution shifts, often involving label\ndistribution shifts, or 2) impose architectural constraints on the model,\nleading to a lack of applicability. To this end, we propose AdapTable, a novel\nTTA framework for tabular data. AdapTable operates in two stages: 1)\ncalibrating model predictions using a shift-aware uncertainty calibrator, and\n2) adjusting these predictions to match the target label distribution with a\nlabel distribution handler. We validate the effectiveness of AdapTable through\ntheoretical analysis and extensive experiments on various distribution shift\nscenarios. Our results demonstrate AdapTable's ability to handle various\nreal-world distribution shifts, achieving up to a 16% improvement on the HELOC\ndataset.\n","authors":["Changhun Kim","Taewon Kim","Seungyeon Woo","June Yong Yang","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2407.10784v3.pdf","comment":"Under Review at AAAI 2025"},{"id":"http://arxiv.org/abs/2408.14014v1","updated":"2024-08-26T04:39:33Z","published":"2024-08-26T04:39:33Z","title":"Category-Theoretical and Topos-Theoretical Frameworks in Machine\n  Learning: A Survey","summary":"  In this survey, we provide an overview of category theory-derived machine\nlearning from four mainstream perspectives: gradient-based learning,\nprobability-based learning, invariance and equivalence-based learning, and\ntopos-based learning. For the first three topics, we primarily review research\nin the past five years, updating and expanding on the previous survey by\nShiebler et al.. The fourth topic, which delves into higher category theory,\nparticularly topos theory, is surveyed for the first time in this paper. In\ncertain machine learning methods, the compositionality of functors plays a\nvital role, prompting the development of specific categorical frameworks.\nHowever, when considering how the global properties of a network reflect in\nlocal structures and how geometric properties are expressed with logic, the\ntopos structure becomes particularly significant and profound.\n","authors":["Yiyang Jia","Guohong Peng","Zheng Yang","Tianhao Chen"],"pdf_url":"https://arxiv.org/pdf/2408.14014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14010v1","updated":"2024-08-26T04:31:55Z","published":"2024-08-26T04:31:55Z","title":"Improving Water Quality Time-Series Prediction in Hong Kong using\n  Sentinel-2 MSI Data and Google Earth Engine Cloud Computing","summary":"  Effective water quality monitoring in coastal regions is crucial due to the\nprogressive deterioration caused by pollution and human activities. To address\nthis, this study develops time-series models to predict chlorophyll-a (Chl-a),\nsuspended solids (SS), and turbidity using Sentinel-2 satellite data and Google\nEarth Engine (GEE) in the coastal regions of Hong Kong. Leveraging Long\nShort-Term Memory (LSTM) Recurrent Neural Networks, the study incorporates\nextensive temporal datasets to enhance prediction accuracy. The models utilize\nspectral data from Sentinel-2, focusing on optically active components, and\ndemonstrate that selected variables closely align with the spectral\ncharacteristics of Chl-a and SS. The results indicate improved predictive\nperformance over previous methods, highlighting the potential for remote\nsensing technology in continuous and comprehensive water quality assessment.\n","authors":["Rohin Sood","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.14010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14001v1","updated":"2024-08-26T03:58:20Z","published":"2024-08-26T03:58:20Z","title":"Decentralized Federated Learning with Model Caching on Mobile Agents","summary":"  Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.\n","authors":["Xiaoyu Wang","Guojun Xiong","Houwei Cao","Jian Li","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14001v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2407.06886v7","updated":"2024-08-26T03:25:12Z","published":"2024-07-09T14:14:47Z","title":"Aligning Cyber Space with Physical World: A Comprehensive Survey on\n  Embodied AI","summary":"  Embodied Artificial Intelligence (Embodied AI) is crucial for achieving\nArtificial General Intelligence (AGI) and serves as a foundation for various\napplications that bridge cyberspace and the physical world. Recently, the\nemergence of Multi-modal Large Models (MLMs) and World Models (WMs) have\nattracted significant attention due to their remarkable perception,\ninteraction, and reasoning capabilities, making them a promising architecture\nfor the brain of embodied agents. However, there is no comprehensive survey for\nEmbodied AI in the era of MLMs. In this survey, we give a comprehensive\nexploration of the latest advancements in Embodied AI. Our analysis firstly\nnavigates through the forefront of representative works of embodied robots and\nsimulators, to fully understand the research focuses and their limitations.\nThen, we analyze four main research targets: 1) embodied perception, 2)\nembodied interaction, 3) embodied agent, and 4) sim-to-real adaptation,\ncovering the state-of-the-art methods, essential paradigms, and comprehensive\ndatasets. Additionally, we explore the complexities of MLMs in virtual and real\nembodied agents, highlighting their significance in facilitating interactions\nin dynamic digital and physical environments. Finally, we summarize the\nchallenges and limitations of embodied AI and discuss their potential future\ndirections. We hope this survey will serve as a foundational reference for the\nresearch community and inspire continued innovation. The associated project can\nbe found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.\n","authors":["Yang Liu","Weixing Chen","Yongjie Bai","Xiaodan Liang","Guanbin Li","Wen Gao","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.06886v7.pdf","comment":"The first comprehensive review of Embodied AI in the era of MLMs, 39\n  pages. We also provide the paper list for Embodied AI:\n  https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List"},{"id":"http://arxiv.org/abs/2408.13991v1","updated":"2024-08-26T03:19:52Z","published":"2024-08-26T03:19:52Z","title":"Dual-CBA: Improving Online Continual Learning via Dual Continual Bias\n  Adaptors from a Bi-level Optimization Perspective","summary":"  In online continual learning (CL), models trained on changing distributions\neasily forget previously learned knowledge and bias toward newly received\ntasks. To address this issue, we present Continual Bias Adaptor (CBA), a\nbi-level framework that augments the classification network to adapt to\ncatastrophic distribution shifts during training, enabling the network to\nachieve a stable consolidation of all seen tasks. However, the CBA module\nadjusts distribution shifts in a class-specific manner, exacerbating the\nstability gap issue and, to some extent, fails to meet the need for continual\ntesting in online CL. To mitigate this challenge, we further propose a novel\nclass-agnostic CBA module that separately aggregates the posterior\nprobabilities of classes from new and old tasks, and applies a stable\nadjustment to the resulting posterior probabilities. We combine the two kinds\nof CBA modules into a unified Dual-CBA module, which thus is capable of\nadapting to catastrophic distribution shifts and simultaneously meets the\nreal-time testing requirements of online CL. Besides, we propose Incremental\nBatch Normalization (IBN), a tailored BN module to re-estimate its population\nstatistics for alleviating the feature bias arising from the inner loop\noptimization problem of our bi-level framework. To validate the effectiveness\nof the proposed method, we theoretically provide some insights into how it\nmitigates catastrophic distribution shifts, and empirically demonstrate its\nsuperiority through extensive experiments based on four rehearsal-based\nbaselines and three public continual learning benchmarks.\n","authors":["Quanziang Wang","Renzhen Wang","Yichen Wu","Xixi Jia","Minghao Zhou","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2408.13991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15951v4","updated":"2024-08-26T03:09:46Z","published":"2023-06-28T06:21:22Z","title":"Reduce Computational Complexity for Convolutional Layers by Skipping\n  Zeros","summary":"  Convolutional neural networks necessitate good algorithms to reduce\ncomplexity, and sufficient utilization of parallel processors for acceleration.\nWithin convolutional layers, there are three types of operators: convolution\nused in forward propagation, deconvolution and dilated-convolution utilized in\nbackward propagation. During the execution of these operators, zeros are\ntypically added to tensors, leading to redundant calculations and unnecessary\nstrain on hardware. To circumvent these inefficiencies, we propose the C-K-S\nalgorithm, accompanied by efficient GPU implementations. C-K-S trims filters to\nexclude zero-padding. For deconvolution and dilated-convolution, C-K-S\ntransforms sparse tensors into dense tensors, and standardizes the local\ncomputational rules to simplify the hardware control. The experimental results\ndemonstrate that C-K-S offers good performance in terms of speed and\nconvergence, surpassing the capabilities of PyTorch and cuDNN in certain\nscenarios.\n","authors":["Zhiyi Zhang","Pengfei Zhang","Zhuopin Xu","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2306.15951v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08713v2","updated":"2024-08-26T03:03:47Z","published":"2024-08-16T12:51:52Z","title":"Beyond KAN: Introducing KarSein for Adaptive High-Order Feature\n  Interaction Modeling in CTR Prediction","summary":"  Modeling feature interactions is crucial for click-through rate (CTR)\nprediction, particularly when it comes to high-order explicit interactions.\nTraditional methods struggle with this task because they often predefine a\nmaximum interaction order, which relies heavily on prior knowledge and can\nlimit the model's effectiveness. Additionally, modeling high-order interactions\ntypically leads to increased computational costs. Therefore, the challenge lies\nin adaptively modeling high-order feature interactions while maintaining\nefficiency. To address this issue, we introduce Kolmogorov-Arnold Represented\nSparse Efficient Interaction Network (KarSein), designed to optimize both\npredictive accuracy and computational efficiency. We firstly identify\nlimitations of directly applying Kolmogorov-Arnold Networks (KAN) to CTR and\nthen introduce KarSein to overcome these issues. It features a novel\narchitecture that reduces the computational costs of KAN and supports embedding\nvectors as feature inputs. Additionally, KarSein employs guided symbolic\nregression to address the challenge of KAN in spontaneously learning\nmultiplicative relationships. Extensive experiments demonstrate KarSein's\nsuperior performance, achieving significant predictive accuracy with minimal\ncomputational overhead. Furthermore, KarSein maintains strong global\nexplainability while enabling the removal of redundant features, resulting in a\nsparse network structure. These advantages also position KarSein as a promising\nmethod for efficient inference.\n","authors":["Yunxiao Shi","Wujiang Xu","Mingyu Jin","Haimin Zhang","Qiang Wu","Yongfeng Zhang","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2408.08713v2.pdf","comment":"KarSein for CTR"},{"id":"http://arxiv.org/abs/2304.06879v2","updated":"2024-08-26T02:59:10Z","published":"2023-04-14T01:12:48Z","title":"Performative Prediction with Neural Networks","summary":"  Performative prediction is a framework for learning models that influence the\ndata they intend to predict. We focus on finding classifiers that are\nperformatively stable, i.e. optimal for the data distribution they induce.\nStandard convergence results for finding a performatively stable classifier\nwith the method of repeated risk minimization assume that the data distribution\nis Lipschitz continuous to the model's parameters. Under this assumption, the\nloss must be strongly convex and smooth in these parameters; otherwise, the\nmethod will diverge for some problems. In this work, we instead assume that the\ndata distribution is Lipschitz continuous with respect to the model's\npredictions, a more natural assumption for performative systems. As a result,\nwe are able to significantly relax the assumptions on the loss function. In\nparticular, we do not need to assume convexity with respect to the model's\nparameters. As an illustration, we introduce a resampling procedure that models\nrealistic distribution shifts and show that it satisfies our assumptions. We\nsupport our theory by showing that one can learn performatively stable\nclassifiers with neural networks making predictions about real data that shift\naccording to our proposed procedure.\n","authors":["Mehrnaz Mofakhami","Ioannis Mitliagkas","Gauthier Gidel"],"pdf_url":"https://arxiv.org/pdf/2304.06879v2.pdf","comment":"Published at AISTATS 2023; Theoretical results extended"},{"id":"http://arxiv.org/abs/2408.13282v1","updated":"2024-08-26T02:53:55Z","published":"2024-08-26T02:53:55Z","title":"Question answering system of bridge design specification based on large\n  language model","summary":"  This paper constructs question answering system for bridge design\nspecification based on large language model. Three implementation schemes are\ntried: full fine-tuning of the Bert pretrained model, parameter-efficient\nfine-tuning of the Bert pretrained model, and self-built language model from\nscratch. Through the self-built question and answer task dataset, based on the\ntensorflow and keras deep learning platform framework, the model is constructed\nand trained to predict the start position and end position of the answer in the\nbridge design specification given by the user. The experimental results show\nthat full fine-tuning of the Bert pretrained model achieves 100% accuracy in\nthe training-dataset, validation-dataset and test-dataset, and the system can\nextract the answers from the bridge design specification given by the user to\nanswer various questions of the user; While parameter-efficient fine-tuning of\nthe Bert pretrained model and self-built language model from scratch perform\nwell in the training-dataset, their generalization ability in the test-dataset\nneeds to be improved. The research of this paper provides a useful reference\nfor the development of question answering system in professional field.\n","authors":["Leye Zhang","Xiangxiang Tian","Hongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.13282v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.13986v1","updated":"2024-08-26T02:36:55Z","published":"2024-08-26T02:36:55Z","title":"AgentMove: Predicting Human Mobility Anywhere Using Large Language Model\n  based Agentic Framework","summary":"  Human mobility prediction plays a crucial role in various real-world\napplications. Although deep learning based models have shown promising results\nover the past decade, their reliance on extensive private mobility data for\ntraining and their inability to perform zero-shot predictions, have hindered\nfurther advancements. Recently, attempts have been made to apply large language\nmodels (LLMs) to mobility prediction task. However, their performance has been\nconstrained by the absence of a systematic design of workflow. They directly\ngenerate the final output using LLMs, which limits the potential of LLMs to\nuncover complex mobility patterns and underestimates their extensive reserve of\nglobal geospatial knowledge. In this paper, we introduce AgentMove, a\nsystematic agentic prediction framework to achieve generalized mobility\nprediction for any cities worldwide. In AgentMove, we first decompose the\nmobility prediction task into three sub-tasks and then design corresponding\nmodules to complete these subtasks, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments on mobility data from two sources in 12 cities\ndemonstrate that AgentMove outperforms the best baseline more than 8% in\nvarious metrics and it shows robust predictions with various LLMs as base and\nalso less geographical bias across cities. Codes and data can be found in\nhttps://github.com/tsinghua-fib-lab/AgentMove.\n","authors":["Jie Feng","Yuwei Du","Jie Zhao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2408.13986v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2209.11691v4","updated":"2024-08-26T02:33:01Z","published":"2022-09-23T16:11:09Z","title":"Linear multidimensional regression with interactive fixed-effects","summary":"  This paper studies a linear and additively separable model for\nmultidimensional panel data of three or more dimensions with unobserved\ninteractive fixed effects. Two approaches are considered to account for these\nunobserved interactive fixed-effects when estimating coefficients on the\nobserved covariates. First, the model is embedded within the standard two\ndimensional panel framework and restrictions are formed under which the factor\nstructure methods in Bai (2009) lead to consistent estimation of model\nparameters, but at slow rates of convergence. The second approach develops a\nkernel weighted fixed-effects method that is more robust to the\nmultidimensional nature of the problem and can achieve the parametric rate of\nconsistency under certain conditions. Theoretical results and simulations show\nsome benefits to standard two-dimensional panel methods when the structure of\nthe interactive fixed-effect term is known, but also highlight how the kernel\nweighted method performs well without knowledge of this structure. The methods\nare implemented to estimate the demand elasticity for beer.\n","authors":["Hugo Freeman"],"pdf_url":"https://arxiv.org/pdf/2209.11691v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12095v2","updated":"2024-08-26T02:26:31Z","published":"2024-08-22T03:08:49Z","title":"uMedSum: A Unified Framework for Advancing Medical Abstractive\n  Summarization","summary":"  Medical abstractive summarization faces the challenge of balancing\nfaithfulness and informativeness. Current methods often sacrifice key\ninformation for faithfulness or introduce confabulations when prioritizing\ninformativeness. While recent advancements in techniques like in-context\nlearning (ICL) and fine-tuning have improved medical summarization, they often\noverlook crucial aspects such as faithfulness and informativeness without\nconsidering advanced methods like model reasoning and self-improvement.\nMoreover, the field lacks a unified benchmark, hindering systematic evaluation\ndue to varied metrics and datasets. This paper addresses these gaps by\npresenting a comprehensive benchmark of six advanced abstractive summarization\nmethods across three diverse datasets using five standardized metrics. Building\non these findings, we propose uMedSum, a modular hybrid summarization framework\nthat introduces novel approaches for sequential confabulation removal followed\nby key missing information addition, ensuring both faithfulness and\ninformativeness. Our work improves upon previous GPT-4-based state-of-the-art\n(SOTA) medical summarization methods, significantly outperforming them in both\nquantitative metrics and qualitative domain expert evaluations. Notably, we\nachieve an average relative performance improvement of 11.8% in reference-free\nmetrics over the previous SOTA. Doctors prefer uMedSum's summaries 6 times more\nthan previous SOTA in difficult cases where there are chances of confabulations\nor missing information. These results highlight uMedSum's effectiveness and\ngeneralizability across various datasets and metrics, marking a significant\nadvancement in medical summarization.\n","authors":["Aishik Nagar","Yutong Liu","Andy T. Liu","Viktor Schlegel","Vijay Prakash Dwivedi","Arun-Kumar Kaliya-Perumal","Guna Pratheep Kalanchiam","Yili Tang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2408.12095v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.10650v2","updated":"2024-08-26T02:19:11Z","published":"2024-03-15T19:35:10Z","title":"PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time\n  Adaptation","summary":"  Real-world vision models in dynamic environments face rapid shifts in domain\ndistributions, leading to decreased recognition performance. Using unlabeled\ntest data, continual test-time adaptation (CTTA) directly adjusts a pre-trained\nsource discriminative model to these changing domains. A highly effective CTTA\nmethod involves applying layer-wise adaptive learning rates for selectively\nadapting pre-trained layers. However, it suffers from the poor estimation of\ndomain shift and the inaccuracies arising from the pseudo-labels. This work\naims to overcome these limitations by identifying layers for adaptation via\nquantifying model prediction uncertainty without relying on pseudo-labels. We\nutilize the magnitude of gradients as a metric, calculated by backpropagating\nthe KL divergence between the softmax output and a uniform distribution, to\nselect layers for further adaptation. Subsequently, for the parameters\nexclusively belonging to these selected layers, with the remaining ones frozen,\nwe evaluate their sensitivity to approximate the domain shift and adjust their\nlearning rates accordingly. We conduct extensive image classification\nexperiments on CIFAR-10C, CIFAR-100C, and ImageNet-C, demonstrating the\nsuperior efficacy of our method compared to prior approaches.\n","authors":["Sarthak Kumar Maharana","Baoming Zhang","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2403.10650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13979v1","updated":"2024-08-26T02:09:05Z","published":"2024-08-26T02:09:05Z","title":"Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models","summary":"  With the prevalence of large-scale pretrained vision-language models (VLMs),\nsuch as CLIP, soft-prompt tuning has become a popular method for adapting these\nmodels to various downstream tasks. However, few works delve into the inherent\nproperties of learnable soft-prompt vectors, specifically the impact of their\nnorms to the performance of VLMs. This motivates us to pose an unexplored\nresearch question: ``Do we need to normalize the soft prompts in VLMs?'' To\nfill this research gap, we first uncover a phenomenon, called the\n\\textbf{Low-Norm Effect} by performing extensive corruption experiments,\nsuggesting that reducing the norms of certain learned prompts occasionally\nenhances the performance of VLMs, while increasing them often degrades it. To\nharness this effect, we propose a novel method named \\textbf{N}ormalizing\nth\\textbf{e} soft-pro\\textbf{m}pt v\\textbf{e}ctors of vi\\textbf{si}on-language\nmodel\\textbf{s} (\\textbf{Nemesis}) to normalize soft-prompt vectors in VLMs. To\nthe best of our knowledge, our work is the first to systematically investigate\nthe role of norms of soft-prompt vector in VLMs, offering valuable insights for\nfuture research in soft-prompt tuning. The code is available at\n\\texttt{\\href{https://github.com/ShyFoo/Nemesis}{https://github.com/ShyFoo/Nemesis}}.\n","authors":["Shuai Fu","Xiequn Wang","Qiushi Huang","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.13979v1.pdf","comment":"Accepted at ICLR 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2408.02679v2","updated":"2024-08-26T01:17:43Z","published":"2024-07-31T08:44:34Z","title":"Visual Analysis of Multi-outcome Causal Graphs","summary":"  We introduce a visual analysis method for multiple causal graphs with\ndifferent outcome variables, namely, multi-outcome causal graphs. Multi-outcome\ncausal graphs are important in healthcare for understanding multimorbidity and\ncomorbidity. To support the visual analysis, we collaborated with medical\nexperts to devise two comparative visualization techniques at different stages\nof the analysis process. First, a progressive visualization method is proposed\nfor comparing multiple state-of-the-art causal discovery algorithms. The method\ncan handle mixed-type datasets comprising both continuous and categorical\nvariables and assist in the creation of a fine-tuned causal graph of a single\noutcome. Second, a comparative graph layout technique and specialized visual\nencodings are devised for the quick comparison of multiple causal graphs. In\nour visual analysis approach, analysts start by building individual causal\ngraphs for each outcome variable, and then, multi-outcome causal graphs are\ngenerated and visualized with our comparative technique for analyzing\ndifferences and commonalities of these causal graphs. Evaluation includes\nquantitative measurements on benchmark datasets, a case study with a medical\nexpert, and expert user studies with real-world health research data.\n","authors":["Mengjie Fan","Jinlu Yu","Daniel Weiskopf","Nan Cao","Huai-Yu Wang","Liang Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10148v2","updated":"2024-08-26T01:08:49Z","published":"2024-06-14T15:59:36Z","title":"A Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with\n  Coupled Constraints","summary":"  Interest in bilevel optimization has grown in recent years, partially due to\nits applications to tackle challenging machine-learning problems. Several\nexciting recent works have been centered around developing efficient\ngradient-based algorithms that can solve bilevel optimization problems with\nprovable guarantees. However, the existing literature mainly focuses on bilevel\nproblems either without constraints, or featuring only simple constraints that\ndo not couple variables across the upper and lower levels, excluding a range of\ncomplex applications. Our paper studies this challenging but less explored\nscenario and develops a (fully) first-order algorithm, which we term BLOCC, to\ntackle BiLevel Optimization problems with Coupled Constraints. We establish\nrigorous convergence theory for the proposed algorithm and demonstrate its\neffectiveness on two well-known real-world applications - hyperparameter\nselection in support vector machine (SVM) and infrastructure planning in\ntransportation networks using the real data from the city of Seville.\n","authors":["Liuyuan Jiang","Quan Xiao","Victor M. Tenorio","Fernando Real-Rojas","Antonio G. Marques","Tianyi Chen"],"pdf_url":"https://arxiv.org/pdf/2406.10148v2.pdf","comment":"In this version, we have made the following updates: (1) Added a\n  sensitivity analysis of the algorithm's hyperparameters (stepsize and penalty\n  constant) in Appendix G. (2) Included a computational complexity analysis and\n  comparison in Appendix H. (3) Explicitly stated the inner-loop stepsizes in\n  Remarks 2 and 3"},{"id":"http://arxiv.org/abs/2406.18747v2","updated":"2024-08-26T01:07:11Z","published":"2024-06-26T20:25:53Z","title":"A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond\n  Four Stems","summary":"  Despite significant recent progress across multiple subtasks of audio source\nseparation, few music source separation systems support separation beyond the\nfour-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current\nsystems that support source separation beyond this setup, most continue to rely\non an inflexible decoder setup that can only support a fixed pre-defined set of\nstems. Increasing stem support in these inflexible systems correspondingly\nrequires increasing computational complexity, rendering extensions of these\nsystems computationally infeasible for long-tail instruments. In this work, we\npropose Banquet, a system that allows source separation of multiple stems using\njust one decoder. A bandsplit source separation model is extended to work in a\nquery-based setup in tandem with a music instrument recognition PaSST model. On\nthe MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached\nthe performance level of the significantly more complex 6-stem Hybrid\nTransformer Demucs on VDBO stems and outperformed it on guitar and piano. The\nquery-based setup allows for the separation of narrow instrument classes such\nas clean acoustic guitars, and can be successfully applied to the extraction of\nless common stems such as reeds and organs. Implementation is available at\nhttps://github.com/kwatcharasupat/query-bandit.\n","authors":["Karn N. Watcharasupat","Alexander Lerch"],"pdf_url":"https://arxiv.org/pdf/2406.18747v2.pdf","comment":"Accepted to the 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024). Camera-ready version"},{"id":"http://arxiv.org/abs/2407.07275v2","updated":"2024-08-26T00:55:01Z","published":"2024-07-09T23:39:37Z","title":"Remastering Divide and Remaster: A Cinematic Audio Source Separation\n  Dataset with Multilingual Support","summary":"  Cinematic audio source separation (CASS), as a problem of extracting the\ndialogue, music, and effects stems from their mixture, is a relatively new\nsubtask of audio source separation. To date, only one publicly available\ndataset exists for CASS, that is, the Divide and Remaster (DnR) dataset, which\nis currently at version 2. While DnR v2 has been an incredibly useful resource\nfor CASS, several areas of improvement have been identified, particularly\nthrough its use in the 2023 Sound Demixing Challenge. In this work, we develop\nversion 3 of the DnR dataset, addressing issues relating to vocal content in\nnon-dialogue stems, loudness distributions, mastering process, and linguistic\ndiversity. In particular, the dialogue stem of DnR v3 includes speech content\nfrom more than 30 languages from multiple families including but not limited to\nthe Germanic, Romance, Indo-Aryan, Dravidian, Malayo-Polynesian, and Bantu\nfamilies. Benchmark results using the Bandit model indicated that training on\nmultilingual data yields significant generalizability to the model even in\nlanguages with low data availability. Even in languages with high data\navailability, the multilingual model often performs on par or better than\ndedicated models trained on monolingual CASS datasets. Dataset and model\nimplementation will be made available at\nhttps://github.com/kwatcharasupat/source-separation-landing.\n","authors":["Karn N. Watcharasupat","Chih-Wei Wu","Iroro Orife"],"pdf_url":"https://arxiv.org/pdf/2407.07275v2.pdf","comment":"Accepted to the 5th IEEE International Symposium on the Internet of\n  Sounds. Camera-ready version"},{"id":"http://arxiv.org/abs/2405.00697v2","updated":"2024-08-26T00:53:17Z","published":"2024-04-10T11:20:52Z","title":"Unveiling Nonlinear Dynamics in Catastrophe Bond Pricing: A Machine\n  Learning Perspective","summary":"  This paper explores the implications of using machine learning models in the\npricing of catastrophe (CAT) bonds. By integrating advanced machine learning\ntechniques, our approach uncovers nonlinear relationships and complex\ninteractions between key risk factors and CAT bond spreads -- dynamics that are\noften overlooked by traditional linear regression models. Using primary market\nCAT bond transaction records between January 1999 and March 2021, our findings\ndemonstrate that machine learning models not only enhance the accuracy of CAT\nbond pricing but also provide a deeper understanding of how various risk\nfactors interact and influence bond prices in a nonlinear way. These findings\nsuggest that investors and issuers can benefit from incorporating machine\nlearning to better capture the intricate interplay between risk factors when\npricing CAT bonds. The results also highlight the potential for machine\nlearning models to refine our understanding of asset pricing in markets\ncharacterized by complex risk structures.\n","authors":["Xiaowei Chen","Hong Li","Yufan Lu","Rui Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.00697v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03588v2","updated":"2024-08-26T00:52:40Z","published":"2024-08-07T07:04:29Z","title":"Facing the Music: Tackling Singing Voice Separation in Cinematic Audio\n  Source Separation","summary":"  Cinematic audio source separation (CASS), as a standalone problem of\nextracting individual stems from their mixture, is a fairly new subtask of\naudio source separation. A typical setup of CASS is a three-stem problem, with\nthe aim of separating the mixture into the dialogue (DX), music (MX), and\neffects (FX) stems. Given the creative nature of cinematic sound production,\nhowever, several edge cases exist; some sound sources do not fit neatly in any\nof these three stems, necessitating the use of additional auxiliary stems in\nproduction. One very common edge case is the singing voice in film audio, which\nmay belong in either the DX or MX or neither, depending heavily on the\ncinematic context. In this work, we demonstrate a very straightforward\nextension of the dedicated-decoder Bandit and query-based single-decoder\nBanquet models to a four-stem problem, treating non-musical dialogue,\ninstrumental music, singing voice, and effects as separate stems.\nInterestingly, the query-based Banquet model outperformed the dedicated-decoder\nBandit model. We hypothesized that this is due to a better feature alignment at\nthe bottleneck as enforced by the band-agnostic FiLM layer. Dataset and model\nimplementation will be made available at\nhttps://github.com/kwatcharasupat/source-separation-landing.\n","authors":["Karn N. Watcharasupat","Chih-Wei Wu","Iroro Orife"],"pdf_url":"https://arxiv.org/pdf/2408.03588v2.pdf","comment":"Submitted to the Late-Breaking Demo Session of the 25th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2024"},{"id":"http://arxiv.org/abs/2402.17131v2","updated":"2024-08-26T23:59:43Z","published":"2024-02-27T01:53:02Z","title":"Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers\n  and RNNs Trained with a New Loss Function","summary":"  Glycosylation, a protein modification, has multiple essential functional and\nstructural roles. O-GlcNAcylation, a subtype of glycosylation, has the\npotential to be an important target for therapeutics, but methods to reliably\npredict O-GlcNAcylation sites had not been available until 2023; a 2021 review\ncorrectly noted that published models were insufficient and failed to\ngeneralize. Moreover, many are no longer usable. In 2023, a considerably better\nRNN model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset\nwas published. This article first sought to improve these metrics using\ntransformer encoders. While transformers displayed high performance on this\ndataset, their performance was inferior to that of the previously published\nRNN. We then created a new loss function, which we call the weighted focal\ndifferentiable MCC, to improve the performance of classification models. RNN\nmodels trained with this new function display superior performance to models\ntrained using the weighted cross-entropy loss; this new function can also be\nused to fine-tune trained models. A two-cell RNN trained with this loss\nachieves state-of-the-art performance in O-GlcNAcylation site prediction with\nan F$_1$ score of 38.88% and an MCC of 38.20% on that large dataset.\n","authors":["Pedro Seber"],"pdf_url":"https://arxiv.org/pdf/2402.17131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.08658v2","updated":"2024-08-26T23:24:52Z","published":"2022-02-17T13:43:06Z","title":"The merged-staircase property: a necessary and nearly sufficient\n  condition for SGD learning of sparse functions on two-layer neural networks","summary":"  It is currently known how to characterize functions that neural networks can\nlearn with SGD for two extremal parameterizations: neural networks in the\nlinear regime, and neural networks with no structural constraints. However, for\nthe main parametrization of interest (non-linear but regular networks) no tight\ncharacterization has yet been achieved, despite significant developments.\n  We take a step in this direction by considering depth-2 neural networks\ntrained by SGD in the mean-field regime. We consider functions on binary inputs\nthat depend on a latent low-dimensional subspace (i.e., small number of\ncoordinates). This regime is of interest since it is poorly understood how\nneural networks routinely tackle high-dimensional datasets and adapt to latent\nlow-dimensional structure without suffering from the curse of dimensionality.\nAccordingly, we study SGD-learnability with $O(d)$ sample complexity in a large\nambient dimension $d$.\n  Our main results characterize a hierarchical property, the \"merged-staircase\nproperty\", that is both necessary and nearly sufficient for learning in this\nsetting.\n  We further show that non-linear training is necessary: for this class of\nfunctions, linear methods on any feature map (e.g., the NTK) are not capable of\nlearning efficiently. The key tools are a new \"dimension-free\" dynamics\napproximation result that applies to functions defined on a latent space of\nlow-dimension, a proof of global convergence based on polynomial identity\ntesting, and an improvement of lower bounds against linear methods for\nnon-almost orthogonal functions.\n","authors":["Emmanuel Abbe","Enric Boix-Adsera","Theodor Misiakiewicz"],"pdf_url":"https://arxiv.org/pdf/2202.08658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14687v1","updated":"2024-08-26T23:24:31Z","published":"2024-08-26T23:24:31Z","title":"A Synthetic Benchmark to Explore Limitations of Localized Drift\n  Detections","summary":"  Concept drift is a common phenomenon in data streams where the statistical\nproperties of the target variable change over time. Traditionally, drift is\nassumed to occur globally, affecting the entire dataset uniformly. However,\nthis assumption does not always hold true in real-world scenarios where only\nspecific subpopulations within the data may experience drift. This paper\nexplores the concept of localized drift and evaluates the performance of\nseveral drift detection techniques in identifying such localized changes. We\nintroduce a synthetic dataset based on the Agrawal generator, where drift is\ninduced in a randomly chosen subgroup. Our experiments demonstrate that\ncommonly adopted drift detection methods may fail to detect drift when it is\nconfined to a small subpopulation. We propose and test various drift detection\napproaches to quantify their effectiveness in this localized drift scenario. We\nmake the source code for the generation of the synthetic benchmark available at\nhttps://github.com/fgiobergia/subgroup-agrawal-drift.\n","authors":["Flavio Giobergia","Eliana Pastor","Luca de Alfaro","Elena Baralis"],"pdf_url":"https://arxiv.org/pdf/2408.14687v1.pdf","comment":"Paper accepted at DELTA Workshop @ KDD 2024"},{"id":"http://arxiv.org/abs/2408.14685v1","updated":"2024-08-26T23:21:44Z","published":"2024-08-26T23:21:44Z","title":"Model-Based Reinforcement Learning for Control of Strongly-Disturbed\n  Unsteady Aerodynamic Flows","summary":"  The intrinsic high dimension of fluid dynamics is an inherent challenge to\ncontrol of aerodynamic flows, and this is further complicated by a flow's\nnonlinear response to strong disturbances. Deep reinforcement learning, which\ntakes advantage of the exploratory aspects of reinforcement learning (RL) and\nthe rich nonlinearity of a deep neural network, provides a promising approach\nto discover feasible control strategies. However, the typical model-free\napproach to reinforcement learning requires a significant amount of interaction\nbetween the flow environment and the RL agent during training, and this high\ntraining cost impedes its development and application. In this work, we propose\na model-based reinforcement learning (MBRL) approach by incorporating a novel\nreduced-order model as a surrogate for the full environment. The model consists\nof a physics-augmented autoencoder, which compresses high-dimensional CFD flow\nfield snaphsots into a three-dimensional latent space, and a latent dynamics\nmodel that is trained to accurately predict the long-time dynamics of\ntrajectories in the latent space in response to action sequences. The\nrobustness and generalizability of the model is demonstrated in two distinct\nflow environments, a pitching airfoil in a highly disturbed environment and a\nvertical-axis wind turbine in a disturbance-free environment. Based on the\ntrained model in the first problem, we realize an MBRL strategy to mitigate\nlift variation during gust-airfoil encounters. We demonstrate that the policy\nlearned in the reduced-order environment translates to an effective control\nstrategy in the full CFD environment.\n","authors":["Zhecheng Liu","Diederik Beckers","Jeff D. Eldredge"],"pdf_url":"https://arxiv.org/pdf/2408.14685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14682v1","updated":"2024-08-26T23:13:38Z","published":"2024-08-26T23:13:38Z","title":"Detecting Interpretable Subgroup Drifts","summary":"  The ability to detect and adapt to changes in data distributions is crucial\nto maintain the accuracy and reliability of machine learning models. Detection\nis generally approached by observing the drift of model performance from a\nglobal point of view. However, drifts occurring in (fine-grained) data\nsubgroups may go unnoticed when monitoring global drift. We take a different\nperspective, and introduce methods for observing drift at the finer granularity\nof subgroups. Relevant data subgroups are identified during training and\nmonitored efficiently throughout the model's life. Performance drifts in any\nsubgroup are detected, quantified and characterized so as to provide an\ninterpretable summary of the model behavior over time. Experimental results\nconfirm that our subgroup-level drift analysis identifies drifts that do not\nshow at the (coarser) global dataset level. The proposed approach provides a\nvaluable tool for monitoring model performance in dynamic real-world\napplications, offering insights into the evolving nature of data and ultimately\ncontributing to more robust and adaptive models.\n","authors":["Flavio Giobergia","Eliana Pastor","Luca de Alfaro","Elena Baralis"],"pdf_url":"https://arxiv.org/pdf/2408.14682v1.pdf","comment":"Currently under submission"},{"id":"http://arxiv.org/abs/2401.10393v3","updated":"2024-08-26T23:10:59Z","published":"2024-01-18T22:06:38Z","title":"Natural Mitigation of Catastrophic Interference: Continual Learning in\n  Power-Law Learning Environments","summary":"  Neural networks often suffer from catastrophic interference (CI): performance\non previously learned tasks drops off significantly when learning a new task.\nThis contrasts strongly with humans, who can continually learn new tasks\nwithout appreciably forgetting previous tasks. Prior work has explored various\ntechniques for mitigating CI and promoting continual learning such as\nregularization, rehearsal, generative replay, and context-specific components.\nThis paper takes a different approach, one guided by cognitive science research\nshowing that in naturalistic environments, the probability of encountering a\ntask decreases as a power-law of the time since it was last performed. We argue\nthat techniques for mitigating CI should be compared against the intrinsic\nmitigation in simulated naturalistic learning environments. Thus, we evaluate\nthe extent of the natural mitigation of CI when training models in power-law\nenvironments, similar to those humans face. Our results show that natural\nrehearsal environments are better at mitigating CI than existing methods,\ncalling for the need for better evaluation processes. The benefits of this\nenvironment include simplicity, rehearsal that is agnostic to both tasks and\nmodels, and the lack of a need for extra neural circuitry. In addition, we\nexplore popular mitigation techniques in power-law environments to create new\nbaselines for continual learning research.\n","authors":["Atith Gandhi","Raj Sanjay Shah","Vijay Marupudi","Sashank Varma"],"pdf_url":"https://arxiv.org/pdf/2401.10393v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14681v1","updated":"2024-08-26T23:10:42Z","published":"2024-08-26T23:10:42Z","title":"Enhancing Neural Network Interpretability Through Conductance-Based\n  Information Plane Analysis","summary":"  The Information Plane is a conceptual framework used to analyze the flow of\ninformation in neural networks, but traditional methods based on activations\nmay not fully capture the dynamics of information processing. This paper\nintroduces a new approach that uses layer conductance, a measure of sensitivity\nto input features, to enhance the Information Plane analysis. By incorporating\ngradient-based contributions, we provide a more precise characterization of\ninformation dynamics within the network. The proposed conductance-based\nInformation Plane and a new Information Transformation Efficiency (ITE) metric\nare evaluated on pretrained ResNet50 and VGG16 models using the ImageNet\ndataset. Our results demonstrate the ability to identify critical hidden layers\nthat contribute significantly to model performance and interpretability, giving\ninsights into information compression, preservation, and utilization across\nlayers. The conductance-based approach offers a granular perspective on feature\nattribution, enhancing our understanding of the decision-making processes\nwithin neural networks. Furthermore, our empirical findings challenge certain\ntheoretical predictions of the Information Bottleneck theory, highlighting the\ncomplexities of information dynamics in real-world data scenarios. The proposed\nmethod not only advances our understanding of information dynamics in neural\nnetworks but also has the potential to significantly impact the broader field\nof Artificial Intelligence by enabling the development of more interpretable,\nefficient, and robust models.\n","authors":["Jaouad Dabounou","Amine Baazzouz"],"pdf_url":"https://arxiv.org/pdf/2408.14681v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.14680v1","updated":"2024-08-26T23:10:01Z","published":"2024-08-26T23:10:01Z","title":"On-Chip Learning with Memristor-Based Neural Networks: Assessing\n  Accuracy and Efficiency Under Device Variations, Conductance Errors, and\n  Input Noise","summary":"  This paper presents a memristor-based compute-in-memory hardware accelerator\nfor on-chip training and inference, focusing on its accuracy and efficiency\nagainst device variations, conductance errors, and input noise. Utilizing\nrealistic SPICE models of commercially available silver-based metal\nself-directed channel (M-SDC) memristors, the study incorporates inherent\ndevice non-idealities into the circuit simulations. The hardware, consisting of\n30 memristors and 4 neurons, utilizes three different M-SDC structures with\ntungsten, chromium, and carbon media to perform binary image classification\ntasks. An on-chip training algorithm precisely tunes memristor conductance to\nachieve target weights. Results show that incorporating moderate noise (<15%)\nduring training enhances robustness to device variations and noisy input data,\nachieving up to 97% accuracy despite conductance variations and input noises.\nThe network tolerates a 10% conductance error without significant accuracy\nloss. Notably, omitting the initial memristor reset pulse during training\nconsiderably reduces training time and energy consumption. The hardware\ndesigned with chromium-based memristors exhibits superior performance,\nachieving a training time of 2.4 seconds and an energy consumption of 18.9 mJ.\nThis research provides insights for developing robust and energy-efficient\nmemristor-based neural networks for on-chip learning in edge applications.\n","authors":["M. Reza Eslami","Dhiman Biswas","Soheib Takhtardeshir","Sarah S. Sharif","Yaser M. Banad"],"pdf_url":"https://arxiv.org/pdf/2408.14680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12320v2","updated":"2024-08-26T23:05:51Z","published":"2024-08-22T11:57:07Z","title":"PolyRouter: A Multi-LLM Querying System","summary":"  With the rapid growth of Large Language Models (LLMs) across various domains,\nnumerous new LLMs have emerged, each possessing domain-specific expertise. This\nproliferation has highlighted the need for quick, high-quality, and\ncost-effective LLM query response methods. Yet, no single LLM exists to\nefficiently balance this trilemma. Some models are powerful but extremely\ncostly, while others are fast and inexpensive but qualitatively inferior. To\naddress this challenge, we present PolyRouter, a non-monolithic LLM querying\nsystem that seamlessly integrates various LLM experts into a single query\ninterface and dynamically routes incoming queries to the most high-performant\nexpert based on query's requirements. Through extensive experiments, we\ndemonstrate that when compared to standalone expert models, PolyRouter improves\nquery efficiency by up to 40%, and leads to significant cost reductions of up\nto 30%, while maintaining or enhancing model performance by up to 10%.\n","authors":["Dimitris Stripelis","Zijian Hu","Jipeng Zhang","Zhaozhuo Xu","Alay Dilipbhai Shah","Han Jin","Yuhang Yao","Salman Avestimehr","Chaoyang He"],"pdf_url":"https://arxiv.org/pdf/2408.12320v2.pdf","comment":"14 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.14678v1","updated":"2024-08-26T23:01:48Z","published":"2024-08-26T23:01:48Z","title":"Bridging the Gap: Unpacking the Hidden Challenges in Knowledge\n  Distillation for Online Ranking Systems","summary":"  Knowledge Distillation (KD) is a powerful approach for compressing a large\nmodel into a smaller, more efficient model, particularly beneficial for\nlatency-sensitive applications like recommender systems. However, current KD\nresearch predominantly focuses on Computer Vision (CV) and NLP tasks,\noverlooking unique data characteristics and challenges inherent to recommender\nsystems. This paper addresses these overlooked challenges, specifically: (1)\nmitigating data distribution shifts between teacher and student models, (2)\nefficiently identifying optimal teacher configurations within time and\nbudgetary constraints, and (3) enabling computationally efficient and rapid\nsharing of teacher labels to support multiple students. We present a robust KD\nsystem developed and rigorously evaluated on multiple large-scale personalized\nvideo recommendation systems within Google. Our live experiment results\ndemonstrate significant improvements in student model performance while\nensuring consistent and reliable generation of high quality teacher labels from\na continuous data stream of data.\n","authors":["Nikhil Khani","Shuo Yang","Aniruddh Nath","Yang Liu","Pendo Abbo","Li Wei","Shawn Andrews","Maciej Kula","Jarrod Kahn","Zhe Zhao","Lichan Hong","Ed Chi"],"pdf_url":"https://arxiv.org/pdf/2408.14678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14677v1","updated":"2024-08-26T22:57:01Z","published":"2024-08-26T22:57:01Z","title":"Can Optimization Trajectories Explain Multi-Task Transfer?","summary":"  Despite the widespread adoption of multi-task training in deep learning,\nlittle is understood about how multi-task learning (MTL) affects\ngeneralization. Prior work has conjectured that the negative effects of MTL are\ndue to optimization challenges that arise during training, and many\noptimization methods have been proposed to improve multi-task performance.\nHowever, recent work has shown that these methods fail to consistently improve\nmulti-task generalization. In this work, we seek to improve our understanding\nof these failures by empirically studying how MTL impacts the optimization of\ntasks, and whether this impact can explain the effects of MTL on\ngeneralization. We show that MTL results in a generalization gap-a gap in\ngeneralization at comparable training loss-between single-task and multi-task\ntrajectories early into training. However, we find that factors of the\noptimization trajectory previously proposed to explain generalization gaps in\nsingle-task settings cannot explain the generalization gaps between single-task\nand multi-task models. Moreover, we show that the amount of gradient conflict\nbetween tasks is correlated with negative effects to task optimization, but is\nnot predictive of generalization. Our work sheds light on the underlying causes\nfor failures in MTL and, importantly, raises questions about the role of\ngeneral purpose multi-task optimization algorithms.\n","authors":["David Mueller","Mark Dredze","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2408.14677v1.pdf","comment":"Pre-print"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.14155v1","updated":"2024-08-26T09:59:45Z","published":"2024-08-26T09:59:45Z","title":"Digital Fingerprinting on Multimedia: A Survey","summary":"  The explosive growth of multimedia content in the digital economy era has\nbrought challenges in content recognition, copyright protection, and data\nmanagement. As an emerging content management technology, perceptual hash-based\ndigital fingerprints, serving as compact summaries of multimedia content, have\nbeen widely adopted for efficient multimedia content identification and\nretrieval across different modalities (e.g., text, image, video, audio),\nattracting significant attention from both academia and industry. Despite the\nincreasing applications of digital fingerprints, there is a lack of systematic\nand comprehensive literature review on multimedia digital fingerprints. This\nsurvey aims to fill this gap and provide an important resource for researchers\nstudying the details and related advancements of multimedia digital\nfingerprints. The survey first introduces the definition, characteristics, and\nrelated concepts (including hash functions, granularity, similarity measures,\netc.) of digital fingerprints. It then focuses on analyzing and summarizing the\nalgorithms for extracting unimodal fingerprints of different types of digital\ncontent, including text fingerprints, image fingerprints, video fingerprints,\nand audio fingerprints. Particularly, it provides an in-depth review and\nsummary of deep learning-based fingerprints. Additionally, the survey\nelaborates on the various practical applications of digital fingerprints and\noutlines the challenges and potential future research directions. The goal is\nto promote the continued development of multimedia digital fingerprint\nresearch.\n","authors":["Wendi Chen","Wensheng Gan","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2408.14155v1.pdf","comment":"Preprint. 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.04284v2","updated":"2024-08-26T08:19:03Z","published":"2024-07-05T06:32:52Z","title":"TSC-PCAC: Voxel Transformer and Sparse Convolution Based Point Cloud\n  Attribute Compression for 3D Broadcasting","summary":"  Point cloud has been the mainstream representation for advanced 3D\napplications, such as virtual reality and augmented reality. However, the\nmassive data amounts of point clouds is one of the most challenging issues for\ntransmission and storage. In this paper, we propose an end-to-end voxel\nTransformer and Sparse Convolution based Point Cloud Attribute Compression\n(TSC-PCAC) for 3D broadcasting. Firstly, we present a framework of the\nTSC-PCAC, which include Transformer and Sparse Convolutional Module (TSCM)\nbased variational autoencoder and channel context module. Secondly, we propose\na two-stage TSCM, where the first stage focuses on modeling local dependencies\nand feature representations of the point clouds, and the second stage captures\nglobal features through spatial and channel pooling encompassing larger\nreceptive fields. This module effectively extracts global and local interpoint\nrelevance to reduce informational redundancy. Thirdly, we design a TSCM based\nchannel context module to exploit interchannel correlations, which improves the\npredicted probability distribution of quantized latent representations and thus\nreduces the bitrate. Experimental results indicate that the proposed TSC-PCAC\nmethod achieves an average of 38.53%, 21.30%, and 11.19% Bjontegaard Delta\nbitrate reductions compared to the Sparse-PCAC, NF-PCAC, and G-PCC v23 methods,\nrespectively. The encoding/decoding time costs are reduced up to 97.68%/98.78%\non average compared to the Sparse-PCAC. The source code and the trained models\nof the TSC-PCAC are available at https://github.com/igizuxo/TSC-PCAC.\n","authors":["Zixi Guo","Yun Zhang","Linwei Zhu","Hanli Wang","Gangyi Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.04284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14084v1","updated":"2024-08-26T08:11:35Z","published":"2024-08-26T08:11:35Z","title":"HABD: a houma alliance book ancient handwritten character recognition\n  database","summary":"  The Houma Alliance Book, one of history's earliest calligraphic examples, was\nunearthed in the 1970s. These artifacts were meticulously organized,\nreproduced, and copied by the Shanxi Provincial Institute of Cultural Relics.\nHowever, because of their ancient origins and severe ink erosion, identifying\ncharacters in the Houma Alliance Book is challenging, necessitating the use of\ndigital technology. In this paper, we propose a new ancient handwritten\ncharacter recognition database for the Houma alliance book, along with a novel\nbenchmark based on deep learning architectures. More specifically, a collection\nof 26,732 characters samples from the Houma Alliance Book were gathered,\nencompassing 327 different types of ancient characters through iterative\nannotation. Furthermore, benchmark algorithms were proposed by combining four\ndeep neural network classifiers with two data augmentation methods. This\nresearch provides valuable resources and technical support for further studies\non the Houma Alliance Book and other ancient characters. This contributes to\nour understanding of ancient culture and history, as well as the preservation\nand inheritance of humanity's cultural heritage.\n","authors":["Xiaoyu Yuan","Xiaohua Huang","Zibo Zhang","Yabo Sun"],"pdf_url":"https://arxiv.org/pdf/2408.14084v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.12321v2","updated":"2024-08-26T04:27:54Z","published":"2024-08-22T11:57:16Z","title":"MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework\n  for Multimodal Large Language Model","summary":"  This paper presents MaVEn, an innovative Multi-granularity Visual Encoding\nframework designed to enhance the capabilities of Multimodal Large Language\nModels (MLLMs) in multi-image reasoning. Current MLLMs primarily focus on\nsingle-image visual understanding, limiting their ability to interpret and\nintegrate information across multiple images. MaVEn addresses this limitation\nby combining discrete visual symbol sequences, which abstract coarse-grained\nsemantic concepts, with traditional continuous representation sequences that\nmodel fine-grained features. This dual approach bridges the semantic gap\nbetween visual and textual data, thereby improving the model's ability to\nprocess and interpret information from multiple images effectively.\nAdditionally, we design a dynamic reduction mechanism by for long-sequence\ncontinuous features to enhance multi-image processing efficiency. Experimental\nresults demonstrate that MaVEn significantly enhances MLLMs' understanding in\ncomplex multi-image scenarios, while also improving performance in single-image\ncontexts.\n","authors":["Chaoya Jiang","Jia Hongrui","Haiyang Xu","Wei Ye","Mengfan Dong","Ming Yan","Ji Zhang","Fei Huang","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.12321v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14547v1","updated":"2024-08-26T18:00:33Z","published":"2024-08-26T18:00:33Z","title":"Revisiting Image Captioning Training Paradigm via Direct CLIP-based\n  Optimization","summary":"  The conventional training approach for image captioning involves pre-training\na network using teacher forcing and subsequent fine-tuning with Self-Critical\nSequence Training to maximize hand-crafted captioning metrics. However, when\nattempting to optimize modern and higher-quality metrics like CLIP-Score and\nPAC-Score, this training method often encounters instability and fails to\nacquire the genuine descriptive capabilities needed to produce fluent and\ninformative captions. In this paper, we propose a new training paradigm termed\nDirect CLIP-Based Optimization (DiCO). Our approach jointly learns and\noptimizes a reward model that is distilled from a learnable captioning\nevaluator with high human correlation. This is done by solving a weighted\nclassification problem directly inside the captioner. At the same time, DiCO\nprevents divergence from the original model, ensuring that fluency is\nmaintained. DiCO not only exhibits improved stability and enhanced quality in\nthe generated captions but also aligns more closely with human preferences\ncompared to existing methods, especially in modern metrics. Additionally, it\nmaintains competitive performance in traditional metrics. Our source code and\ntrained models are publicly available at https://github.com/aimagelab/DiCO.\n","authors":["Nicholas Moratelli","Davide Caffagni","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2408.14547v1.pdf","comment":"BMVC 2024"}]},"2024-08-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.16553v7","updated":"2024-08-27T17:57:07Z","published":"2024-01-29T20:44:10Z","title":"SelectLLM: Can LLMs Select Important Instructions to Annotate?","summary":"  Instruction tuning benefits from large and diverse datasets; however,\ncreating such datasets involves a high cost of human labeling. While synthetic\ndatasets generated by large language models (LLMs) have partly solved this\nissue, they often contain low-quality data. One effective solution is\nselectively annotating unlabelled instructions, especially given the relative\nease of acquiring unlabeled instructions or texts from various sources.\nHowever, how to select unlabelled instructions is not well-explored, especially\nin the context of LLMs. Therefore, we introduce SelectLLM, an alternative\nframework that leverages the capabilities of LLMs to select unlabeled\ninstructions more effectively. Specifically, SelectLLM consists of two key\nsteps: Coreset-based clustering of unlabelled instructions for enlarging\ndiversity and prompting of LLM to identify the most beneficial instructions\nwithin each cluster. We evaluate SelectLLM on AlpacaEval2 and MT-Bench,\ndemonstrating its ability to outperform state-of-the-art methods like\nAlpagasus. In addition, we compare the performance and compatibility of\nSelectLLM with various LLMs, such as ChatGPT, LLaMA-3.1-70B, and Gemma-2-27b.\nSelectLLM's adaptability and robustness are further evidenced by its ability to\nmaintain high performance across both human and synthetic datasets. All code\nand data are publicly available (https://github.com/minnesotanlp/select-llm).\n","authors":["Ritik Sachin Parkar","Jaehyung Kim","Jong Inn Park","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2401.16553v7.pdf","comment":"First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author:\n  Jong Inn Park | PI: Dongyeop Kang"},{"id":"http://arxiv.org/abs/2408.15232v1","updated":"2024-08-27T17:50:03Z","published":"2024-08-27T17:50:03Z","title":"Into the Unknown Unknowns: Engaged Human Learning through Participation\n  in Language Model Agent Conversations","summary":"  While language model (LM)-powered chatbots and generative search engines\nexcel at answering concrete queries, discovering information in the terrain of\nunknown unknowns remains challenging for users. To emulate the common\neducational scenario where children/students learn by listening to and\nparticipating in conversations of their parents/teachers, we create\nCollaborative STORM (Co-STORM). Unlike QA systems that require users to ask all\nthe questions, Co-STORM lets users observe and occasionally steer the discourse\namong several LM agents. The agents ask questions on the user's behalf,\nallowing the user to discover unknown unknowns serendipitously. To facilitate\nuser interaction, Co-STORM assists users in tracking the discourse by\norganizing the uncovered information into a dynamic mind map, ultimately\ngenerating a comprehensive report as takeaways. For automatic evaluation, we\nconstruct the WildSeek dataset by collecting real information-seeking records\nwith user goals. Co-STORM outperforms baseline methods on both discourse trace\nand report quality. In a further human evaluation, 70% of participants prefer\nCo-STORM over a search engine, and 78% favor it over a RAG chatbot.\n","authors":["Yucheng Jiang","Yijia Shao","Dekun Ma","Sina J. Semnani","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2408.15232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03334v2","updated":"2024-08-27T17:46:31Z","published":"2024-03-05T21:36:23Z","title":"DIVERSE: A Dataset of YouTube Video Comment Stances with a Data\n  Programming Model","summary":"  Stance detection of social media text is a key component of many real-world\napplications like evaluating marketing campaigns, evaluating political policies\nor candidates, or evaluating information environments. However, creating\nautomatic stance labeling systems requires the manual annotation of stances,\nwhich is both tedious and resource-intensive. This paper introduces a stance\nlabeling method that makes use of weak signals of sentence tone, then\nconsolidating these signals with a Data Programmingmodel for the final stance\nlabel. In a time of international conflict, understanding the public opinion\ntowards the country's military is crucial for recruitment. We present DIVERSE,\na dataset involve stances towards YouTube videos of the US military (Dataset\navailable at https://doi.org/10.5281/zenodo.10493803). On average, the videos\nhave 200 comments each, and the stances skew slightly towards the \"against\"\ncharacterization for both the US army and the video.\n","authors":["Iain J. Cruickshank","Amir Soofi","Lynnette Hui Xian Ng"],"pdf_url":"https://arxiv.org/pdf/2403.03334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15221v1","updated":"2024-08-27T17:33:30Z","published":"2024-08-27T17:33:30Z","title":"LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet","summary":"  Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses.\n","authors":["Nathaniel Li","Ziwen Han","Ian Steneker","Willow Primack","Riley Goodside","Hugh Zhang","Zifan Wang","Cristina Menghini","Summer Yue"],"pdf_url":"https://arxiv.org/pdf/2408.15221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15213v1","updated":"2024-08-27T17:19:57Z","published":"2024-08-27T17:19:57Z","title":"Classifying populist language in American presidential and governor\n  speeches using automatic text analysis","summary":"  Populism is a concept that is often used but notoriously difficult to\nmeasure. Common qualitative measurements like holistic grading or content\nanalysis require great amounts of time and labour, making it difficult to\nquickly scope out which politicians should be classified as populist and which\nshould not, while quantitative methods show mixed results when it comes to\nclassifying populist rhetoric. In this paper, we develop a pipeline to train\nand validate an automated classification model to estimate the use of populist\nlanguage. We train models based on sentences that were identified as populist\nand pluralist in 300 US governors' speeches from 2010 to 2018 and in 45\nspeeches of presidential candidates in 2016. We find that these models classify\nmost speeches correctly, including 84% of governor speeches and 89% of\npresidential speeches. These results extend to different time periods (with 92%\naccuracy on more recent American governors), different amounts of data (with as\nfew as 70 training sentences per category achieving similar results), and when\nclassifying politicians instead of individual speeches. This pipeline is thus\nan effective tool that can optimise the systematic and swift classification of\nthe use of populist language in politicians' speeches.\n","authors":["Olaf van der Veen","Semir Dzebo","Levi Littvay","Kirk Hawkins","Oren Dar"],"pdf_url":"https://arxiv.org/pdf/2408.15213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15204v1","updated":"2024-08-27T17:03:18Z","published":"2024-08-27T17:03:18Z","title":"Can Unconfident LLM Annotations Be Used for Confident Conclusions?","summary":"  Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems.\n","authors":["Kristina GligoriÄ","Tijana Zrnic","Cinoo Lee","Emmanuel J. CandÃ¨s","Dan Jurafsky"],"pdf_url":"https://arxiv.org/pdf/2408.15204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15188v1","updated":"2024-08-27T16:44:41Z","published":"2024-08-27T16:44:41Z","title":"Infusing Acoustic Pause Context into Text-Based Dementia Assessment","summary":"  Speech pauses, alongside content and structure, offer a valuable and\nnon-invasive biomarker for detecting dementia. This work investigates the use\nof pause-enriched transcripts in transformer-based language models to\ndifferentiate the cognitive states of subjects with no cognitive impairment,\nmild cognitive impairment, and Alzheimer's dementia based on their speech from\na clinical assessment. We address three binary classification tasks: Onset,\nmonitoring, and dementia exclusion. The performance is evaluated through\nexperiments on a German Verbal Fluency Test and a Picture Description Test,\ncomparing the model's effectiveness across different speech production\ncontexts. Starting from a textual baseline, we investigate the effect of\nincorporation of pause information and acoustic context. We show the test\nshould be chosen depending on the task, and similarly, lexical pause\ninformation and acoustic cross-attention contribute differently.\n","authors":["Franziska Braun","Sebastian P. Bayerl","Florian HÃ¶nig","Hartmut Lehfeld","Thomas Hillemacher","Tobias Bocklet","Korbinian Riedhammer"],"pdf_url":"https://arxiv.org/pdf/2408.15188v1.pdf","comment":"Accepted at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2408.15176v1","updated":"2024-08-27T16:18:51Z","published":"2024-08-27T16:18:51Z","title":"Unlocking Potential in Pre-Trained Music Language Models for Versatile\n  Multi-Track Music Arrangement","summary":"  Large language models have shown significant capabilities across various\ndomains, including symbolic music generation. However, leveraging these\npre-trained models for controllable music arrangement tasks, each requiring\ndifferent forms of musical information as control, remains a novel challenge.\nIn this paper, we propose a unified sequence-to-sequence framework that enables\nthe fine-tuning of a symbolic music language model for multiple multi-track\narrangement tasks, including band arrangement, piano reduction, drum\narrangement, and voice separation. Our experiments demonstrate that the\nproposed approach consistently achieves higher musical quality compared to\ntask-specific baselines across all four tasks. Furthermore, through additional\nexperiments on probing analysis, we show the pre-training phase equips the\nmodel with essential knowledge to understand musical conditions, which is hard\nto acquired solely through task-specific fine-tuning.\n","authors":["Longshen Ou","Jingwei Zhao","Ziyu Wang","Gus Xia","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2408.15176v1.pdf","comment":"Submitted to AAAI 2025"},{"id":"http://arxiv.org/abs/2408.15172v1","updated":"2024-08-27T16:10:21Z","published":"2024-08-27T16:10:21Z","title":"X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation","summary":"  Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been\nshown to enhance the effectiveness of enriching item descriptions, thereby\nimproving the accuracy of recommendation systems. However, most existing\napproaches either rely on text-only prompting or employ basic multimodal\nstrategies that do not fully exploit the complementary information available\nfrom both textual and visual modalities. This paper introduces a novel\nframework, Cross-Reflection Prompting, termed X-Reflect, designed to address\nthese limitations by prompting LMMs to explicitly identify and reconcile\nsupportive and conflicting information between text and images. By capturing\nnuanced insights from both modalities, this approach generates more\ncomprehensive and contextually richer item representations. Extensive\nexperiments conducted on two widely used benchmarks demonstrate that our method\noutperforms existing prompting baselines in downstream recommendation accuracy.\nAdditionally, we evaluate the generalizability of our framework across\ndifferent LMM backbones and the robustness of the prompting strategies,\noffering insights for optimization. This work underscores the importance of\nintegrating multimodal information and presents a novel solution for improving\nitem understanding in multimodal recommendation systems.\n","authors":["Hanjia Lyu","Ryan Rossi","Xiang Chen","Md Mehrab Tanjim","Stefano Petrangeli","Somdeb Sarkhel","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2408.15172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15171v1","updated":"2024-08-27T16:09:56Z","published":"2024-08-27T16:09:56Z","title":"Measuring text summarization factuality using atomic facts entailment\n  metrics in the context of retrieval augmented generation","summary":"  The use of large language models (LLMs) has significantly increased since the\nintroduction of ChatGPT in 2022, demonstrating their value across various\napplications. However, a major challenge for enterprise and commercial adoption\nof LLMs is their tendency to generate inaccurate information, a phenomenon\nknown as \"hallucination.\" This project proposes a method for estimating the\nfactuality of a summary generated by LLMs when compared to a source text. Our\napproach utilizes Naive Bayes classification to assess the accuracy of the\ncontent produced.\n","authors":["N. E. Kriman"],"pdf_url":"https://arxiv.org/pdf/2408.15171v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2408.15138v1","updated":"2024-08-27T15:23:09Z","published":"2024-08-27T15:23:09Z","title":"How transformers learn structured data: insights from hierarchical\n  filtering","summary":"  We introduce a hierarchical filtering procedure for generative models of\nsequences on trees, enabling control over the range of positional correlations\nin the data. Leveraging this controlled setting, we provide evidence that\nvanilla encoder-only transformer architectures can implement the optimal Belief\nPropagation algorithm on both root classification and masked language modeling\ntasks. Correlations at larger distances corresponding to increasing layers of\nthe hierarchy are sequentially included as the network is trained. We analyze\nhow the transformer layers succeed by focusing on attention maps from models\ntrained with varying degrees of filtering. These attention maps show clear\nevidence for iterative hierarchical reconstruction of correlations, and we can\nrelate these observations to a plausible implementation of the exact inference\nalgorithm for the network sizes considered.\n","authors":["Jerome Garnier-Brun","Marc MÃ©zard","Emanuele Moscato","Luca Saglietti"],"pdf_url":"https://arxiv.org/pdf/2408.15138v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.07531v2","updated":"2024-08-27T15:16:06Z","published":"2024-08-14T13:03:41Z","title":"Development of a Large Language Model-based Multi-Agent Clinical\n  Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based\n  Triage and Treatment Planning in Emergency Departments","summary":"  Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation.\n","authors":["Seungjun Han","Wongyung Choi"],"pdf_url":"https://arxiv.org/pdf/2408.07531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15091v1","updated":"2024-08-27T14:22:02Z","published":"2024-08-27T14:22:02Z","title":"Relation Also Knows: Rethinking the Recall and Editing of Factual\n  Associations in Auto-Regressive Transformer Language Models","summary":"  The storage and recall of factual associations in auto-regressive transformer\nlanguage models (LMs) have drawn a great deal of attention, inspiring knowledge\nediting by directly modifying the located model weights. Most editing works\nachieve knowledge editing under the guidance of existing interpretations of\nknowledge recall that mainly focus on subject knowledge. However, these\ninterpretations are seriously flawed, neglecting relation information and\nleading to the over-generalizing problem for editing. In this work, we discover\na novel relation-focused perspective to interpret the knowledge recall of\ntransformer LMs during inference and apply it on knowledge editing to avoid\nover-generalizing. Experimental results on the dataset supplemented with a new\nR-Specificity criterion demonstrate that our editing approach significantly\nalleviates over-generalizing while remaining competitive on other criteria,\nbreaking the domination of subject-focused editing for future research.\n","authors":["Xiyu Liu","Zhengxiao Liu","Naibin Gu","Zheng Lin","Wanli Ma","Ji Xiang","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2408.15091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05195v2","updated":"2024-08-27T14:20:57Z","published":"2023-11-09T08:19:34Z","title":"PRODIGy: a PROfile-based DIalogue Generation dataset","summary":"  Providing dialogue agents with a profile representation can improve their\nconsistency and coherence, leading to better conversations. However, current\nprofile-based dialogue datasets for training such agents contain either\nexplicit profile representations that are simple and dialogue-specific, or\nimplicit representations that are difficult to collect. In this work, we\npropose a unified framework in which we bring together both standard and more\nsophisticated profile representations by creating a new resource where each\ndialogue is aligned with all possible speaker representations such as\ncommunication style, biographies, and personality. This framework allows to\ntest several baselines built using generative language models with several\nprofile configurations. The automatic evaluation shows that profile-based\nmodels have better generalisation capabilities than models trained on dialogues\nonly, both in-domain and cross-domain settings. These results are consistent\nfor fine-tuned models and instruction-based LLMs. Additionally, human\nevaluation demonstrates a clear preference for generations consistent with both\nprofile and context. Finally, to account for possible privacy concerns, all\nexperiments are done under two configurations: inter-character and\nintra-character. In the former, the LM stores the information about the\ncharacter in its internal representation, while in the latter, the LM does not\nretain any personal information but uses it only at inference time.\n","authors":["Daniela Occhipinti","Serra Sinem Tekiroglu","Marco Guerini"],"pdf_url":"https://arxiv.org/pdf/2311.05195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14340v2","updated":"2024-08-27T14:09:44Z","published":"2024-08-26T15:13:14Z","title":"Foundation Models for Music: A Survey","summary":"  In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.\n","authors":["Yinghao Ma","Anders Ãland","Anton Ragni","Bleiz MacSen Del Sette","Charalampos Saitis","Chris Donahue","Chenghua Lin","Christos Plachouras","Emmanouil Benetos","Elio Quinton","Elona Shatri","Fabio Morreale","Ge Zhang","GyÃ¶rgy Fazekas","Gus Xia","Huan Zhang","Ilaria Manco","Jiawen Huang","Julien Guinot","Liwei Lin","Luca Marinelli","Max W. Y. Lam","Megha Sharma","Qiuqiang Kong","Roger B. Dannenberg","Ruibin Yuan","Shangda Wu","Shih-Lun Wu","Shuqi Dai","Shun Lei","Shiyin Kang","Simon Dixon","Wenhu Chen","Wenhao Huang","Xingjian Du","Xingwei Qu","Xu Tan","Yizhi Li","Zeyue Tian","Zhiyong Wu","Zhizheng Wu","Ziyang Ma","Ziyu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15079v1","updated":"2024-08-27T14:08:23Z","published":"2024-08-27T14:08:23Z","title":"BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and\n  Deduplication by Introducing a Competitive Large Language Model Baseline","summary":"  The general capabilities of Large Language Models (LLM) highly rely on the\ncomposition and selection on extensive pretraining datasets, treated as\ncommercial secrets by several institutions. To mitigate this issue, we\nopen-source the details of a universally applicable data processing pipeline\nand validate its effectiveness and potential by introducing a competitive LLM\nbaseline. Specifically, the data processing pipeline consists of broad\ncollection to scale up and reweighting to improve quality. We then pretrain a\n7B model BaichuanSEED with 3T tokens processed by our pipeline without any\ndeliberate downstream task-related optimization, followed by an easy but\neffective supervised fine-tuning stage. BaichuanSEED demonstrates consistency\nand predictability throughout training and achieves comparable performance on\ncomprehensive benchmarks with several commercial advanced large language\nmodels, such as Qwen1.5 and Llama3. We also conduct several heuristic\nexperiments to discuss the potential for further optimization of downstream\ntasks, such as mathematics and coding.\n","authors":["Guosheng Dong","Da Pan","Yiding Sun","Shusen Zhang","Zheng Liang","Xin Wu","Yanjun Shen","Fan Yang","Haoze Sun","Tianpeng Li","Mingan Lin","Jianhua Xu","Yufan Zhang","Xiaonan Nie","Lei Su","Bingning Wang","Wentao Zhang","Jiaxin Mao","Zenan Zhou","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.15079v1.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.15050v1","updated":"2024-08-27T13:19:32Z","published":"2024-08-27T13:19:32Z","title":"Self-supervised Topic Taxonomy Discovery in the Box Embedding Space","summary":"  Topic taxonomy discovery aims at uncovering topics of different abstraction\nlevels and constructing hierarchical relations between them. Unfortunately,\nmost of prior work can hardly model semantic scopes of words and topics by\nholding the Euclidean embedding space assumption. What's worse, they infer\nasymmetric hierarchical relations by symmetric distances between topic\nembeddings. As a result, existing methods suffer from problems of low-quality\ntopics at high abstraction levels and inaccurate hierarchical relations. To\nalleviate these problems, this paper develops a Box embedding-based Topic Model\n(BoxTM) that maps words and topics into the box embedding space, where the\nasymmetric metric is defined to properly infer hierarchical relations among\ntopics. Additionally, our BoxTM explicitly infers upper-level topics based on\ncorrelation between specific topics through recursive clustering on topic\nboxes. Finally, extensive experiments validate high-quality of the topic\ntaxonomy learned by BoxTM.\n","authors":["Yuyin Lu","Hegang Chen","Pengbo Mao","Yanghui Rao","Haoran Xie","Fu Lee Wang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2408.15050v1.pdf","comment":"to be published in TACL"},{"id":"http://arxiv.org/abs/2408.15040v1","updated":"2024-08-27T13:10:05Z","published":"2024-08-27T13:10:05Z","title":"A Survey of Large Language Models for European Languages","summary":"  Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining LLMs.\n","authors":["Wazir Ali","Sampo Pyysalo"],"pdf_url":"https://arxiv.org/pdf/2408.15040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15037v1","updated":"2024-08-27T13:07:07Z","published":"2024-08-27T13:07:07Z","title":"Evidence-Enhanced Triplet Generation Framework for Hallucination\n  Alleviation in Generative Question Answering","summary":"  To address the hallucination in generative question answering (GQA) where the\nanswer can not be derived from the document, we propose a novel\nevidence-enhanced triplet generation framework, EATQA, encouraging the model to\npredict all the combinations of (Question, Evidence, Answer) triplet by\nflipping the source pair and the target label to understand their logical\nrelationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a\nQE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap\nto distill the knowledge from evidence in inference stage. Our framework\nensures the model to learn the logical relation between query, evidence and\nanswer, which simultaneously improves the evidence generation and query\nanswering. In this paper, we apply EATQA to LLama and it outperforms other\nLLMs-based methods and hallucination mitigation approaches on two challenging\nGQA benchmarks. Further analysis shows that our method not only keeps prior\nknowledge within LLM, but also mitigates hallucination and generates faithful\nanswers.\n","authors":["Haowei Du","Huishuai Zhang","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.15037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14991v1","updated":"2024-08-27T12:15:43Z","published":"2024-08-27T12:15:43Z","title":"Speech Recognition Transformers: Topological-lingualism Perspective","summary":"  Transformers have evolved with great success in various artificial\nintelligence tasks. Thanks to our recent prevalence of self-attention\nmechanisms, which capture long-term dependency, phenomenal outcomes in speech\nprocessing and recognition tasks have been produced. The paper presents a\ncomprehensive survey of transformer techniques oriented in speech modality. The\nmain contents of this survey include (1) background of traditional ASR,\nend-to-end transformer ecosystem, and speech transformers (2) foundational\nmodels in a speech via lingualism paradigm, i.e., monolingual, bilingual,\nmultilingual, and cross-lingual (3) dataset and languages, acoustic features,\narchitecture, decoding, and evaluation metric from a specific topological\nlingualism perspective (4) popular speech transformer toolkit for building\nend-to-end ASR systems. Finally, highlight the discussion of open challenges\nand potential research directions for the community to conduct further research\nin this domain.\n","authors":["Shruti Singh","Muskaan Singh","Virender Kadyan"],"pdf_url":"https://arxiv.org/pdf/2408.14991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14972v1","updated":"2024-08-27T11:24:38Z","published":"2024-08-27T11:24:38Z","title":"AgentMonitor: A Plug-and-Play Framework for Predictive and Secure\n  Multi-Agent Systems","summary":"  The rapid advancement of large language models (LLMs) has led to the rise of\nLLM-based agents. Recent research shows that multi-agent systems (MAS), where\neach agent plays a specific role, can outperform individual LLMs. However,\nconfiguring an MAS for a task remains challenging, with performance only\nobservable post-execution. Inspired by scaling laws in LLM development, we\ninvestigate whether MAS performance can be predicted beforehand. We introduce\nAgentMonitor, a framework that integrates at the agent level to capture inputs\nand outputs, transforming them into statistics for training a regression model\nto predict task performance. Additionally, it can further apply real-time\ncorrections to address security risks posed by malicious agents, mitigating\nnegative impacts and enhancing MAS security. Experiments demonstrate that an\nXGBoost model achieves a Spearman correlation of 0.89 in-domain and 0.58 in\nmore challenging scenarios. Furthermore, using AgentMonitor reduces harmful\ncontent by 6.2% and increases helpful content by 1.8% on average, enhancing\nsafety and reliability. Code is available at\n\\url{https://github.com/chanchimin/AgentMonitor}.\n","authors":["Chi-Min Chan","Jianxuan Yu","Weize Chen","Chunyang Jiang","Xinyu Liu","Weijie Shi","Zhiyuan Liu","Wei Xue","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2408.14972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14968v1","updated":"2024-08-27T11:21:19Z","published":"2024-08-27T11:21:19Z","title":"MRSE: An Efficient Multi-modality Retrieval System for Large Scale\n  E-commerce","summary":"  Providing high-quality item recall for text queries is crucial in large-scale\ne-commerce search systems. Current Embedding-based Retrieval Systems (ERS)\nembed queries and items into a shared low-dimensional space, but uni-modality\nERS rely too heavily on textual features, making them unreliable in complex\ncontexts. While multi-modality ERS incorporate various data sources, they often\noverlook individual preferences for different modalities, leading to suboptimal\nresults. To address these issues, we propose MRSE, a Multi-modality Retrieval\nSystem that integrates text, item images, and user preferences through\nlightweight mixture-of-expert (LMoE) modules to better align features across\nand within modalities. MRSE also builds user profiles at a multi-modality level\nand introduces a novel hybrid loss function that enhances consistency and\nrobustness using hard negative sampling. Experiments on a large-scale dataset\nfrom Shopee and online A/B testing show that MRSE achieves an 18.9% improvement\nin offline relevance and a 3.7% gain in online core metrics compared to\nShopee's state-of-the-art uni-modality system.\n","authors":["Hao Jiang","Haoxiang Zhang","Qingshan Hou","Chaofeng Chen","Weisi Lin","Jingchang Zhang","Annan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14960v1","updated":"2024-08-27T11:07:15Z","published":"2024-08-27T11:07:15Z","title":"Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual\n  Progress","summary":"  The use of synthetic data has played a critical role in recent state-of-art\nbreakthroughs. However, overly relying on a single oracle teacher model to\ngenerate data has been shown to lead to model collapse and invite propagation\nof biases. These limitations are particularly evident in multilingual settings,\nwhere the absence of a universally effective teacher model that excels across\nall languages presents significant challenges. In this work, we address these\nextreme difference by introducing \"multilingual arbitrage\", which capitalizes\non performance variations between multiple models for a given language. To do\nso, we strategically route samples through a diverse pool of models, each with\nunique strengths in different languages. Across exhaustive experiments on\nstate-of-art models, our work suggests that arbitrage techniques allow for\nspectacular gains in performance that far outperform relying on a single\nteacher. In particular, compared to the best single teacher, we observe gains\nof up to 56.5% improvement in win rates averaged across all languages when\nswitching to multilingual arbitrage. We observe the most significant gains for\nthe least resourced languages in our pool.\n","authors":["Ayomide Odumakinde","Daniel D'souza","Pat Verga","Beyza Ermis","Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2408.14960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15504v2","updated":"2024-08-27T10:07:27Z","published":"2024-06-19T16:43:56Z","title":"Dr.E Bridges Graphs with Large Language Models through Words","summary":"  Significant efforts have been dedicated to integrating the powerful Large\nLanguage Models (LLMs) with diverse modalities, particularly focusing on the\nfusion of language, vision and audio data. However, the graph-structured data,\nwhich is inherently rich in structural and domain-specific knowledge, has not\nyet been gracefully adapted to LLMs. Existing methods either describe the graph\nwith raw text, suffering the loss of graph structural information, or feed\nGraph Neural Network (GNN) embeddings into LLMs at the cost of losing\nexplainable prompt semantics. To bridge this gap, we introduce an end-to-end\nmodality-aligning framework for LLM-graph alignment: Dual-Residual Vector\nQuantized-Variational AutoEncoder, namely Dr.E. Our approach is purposefully\ndesigned to facilitate token-level alignment with LLMs, enabling an effective\ntranslation of the intrinsic `language' of graphs into comprehensible natural\nlanguage. We also manage to enhance LLMs' more robust structural understanding\nof graphs by incorporating multiple views of the central nodes based on their\nsurrounding nodes at various distances. Our experimental evaluations on\nstandard graph tasks demonstrate competitive performance against other\nstate-of-the-art (SOTA) approaches. Additionally, our framework ensures certain\nvisual interpretability, efficiency, and robustness, marking the promising\nsuccessful endeavor to achieve token-level alignment between LLMs and GNNs. Our\ncode is available at: https://anonymous.4open.science/r/dre-817.\n","authors":["Zipeng Liu","Likang Wu","Ming He","Zhong Guan","Hongke Zhao","Nan Feng"],"pdf_url":"https://arxiv.org/pdf/2406.15504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14909v1","updated":"2024-08-27T09:35:49Z","published":"2024-08-27T09:35:49Z","title":"SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models","summary":"  Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs.\n","authors":["Shuaijie Shen","Chao Wang","Renzhuo Huang","Yan Zhong","Qinghai Guo","Zhichao Lu","Jianguo Zhang","Luziwei Leng"],"pdf_url":"https://arxiv.org/pdf/2408.14909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14908v1","updated":"2024-08-27T09:35:13Z","published":"2024-08-27T09:35:13Z","title":"TriplÃ¨toile: Extraction of Knowledge from Microblogging Text","summary":"  Numerous methods and pipelines have recently emerged for the automatic\nextraction of knowledge graphs from documents such as scientific publications\nand patents. However, adapting these methods to incorporate alternative text\nsources like micro-blogging posts and news has proven challenging as they\nstruggle to model open-domain entities and relations, typically found in these\nsources. In this paper, we propose an enhanced information extraction pipeline\ntailored to the extraction of a knowledge graph comprising open-domain entities\nfrom micro-blogging posts on social media platforms. Our pipeline leverages\ndependency parsing and classifies entity relations in an unsupervised manner\nthrough hierarchical clustering over word embeddings. We provide a use case on\nextracting semantic triples from a corpus of 100 thousand tweets about digital\ntransformation and publicly release the generated knowledge graph. On the same\ndataset, we conduct two experimental evaluations, showing that the system\nproduces triples with precision over 95% and outperforms similar pipelines of\naround 5% in terms of precision, while generating a comparatively higher number\nof triples.\n","authors":["Vanni Zavarella","Sergio Consoli","Diego Reforgiato Recupero","Gianni Fenu","Simone Angioni","Davide Buscaldi","Danilo DessÃ¬","Francesco Osborne"],"pdf_url":"https://arxiv.org/pdf/2408.14908v1.pdf","comment":"42 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.14906v1","updated":"2024-08-27T09:34:38Z","published":"2024-08-27T09:34:38Z","title":"Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval","summary":"  In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.\n","authors":["Melisa Russak","Umar Jamil","Christopher Bryant","Kiran Kamble","Axel Magnuson","Mateusz Russak","Waseem AlShikh"],"pdf_url":"https://arxiv.org/pdf/2408.14906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14895v1","updated":"2024-08-27T09:18:57Z","published":"2024-08-27T09:18:57Z","title":"VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view\n  Videos of Daily Activities","summary":"  Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data\n(e.g., images and videos) into symbols, have attracted attention as resources\nenabling knowledge processing and machine learning across modalities. However,\nthe construction of MMKGs for videos consisting of multiple events, such as\ndaily activities, is still in the early stages. In this paper, we construct an\nMMKG based on synchronized multi-view simulated videos of daily activities.\nBesides representing the content of daily life videos as event-centric\nknowledge, our MMKG also includes frame-by-frame fine-grained changes, such as\nbounding boxes within video frames. In addition, we provide support tools for\nquerying our MMKG. As an application example, we demonstrate that our MMKG\nfacilitates benchmarking vision-language models by providing the necessary\nvision-language datasets for a tailored task.\n","authors":["Shusaku Egami","Takahiro Ugai","Ken Fukuda"],"pdf_url":"https://arxiv.org/pdf/2408.14895v1.pdf","comment":"5 pages,4 figures, accepted by CIKM2024 Resource Track"},{"id":"http://arxiv.org/abs/2408.14892v1","updated":"2024-08-27T09:07:37Z","published":"2024-08-27T09:07:37Z","title":"A Functional Trade-off between Prosodic and Semantic Cues in Conveying\n  Sarcasm","summary":"  This study investigates the acoustic features of sarcasm and disentangles the\ninterplay between the propensity of an utterance being used sarcastically and\nthe presence of prosodic cues signaling sarcasm. Using a dataset of sarcastic\nutterances compiled from television shows, we analyze the prosodic features\nwithin utterances and key phrases belonging to three distinct sarcasm\ncategories (embedded, propositional, and illocutionary), which vary in the\ndegree of semantic cues present, and compare them to neutral expressions.\nResults show that in phrases where the sarcastic meaning is salient from the\nsemantics, the prosodic cues are less relevant than when the sarcastic meaning\nis not evident from the semantics, suggesting a trade-off between prosodic and\nsemantic cues of sarcasm at the phrase level. These findings highlight a\nlessened reliance on prosodic modulation in semantically dense sarcastic\nexpressions and a nuanced interaction that shapes the communication of\nsarcastic intent.\n","authors":["Zhu Li","Xiyuan Gao","Yuqing Zhang","Shekhar Nayak","Matt Coler"],"pdf_url":"https://arxiv.org/pdf/2408.14892v1.pdf","comment":"accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2408.14874v1","updated":"2024-08-27T08:43:32Z","published":"2024-08-27T08:43:32Z","title":"Inverse-Q*: Token Level Reinforcement Learning for Aligning Large\n  Language Models Without Preference Data","summary":"  Reinforcement Learning from Human Feedback (RLHF) has proven effective in\naligning large language models with human intentions, yet it often relies on\ncomplex methodologies like Proximal Policy Optimization (PPO) that require\nextensive hyper-parameter tuning and present challenges in sample efficiency\nand stability. In this paper, we introduce Inverse-Q*, an innovative framework\nthat transcends traditional RL methods by optimizing token-level reinforcement\nlearning without the need for additional reward or value models. Inverse-Q*\nleverages direct preference optimization techniques but extends them by\nestimating the conditionally optimal policy directly from the model's\nresponses, facilitating more granular and flexible policy shaping. Our approach\nreduces reliance on human annotation and external supervision, making it\nespecially suitable for low-resource settings. We present extensive\nexperimental results demonstrating that Inverse-Q* not only matches but\npotentially exceeds the effectiveness of PPO in terms of convergence speed and\nthe alignment of model responses with human preferences. Our findings suggest\nthat Inverse-Q* offers a practical and robust alternative to conventional RLHF\napproaches, paving the way for more efficient and adaptable model training\napproaches.\n","authors":["Han Xia","Songyang Gao","Qiming Ge","Zhiheng Xi","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2408.14874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14866v1","updated":"2024-08-27T08:38:48Z","published":"2024-08-27T08:38:48Z","title":"Advancing Adversarial Suffix Transfer Learning on Aligned Large Language\n  Models","summary":"  Language Language Models (LLMs) face safety concerns due to potential misuse\nby malicious users. Recent red-teaming efforts have identified adversarial\nsuffixes capable of jailbreaking LLMs using the gradient-based search algorithm\nGreedy Coordinate Gradient (GCG). However, GCG struggles with computational\ninefficiency, limiting further investigations regarding suffix transferability\nand scalability across models and data. In this work, we bridge the connection\nbetween search efficiency and suffix transferability. We propose a two-stage\ntransfer learning framework, DeGCG, which decouples the search process into\nbehavior-agnostic pre-searching and behavior-relevant post-searching.\nSpecifically, we employ direct first target token optimization in pre-searching\nto facilitate the search process. We apply our approach to cross-model,\ncross-data, and self-transfer scenarios. Furthermore, we introduce an\ninterleaved variant of our approach, i-DeGCG, which iteratively leverages\nself-transferability to accelerate the search process. Experiments on HarmBench\ndemonstrate the efficiency of our approach across various models and domains.\nNotably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of\n$43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively.\nFurther analysis on cross-model transfer indicates the pivotal role of first\ntarget token optimization in leveraging suffix transferability for efficient\nsearching.\n","authors":["Hongfu Liu","Yuxi Xie","Ye Wang","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2408.14866v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.03848v7","updated":"2024-08-27T08:33:29Z","published":"2024-02-06T09:50:08Z","title":"ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models","summary":"  Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, and more than 10 different GLLMs\ntogether with 3 different prompting methods using the ANLS* metric is also\nprovided, demonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In almost all cases, SFT\noutperforms other techniques and improves the state-of-the-art, sometimes by as\nmuch as $10$ percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric\n","authors":["David Peer","Philemon SchÃ¶pf","Volckmar Nebendahl","Alexander Rietzler","Sebastian Stabinger"],"pdf_url":"https://arxiv.org/pdf/2402.03848v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03870v2","updated":"2024-08-27T08:31:04Z","published":"2024-03-06T17:23:28Z","title":"Learning to Decode Collaboratively with Multiple Language Models","summary":"  We propose a method to teach multiple large language models (LLM) to\ncollaborate by interleaving their generations at the token level. We model the\ndecision of which LLM generates the next token as a latent variable. By\noptimizing the marginal likelihood of a training set under our latent variable\nmodel, the base LLM automatically learns when to generate itself and when to\ncall on one of the ``assistant'' language models to generate, all without\ndirect supervision. Token-level collaboration during decoding allows for a\nfusion of each model's expertise in a manner tailored to the specific task at\nhand. Our collaborative decoding is especially useful in cross-domain settings\nwhere a generalist base LLM learns to invoke domain expert models. On\ninstruction-following, domain-specific QA, and reasoning tasks, we show that\nthe performance of the joint system exceeds that of the individual models.\nThrough qualitative analysis of the learned latent decisions, we show models\ntrained with our method exhibit several interesting collaboration patterns,\ne.g., template-filling. Our code is available at\nhttps://github.com/clinicalml/co-llm.\n","authors":["Shannon Zejiang Shen","Hunter Lang","Bailin Wang","Yoon Kim","David Sontag"],"pdf_url":"https://arxiv.org/pdf/2403.03870v2.pdf","comment":"16 pages, 4 figures, 11 tables"},{"id":"http://arxiv.org/abs/2408.14853v1","updated":"2024-08-27T08:12:08Z","published":"2024-08-27T08:12:08Z","title":"Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language\n  Models","summary":"  Large Language Models (LLMs) have become a focal point in the rapidly\nevolving field of artificial intelligence. However, a critical concern is the\npresence of toxic content within the pre-training corpus of these models, which\ncan lead to the generation of inappropriate outputs. Investigating methods for\ndetecting internal faults in LLMs can help us understand their limitations and\nimprove their security. Existing methods primarily focus on jailbreaking\nattacks, which involve manually or automatically constructing adversarial\ncontent to prompt the target LLM to generate unexpected responses. These\nmethods rely heavily on prompt engineering, which is time-consuming and usually\nrequires specially designed questions. To address these challenges, this paper\nproposes a target-driven attack paradigm that focuses on directly eliciting the\ntarget response instead of optimizing the prompts. We introduce the use of\nanother LLM as the detector for toxic content, referred to as ToxDet. Given a\ntarget toxic response, ToxDet can generate a possible question and a\npreliminary answer to provoke the target model into producing desired toxic\nresponses with meanings equivalent to the provided one. ToxDet is trained by\ninteracting with the target LLM and receiving reward signals from it, utilizing\nreinforcement learning for the optimization process. While the primary focus of\nthe target models is on open-source LLMs, the fine-tuned ToxDet can also be\ntransferred to attack black-box models such as GPT-4o, achieving notable\nresults. Experimental results on AdvBench and HH-Harmless datasets demonstrate\nthe effectiveness of our methods in detecting the tendencies of target LLMs to\ngenerate harmful responses. This algorithm not only exposes vulnerabilities but\nalso provides a valuable resource for researchers to strengthen their models\nagainst such attacks.\n","authors":["Yuhao Du","Zhuo Li","Pengyu Cheng","Xiang Wan","Anningzhe Gao"],"pdf_url":"https://arxiv.org/pdf/2408.14853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14053v2","updated":"2024-08-27T08:05:07Z","published":"2024-08-26T07:19:07Z","title":"Enhancing Depression Diagnosis with Chain-of-Thought Prompting","summary":"  When using AI to detect signs of depressive disorder, AI models habitually\ndraw preemptive conclusions. We theorize that using chain-of-thought (CoT)\nprompting to evaluate Patient Health Questionnaire-8 (PHQ-8) scores will\nimprove the accuracy of the scores determined by AI models. In our findings,\nwhen the models reasoned with CoT, the estimated PHQ-8 scores were consistently\ncloser on average to the accepted true scores reported by each participant\ncompared to when not using CoT. Our goal is to expand upon AI models'\nunderstanding of the intricacies of human conversation, allowing them to more\neffectively assess a patient's feelings and tone, therefore being able to more\naccurately discern mental disorder symptoms; ultimately, we hope to augment AI\nmodels' abilities, so that they can be widely accessible and used in the\nmedical field.\n","authors":["Elysia Shi","Adithri Manda","London Chowdhury","Runeema Arun","Kevin Zhu","Michael Lam"],"pdf_url":"https://arxiv.org/pdf/2408.14053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14849v1","updated":"2024-08-27T08:01:13Z","published":"2024-08-27T08:01:13Z","title":"Project SHADOW: Symbolic Higher-order Associative Deductive reasoning On\n  Wikidata using LM probing","summary":"  We introduce SHADOW, a fine-tuned language model trained on an intermediate\ntask using associative deductive reasoning, and measure its performance on a\nknowledge base construction task using Wikidata triple completion. We evaluate\nSHADOW on the LM-KBC 2024 challenge and show that it outperforms the baseline\nsolution by 20% with a F1 score of 68.72%.\n","authors":["Hanna Abi Akl"],"pdf_url":"https://arxiv.org/pdf/2408.14849v1.pdf","comment":"6 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.03600v2","updated":"2024-08-27T08:00:03Z","published":"2024-07-04T03:20:31Z","title":"Chain-of-Thought Augmentation with Logit Contrast for Enhanced Reasoning\n  in Language Models","summary":"  Rapidly increasing model scales coupled with steering methods such as\nchain-of-thought prompting have led to drastic improvements in language model\nreasoning. At the same time, models struggle with compositional generalization\nand are far from human performance on many reasoning-based benchmarks.\nLeveraging the success of chain-of-thought prompting, and also taking\ninspiration from context-aware decoding (CAD), we explore input-based\ncontrasting methods to further encourage the type of reasoning induced by\nchain-of-thought prompting. While work remains to stabilize these results\nacross datasets and models, the improvements we find warrant further\ninvestigation into input-based steering methods for context-aware reasoning.\n","authors":["Jay Shim","Grant Kruttschnitt","Alyssa Ma","Daniel Kim","Benjamin Chek","Athul Anand","Kevin Zhu","Sean O'Brien"],"pdf_url":"https://arxiv.org/pdf/2407.03600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14845v1","updated":"2024-08-27T07:56:35Z","published":"2024-08-27T07:56:35Z","title":"AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark","summary":"  Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live.\n","authors":["Abhay Gupta","Philip Meng","Ece Yurtseven","Sean O'Brien","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.14845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14840v1","updated":"2024-08-27T07:51:26Z","published":"2024-08-27T07:51:26Z","title":"CL4KGE: A Curriculum Learning Method for Knowledge Graph Embedding","summary":"  Knowledge graph embedding (KGE) constitutes a foundational task, directed\ntowards learning representations for entities and relations within knowledge\ngraphs (KGs), with the objective of crafting representations comprehensive\nenough to approximate the logical and symbolic interconnections among entities.\nIn this paper, we define a metric Z-counts to measure the difficulty of\ntraining each triple ($<$head entity, relation, tail entity$>$) in KGs with\ntheoretical analysis. Based on this metric, we propose \\textbf{CL4KGE}, an\nefficient \\textbf{C}urriculum \\textbf{L}earning based training strategy for\n\\textbf{KGE}. This method includes a difficulty measurer and a training\nscheduler that aids in the training of KGE models. Our approach possesses the\nflexibility to act as a plugin within a wide range of KGE models, with the\nadded advantage of adaptability to the majority of KGs in existence. The\nproposed method has been evaluated on popular KGE models, and the results\ndemonstrate that it enhances the state-of-the-art methods. The use of Z-counts\nas a metric has enabled the identification of challenging triples in KGs, which\nhelps in devising effective training strategies.\n","authors":["Yang Liu","Chuan Zhou","Peng Zhang","Yanan Cao","Yongchao Liu","Zhao Li","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2408.14840v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.14830v1","updated":"2024-08-27T07:27:16Z","published":"2024-08-27T07:27:16Z","title":"PolicyLR: A Logic Representation For Privacy Policies","summary":"  Privacy policies are crucial in the online ecosystem, defining how services\nhandle user data and adhere to regulations such as GDPR and CCPA. However,\ntheir complexity and frequent updates often make them difficult for\nstakeholders to understand and analyze. Current automated analysis methods,\nwhich utilize natural language processing, have limitations. They typically\nfocus on individual tasks and fail to capture the full context of the policies.\nWe propose PolicyLR, a new paradigm that offers a comprehensive\nmachine-readable representation of privacy policies, serving as an all-in-one\nsolution for multiple downstream tasks. PolicyLR converts privacy policies into\na machine-readable format using valuations of atomic formulae, allowing for\nformal definitions of tasks like compliance and consistency. We have developed\na compiler that transforms unstructured policy text into this format using\noff-the-shelf Large Language Models (LLMs). This compiler breaks down the\ntransformation task into a two-stage translation and entailment procedure. This\nprocedure considers the full context of the privacy policy to infer a complex\nformula, where each formula consists of simpler atomic formulae. The advantage\nof this model is that PolicyLR is interpretable by design and grounded in\nsegments of the privacy policy. We evaluated the compiler using ToS;DR, a\ncommunity-annotated privacy policy entailment dataset. Utilizing open-source\nLLMs, our compiler achieves precision and recall values of 0.91 and 0.88,\nrespectively. Finally, we demonstrate the utility of PolicyLR in three privacy\ntasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison\nShopping.\n","authors":["Ashish Hooda","Rishabh Khandelwal","Prasad Chalasani","Kassem Fawaz","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2408.14830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16349v3","updated":"2024-08-27T07:22:50Z","published":"2023-08-30T22:50:32Z","title":"Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning\n  Based on Visually Grounded Conversations","summary":"  We introduce Affective Visual Dialog, an emotion explanation and reasoning\ntask as a testbed for research on understanding the formation of emotions in\nvisually grounded conversations. The task involves three skills: (1)\nDialog-based Question Answering (2) Dialog-based Emotion Prediction and (3)\nAffective emotion explanation generation based on the dialog. Our key\ncontribution is the collection of a large-scale dataset, dubbed AffectVisDial,\nconsisting of 50K 10-turn visually grounded dialogs as well as concluding\nemotion attributions and dialog-informed textual emotion explanations,\nresulting in a total of 27,180 working hours. We explain our design decisions\nin collecting the dataset and introduce the questioner and answerer tasks that\nare associated with the participants in the conversation. We train and\ndemonstrate solid Affective Visual Dialog baselines adapted from\nstate-of-the-art models. Remarkably, the responses generated by our models show\npromising emotional reasoning abilities in response to visually grounded\nconversations. Our project page is available at\nhttps://affective-visual-dialog.github.io.\n","authors":["Kilichbek Haydarov","Xiaoqian Shen","Avinash Madasu","Mahmoud Salem","Li-Jia Li","Gamaleldin Elsayed","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2308.16349v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14825v1","updated":"2024-08-27T07:11:45Z","published":"2024-08-27T07:11:45Z","title":"From Rule-Based Models to Deep Learning Transformers Architectures for\n  Natural Language Processing and Sign Language Translation Systems: Survey,\n  Taxonomy and Performance Evaluation","summary":"  With the growing Deaf and Hard of Hearing population worldwide and the\npersistent shortage of certified sign language interpreters, there is a\npressing need for an efficient, signs-driven, integrated end-to-end translation\nsystem, from sign to gloss to text and vice-versa. There has been a wealth of\nresearch on machine translations and related reviews. However, there are few\nworks on sign language machine translation considering the particularity of the\nlanguage being continuous and dynamic. This paper aims to address this void,\nproviding a retrospective analysis of the temporal evolution of sign language\nmachine translation algorithms and a taxonomy of the Transformers\narchitectures, the most used approach in language translation. We also present\nthe requirements of a real-time Quality-of-Service sign language ma-chine\ntranslation system underpinned by accurate deep learning algorithms. We propose\nfuture research directions for sign language translation systems.\n","authors":["Nada Shahin","Leila Ismail"],"pdf_url":"https://arxiv.org/pdf/2408.14825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05885v2","updated":"2024-08-27T06:53:16Z","published":"2024-06-09T18:45:41Z","title":"Are Large Language Models Actually Good at Text Style Transfer?","summary":"  We analyze the performance of large language models (LLMs) on Text Style\nTransfer (TST), specifically focusing on sentiment transfer and text\ndetoxification across three languages: English, Hindi, and Bengali. Text Style\nTransfer involves modifying the linguistic style of a text while preserving its\ncore content. We evaluate the capabilities of pre-trained LLMs using zero-shot\nand few-shot prompting as well as parameter-efficient finetuning on publicly\navailable datasets. Our evaluation using automatic metrics, GPT-4 and human\nevaluations reveals that while some prompted LLMs perform well in English,\ntheir performance in on other languages (Hindi, Bengali) remains average.\nHowever, finetuning significantly improves results compared to zero-shot and\nfew-shot prompting, making them comparable to previous state-of-the-art. This\nunderscores the necessity of dedicated datasets and specialized models for\neffective TST.\n","authors":["Sourabrata Mukherjee","Atul Kr. Ojha","OndÅej DuÅ¡ek"],"pdf_url":"https://arxiv.org/pdf/2406.05885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20805v3","updated":"2024-08-27T06:51:00Z","published":"2024-05-31T14:05:27Z","title":"Multilingual Text Style Transfer: Datasets & Models for Indian Languages","summary":"  Text style transfer (TST) involves altering the linguistic style of a text\nwhile preserving its core content. This paper focuses on sentiment transfer, a\npopular TST subtask, across a spectrum of Indian languages: Hindi, Magahi,\nMalayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous\nwork on English-Bangla sentiment transfer (Mukherjee et al., 2023). We\nintroduce dedicated datasets of 1,000 positive and 1,000 negative\nstyle-parallel sentences for each of these eight languages. We then evaluate\nthe performance of various benchmark models categorized into parallel,\nnon-parallel, cross-lingual, and shared learning approaches, including the\nLlama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the\nsignificance of parallel data in TST and demonstrate the effectiveness of the\nMasked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel\ntechniques. Moreover, cross-lingual and joint multilingual learning methods\nshow promise, offering insights into selecting optimal models tailored to the\nspecific language and task requirements. To the best of our knowledge, this\nwork represents the first comprehensive exploration of the TST task as\nsentiment transfer across a diverse set of languages.\n","authors":["Sourabrata Mukherjee","Atul Kr. Ojha","Akanksha Bansal","Deepak Alok","John P. McCrae","OndÅej DuÅ¡ek"],"pdf_url":"https://arxiv.org/pdf/2405.20805v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14809v1","updated":"2024-08-27T06:44:28Z","published":"2024-08-27T06:44:28Z","title":"GSIFN: A Graph-Structured and Interlaced-Masked Multimodal Transformer\n  Based Fusion Network for Multimodal Sentiment Analysis","summary":"  Multimodal Sentiment Analysis (MSA) leverages multiple modals to analyze\nsentiments. Typically, advanced fusion methods and representation\nlearning-based methods are designed to tackle it. Our proposed GSIFN solves two\nkey problems to be solved in MSA: (i) In multimodal fusion, the decoupling of\nmodal combinations and tremendous parameter redundancy in existing fusion\nmethods, which lead to poor fusion performance and efficiency. (ii) The\ntrade-off between representation capability and computation overhead of the\nunimodal feature extractors and enhancers. GSIFN incorporates two main\ncomponents to solve these problems: (i) Graph-Structured and Interlaced-Masked\nMultimodal Transformer. It adopts the Interlaced Mask mechanism to construct\nrobust multimodal graph embedding, achieve all-modal-in-one Transformer-based\nfusion, and greatly reduce the computation overhead. (ii) A self-supervised\nlearning framework with low computation overhead and high performance, which\nutilizes a parallelized LSTM with matrix memory to enhance non-verbal modal\nfeature for unimodal label generation. Evaluated on the MSA datasets CMU-MOSI,\nCMU-MOSEI, and CH-SIMS, GSIFN demonstrates superior performance with\nsignificantly lower computation overhead compared with state-of-the-art\nmethods.\n","authors":["Yijie Jin"],"pdf_url":"https://arxiv.org/pdf/2408.14809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08841v2","updated":"2024-08-27T06:23:45Z","published":"2024-08-16T17:00:11Z","title":"FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats","summary":"  The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods.\n","authors":["Xuanliang Zhang","Dingzirui Wang","Longxu Dou","Baoxin Wang","Dayong Wu","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2408.08841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14043v2","updated":"2024-08-27T06:18:05Z","published":"2024-06-20T07:06:58Z","title":"Taxonomy-Guided Zero-Shot Recommendations with LLMs","summary":"  With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec.\n","authors":["Yueqing Liang","Liangwei Yang","Chen Wang","Xiongxiao Xu","Philip S. Yu","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2406.14043v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08779v2","updated":"2024-08-27T06:14:54Z","published":"2024-08-16T14:43:15Z","title":"DAC: Decomposed Automation Correction for Text-to-SQL","summary":"  Text-to-SQL is an important task that helps people obtain information from\ndatabases by automatically generating SQL queries. Considering the brilliant\nperformance, approaches based on Large Language Models (LLMs) become the\nmainstream for text-to-SQL. Among these approaches, automated correction is an\neffective approach that further enhances performance by correcting the mistakes\nin the generated results. The existing correction methods require LLMs to\ndirectly correct with generated SQL, while previous research shows that LLMs do\nnot know how to detect mistakes, leading to poor performance. Therefore, in\nthis paper, we propose to employ the decomposed correction to enhance\ntext-to-SQL performance. We first demonstrate that decomposed correction\noutperforms direct correction since detecting and fixing mistakes with the\nresults of the decomposed sub-tasks is easier than with SQL. Based on this\nanalysis, we introduce Decomposed Automation Correction (DAC), which corrects\nSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC\nfirst generates the entity and skeleton corresponding to the question and then\ncompares the differences between the initial SQL and the generated entities and\nskeleton as feedback for correction. Experimental results show that our method\nimproves performance by $3.7\\%$ on average of Spider, Bird, and KaggleDBQA\ncompared with the baseline method, demonstrating the effectiveness of DAC.\n","authors":["Dingzirui Wang","Longxu Dou","Xuanliang Zhang","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2408.08779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08072v2","updated":"2024-08-27T04:50:12Z","published":"2024-08-15T10:44:38Z","title":"I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm","summary":"  Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}.\n","authors":["Yiming Liang","Ge Zhang","Xingwei Qu","Tianyu Zheng","Jiawei Guo","Xinrun Du","Zhenzhu Yang","Jiaheng Liu","Chenghua Lin","Lei Ma","Wenhao Huang","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.08072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06537v2","updated":"2024-08-27T04:43:59Z","published":"2024-07-09T04:17:39Z","title":"Efficient and Accurate Memorable Conversation Model using DPO based on\n  sLLM","summary":"  In multi-session dialog system, it is essential to continuously update the\nmemory as the session progresses. Simply accumulating memory can make it\ndifficult to focus on the content of the conversation for inference due to the\nlimited input sentence size. Therefore, efficient and accurate conversation\nmodel that is capable of managing memory to reflect the conversation history\ncontinuously is necessary. This paper presents a conversation model that\nefficiently manages memory as sessions progress and incorporates this into the\nmodel to reflect the conversation history accurately with 3 methodologies: SFT,\nDPO and DPO with SFT model. Our model using DPO algorithm shows an improvement\nabout 0.0591 of BERTScore in memory accuracy, and the rate of responses\nreflecting the memory increased as well. Also, response generation performance\nenhanced about 4.292 in fluency, 3.935 in coherence, and 2.896 in consistency.\nThis paper describes a training method that yields better performance than\nmodels with more than twice the parameter size, even when the model size is\nsmaller. Thus, our model demonstrates efficiency not only in terms of accuracy\nbut also in resource utilization.\n","authors":["Youngkyung Seo","Yoonseok Heo","Jun-Seok Koh","Du-Seong Chang"],"pdf_url":"https://arxiv.org/pdf/2407.06537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08374v2","updated":"2024-08-27T04:35:20Z","published":"2023-06-14T09:04:29Z","title":"SpeechGLUE: How Well Can Self-Supervised Speech Models Capture\n  Linguistic Knowledge?","summary":"  Self-supervised learning (SSL) for speech representation has been\nsuccessfully applied in various downstream tasks, such as speech and speaker\nrecognition. More recently, speech SSL models have also been shown to be\nbeneficial in advancing spoken language understanding tasks, implying that the\nSSL models have the potential to learn not only acoustic but also linguistic\ninformation. In this paper, we aim to clarify if speech SSL techniques can well\ncapture linguistic knowledge. For this purpose, we introduce SpeechGLUE, a\nspeech version of the General Language Understanding Evaluation (GLUE)\nbenchmark. Since GLUE comprises a variety of natural language understanding\ntasks, SpeechGLUE can elucidate the degree of linguistic ability of speech SSL\nmodels. Experiments demonstrate that speech SSL models, although inferior to\ntext-based SSL models, perform better than baselines, suggesting that they can\nacquire a certain amount of general linguistic knowledge from just unlabeled\nspeech data.\n","authors":["Takanori Ashihara","Takafumi Moriya","Kohei Matsuura","Tomohiro Tanaka","Yusuke Ijima","Taichi Asami","Marc Delcroix","Yukinori Honma"],"pdf_url":"https://arxiv.org/pdf/2306.08374v2.pdf","comment":"Accepted at INTERSPEECH 2023. This paper has been extended in a\n  subsequent journal paper, see\n  https://ieeexplore.ieee.org/abstract/document/10597571"},{"id":"http://arxiv.org/abs/2408.14774v1","updated":"2024-08-27T04:31:58Z","published":"2024-08-27T04:31:58Z","title":"Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning","summary":"  We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.\n","authors":["Simran Kaur","Simon Park","Anirudh Goyal","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2408.14774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14856v2","updated":"2024-08-27T04:30:29Z","published":"2023-07-27T13:37:06Z","title":"Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners","summary":"  In-context learning, which offers substantial advantages over fine-tuning, is\npredominantly observed in decoder-only models, while encoder-decoder (i.e.,\nseq2seq) models excel in methods that rely on weight updates. Recently, a few\nstudies have demonstrated the feasibility of few-shot learning with seq2seq\nmodels; however, this has been limited to tasks that align well with the\nseq2seq architecture, such as summarization and translation. Inspired by these\ninitial studies, we provide a first-ever extensive experiment comparing the\nin-context few-shot learning capabilities of decoder-only and encoder-decoder\nmodels on a broad range of tasks. Furthermore, we propose two methods to more\neffectively elicit in-context learning ability in seq2seq models:\nobjective-aligned prompting and a fusion-based approach. Remarkably, our\napproach outperforms a decoder-only model that is six times larger and exhibits\nsignificant performance improvements compared to conventional seq2seq models\nacross a variety of settings. We posit that, with the right configuration and\nprompt design, seq2seq models can be highly effective few-shot learners for a\nwide spectrum of applications.\n","authors":["Jihyeon Lee","Dain Kim","Doohae Jung","Boseop Kim","Kyoung-Woon On"],"pdf_url":"https://arxiv.org/pdf/2307.14856v2.pdf","comment":"Accepted to COLM'2024"},{"id":"http://arxiv.org/abs/2408.14772v1","updated":"2024-08-27T04:20:10Z","published":"2024-08-27T04:20:10Z","title":"A global AI community requires language-diverse publishing","summary":"  In this provocation, we discuss the English dominance of the AI research\ncommunity, arguing that the requirement for English language publishing upholds\nand reinforces broader regimes of extraction in AI. While large language models\nand machine translation have been celebrated as a way to break down barriers,\nwe regard their use as a symptom of linguistic exclusion of scientists and\npotential readers. We propose alternative futures for a healthier publishing\nculture, organized around three themes: administering conferences in the\nlanguages of the country in which they are held, instructing peer reviewers not\nto adjudicate the language appropriateness of papers, and offering\nopportunities to publish and present in multiple languages. We welcome new\ntranslations of this piece. Please contact the authors if you would like to\ncontribute one.\n","authors":["Haley Lepp","Parth Sarin"],"pdf_url":"https://arxiv.org/pdf/2408.14772v1.pdf","comment":"Translations by Michael Hardy (Guarani), Vandana Sarin and Vivek\n  Sarin (Hindi), Roshna Omer Abdulrahman (Soran\\^i Kurdish), Gabriel Poesia\n  (Portuguese), and Mat\\'ias Grinberg (Spanish). In the proceedings of the\n  Global AI Cultures Workshop at the Twelfth International Conference on\n  Learning Representations (ICLR) 2024, Vienna, Austria, May 7-11, 2024"},{"id":"http://arxiv.org/abs/2408.14470v2","updated":"2024-08-27T03:56:11Z","published":"2024-08-26T17:58:53Z","title":"Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models","summary":"  Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification.\n","authors":["Aradhye Agarwal","Suhas K Ramesh","Ayan Sengupta","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2408.14470v2.pdf","comment":"15 pages, 7 tables, 9 figures"},{"id":"http://arxiv.org/abs/2402.10260v2","updated":"2024-08-27T03:32:47Z","published":"2024-02-15T18:58:09Z","title":"A StrongREJECT for Empty Jailbreaks","summary":"  Most jailbreak papers claim the jailbreaks they propose are highly effective,\noften boasting near-100% attack success rates. However, it is perhaps more\ncommon than not for jailbreak developers to substantially exaggerate the\neffectiveness of their jailbreaks. We suggest this problem arises because\njailbreak researchers lack a standard, high-quality benchmark for evaluating\njailbreak performance, leaving researchers to create their own. To create a\nbenchmark, researchers must choose a dataset of forbidden prompts to which a\nvictim model will respond, along with an evaluation method that scores the\nharmfulness of the victim model's responses. We show that existing benchmarks\nsuffer from significant shortcomings and introduce the StrongREJECT benchmark\nto address these issues. StrongREJECT's dataset contains prompts that victim\nmodels must answer with specific, harmful information, while its automated\nevaluator measures the extent to which a response gives useful information to\nforbidden prompts. In doing so, the StrongREJECT evaluator achieves\nstate-of-the-art agreement with human judgments of jailbreak effectiveness.\nNotably, we find that existing evaluation methods significantly overstate\njailbreak effectiveness compared to human judgments and the StrongREJECT\nevaluator. We describe a surprising and novel phenomenon that explains this\ndiscrepancy: jailbreaks bypassing a victim model's safety fine-tuning tend to\nreduce its capabilities. Together, our findings underscore the need for\nresearchers to use a high-quality benchmark, such as StrongREJECT, when\ndeveloping new jailbreak attacks. We release the StrongREJECT code and data at\nhttps://strong-reject.readthedocs.io/en/latest/.\n","authors":["Alexandra Souly","Qingyuan Lu","Dillon Bowen","Tu Trinh","Elvis Hsieh","Sana Pandey","Pieter Abbeel","Justin Svegliato","Scott Emmons","Olivia Watkins","Sam Toyer"],"pdf_url":"https://arxiv.org/pdf/2402.10260v2.pdf","comment":"Code and data at https://strong-reject.readthedocs.io/en/latest/"},{"id":"http://arxiv.org/abs/2408.13184v2","updated":"2024-08-27T03:27:08Z","published":"2024-08-23T16:02:54Z","title":"Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning","summary":"  Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering.\n","authors":["Hourui Deng","Hongjie Zhang","Jie Ou","Chaosheng Feng"],"pdf_url":"https://arxiv.org/pdf/2408.13184v2.pdf","comment":"Submitted to ICASSP"},{"id":"http://arxiv.org/abs/2408.01262v3","updated":"2024-08-27T03:13:50Z","published":"2024-08-02T13:35:11Z","title":"RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework","summary":"  Retrieval-Augmented Generation (RAG) systems have demonstrated their\nadvantages in alleviating the hallucination of Large Language Models (LLMs).\nExisting RAG benchmarks mainly focus on evaluating whether LLMs can correctly\nanswer the general knowledge. However, they are unable to evaluate the\neffectiveness of the RAG system in dealing with the data from different\nvertical domains. This paper introduces RAGEval, a framework for automatically\ngenerating evaluation datasets to evaluate the knowledge usage ability of\ndifferent LLMs in different scenarios. Specifically, RAGEval summarizes a\nschema from seed documents, applies the configurations to generate diverse\ndocuments, and constructs question-answering pairs according to both articles\nand configurations. We propose three novel metrics, Completeness,\nHallucination, and Irrelevance, to carefully evaluate the responses generated\nby LLMs. By benchmarking RAG models in vertical domains, RAGEval has the\nability to better evaluate the knowledge usage ability of LLMs, which avoids\nthe confusion regarding the source of knowledge in answering question in\nexisting QA datasets--whether it comes from parameterized memory or retrieval.\nThe code and dataset will be released.\n","authors":["Kunlun Zhu","Yifan Luo","Dingling Xu","Ruobing Wang","Shi Yu","Shuo Wang","Yukun Yan","Zhenghao Liu","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01262v3.pdf","comment":"add github repo"},{"id":"http://arxiv.org/abs/2408.14750v1","updated":"2024-08-27T03:01:48Z","published":"2024-08-27T03:01:48Z","title":"LyCon: Lyrics Reconstruction from the Bag-of-Words Using Large Language\n  Models","summary":"  This paper addresses the unique challenge of conducting research in lyric\nstudies, where direct use of lyrics is often restricted due to copyright\nconcerns. Unlike typical data, internet-sourced lyrics are frequently protected\nunder copyright law, necessitating alternative approaches. Our study introduces\na novel method for generating copyright-free lyrics from publicly available\nBag-of-Words (BoW) datasets, which contain the vocabulary of lyrics but not the\nlyrics themselves. Utilizing metadata associated with BoW datasets and large\nlanguage models, we successfully reconstructed lyrics. We have compiled and\nmade available a dataset of reconstructed lyrics, LyCon, aligned with metadata\nfrom renowned sources including the Million Song Dataset, Deezer Mood Detection\nDataset, and AllMusic Genre Dataset, available for public access. We believe\nthat the integration of metadata such as mood annotations or genres enables a\nvariety of academic experiments on lyrics, such as conditional lyric\ngeneration.\n","authors":["Haven Kim","Kahyun Choi"],"pdf_url":"https://arxiv.org/pdf/2408.14750v1.pdf","comment":"Dataset downlodable at https://github.com/havenpersona/lycon"},{"id":"http://arxiv.org/abs/2408.10903v4","updated":"2024-08-27T02:58:39Z","published":"2024-08-20T14:47:38Z","title":"BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model","summary":"  The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.\n","authors":["Yeyong Yu","Runsheng Yu","Haojie Wei","Zhanqiu Zhang","Quan Qian"],"pdf_url":"https://arxiv.org/pdf/2408.10903v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05706v2","updated":"2024-08-27T02:24:30Z","published":"2024-02-08T14:35:09Z","title":"Integrating Paralinguistics in Speech-Empowered Large Language Models\n  for Natural Conversation","summary":"  Recent work shows promising results in expanding the capabilities of large\nlanguage models (LLM) to directly understand and synthesize speech. However, an\nLLM-based strategy for modeling spoken dialogs remains elusive, calling for\nfurther investigation. This paper introduces an extensive speech-text LLM\nframework, the Unified Spoken Dialog Model (USDM), designed to generate\ncoherent spoken responses with naturally occurring prosodic features relevant\nto the given input speech without relying on explicit automatic speech\nrecognition (ASR) or text-to-speech (TTS) systems. We have verified the\ninclusion of prosody in speech tokens that predominantly contain semantic\ninformation and have used this foundation to construct a prosody-infused\nspeech-text model. Additionally, we propose a generalized speech-text\npretraining scheme that enhances the capture of cross-modal semantics. To\nconstruct USDM, we fine-tune our speech-text model on spoken dialog data using\na multi-step spoken dialog template that stimulates the chain-of-reasoning\ncapabilities exhibited by the underlying LLM. Automatic and human evaluations\non the DailyTalk dataset demonstrate that our approach effectively generates\nnatural-sounding spoken responses, surpassing previous and cascaded baselines.\nWe will make our code and checkpoints publicly available.\n","authors":["Heeseung Kim","Soonshin Seo","Kyeongseok Jeong","Ohsung Kwon","Soyoon Kim","Jungwhan Kim","Jaehong Lee","Eunwoo Song","Myungwoo Oh","Jung-Woo Ha","Sungroh Yoon","Kang Min Yoo"],"pdf_url":"https://arxiv.org/pdf/2402.05706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12654v3","updated":"2024-08-27T02:22:00Z","published":"2024-02-20T02:04:38Z","title":"OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech\n  Recognition, Translation, and Language Identification","summary":"  There has been an increasing interest in large speech models that can perform\nmultiple tasks in a single model. Such models usually adopt an encoder-decoder\nor decoder-only architecture due to their popularity and good performance in\nmany domains. However, autoregressive models can be slower during inference\ncompared to non-autoregressive models and also have potential risks of\nhallucination. Though prior studies observed promising results of\nnon-autoregressive models for certain tasks at small scales, it remains unclear\nif they can be scaled to speech-to-text generation in diverse languages and\ntasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we\npropose OWSM-CTC, a novel encoder-only speech foundation model based on\nConnectionist Temporal Classification (CTC). It is trained on 180k hours of\npublic audio data for multilingual automatic speech recognition (ASR), speech\ntranslation (ST), and language identification (LID). Compared to\nencoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up\nto 24% relative improvement on ST, while it is more robust and 3 to 4 times\nfaster for inference. OWSM-CTC also improves the long-form ASR result with 20x\nspeed-up. We will publicly release our code, pre-trained model, and training\nlogs to promote open science in speech foundation models.\n","authors":["Yifan Peng","Yui Sudo","Muhammad Shakeel","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2402.12654v3.pdf","comment":"Accepted at ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2401.16658v3","updated":"2024-08-27T02:15:49Z","published":"2024-01-30T01:22:18Z","title":"OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on\n  E-Branchformer","summary":"  Recent studies have highlighted the importance of fully open foundation\nmodels. The Open Whisper-style Speech Model (OWSM) is an initial step towards\nreproducing OpenAI Whisper using public data and open-source toolkits. However,\nprevious versions of OWSM (v1 to v3) are still based on standard Transformer,\nwhich might lead to inferior performance compared to state-of-the-art speech\nencoder architectures. This work aims to improve the performance and efficiency\nof OWSM without additional data. We present a series of E-Branchformer-based\nmodels named OWSM v3.1, ranging from 100M to 1B parameters. OWSM v3.1\noutperforms its predecessor, OWSM v3, in most evaluation benchmarks, while\nshowing an improved inference speed of up to 25%. We further reveal the\nemergent ability of OWSM v3.1 in zero-shot contextual biasing speech\nrecognition. We also provide a model trained on a subset of data with low\nlicense restrictions. We will publicly release the code, pre-trained models,\nand training logs.\n","authors":["Yifan Peng","Jinchuan Tian","William Chen","Siddhant Arora","Brian Yan","Yui Sudo","Muhammad Shakeel","Kwanghee Choi","Jiatong Shi","Xuankai Chang","Jee-weon Jung","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2401.16658v3.pdf","comment":"Accepted at INTERSPEECH 2024. Webpage:\n  https://www.wavlab.org/activities/2024/owsm/"},{"id":"http://arxiv.org/abs/2404.02342v2","updated":"2024-08-27T02:12:57Z","published":"2024-04-02T22:31:38Z","title":"A Computational Analysis of Lyric Similarity Perception","summary":"  In musical compositions that include vocals, lyrics significantly contribute\nto artistic expression. Consequently, previous studies have introduced the\nconcept of a recommendation system that suggests lyrics similar to a user's\nfavorites or personalized preferences, aiding in the discovery of lyrics among\nmillions of tracks. However, many of these systems do not fully consider human\nperceptions of lyric similarity, primarily due to limited research in this\narea. To bridge this gap, we conducted a comparative analysis of computational\nmethods for modeling lyric similarity with human perception. Results indicated\nthat computational models based on similarities between embeddings from\npre-trained BERT-based models, the audio from which the lyrics are derived, and\nphonetic components are indicative of perceptual lyric similarity. This finding\nunderscores the importance of semantic, stylistic, and phonetic similarities in\nhuman perception about lyric similarity. We anticipate that our findings will\nenhance the development of similarity-based lyric recommendation systems by\noffering pseudo-labels for neural network development and introducing objective\nevaluation metrics.\n","authors":["Haven Kim","Taketo Akama"],"pdf_url":"https://arxiv.org/pdf/2404.02342v2.pdf","comment":"In the process of a detailed revision"},{"id":"http://arxiv.org/abs/2408.11247v2","updated":"2024-08-27T02:11:32Z","published":"2024-08-20T23:54:26Z","title":"Unboxing Occupational Bias: Grounded Debiasing of LLMs with U.S. Labor\n  Data","summary":"  Large Language Models (LLMs) are prone to inheriting and amplifying societal\nbiases embedded within their training data, potentially reinforcing harmful\nstereotypes related to gender, occupation, and other sensitive categories. This\nissue becomes particularly problematic as biased LLMs can have far-reaching\nconsequences, leading to unfair practices and exacerbating social inequalities\nacross various domains, such as recruitment, online content moderation, or even\nthe criminal justice system. Although prior research has focused on detecting\nbias in LLMs using specialized datasets designed to highlight intrinsic biases,\nthere has been a notable lack of investigation into how these findings\ncorrelate with authoritative datasets, such as those from the U.S. National\nBureau of Labor Statistics (NBLS). To address this gap, we conduct empirical\nresearch that evaluates LLMs in a ``bias-out-of-the-box\" setting, analyzing how\nthe generated outputs compare with the distributions found in NBLS data.\nFurthermore, we propose a straightforward yet effective debiasing mechanism\nthat directly incorporates NBLS instances to mitigate bias within LLMs. Our\nstudy spans seven different LLMs, including instructable, base, and\nmixture-of-expert models, and reveals significant levels of bias that are often\noverlooked by existing bias detection techniques. Importantly, our debiasing\nmethod, which does not rely on external datasets, demonstrates a substantial\nreduction in bias scores, highlighting the efficacy of our approach in creating\nfairer and more reliable LLMs.\n","authors":["Atmika Gorti","Manas Gaur","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2408.11247v2.pdf","comment":"Accepted in AAAI Spring Symposium 2024"},{"id":"http://arxiv.org/abs/2312.06635v6","updated":"2024-08-27T01:27:29Z","published":"2023-12-11T18:51:59Z","title":"Gated Linear Attention Transformers with Hardware-Efficient Training","summary":"  Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n","authors":["Songlin Yang","Bailin Wang","Yikang Shen","Rameswar Panda","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2312.06635v6.pdf","comment":"minor update"},{"id":"http://arxiv.org/abs/2408.14721v1","updated":"2024-08-27T01:04:14Z","published":"2024-08-27T01:04:14Z","title":"PAT: Pruning-Aware Tuning for Large Language Models","summary":"  Large language models (LLMs) excel in language tasks, especially with\nsupervised fine-tuning after pre-training. However, their substantial memory\nand computational requirements hinder practical applications. Structural\npruning, which reduces less significant weight dimensions, is one solution.\nYet, traditional post-hoc pruning often leads to significant performance loss,\nwith limited recovery from further fine-tuning due to reduced capacity. Since\nthe model fine-tuning refines the general and chaotic knowledge in pre-trained\nmodels, we aim to incorporate structural pruning with the fine-tuning, and\npropose the Pruning-Aware Tuning (PAT) paradigm to eliminate model redundancy\nwhile preserving the model performance to the maximum extend. Specifically, we\ninsert the innovative Hybrid Sparsification Modules (HSMs) between the\nAttention and FFN components to accordingly sparsify the upstream and\ndownstream linear modules. The HSM comprises a lightweight operator and a\nglobally shared trainable mask. The lightweight operator maintains a training\noverhead comparable to that of LoRA, while the trainable mask unifies the\nchannels to be sparsified, ensuring structural pruning. Additionally, we\npropose the Identity Loss which decouples the transformation and scaling\nproperties of the HSMs to enhance training robustness. Extensive experiments\ndemonstrate that PAT excels in both performance and efficiency. For example,\nour Llama2-7b model with a 25\\% pruning ratio achieves 1.33$\\times$ speedup\nwhile outperforming the LoRA-finetuned model by up to 1.26\\% in accuracy with a\nsimilar training cost. Code:\nhttps://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning\n","authors":["Yijiang Liu","Huanrui Yang","Youxin Chen","Rongyu Zhang","Miao Wang","Yuan Du","Li Du"],"pdf_url":"https://arxiv.org/pdf/2408.14721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12023v4","updated":"2024-08-27T00:48:35Z","published":"2023-11-20T18:57:41Z","title":"LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient\n  Language Model Finetuning","summary":"  We propose a simple approach for memory-efficient adaptation of pretrained\nlanguage models. Our approach uses an iterative algorithm to decompose each\npretrained matrix into a high-precision low-rank component and a\nmemory-efficient quantized component. During finetuning, the quantized\ncomponent remains fixed and only the low-rank component is updated. We present\nan integer linear programming formulation of the quantization component which\nenables dynamic configuration of quantization parameters (e.g., bit-width,\nblock size) for each matrix given an overall target memory budget. We further\nexplore a data-aware version of the algorithm which uses an approximation of\nthe Fisher information matrix to weight the reconstruction objective during\nmatrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and\n70B) demonstrate that our low-rank plus quantized matrix decomposition approach\n(LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables\naggressive quantization to sub-3 bits with only minor performance degradations.\nWhen finetuned on a language modeling calibration dataset, LQ-LoRA can also be\nused for model compression; in this setting our 2.75-bit LLaMA-2-70B model\n(which has 2.85 bits on average when including the low-rank components and\nrequires 27GB of GPU memory) performs respectably compared to the 16-bit\nbaseline.\n","authors":["Han Guo","Philip Greengard","Eric P. Xing","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2311.12023v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10960v2","updated":"2024-08-27T00:27:12Z","published":"2024-07-15T17:55:42Z","title":"Fast Matrix Multiplications for Lookup Table-Quantized LLMs","summary":"  The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.\n","authors":["Han Guo","William Brandon","Radostin Cholakov","Jonathan Ragan-Kelley","Eric P. Xing","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.10960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08816v2","updated":"2024-08-27T22:51:57Z","published":"2024-04-12T21:16:53Z","title":"Measuring the Quality of Answers in Political Q&As with Large Language\n  Models","summary":"  This paper introduces a new approach for measuring the quality of answers in\npolitical question-and-answer sessions. We propose to measure answer quality\nbased on the degree to which it allows to infer the initial question\naccurately. This measure of answer quality reflects how well the answer engages\nwith and addresses the initial question. Drawing an analogy with semantic\nsearch, we demonstrate that this measurement approach can be implemented by\nfine-tuning a large language model on the corpus of observed questions and\nanswers without additional labeled data. We showcase our approach within the\ncontext of the Question Period in the Canadian House of Commons, providing\nvaluable insights into the correlates of answer quality. Our findings reveal\nsignificant variations in answer quality based on the party affiliation of the\nmembers of Parliament asking the question. Additionally, we find a meaningful\ncorrelation between answer quality and the topic raised in the question.\n","authors":["R. Michael Alvarez","Jacob Morrier"],"pdf_url":"https://arxiv.org/pdf/2404.08816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17678v2","updated":"2024-08-27T22:06:20Z","published":"2024-07-25T00:27:07Z","title":"Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads","summary":"  Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.\n","authors":["Xihui Lin","Yunan Zhang","Suyu Ge","Barun Patra","Vishrav Chaudhary","Hao Peng","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2407.17678v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2408.15417v1","updated":"2024-08-27T21:46:47Z","published":"2024-08-27T21:46:47Z","title":"Implicit Geometry of Next-token Prediction: From Language Sparsity\n  Patterns to Model Representations","summary":"  Next-token prediction (NTP) over large text corpora has become the go-to\nparadigm to train large language models. Yet, it remains unclear how NTP\ninfluences the mapping of linguistic patterns to geometric properties of the\nresulting model representations. We frame training of large language models as\nsoft-label classification over sparse probabilistic label vectors, coupled with\nan analytical approximation that allows unrestricted generation of context\nembeddings. This approach links NTP training to rank-constrained, nuclear-norm\nregularized optimization in the logit domain, offering a framework for\nanalyzing the geometry of word and context embeddings. In large embedding\nspaces, we find that NTP implicitly favors learning logits with a sparse plus\nlow-rank structure. While the sparse component captures the co-occurrence\nfrequency of context-word pairs, the orthogonal low-rank component, which\nbecomes dominant as training progresses, depends solely on the sparsity pattern\nof the co-occurrence matrix. Consequently, when projected onto an appropriate\nsubspace, representations of contexts that are followed by the same set of\nnext-tokens collapse, a phenomenon we term subspace-collapse. We validate our\nfindings on synthetic and small-scale real language datasets. Finally, we\noutline potential research directions aimed at deepening the understanding of\nNTP's influence on the learning of linguistic patterns and regularities.\n","authors":["Yize Zhao","Tina Behnia","Vala Vakilian","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2408.15417v1.pdf","comment":"Accepted at COLM 2024"},{"id":"http://arxiv.org/abs/2310.07819v3","updated":"2024-08-27T21:37:57Z","published":"2023-10-11T19:00:40Z","title":"Faithfulness Measurable Masked Language Models","summary":"  A common approach to explaining NLP models is to use importance measures that\nexpress which tokens are important for a prediction. Unfortunately, such\nexplanations are often wrong despite being persuasive. Therefore, it is\nessential to measure their faithfulness. One such metric is if tokens are truly\nimportant, then masking them should result in worse model performance. However,\ntoken masking introduces out-of-distribution issues, and existing solutions\nthat address this are computationally expensive and employ proxy models.\nFurthermore, other metrics are very limited in scope. This work proposes an\ninherently faithfulness measurable model that addresses these challenges. This\nis achieved using a novel fine-tuning method that incorporates masking, such\nthat masking tokens become in-distribution by design. This differs from\nexisting approaches, which are completely model-agnostic but are inapplicable\nin practice. We demonstrate the generality of our approach by applying it to 16\ndifferent datasets and validate it using statistical in-distribution tests. The\nfaithfulness is then measured with 9 different importance measures. Because\nmasking is in-distribution, importance measures that themselves use masking\nbecome consistently more faithful. Additionally, because the model makes\nfaithfulness cheap to measure, we can optimize explanations towards maximal\nfaithfulness; thus, our model becomes indirectly inherently explainable.\n","authors":["Andreas Madsen","Siva Reddy","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2310.07819v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15409v1","updated":"2024-08-27T21:19:37Z","published":"2024-08-27T21:19:37Z","title":"Awes, Laws, and Flaws From Today's LLM Research","summary":"  We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works based on criteria typical of what is considered good research\n(e.g. presence of statistical tests and reproducibility) and cross-validate it\nwith arguments that are at the centre of controversy (e.g., claims of emergent\nbehaviour, the use of LLMs as evaluators). We find multiple trends, such as\ndeclines in claims of emergent behaviour and the presence of ethics\ndisclaimers; and the rise of LLMs as evaluators. This paper underscores the\nneed for more scrutiny and rigour by and from this field. Critical reading and\nfamiliarity with the literature are crucial to live up to the fundamentals of a\nresponsible scientific method that is ethical, reproducible, systematic, and\nopen to criticism.\n","authors":["Adrian de Wynter"],"pdf_url":"https://arxiv.org/pdf/2408.15409v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2408.15406v1","updated":"2024-08-27T21:03:42Z","published":"2024-08-27T21:03:42Z","title":"Intertwined Biases Across Social Media Spheres: Unpacking Correlations\n  in Media Bias Dimensions","summary":"  Media bias significantly shapes public perception by reinforcing stereotypes\nand exacerbating societal divisions. Prior research has often focused on\nisolated media bias dimensions such as \\textit{political bias} or\n\\textit{racial bias}, neglecting the complex interrelationships among various\nbias dimensions across different topic domains. Moreover, we observe that\nmodels trained on existing media bias benchmarks fail to generalize effectively\non recent social media posts, particularly in certain bias identification\ntasks. This shortfall primarily arises because these benchmarks do not\nadequately reflect the rapidly evolving nature of social media content, which\nis characterized by shifting user behaviors and emerging trends. In response to\nthese limitations, our research introduces a novel dataset collected from\nYouTube and Reddit over the past five years. Our dataset includes automated\nannotations for YouTube content across a broad spectrum of bias dimensions,\nsuch as gender, racial, and political biases, as well as hate speech, among\nothers. It spans diverse domains including politics, sports, healthcare,\neducation, and entertainment, reflecting the complex interplay of biases across\ndifferent societal sectors. Through comprehensive statistical analysis, we\nidentify significant differences in bias expression patterns and intra-domain\nbias correlations across these domains. By utilizing our understanding of the\ncorrelations among various bias dimensions, we lay the groundwork for creating\nadvanced systems capable of detecting multiple biases simultaneously. Overall,\nour dataset advances the field of media bias identification, contributing to\nthe development of tools that promote fairer media consumption. The\ncomprehensive awareness of existing media bias fosters more ethical journalism,\npromotes cultural sensitivity, and supports a more informed and equitable\npublic discourse.\n","authors":["Yifan Liu","Yike Li","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2408.15406v1.pdf","comment":"Accepted to ASONAM 2024"},{"id":"http://arxiv.org/abs/2408.15399v1","updated":"2024-08-27T20:51:06Z","published":"2024-08-27T20:51:06Z","title":"A Statistical Framework for Data-dependent Retrieval-Augmented Models","summary":"  Modern ML systems increasingly augment input instances with additional\nrelevant information to enhance final prediction. Despite growing interest in\nsuch retrieval-augmented models, their fundamental properties and training are\nnot well understood. We propose a statistical framework to study such models\nwith two components: 1) a {\\em retriever} to identify the relevant information\nout of a large corpus via a data-dependent metric; and 2) a {\\em predictor}\nthat consumes the input instances along with the retrieved information to make\nthe final predictions. We present a principled method for end-to-end training\nof both components and draw connections with various training approaches in the\nliterature. Furthermore, we establish excess risk bounds for\nretrieval-augmented models while delineating the contributions of both\nretriever and predictor towards the model performance. We validate the utility\nof our proposed training methods along with the key takeaways from our\nstatistical analysis on open domain question answering task where retrieval\naugmentation is important.\n","authors":["Soumya Basu","Ankit Singh Rawat","Manzil Zaheer"],"pdf_url":"https://arxiv.org/pdf/2408.15399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01966v2","updated":"2024-08-27T20:05:59Z","published":"2024-08-04T09:04:44Z","title":"ML-EAT: A Multilevel Embedding Association Test for Interpretable and\n  Transparent Social Science","summary":"  This research introduces the Multilevel Embedding Association Test (ML-EAT),\na method designed for interpretable and transparent measurement of intrinsic\nbias in language technologies. The ML-EAT addresses issues of ambiguity and\ndifficulty in interpreting the traditional EAT measurement by quantifying bias\nat three levels of increasing granularity: the differential association between\ntwo target concepts with two attribute concepts; the individual effect size of\neach target concept with two attribute concepts; and the association between\neach individual target concept and each individual attribute concept. Using the\nML-EAT, this research defines a taxonomy of EAT patterns describing the nine\npossible outcomes of an embedding association test, each of which is associated\nwith a unique EAT-Map, a novel four-quadrant visualization for interpreting the\nML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2\nlanguage models, and a CLIP language-and-image model shows that EAT patterns\nadd otherwise unobservable information about the component biases that make up\nan EAT; reveal the effects of prompting in zero-shot models; and can also\nidentify situations when cosine similarity is an ineffective metric, rendering\nan EAT unreliable. Our work contributes a method for rendering bias more\nobservable and interpretable, improving the transparency of computational\ninvestigations into human minds and societies.\n","authors":["Robert Wolfe","Alexis Hiniker","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.01966v2.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2408.01959v2","updated":"2024-08-27T19:57:45Z","published":"2024-08-04T08:26:58Z","title":"Dataset Scale and Societal Consistency Mediate Facial Impression Bias in\n  Vision-Language AI","summary":"  Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting.\n","authors":["Robert Wolfe","Aayushi Dangol","Alexis Hiniker","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.01959v2.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2408.15379v1","updated":"2024-08-27T19:33:15Z","published":"2024-08-27T19:33:15Z","title":"DualKanbaFormer: Kolmogorov-Arnold Networks and State Space Model\n  DualKanbaFormer: Kolmogorov-Arnold Networks and State Space Model Transformer\n  for Multimodal Aspect-based Sentiment Analysis","summary":"  Multimodal aspect-based sentiment analysis (MABSA) enhances sentiment\ndetection by combining text with other data types like images. However, despite\nsetting significant benchmarks, attention mechanisms exhibit limitations in\nefficiently modelling long-range dependencies between aspect and opinion\ntargets within the text. They also face challenges in capturing global-context\ndependencies for visual representations. To this end, we propose\nKolmogorov-Arnold Networks (KANs) and Selective State Space model (Mamba)\ntransformer (DualKanbaFormer), a novel architecture to address the above\nissues. We leverage the power of Mamba to capture global context dependencies,\nMulti-head Attention (MHA) to capture local context dependencies, and KANs to\ncapture non-linear modelling patterns for both textual representations (textual\nKanbaFormer) and visual representations (visual KanbaFormer). Furthermore, we\nfuse the textual KanbaFormer and visual KanbaFomer with a gated fusion layer to\ncapture the inter-modality dynamics. According to extensive experimental\nresults, our model outperforms some state-of-the-art (SOTA) studies on two\npublic datasets.\n","authors":["Adamu Lawan","Juhua Pu","Haruna Yunusa","Muhammad Lawan","Aliyu Umar","Adamu Sani Yahya"],"pdf_url":"https://arxiv.org/pdf/2408.15379v1.pdf","comment":"10 pages, 2 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2408.15366v1","updated":"2024-08-27T19:03:11Z","published":"2024-08-27T19:03:11Z","title":"Pitfalls and Outlooks in Using COMET","summary":"  Since its introduction, the COMET metric has blazed a trail in the machine\ntranslation community, given its strong correlation with human judgements of\ntranslation quality. Its success stems from being a modified pre-trained\nmultilingual model finetuned for quality assessment. However, it being a\nmachine learning model also gives rise to a new set of pitfalls that may not be\nwidely known. We investigate these unexpected behaviours from three aspects: 1)\ntechnical: obsolete software versions and compute precision; 2) data: empty\ncontent, language mismatch, and translationese at test time as well as\ndistribution and domain biases in training; 3) usage and reporting:\nmulti-reference support and model referencing in the literature. All of these\nproblems imply that COMET scores is not comparable between papers or even\ntechnical setups and we put forward our perspective on fixing each issue.\nFurthermore, we release the SacreCOMET package that can generate a signature\nfor the software and model configuration as well as an appropriate citation.\nThe goal of this work is to help the community make more sound use of the COMET\nmetric.\n","authors":["VilÃ©m Zouhar","Pinzhen Chen","Tsz Kin Lam","Nikita Moghe","Barry Haddow"],"pdf_url":"https://arxiv.org/pdf/2408.15366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06062v3","updated":"2024-08-27T18:47:13Z","published":"2024-08-12T11:23:24Z","title":"On Tables with Numbers, with Numbers","summary":"  This paper is a critical reflection on the epistemic culture of contemporary\ncomputational linguistics, framed in the context of its growing obsession with\ntables with numbers. We argue against tables with numbers on the basis of their\nepistemic irrelevance, their environmental impact, their role in enabling and\nexacerbating social inequalities, and their deep ties to commercial\napplications and profit-driven research. We substantiate our arguments with\nempirical evidence drawn from a meta-analysis of computational linguistics\nresearch over the last decade.\n","authors":["Konstantinos Kogkalidis","Stergios Chatzikyriakidis"],"pdf_url":"https://arxiv.org/pdf/2408.06062v3.pdf","comment":"v3: Stergios' acknowledgements"},{"id":"http://arxiv.org/abs/2408.15339v1","updated":"2024-08-27T18:04:07Z","published":"2024-08-27T18:04:07Z","title":"UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized\n  Implicit Reward Function","summary":"  An LLM is pretrained on trillions of tokens, but the pretrained LLM may still\ngenerate undesired responses. To solve this problem, alignment techniques such\nas RLHF, DPO and KTO are proposed. However, these alignment techniques have\nlimitations. For example, RLHF requires training the reward model and policy\nseparately, which is complex, time-consuming, memory intensive and unstable\nduring training processes. DPO proposes a mapping between an optimal policy and\na reward, greatly simplifying the training process of RLHF. However, it can not\ntake full advantages of a reward model and it is limited to pairwise preference\ndata.\n  In this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which\nunifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the\nclassical RLHF objective, the optimal policy is induced by a generalize\nimplicit reward function. With this novel mapping between a reward model and an\noptimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised\nlearning of minimizing the difference between an implicit reward and an\nexplicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and\nreduce memory burden of RL fine-tuning process; 3. accommodate different\nfeedback types including pairwise, binary and scalar feedback. Downstream\nexperiments show UNA outperforms DPO, KTO and RLHF.\n","authors":["Zhichao Wang","Bin Bi","Can Huang","Shiva Kumar Pentyala","Zixu James Zhu","Sitaram Asur","Na Claire Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.15339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15313v1","updated":"2024-08-27T17:31:21Z","published":"2024-08-27T17:31:21Z","title":"Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in\n  Language Models","summary":"  Fine-tuning large language models (LLMs) on human preferences, typically\nthrough reinforcement learning from human feedback (RLHF), has proven\nsuccessful in enhancing their capabilities. However, ensuring the safety of\nLLMs during the fine-tuning remains a critical concern, and mitigating the\npotential conflicts in safety and helpfulness is costly in RLHF. To address\nthis issue, we propose a supervised learning framework called Bi-Factorial\nPreference Optimization (BFPO), which re-parameterizes a joint RLHF objective\nof both safety and helpfulness into a single supervised learning objective. In\nthe supervised optimization, a labeling function is used to capture global\npreferences ranking to balance both safety and helpfulness. To evaluate BFPO,\nwe develop a benchmark including comprehensive discriminative and generative\ntasks for helpfulness and harmlessness. The results indicate that our method\nsignificantly outperforms existing approaches in both safety and helpfulness.\nMoreover, BFPO eliminates the need for human prompting and annotation in LLM\nfine-tuning while achieving the same level of safety as methods that heavily\nrely on human labor, with less than 10% of the computational resources. The\ntraining recipes and models will be released.\n","authors":["Wenxuan Zhang","Philip H. S. Torr","Mohamed Elhoseiny","Adel Bibi"],"pdf_url":"https://arxiv.org/pdf/2408.15313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15297v1","updated":"2024-08-27T11:31:12Z","published":"2024-08-27T11:31:12Z","title":"YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection","summary":"  Dysfluent speech detection is the bottleneck for disordered speech analysis\nand spoken language learning. Current state-of-the-art models are governed by\nrule-based systems which lack efficiency and robustness, and are sensitive to\ntemplate design. In this paper, we propose YOLO-Stutter: a first end-to-end\nmethod that detects dysfluencies in a time-accurate manner. YOLO-Stutter takes\nimperfect speech-text alignment as input, followed by a spatial feature\naggregator, and a temporal dependency extractor to perform region-wise boundary\nand class predictions. We also introduce two dysfluency corpus, VCTK-Stutter\nand VCTK-TTS, that simulate natural spoken dysfluencies including repetition,\nblock, missing, replacement, and prolongation. Our end-to-end method achieves\nstate-of-the-art performance with a minimum number of trainable parameters for\non both simulated data and real aphasia speech. Code and datasets are\nopen-sourced at https://github.com/rorizzz/YOLO-Stutter\n","authors":["Xuanru Zhou","Anshul Kashyap","Steve Li","Ayati Sharma","Brittany Morin","David Baquirin","Jet Vonk","Zoe Ezzes","Zachary Miller","Maria Luisa Gorno Tempini","Jiachen Lian","Gopala Krishna Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2408.15297v1.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2408.15293v1","updated":"2024-08-27T08:19:34Z","published":"2024-08-27T08:19:34Z","title":"Learning Granularity Representation for Temporal Knowledge Graph\n  Completion","summary":"  Temporal Knowledge Graphs (TKGs) incorporate temporal information to reflect\nthe dynamic structural knowledge and evolutionary patterns of real-world facts.\nNevertheless, TKGs are still limited in downstream applications due to the\nproblem of incompleteness. Consequently, TKG completion (also known as link\nprediction) has been widely studied, with recent research focusing on\nincorporating independent embeddings of time or combining them with entities\nand relations to form temporal representations. However, most existing methods\noverlook the impact of history from a multi-granularity aspect. The inherent\nsemantics of human-defined temporal granularities, such as ordinal dates,\nreveal general patterns to which facts typically adhere. To counter this\nlimitation, this paper proposes \\textbf{L}earning \\textbf{G}ranularity\n\\textbf{Re}presentation (termed $\\mathsf{LGRe}$) for TKG completion. It\ncomprises two main components: Granularity Representation Learning (GRL) and\nAdaptive Granularity Balancing (AGB). Specifically, GRL employs time-specific\nmulti-layer convolutional neural networks to capture interactions between\nentities and relations at different granularities. After that, AGB generates\nadaptive weights for these embeddings according to temporal semantics,\nresulting in expressive representations of predictions. Moreover, to reflect\nsimilar semantics of adjacent timestamps, a temporal loss function is\nintroduced. Extensive experimental results on four event benchmarks demonstrate\nthe effectiveness of $\\mathsf{LGRe}$ in learning time-related representations.\nTo ensure reproducibility, our code is available at\nhttps://github.com/KcAcoZhang/LGRe.\n","authors":["Jinchuan Zhang","Tianqi Wan","Chong Mu","Guangxi Lu","Ling Tian"],"pdf_url":"https://arxiv.org/pdf/2408.15293v1.pdf","comment":"15 pages. Accepted at ICONIP 2024"},{"id":"http://arxiv.org/abs/2408.13609v2","updated":"2024-08-27T04:49:46Z","published":"2024-08-24T15:43:02Z","title":"GNN: Graph Neural Network and Large Language Model for Data Discovery","summary":"  Our algorithm GNN: Graph Neural Network and Large Language Model for Data\nDiscovery inherit the benefits of \\cite{hoang2024plod} (PLOD: Predictive\nLearning Optimal Data Discovery), \\cite{Hoang2024BODBO} (BOD: Blindly Optimal\nData Discovery) in terms of overcoming the challenges of having to predefine\nutility function and the human input for attribute ranking, which helps prevent\nthe time-consuming loop process. In addition to these previous works, our\nalgorithm GNN leverages the advantages of graph neural networks and large\nlanguage models to understand text type values that cannot be understood by\nPLOD and MOD, thus making the task of predicting outcomes more reliable. GNN\ncould be seen as an extension of PLOD in terms of understanding the text type\nvalue and the user's preferences, not only numerical values but also text\nvalues, making the promise of data science and analytics purposes.\n","authors":["Thomas Hoang"],"pdf_url":"https://arxiv.org/pdf/2408.13609v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.15242v1","updated":"2024-08-27T17:59:55Z","published":"2024-08-27T17:59:55Z","title":"Drone-assisted Road Gaussian Splatting with Cross-view Uncertainty","summary":"  Robust and realistic rendering for large-scale road scenes is essential in\nautonomous driving simulation. Recently, 3D Gaussian Splatting (3D-GS) has made\ngroundbreaking progress in neural rendering, but the general fidelity of\nlarge-scale road scene renderings is often limited by the input imagery, which\nusually has a narrow field of view and focuses mainly on the street-level local\narea. Intuitively, the data from the drone's perspective can provide a\ncomplementary viewpoint for the data from the ground vehicle's perspective,\nenhancing the completeness of scene reconstruction and rendering. However,\ntraining naively with aerial and ground images, which exhibit large view\ndisparity, poses a significant convergence challenge for 3D-GS, and does not\ndemonstrate remarkable improvements in performance on road views. In order to\nenhance the novel view synthesis of road views and to effectively use the\naerial information, we design an uncertainty-aware training method that allows\naerial images to assist in the synthesis of areas where ground images have poor\nlearning outcomes instead of weighting all pixels equally in 3D-GS training\nlike prior work did. We are the first to introduce the cross-view uncertainty\nto 3D-GS by matching the car-view ensemble-based rendering uncertainty to\naerial images, weighting the contribution of each pixel to the training\nprocess. Additionally, to systematically quantify evaluation metrics, we\nassemble a high-quality synthesized dataset comprising both aerial and ground\nimages for road scenes.\n","authors":["Saining Zhang","Baijun Ye","Xiaoxue Chen","Yuantao Chen","Zongzheng Zhang","Cheng Peng","Yongliang Shi","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.15242v1.pdf","comment":"BMVC2024 Project Page: https://sainingzhang.github.io/project/uc-gs/\n  Code: https://github.com/SainingZhang/uc-gs/"},{"id":"http://arxiv.org/abs/2408.15241v1","updated":"2024-08-27T17:59:41Z","published":"2024-08-27T17:59:41Z","title":"GenRec: Unifying Video Generation and Recognition with Diffusion Models","summary":"  Video diffusion models are able to generate high-quality videos by learning\nstrong spatial-temporal priors on large-scale datasets. In this paper, we aim\nto investigate whether such priors derived from a generative process are\nsuitable for video recognition, and eventually joint optimization of generation\nand recognition. Building upon Stable Video Diffusion, we introduce GenRec, the\nfirst unified framework trained with a random-frame conditioning process so as\nto learn generalized spatial-temporal representations. The resulting framework\ncan naturally supports generation and recognition, and more importantly is\nrobust even when visual inputs contain limited information. Extensive\nexperiments demonstrate the efficacy of GenRec for both recognition and\ngeneration. In particular, GenRec achieves competitive recognition performance,\noffering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. GenRec also\nperforms the best class-conditioned image-to-video generation results,\nachieving 46.5 and 49.3 FVD scores on SSV2 and EK-100 datasets. Furthermore,\nGenRec demonstrates extraordinary robustness in scenarios that only limited\nframes can be observed.\n","authors":["Zejia Weng","Xitong Yang","Zhen Xing","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.15241v1.pdf","comment":"17 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2408.15239v1","updated":"2024-08-27T17:57:14Z","published":"2024-08-27T17:57:14Z","title":"Generative Inbetweening: Adapting Image-to-Video Models for Keyframe\n  Interpolation","summary":"  We present a method for generating video sequences with coherent motion\nbetween a pair of input key frames. We adapt a pretrained large-scale\nimage-to-video diffusion model (originally trained to generate videos moving\nforward in time from a single input image) for key frame interpolation, i.e.,\nto produce a video in between two input frames. We accomplish this adaptation\nthrough a lightweight fine-tuning technique that produces a version of the\nmodel that instead predicts videos moving backwards in time from a single input\nimage. This model (along with the original forward-moving model) is\nsubsequently used in a dual-directional diffusion sampling process that\ncombines the overlapping model estimates starting from each of the two\nkeyframes. Our experiments show that our method outperforms both existing\ndiffusion-based methods and traditional frame interpolation techniques.\n","authors":["Xiaojuan Wang","Boyang Zhou","Brian Curless","Ira Kemelmacher-Shlizerman","Aleksander Holynski","Steven M. Seitz"],"pdf_url":"https://arxiv.org/pdf/2408.15239v1.pdf","comment":"project page: https://svd-keyframe-interpolation.github.io/"},{"id":"http://arxiv.org/abs/2408.15235v1","updated":"2024-08-27T17:53:18Z","published":"2024-08-27T17:53:18Z","title":"Learning-based Multi-View Stereo: A Survey","summary":"  3D reconstruction aims to recover the dense 3D structure of a scene. It plays\nan essential role in various applications such as Augmented/Virtual Reality\n(AR/VR), autonomous driving and robotics. Leveraging multiple views of a scene\ncaptured from different viewpoints, Multi-View Stereo (MVS) algorithms\nsynthesize a comprehensive 3D representation, enabling precise reconstruction\nin complex environments. Due to its efficiency and effectiveness, MVS has\nbecome a pivotal method for image-based 3D reconstruction. Recently, with the\nsuccess of deep learning, many learning-based MVS methods have been proposed,\nachieving impressive performance against traditional methods. We categorize\nthese learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D\nGaussian Splatting-based, and large feed-forward methods. Among these, we focus\nsignificantly on depth map-based methods, which are the main family of MVS due\nto their conciseness, flexibility and scalability. In this survey, we provide a\ncomprehensive review of the literature at the time of this writing. We\ninvestigate these learning-based methods, summarize their performances on\npopular benchmarks, and discuss promising future research directions in this\narea.\n","authors":["Fangjinhua Wang","Qingtian Zhu","Di Chang","Quankai Gao","Junlin Han","Tong Zhang","Richard Hartley","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2408.15235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15231v1","updated":"2024-08-27T17:48:29Z","published":"2024-08-27T17:48:29Z","title":"DCT-CryptoNets: Scaling Private Inference in the Frequency Domain","summary":"  The convergence of fully homomorphic encryption (FHE) and machine learning\noffers unprecedented opportunities for private inference of sensitive data. FHE\nenables computation directly on encrypted data, safeguarding the entire machine\nlearning pipeline, including data and model confidentiality. However, existing\nFHE-based implementations for deep neural networks face significant challenges\nin computational cost, latency, and scalability, limiting their practical\ndeployment. This paper introduces DCT-CryptoNets, a novel approach that\nleverages frequency-domain learning to tackle these issues. Our method operates\ndirectly in the frequency domain, utilizing the discrete cosine transform (DCT)\ncommonly employed in JPEG compression. This approach is inherently compatible\nwith remote computing services, where images are usually transmitted and stored\nin compressed formats. DCT-CryptoNets reduces the computational burden of\nhomomorphic operations by focusing on perceptually relevant low-frequency\ncomponents. This is demonstrated by substantial latency reduction of up to\n5.3$\\times$ compared to prior work on image classification tasks, including a\nnovel demonstration of ImageNet inference within 2.5 hours, down from 12.5\nhours compared to prior work on equivalent compute resources. Moreover,\nDCT-CryptoNets improves the reliability of encrypted accuracy by reducing\nvariability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet). This study\ndemonstrates a promising avenue for achieving efficient and practical\nprivacy-preserving deep learning on high resolution images seen in real-world\napplications.\n","authors":["Arjun Roy","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2408.15231v1.pdf","comment":"Under Review; 10 pages content, 3 pages appendix, 4 figures, 8\n  tables; Code TBD"},{"id":"http://arxiv.org/abs/2408.15224v1","updated":"2024-08-27T17:39:33Z","published":"2024-08-27T17:39:33Z","title":"SAM & SAM 2 in 3D Slicer: SegmentWithSAM Extension for Annotating\n  Medical Images","summary":"  Creating annotations for 3D medical data is time-consuming and often requires\nhighly specialized expertise. Various tools have been implemented to aid this\nprocess. Segment Anything Model 2 (SAM 2) offers a general-purpose prompt-based\nsegmentation algorithm designed to annotate videos. In this paper, we adapt\nthis model to the annotation of 3D medical images and offer our implementation\nin the form of an extension to the popular annotation software: 3D Slicer. Our\nextension allows users to place point prompts on 2D slices to generate\nannotation masks and propagate these annotations across entire volumes in\neither single-directional or bi-directional manners. Our code is publicly\navailable on https://github.com/mazurowski-lab/SlicerSegmentWithSAM and can be\neasily installed directly from the Extension Manager of 3D Slicer as well.\n","authors":["Zafer Yildiz","Yuwen Chen","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2408.15224v1.pdf","comment":"Future work: support for box and mask inputs for the video predictor\n  of SAM 2"},{"id":"http://arxiv.org/abs/2408.15218v1","updated":"2024-08-27T17:31:00Z","published":"2024-08-27T17:31:00Z","title":"Histo-Diffusion: A Diffusion Super-Resolution Method for Digital\n  Pathology with Comprehensive Quality Assessment","summary":"  Digital pathology has advanced significantly over the last decade, with Whole\nSlide Images (WSIs) encompassing vast amounts of data essential for accurate\ndisease diagnosis. High-resolution WSIs are essential for precise diagnosis but\ntechnical limitations in scanning equipment and variablity in slide preparation\ncan hinder obtaining these images. Super-resolution techniques can enhance\nlow-resolution images; while Generative Adversarial Networks (GANs) have been\neffective in natural image super-resolution tasks, they often struggle with\nhistopathology due to overfitting and mode collapse. Traditional evaluation\nmetrics fall short in assessing the complex characteristics of histopathology\nimages, necessitating robust histology-specific evaluation methods.\n  We introduce Histo-Diffusion, a novel diffusion-based method specially\ndesigned for generating and evaluating super-resolution images in digital\npathology. It includes a restoration module for histopathology prior and a\ncontrollable diffusion module for generating high-quality images. We have\ncurated two histopathology datasets and proposed a comprehensive evaluation\nstrategy which incorporates both full-reference and no-reference metrics to\nthoroughly assess the quality of digital pathology images.\n  Comparative analyses on multiple datasets with state-of-the-art methods\nreveal that Histo-Diffusion outperforms GANs. Our method offers a versatile\nsolution for histopathology image super-resolution, capable of handling\nmulti-resolution generation from varied input sizes, providing valuable support\nin diagnostic processes.\n","authors":["Xuan Xu","Saarthak Kapse","Prateek Prasanna"],"pdf_url":"https://arxiv.org/pdf/2408.15218v1.pdf","comment":"We have submitted our paper to Medical Image Analysis and are\n  currently awaiting feedback"},{"id":"http://arxiv.org/abs/2408.15217v1","updated":"2024-08-27T17:30:49Z","published":"2024-08-27T17:30:49Z","title":"Fundus2Video: Cross-Modal Angiography Video Generation from Static\n  Fundus Photography with Clinical Knowledge Guidance","summary":"  Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal\nvascular dynamics and aiding in the diagnosis of eye diseases. However, its\ninvasive nature and less accessibility compared to Color Fundus (CF) images\npose significant challenges. Current CF to FFA translation methods are limited\nto static generation. In this work, we pioneer dynamic FFA video generation\nfrom static CF images. We introduce an autoregressive GAN for smooth,\nmemory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic\nlesion changes in FFA regions, we design a knowledge mask based on clinical\nexperience. Leveraging this mask, our approach integrates innovative knowledge\nmask-guided techniques, including knowledge-boosted attention, knowledge-aware\ndiscriminators, and mask-enhanced patchNCE loss, aimed at refining generation\nin critical areas and addressing the pixel misalignment challenge. Our method\nachieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common\nvideo generation approaches. Human assessment by an ophthalmologist confirms\nits high generation quality. Notably, our knowledge mask surpasses supervised\nlesion segmentation masks, offering a promising non-invasive alternative to\ntraditional FFA for research and clinical applications. The code is available\nat https://github.com/Michi-3000/Fundus2Video.\n","authors":["Weiyi Zhang","Siyu Huang","Jiancheng Yang","Ruoyu Chen","Zongyuan Ge","Yingfeng Zheng","Danli Shi","Mingguang He"],"pdf_url":"https://arxiv.org/pdf/2408.15217v1.pdf","comment":"The paper has been accepted by Medical Image Computing and Computer\n  Assisted Intervention Society (MICCAI) 2024"},{"id":"http://arxiv.org/abs/2407.05921v2","updated":"2024-08-27T17:14:16Z","published":"2024-07-08T13:28:47Z","title":"TAPVid-3D: A Benchmark for Tracking Any Point in 3D","summary":"  We introduce a new benchmark, TAPVid-3D, for evaluating the task of\nlong-range Tracking Any Point in 3D (TAP-3D). While point tracking in two\ndimensions (TAP) has many benchmarks measuring performance on real-world\nvideos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To\nthis end, leveraging existing footage, we build a new benchmark for 3D point\ntracking featuring 4,000+ real-world videos, composed of three different data\nsources spanning a variety of object types, motion patterns, and indoor and\noutdoor environments. To measure performance on the TAP-3D task, we formulate a\ncollection of metrics that extend the Jaccard-based metric used in TAP to\nhandle the complexities of ambiguous depth scales across models, occlusions,\nand multi-track spatio-temporal smoothness. We manually verify a large sample\nof trajectories to ensure correct video annotations, and assess the current\nstate of the TAP-3D task by constructing competitive baselines using existing\ntracking models. We anticipate this benchmark will serve as a guidepost to\nimprove our ability to understand precise 3D motion and surface deformation\nfrom monocular video. Code for dataset download, generation, and model\nevaluation is available at https://tapvid3d.github.io\n","authors":["Skanda Koppula","Ignacio Rocco","Yi Yang","Joe Heyward","JoÃ£o Carreira","Andrew Zisserman","Gabriel Brostow","Carl Doersch"],"pdf_url":"https://arxiv.org/pdf/2407.05921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15205v1","updated":"2024-08-27T17:06:22Z","published":"2024-08-27T17:06:22Z","title":"Leveraging Hallucinations to Reduce Manual Prompt Dependency in\n  Promptable Segmentation","summary":"  Promptable segmentation typically requires instance-specific manual prompts\nto guide the segmentation of each desired object. To minimize such a need,\ntask-generic promptable segmentation has been introduced, which employs a\nsingle task-generic prompt to segment various images of different objects in\nthe same task. Current methods use Multimodal Large Language Models (MLLMs) to\nreason detailed instance-specific prompts from a task-generic prompt for\nimproving segmentation accuracy. The effectiveness of this segmentation heavily\ndepends on the precision of these derived prompts. However, MLLMs often suffer\nhallucinations during reasoning, resulting in inaccurate prompting. While\nexisting methods focus on eliminating hallucinations to improve a model, we\nargue that MLLM hallucinations can reveal valuable contextual insights when\nleveraged correctly, as they represent pre-trained large-scale knowledge beyond\nindividual images. In this paper, we utilize hallucinations to mine\ntask-related information from images and verify its accuracy for enhancing\nprecision of the generated prompts. Specifically, we introduce an iterative\nPrompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a\nmask generator.The prompt generator uses a multi-scale chain of thought\nprompting, initially exploring hallucinations for extracting extended\ncontextual knowledge on a test image.These hallucinations are then reduced to\nformulate precise instance-specific prompts, directing the mask generator to\nproduce masks that are consistent with task semantics by mask semantic\nalignment. The generated masks iteratively induce the prompt generator to focus\nmore on task-relevant image areas and reduce irrelevant hallucinations,\nresulting jointly in better prompts and masks. Experiments on 5 benchmarks\ndemonstrate the effectiveness of ProMaC. Code given in\nhttps://lwpyh.github.io/ProMaC/.\n","authors":["Jian Hu","Jiayi Lin","Junchi Yan","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2408.15205v1.pdf","comment":"We propose using hallucinations as prior knowledge to extract and\n  validate task-related information, which helps generate instance-specific\n  prompts for reducing reliance on manual prompts in promptable segmentation"},{"id":"http://arxiv.org/abs/2408.15201v1","updated":"2024-08-27T17:02:03Z","published":"2024-08-27T17:02:03Z","title":"An Investigation on The Position Encoding in Vision-Based Dynamics\n  Prediction","summary":"  Despite the success of vision-based dynamics prediction models, which predict\nobject states by utilizing RGB images and simple object descriptions, they were\nchallenged by environment misalignments. Although the literature has\ndemonstrated that unifying visual domains with both environment context and\nobject abstract, such as semantic segmentation and bounding boxes, can\neffectively mitigate the visual domain misalignment challenge, discussions were\nfocused on the abstract of environment context, and the insight of using\nbounding box as the object abstract is under-explored. Furthermore, we notice\nthat, as empirical results shown in the literature, even when the visual\nappearance of objects is removed, object bounding boxes alone, instead of being\ndirectly fed into the network, can indirectly provide sufficient position\ninformation via the Region of Interest Pooling operation for dynamics\nprediction. However, previous literature overlooked discussions regarding how\nsuch position information is implicitly encoded in the dynamics prediction\nmodel. Thus, in this paper, we provide detailed studies to investigate the\nprocess and necessary conditions for encoding position information via using\nthe bounding box as the object abstract into output features. Furthermore, we\nstudy the limitation of solely using object abstracts, such that the dynamics\nprediction performance will be jeopardized when the environment context varies.\n","authors":["Jiageng Zhu","Hanchen Xie","Jiazhi Li","Mahyar Khayatkhoei","Wael AbdAlmageed"],"pdf_url":"https://arxiv.org/pdf/2408.15201v1.pdf","comment":"13 pages, 4 tables, and 3 figures. Accepted to ECCV2024 eXCV workshop"},{"id":"http://arxiv.org/abs/2408.02088v3","updated":"2024-08-27T16:46:53Z","published":"2024-08-04T16:54:49Z","title":"KAN-RCBEVDepth: A multi-modal fusion algorithm in object detection for\n  autonomous driving","summary":"  Accurate 3D object detection in autonomous driving is critical yet\nchallenging due to occlusions, varying object sizes, and complex urban\nenvironments. This paper introduces the KAN-RCBEVDepth method, an innovative\napproach aimed at enhancing 3D object detection by fusing multimodal sensor\ndata from cameras, LiDAR, and millimeter-wave radar. Our unique Bird's Eye\nView-based approach significantly improves detection accuracy and efficiency by\nseamlessly integrating diverse sensor inputs, refining spatial relationship\nunderstanding, and optimizing computational procedures. Experimental results\nshow that the proposed method outperforms existing techniques across multiple\ndetection metrics, achieving a higher Mean Distance AP (0.389, 23\\%\nimprovement), a better ND Score (0.485, 17.1\\% improvement), and a faster\nEvaluation Time (71.28s, 8\\% faster). Additionally, the KAN-RCBEVDepth method\nsignificantly reduces errors compared to BEVDepth, with lower Transformation\nError (0.6044, 13.8\\% improvement), Scale Error (0.2780, 2.6\\% improvement),\nOrientation Error (0.5830, 7.6\\% improvement), Velocity Error (0.4244, 28.3\\%\nimprovement), and Attribute Error (0.2129, 3.2\\% improvement). These findings\nsuggest that our method offers enhanced accuracy, reliability, and efficiency,\nmaking it well-suited for dynamic and demanding autonomous driving scenarios.\nThe code will be released in \\url{https://github.com/laitiamo/RCBEVDepth-KAN}.\n","authors":["Zhihao Lai","Chuanhao Liu","Shihui Sheng","Zhiqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02088v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08789v4","updated":"2024-08-27T16:43:17Z","published":"2023-07-17T19:17:10Z","title":"Creating Image Datasets in Agricultural Environments using DALL.E:\n  Generative AI-Powered Large Language Model","summary":"  This research investigated the role of artificial intelligence (AI),\nspecifically the DALL.E model by OpenAI, in advancing data generation and\nvisualization techniques in agriculture. DALL.E, an advanced AI image\ngenerator, works alongside ChatGPT's language processing to transform text\ndescriptions and image clues into realistic visual representations of the\ncontent. The study used both approaches of image generation: text-to-image and\nimage-to image (variation). Six types of datasets depicting fruit crop\nenvironment were generated. These AI-generated images were then compared\nagainst ground truth images captured by sensors in real agricultural fields.\nThe comparison was based on Peak Signal-to-Noise Ratio (PSNR) and Feature\nSimilarity Index (FSIM) metrics. The image-to-image generation exhibited a\n5.78% increase in average PSNR over text-to-image methods, signifying superior\nimage clarity and quality. However, this method also resulted in a 10.23%\ndecrease in average FSIM, indicating a diminished structural and textural\nsimilarity to the original images. Similar to these measures, human evaluation\nalso showed that images generated using image-to-image-based method were more\nrealistic compared to those generated with text-to-image approach. The results\nhighlighted DALL.E's potential in generating realistic agricultural image\ndatasets and thus accelerating the development and adoption of imaging-based\nprecision agricultural solutions.\n","authors":["Ranjan Sapkota","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2307.08789v4.pdf","comment":"9 Figures, 1 table, 17 pages"},{"id":"http://arxiv.org/abs/2408.15185v1","updated":"2024-08-27T16:40:14Z","published":"2024-08-27T16:40:14Z","title":"PoseWatch: A Transformer-based Architecture for Human-centric Video\n  Anomaly Detection Using Spatio-temporal Pose Tokenization","summary":"  Video Anomaly Detection (VAD) presents a significant challenge in computer\nvision, particularly due to the unpredictable and infrequent nature of\nanomalous events, coupled with the diverse and dynamic environments in which\nthey occur. Human-centric VAD, a specialized area within this domain, faces\nadditional complexities, including variations in human behavior, potential\nbiases in data, and substantial privacy concerns related to human subjects.\nThese issues complicate the development of models that are both robust and\ngeneralizable. To address these challenges, recent advancements have focused on\npose-based VAD, which leverages human pose as a high-level feature to mitigate\nprivacy concerns, reduce appearance biases, and minimize background\ninterference. In this paper, we introduce PoseWatch, a novel transformer-based\narchitecture designed specifically for human-centric pose-based VAD. PoseWatch\nfeatures an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP)\ntokenization method that enhances the representation of human motion over time,\nwhich is also beneficial for broader human behavior analysis tasks. The\narchitecture's core, a Unified Encoder Twin Decoders (UETD) transformer,\nsignificantly improves the detection of anomalous behaviors in video data.\nExtensive evaluations across multiple benchmark datasets demonstrate that\nPoseWatch consistently outperforms existing methods, establishing a new\nstate-of-the-art in pose-based VAD. This work not only demonstrates the\nefficacy of PoseWatch but also highlights the potential of integrating Natural\nLanguage Processing techniques with computer vision to advance human behavior\nanalysis.\n","authors":["Ghazal Alinezhad Noghre","Armin Danesh Pazho","Hamed Tabkhi"],"pdf_url":"https://arxiv.org/pdf/2408.15185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12040v3","updated":"2024-08-27T16:25:47Z","published":"2024-07-01T17:59:55Z","title":"Comprehensive Performance Evaluation of YOLOv10, YOLOv9 and YOLOv8 on\n  Detecting and Counting Fruitlet in Complex Orchard Environments","summary":"  This study performed an extensive evaluation of the performances of all\nconfigurations of YOLOv8, YOLOv9, and YOLOv10 object detection algorithms for\nfruitlet (of green fruit) detection in commercial orchards. Additionally, this\nresearch performed and validated in-field counting of fruitlets using an iPhone\nand machine vision sensors in 5 different apple varieties (Scifresh, Scilate,\nHoneycrisp, Cosmic crisp & Golden delicious). This comprehensive investigation\nof total 17 different configurations (5 for YOLOv8, 6 for YOLOv9 and 6 for\nYOLOv10) revealed that YOLOv9 outperforms YOLOv10 and YOLOv8 in terms of\nmAP@50, while YOLOv10x outperformed all 17 configurations tested in terms of\nprecision and recall. Specifically, YOLOv9 Gelan-e achieved the highest mAP@50\nof 0.935, outperforming YOLOv10n's 0.921 and YOLOv8s's 0.924. In terms of\nprecision, YOLOv10x achieved the highest precision of 0.908, indicating\nsuperior object identification accuracy compared to other configurations tested\n(e.g. YOLOv9 Gelan-c with a precision of 0.903 and YOLOv8m with 0.897. In terms\nof recall, YOLOv10s achieved the highest in its series (0.872), while YOLOv9\nGelan m performed the best among YOLOv9 configurations (0.899), and YOLOv8n\nperformed the best among the YOLOv8 configurations (0.883). Meanwhile, three\nconfigurations of YOLOv10: YOLOv10b, YOLOv10l, and YOLOv10x achieved superior\npost-processing speeds of 1.5 milliseconds, outperforming all other\nconfigurations within the YOLOv9 and YOLOv8 families. Specifically, YOLOv9\nGelan-e recorded a post-processing speed of 1.9 milliseconds, and YOLOv8m\nachieved 2.1 milliseconds. Furthermore, YOLOv8n exhibited the highest inference\nspeed among all configurations tested, achieving a processing time of 4.1\nmilliseconds while YOLOv9 Gelan-t and YOLOv10n also demonstrated comparatively\nslower inference speeds of 9.3 ms and 5.5 ms, respectively.\n","authors":["Ranjan Sapkota","Zhichao Meng","Martin Churuvija","Xiaoqiang Du","Zenghong Ma","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2407.12040v3.pdf","comment":"14 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.15178v1","updated":"2024-08-27T16:22:18Z","published":"2024-08-27T16:22:18Z","title":"A Review of Transformer-Based Models for Computer Vision Tasks:\n  Capturing Global Context and Spatial Relationships","summary":"  Transformer-based models have transformed the landscape of natural language\nprocessing (NLP) and are increasingly applied to computer vision tasks with\nremarkable success. These models, renowned for their ability to capture\nlong-range dependencies and contextual information, offer a promising\nalternative to traditional convolutional neural networks (CNNs) in computer\nvision. In this review paper, we provide an extensive overview of various\ntransformer architectures adapted for computer vision tasks. We delve into how\nthese models capture global context and spatial relationships in images,\nempowering them to excel in tasks such as image classification, object\ndetection, and segmentation. Analyzing the key components, training\nmethodologies, and performance metrics of transformer-based models, we\nhighlight their strengths, limitations, and recent advancements. Additionally,\nwe discuss potential research directions and applications of transformer-based\nmodels in computer vision, offering insights into their implications for future\nadvancements in the field.\n","authors":["Gracile Astlin Pereira","Muhammad Hussain"],"pdf_url":"https://arxiv.org/pdf/2408.15178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10636v2","updated":"2024-08-27T16:19:22Z","published":"2024-08-20T08:22:29Z","title":"UWF-RI2FA: Generating Multi-frame Ultrawide-field Fluorescein\n  Angiography from Ultrawide-field Retinal Imaging Improves Diabetic\n  Retinopathy Stratification","summary":"  Ultrawide-field fluorescein angiography (UWF-FA) facilitates diabetic\nretinopathy (DR) detection by providing a clear visualization of peripheral\nretinal lesions. However, the intravenous dye injection with potential risks\nhamper its application. We aim to acquire dye-free UWF-FA images from\nnoninvasive UWF retinal imaging (UWF-RI) using generative artificial\nintelligence (GenAI) and evaluate its effectiveness in DR screening. A total of\n18,321 UWF-FA images of different phases were registered with corresponding\nUWF-RI images and fed into a generative adversarial networks (GAN)-based model\nfor training. The quality of generated UWF-FA images was evaluated through\nquantitative metrics and human evaluation. The DeepDRiD dataset was used to\nexternally assess the contribution of generated UWF-FA images to DR\nclassification, using area under the receiver operating characteristic curve\n(AUROC) as outcome metrics. The generated early, mid, and late phase UWF-FA\nimages achieved high authenticity, with multi-scale similarity scores ranging\nfrom 0.70 to 0.91 and qualitative visual scores ranging from 1.64 to 1.98\n(1=real UWF-FA quality). In fifty randomly selected images, 56% to 76% of the\ngenerated images were difficult to distinguish from real images in the Turing\ntest. Moreover, adding these generated UWF-FA images for DR classification\nsignificantly increased the AUROC from 0.869 to 0.904 compared to the baseline\nmodel using UWF-RI images (P < .001). The model successfully generates\nrealistic multi-frame UWF-FA images for enhancing DR stratification without\nintravenous dye injection.\n","authors":["Ruoyu Chen","Kezheng Xu","Kangyan Zheng","Weiyi Zhang","Yan Lu","Danli Shi","Mingguang He"],"pdf_url":"https://arxiv.org/pdf/2408.10636v2.pdf","comment":"22 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.13698v2","updated":"2024-08-27T16:11:44Z","published":"2024-08-25T01:27:35Z","title":"CNN-Transformer Rectified Collaborative Learning for Medical Image\n  Segmentation","summary":"  Automatic and precise medical image segmentation (MIS) is of vital importance\nfor clinical diagnosis and analysis. Current MIS methods mainly rely on the\nconvolutional neural network (CNN) or self-attention mechanism (Transformer)\nfor feature modeling. However, CNN-based methods suffer from the inaccurate\nlocalization owing to the limited global dependency while Transformer-based\nmethods always present the coarse boundary for the lack of local emphasis.\nAlthough some CNN-Transformer hybrid methods are designed to synthesize the\ncomplementary local and global information for better performance, the\ncombination of CNN and Transformer introduces numerous parameters and increases\nthe computation cost. To this end, this paper proposes a CNN-Transformer\nrectified collaborative learning (CTRCL) framework to learn stronger CNN-based\nand Transformer-based models for MIS tasks via the bi-directional knowledge\ntransfer between them. Specifically, we propose a rectified logit-wise\ncollaborative learning (RLCL) strategy which introduces the ground truth to\nadaptively select and rectify the wrong regions in student soft labels for\naccurate knowledge transfer in the logit space. We also propose a class-aware\nfeature-wise collaborative learning (CFCL) strategy to achieve effective\nknowledge transfer between CNN-based and Transformer-based models in the\nfeature space by granting their intermediate features the similar capability of\ncategory perception. Extensive experiments on three popular MIS benchmarks\ndemonstrate that our CTRCL outperforms most state-of-the-art collaborative\nlearning methods under different evaluation metrics.\n","authors":["Lanhu Wu","Miao Zhang","Yongri Piao","Zhenyan Yao","Weibing Sun","Feng Tian","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2408.13698v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15172v1","updated":"2024-08-27T16:10:21Z","published":"2024-08-27T16:10:21Z","title":"X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation","summary":"  Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been\nshown to enhance the effectiveness of enriching item descriptions, thereby\nimproving the accuracy of recommendation systems. However, most existing\napproaches either rely on text-only prompting or employ basic multimodal\nstrategies that do not fully exploit the complementary information available\nfrom both textual and visual modalities. This paper introduces a novel\nframework, Cross-Reflection Prompting, termed X-Reflect, designed to address\nthese limitations by prompting LMMs to explicitly identify and reconcile\nsupportive and conflicting information between text and images. By capturing\nnuanced insights from both modalities, this approach generates more\ncomprehensive and contextually richer item representations. Extensive\nexperiments conducted on two widely used benchmarks demonstrate that our method\noutperforms existing prompting baselines in downstream recommendation accuracy.\nAdditionally, we evaluate the generalizability of our framework across\ndifferent LMM backbones and the robustness of the prompting strategies,\noffering insights for optimization. This work underscores the importance of\nintegrating multimodal information and presents a novel solution for improving\nitem understanding in multimodal recommendation systems.\n","authors":["Hanjia Lyu","Ryan Rossi","Xiang Chen","Md Mehrab Tanjim","Stefano Petrangeli","Somdeb Sarkhel","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2408.15172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13993v3","updated":"2024-08-27T15:56:33Z","published":"2024-04-22T08:59:35Z","title":"Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion","summary":"  Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.\n","authors":["Yingxuan Li","Ryota Hinami","Kiyoharu Aizawa","Yusuke Matsui"],"pdf_url":"https://arxiv.org/pdf/2404.13993v3.pdf","comment":"Accepted to ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.15159v1","updated":"2024-08-27T15:55:18Z","published":"2024-08-27T15:55:18Z","title":"Empowering Sign Language Communication: Integrating Sentiment and\n  Semantics for Facial Expression Synthesis","summary":"  Translating written sentences from oral languages to a sequence of manual and\nnon-manual gestures plays a crucial role in building a more inclusive society\nfor deaf and hard-of-hearing people. Facial expressions (non-manual), in\nparticular, are responsible for encoding the grammar of the sentence to be\nspoken, applying punctuation, pronouns, or emphasizing signs. These non-manual\ngestures are closely related to the semantics of the sentence being spoken and\nalso to the utterance of the speaker's emotions. However, most Sign Language\nProduction (SLP) approaches are centered on synthesizing manual gestures and do\nnot focus on modeling the speakers expression. This paper introduces a new\nmethod focused in synthesizing facial expressions for sign language. Our goal\nis to improve sign language production by integrating sentiment information in\nfacial expression generation. The approach leverages a sentence sentiment and\nsemantic features to sample from a meaningful representation space, integrating\nthe bias of the non-manual components into the sign language production\nprocess. To evaluate our method, we extend the Frechet Gesture Distance (FGD)\nand propose a new metric called Frechet Expression Distance (FED) and apply an\nextensive set of metrics to assess the quality of specific regions of the face.\nThe experimental results showed that our method achieved state of the art,\nbeing superior to the competitors on How2Sign and PHOENIX14T datasets.\nMoreover, our architecture is based on a carefully designed graph pyramid that\nmakes it simpler, easier to train, and capable of leveraging emotions to\nproduce facial expressions.\n","authors":["Rafael Azevedo","Thiago Coutinho","JoÃ£o Ferreira","Thiago Gomes","Erickson Nascimento"],"pdf_url":"https://arxiv.org/pdf/2408.15159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15143v1","updated":"2024-08-27T15:31:45Z","published":"2024-08-27T15:31:45Z","title":"A Preliminary Exploration Towards General Image Restoration","summary":"  Despite the tremendous success of deep models in various individual image\nrestoration tasks, there are at least two major technical challenges preventing\nthese works from being applied to real-world usages: (1) the lack of\ngeneralization ability and (2) the complex and unknown degradations in\nreal-world scenarios. Existing deep models, tailored for specific individual\nimage restoration tasks, often fall short in effectively addressing these\nchallenges. In this paper, we present a new problem called general image\nrestoration (GIR) which aims to address these challenges within a unified\nmodel. GIR covers most individual image restoration tasks (\\eg, image\ndenoising, deblurring, deraining and super-resolution) and their combinations\nfor general purposes. This paper proceeds to delineate the essential aspects of\nGIR, including problem definition and the overarching significance of\ngeneralization performance. Moreover, the establishment of new datasets and a\nthorough evaluation framework for GIR models is discussed. We conduct a\ncomprehensive evaluation of existing approaches for tackling the GIR challenge,\nilluminating their strengths and pragmatic challenges. By analyzing these\napproaches, we not only underscore the effectiveness of GIR but also highlight\nthe difficulties in its practical implementation. At last, we also try to\nunderstand and interpret these models' behaviors to inspire the future\ndirection. Our work can open up new valuable research directions and contribute\nto the research of general vision.\n","authors":["Xiangtao Kong","Jinjin Gu","Yihao Liu","Wenlong Zhang","Xiangyu Chen","Yu Qiao","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.15143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13896v2","updated":"2024-08-27T15:13:01Z","published":"2024-08-25T17:33:40Z","title":"RT-Attack: Jailbreaking Text-to-Image Models via Random Token","summary":"  Recently, Text-to-Image(T2I) models have achieved remarkable success in image\ngeneration and editing, yet these models still have many potential issues,\nparticularly in generating inappropriate or Not-Safe-For-Work(NSFW) content.\nStrengthening attacks and uncovering such vulnerabilities can advance the\ndevelopment of reliable and practical T2I models. Most of the previous works\ntreat T2I models as white-box systems, using gradient optimization to generate\nadversarial prompts. However, accessing the model's gradient is often\nimpossible in real-world scenarios. Moreover, existing defense methods, those\nusing gradient masking, are designed to prevent attackers from obtaining\naccurate gradient information. While some black-box jailbreak attacks have been\nexplored, these typically rely on simply replacing sensitive words, leading to\nsuboptimal attack performance. To address this issue, we introduce a two-stage\nquery-based black-box attack method utilizing random search. In the first\nstage, we establish a preliminary prompt by maximizing the semantic similarity\nbetween the adversarial and target harmful prompts. In the second stage, we use\nthis initial prompt to refine our approach, creating a detailed adversarial\nprompt aimed at jailbreaking and maximizing the similarity in image features\nbetween the images generated from this prompt and those produced by the target\nharmful prompt. Extensive experiments validate the effectiveness of our method\nin attacking the latest prompt checkers, post-hoc image checkers, securely\ntrained T2I models, and online commercial models.\n","authors":["Sensen Gao","Xiaojun Jia","Yihao Huang","Ranjie Duan","Jindong Gu","Yang Liu","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2408.13896v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15127v1","updated":"2024-08-27T15:07:58Z","published":"2024-08-27T15:07:58Z","title":"T-FAKE: Synthesizing Thermal Images for Facial Landmarking","summary":"  Facial analysis is a key component in a wide range of applications such as\nsecurity, autonomous driving, entertainment, and healthcare. Despite the\navailability of various facial RGB datasets, the thermal modality, which plays\na crucial role in life sciences, medicine, and biometrics, has been largely\noverlooked. To address this gap, we introduce the T-FAKE dataset, a new\nlarge-scale synthetic thermal dataset with sparse and dense landmarks. To\nfacilitate the creation of the dataset, we propose a novel RGB2Thermal loss\nfunction, which enables the transfer of thermal style to RGB faces. By\nutilizing the Wasserstein distance between thermal and RGB patches and the\nstatistical analysis of clinical temperature distributions on faces, we ensure\nthat the generated thermal images closely resemble real samples. Using\nRGB2Thermal style transfer based on our RGB2Thermal loss function, we create\nthe T-FAKE dataset, a large-scale synthetic thermal dataset of faces.\nLeveraging our novel T-FAKE dataset, probabilistic landmark prediction, and\nlabel adaptation networks, we demonstrate significant improvements in landmark\ndetection methods on thermal images across different landmark conventions. Our\nmodels show excellent performance with both sparse 70-point landmarks and dense\n478-point landmark annotations. Our code and models are available at\nhttps://github.com/phflot/tfake.\n","authors":["Philipp Flotho","Moritz Piening","Anna Kukleva","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2408.15127v1.pdf","comment":"22 pages, 12 figures, Philipp Flotho and Moritz Piening share equal\n  contribution"},{"id":"http://arxiv.org/abs/2408.15122v1","updated":"2024-08-27T15:03:20Z","published":"2024-08-27T15:03:20Z","title":"Machine Learning for Methane Detection and Quantification from Space --\n  A survey","summary":"  Methane (CH_4) is a potent anthropogenic greenhouse gas, contributing 86\ntimes more to global warming than Carbon Dioxide (CO_2) over 20 years, and it\nalso acts as an air pollutant. Given its high radiative forcing potential and\nrelatively short atmospheric lifetime (9\\textpm1 years), methane has important\nimplications for climate change, therefore, cutting methane emissions is\ncrucial for effective climate change mitigation. This work expands existing\ninformation on operational methane point source detection sensors in the\nShort-Wave Infrared (SWIR) bands. It reviews the state-of-the-art for\ntraditional as well as Machine Learning (ML) approaches. The architecture and\ndata used in such ML models will be discussed separately for methane plume\nsegmentation and emission rate estimation. Traditionally, experts rely on\nlabor-intensive manually adjusted methods for methane detection. However, ML\napproaches offer greater scalability. Our analysis reveals that ML models\noutperform traditional methods, particularly those based on convolutional\nneural networks (CNN), which are based on the U-net and transformer\narchitectures. These ML models extract valuable information from\nmethane-sensitive spectral data, enabling a more accurate detection. Challenges\narise when comparing these methods due to variations in data, sensor\nspecifications, and evaluation metrics. To address this, we discuss existing\ndatasets and metrics, providing an overview of available resources and\nidentifying open research problems. Finally, we explore potential future\nadvances in ML, emphasizing approaches for model comparability, large dataset\ncreation, and the European Union's forthcoming methane strategy.\n","authors":["Enno Tiemann","Shanyu Zhou","Alexander KlÃ¤ser","Konrad Heidler","Rochelle Schneider","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.15122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05892v3","updated":"2024-08-27T15:00:53Z","published":"2024-08-12T02:10:18Z","title":"Polyp SAM 2: Advancing Zero shot Polyp Segmentation in Colorectal Cancer\n  Detection","summary":"  Polyp segmentation plays a crucial role in the early detection and diagnosis\nof colorectal cancer. However, obtaining accurate segmentations often requires\nlabor-intensive annotations and specialized models. Recently, Meta AI Research\nreleased a general Segment Anything Model 2 (SAM 2), which has demonstrated\npromising performance in several segmentation tasks. In this manuscript, we\nevaluate the performance of SAM 2 in segmenting polyps under various prompted\nsettings. We hope this report will provide insights to advance the field of\npolyp segmentation and promote more interesting work in the future. This\nproject is publicly available at https://github.com/ sajjad-sh33/Polyp-SAM-2.\n","authors":["Mobina Mansoori","Sajjad Shahabodini","Jamshid Abouei","Konstantinos N. Plataniotis","Arash Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2408.05892v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15119v1","updated":"2024-08-27T14:58:13Z","published":"2024-08-27T14:58:13Z","title":"Urdu Digital Text Word Optical Character Recognition Using Permuted Auto\n  Regressive Sequence Modeling","summary":"  This research paper introduces an innovative word-level Optical Character\nRecognition (OCR) model specifically designed for digital Urdu text\nrecognition. Utilizing transformer-based architectures and attention\nmechanisms, the model was trained on a comprehensive dataset of approximately\n160,000 Urdu text images, achieving a character error rate (CER) of 0.178,\nwhich highlights its superior accuracy in recognizing Urdu characters. The\nmodel's strength lies in its unique architecture, incorporating the permuted\nautoregressive sequence (PARSeq) model, which allows for context-aware\ninference and iterative refinement by leveraging bidirectional context\ninformation to enhance recognition accuracy. Furthermore, its capability to\nhandle a diverse range of Urdu text styles, fonts, and variations enhances its\napplicability in real-world scenarios. Despite its promising results, the model\nhas some limitations, such as difficulty with blurred images, non-horizontal\norientations, and overlays of patterns, lines, or other text, which can\noccasionally lead to suboptimal performance. Additionally, trailing or\nfollowing punctuation marks can introduce noise into the recognition process.\nAddressing these challenges will be a focus of future research, aiming to\nrefine the model further, explore data augmentation techniques, optimize\nhyperparameters, and integrate contextual improvements for more accurate and\nefficient Urdu text recognition.\n","authors":["Ahmed Mustafa","Ijlal Baig","Hasan Sajid"],"pdf_url":"https://arxiv.org/pdf/2408.15119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15118v1","updated":"2024-08-27T14:58:08Z","published":"2024-08-27T14:58:08Z","title":"DIFR3CT: Latent Diffusion for Probabilistic 3D CT Reconstruction from\n  Few Planar X-Rays","summary":"  Computed Tomography (CT) scans are the standard-of-care for the visualization\nand diagnosis of many clinical ailments, and are needed for the treatment\nplanning of external beam radiotherapy. Unfortunately, the availability of CT\nscanners in low- and mid-resource settings is highly variable. Planar x-ray\nradiography units, in comparison, are far more prevalent, but can only provide\nlimited 2D observations of the 3D anatomy. In this work we propose DIFR3CT, a\n3D latent diffusion model, that can generate a distribution of plausible CT\nvolumes from one or few (<10) planar x-ray observations. DIFR3CT works by\nfusing 2D features from each x-ray into a joint 3D space, and performing\ndiffusion conditioned on these fused features in a low-dimensional latent\nspace. We conduct extensive experiments demonstrating that DIFR3CT is better\nthan recent sparse CT reconstruction baselines in terms of standard pixel-level\n(PSNR, SSIM) on both the public LIDC and in-house post-mastectomy CT datasets.\nWe also show that DIFR3CT supports uncertainty quantification via Monte Carlo\nsampling, which provides an opportunity to measure reconstruction reliability.\nFinally, we perform a preliminary pilot study evaluating DIFR3CT for automated\nbreast radiotherapy contouring and planning -- and demonstrate promising\nfeasibility. Our code is available at https://github.com/yransun/DIFR3CT.\n","authors":["Yiran Sun","Hana Baroudi","Tucker Netherton","Laurence Court","Osama Mawlawi","Ashok Veeraraghavan","Guha Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2408.15118v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.15114v1","updated":"2024-08-27T14:54:33Z","published":"2024-08-27T14:54:33Z","title":"Few-Shot Unsupervised Implicit Neural Shape Representation Learning with\n  Spatial Adversaries","summary":"  Implicit Neural Representations have gained prominence as a powerful\nframework for capturing complex data modalities, encompassing a wide range from\n3D shapes to images and audio. Within the realm of 3D shape representation,\nNeural Signed Distance Functions (SDF) have demonstrated remarkable potential\nin faithfully encoding intricate shape geometry. However, learning SDFs from\nsparse 3D point clouds in the absence of ground truth supervision remains a\nvery challenging task. While recent methods rely on smoothness priors to\nregularize the learning, our method introduces a regularization term that\nleverages adversarial samples around the shape to improve the learned SDFs.\nThrough extensive experiments and evaluations, we illustrate the efficacy of\nour proposed method, highlighting its capacity to improve SDF learning with\nrespect to baselines and the state-of-the-art using synthetic and real data.\n","authors":["Amine Ouasfi","Adnane Boukhayma"],"pdf_url":"https://arxiv.org/pdf/2408.15114v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.15113v1","updated":"2024-08-27T14:51:34Z","published":"2024-08-27T14:51:34Z","title":"AnomalousPatchCore: Exploring the Use of Anomalous Samples in Industrial\n  Anomaly Detection","summary":"  Visual inspection, or industrial anomaly detection, is one of the most common\nquality control types in manufacturing. The task is to identify the presence of\nan anomaly given an image, e.g., a missing component on an image of a circuit\nboard, for subsequent manual inspection. While industrial anomaly detection has\nseen a surge in recent years, most anomaly detection methods still utilize\nknowledge only from normal samples, failing to leverage the information from\nthe frequently available anomalous samples. Additionally, they heavily rely on\nvery general feature extractors pre-trained on common image classification\ndatasets. In this paper, we address these shortcomings and propose the new\nanomaly detection system AnomalousPatchCore~(APC) based on a feature extractor\nfine-tuned with normal and anomalous in-domain samples and a subsequent memory\nbank for identifying unusual features. To fine-tune the feature extractor in\nAPC, we propose three auxiliary tasks that address the different aspects of\nanomaly detection~(classification vs. localization) and mitigate the effect of\nthe imbalance between normal and anomalous samples. Our extensive evaluation on\nthe MVTec dataset shows that APC outperforms state-of-the-art systems in\ndetecting anomalies, which is especially important in industrial anomaly\ndetection given the subsequent manual inspection. In detailed ablation studies,\nwe further investigate the properties of our APC.\n","authors":["Mykhailo Koshil","Tilman Wegener","Detlef Mentrup","Simone Frintrop","Christian Wilms"],"pdf_url":"https://arxiv.org/pdf/2408.15113v1.pdf","comment":"Accepted at the 2nd workshop on Vision-based InduStrial InspectiON\n  (VISION) @ ECCV"},{"id":"http://arxiv.org/abs/2408.15103v1","updated":"2024-08-27T14:40:19Z","published":"2024-08-27T14:40:19Z","title":"Enhancing License Plate Super-Resolution: A Layout-Aware and\n  Character-Driven Approach","summary":"  Despite significant advancements in License Plate Recognition (LPR) through\ndeep learning, most improvements rely on high-resolution images with clear\ncharacters. This scenario does not reflect real-world conditions where traffic\nsurveillance often captures low-resolution and blurry images. Under these\nconditions, characters tend to blend with the background or neighboring\ncharacters, making accurate LPR challenging. To address this issue, we\nintroduce a novel loss function, Layout and Character Oriented Focal Loss\n(LCOFL), which considers factors such as resolution, texture, and structural\ndetails, as well as the performance of the LPR task itself. We enhance\ncharacter feature learning using deformable convolutions and shared weights in\nan attention module and employ a GAN-based training approach with an Optical\nCharacter Recognition (OCR) model as the discriminator to guide the\nsuper-resolution process. Our experimental results show significant\nimprovements in character reconstruction quality, outperforming two\nstate-of-the-art methods in both quantitative and qualitative measures. Our\ncode is publicly available at https://github.com/valfride/lpsr-lacd\n","authors":["Valfride Nascimento","Rayson Laroca","Rafael O. Ribeiro","William Robson Schwartz","David Menotti"],"pdf_url":"https://arxiv.org/pdf/2408.15103v1.pdf","comment":"Accepted for presentation at the Conference on Graphics, Patterns and\n  Images (SIBGRAPI) 2024"},{"id":"http://arxiv.org/abs/2408.15101v1","updated":"2024-08-27T14:36:46Z","published":"2024-08-27T14:36:46Z","title":"MTMamba++: Enhancing Multi-Task Dense Scene Understanding via\n  Mamba-Based Decoders","summary":"  Multi-task dense scene understanding, which trains a model for multiple dense\nprediction tasks, has a wide range of application scenarios. Capturing\nlong-range dependency and enhancing cross-task interactions are crucial to\nmulti-task dense prediction. In this paper, we propose MTMamba++, a novel\narchitecture for multi-task scene understanding featuring with a Mamba-based\ndecoder. It contains two types of core blocks: self-task Mamba (STM) block and\ncross-task Mamba (CTM) block. STM handles long-range dependency by leveraging\nstate-space models, while CTM explicitly models task interactions to facilitate\ninformation exchange across tasks. We design two types of CTM block, namely\nF-CTM and S-CTM, to enhance cross-task interaction from feature and semantic\nperspectives, respectively. Experiments on NYUDv2, PASCAL-Context, and\nCityscapes datasets demonstrate the superior performance of MTMamba++ over\nCNN-based and Transformer-based methods. The code is available at\nhttps://github.com/EnVision-Research/MTMamba.\n","authors":["Baijiong Lin","Weisen Jiang","Pengguang Chen","Shu Liu","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.15101v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.02228"},{"id":"http://arxiv.org/abs/2408.15098v1","updated":"2024-08-27T14:30:36Z","published":"2024-08-27T14:30:36Z","title":"CLIP-AGIQA: Boosting the Performance of AI-Generated Image Quality\n  Assessment with CLIP","summary":"  With the rapid development of generative technologies, AI-Generated Images\n(AIGIs) have been widely applied in various aspects of daily life. However, due\nto the immaturity of the technology, the quality of the generated images\nvaries, so it is important to develop quality assessment techniques for the\ngenerated images. Although some models have been proposed to assess the quality\nof generated images, they are inadequate when faced with the ever-increasing\nand diverse categories of generated images. Consequently, the development of\nmore advanced and effective models for evaluating the quality of generated\nimages is urgently needed. Recent research has explored the significant\npotential of the visual language model CLIP in image quality assessment,\nfinding that it performs well in evaluating the quality of natural images.\nHowever, its application to generated images has not been thoroughly\ninvestigated. In this paper, we build on this idea and further explore the\npotential of CLIP in evaluating the quality of generated images. We design\nCLIP-AGIQA, a CLIP-based regression model for quality assessment of generated\nimages, leveraging rich visual and textual knowledge encapsulated in CLIP.\nParticularly, we implement multi-category learnable prompts to fully utilize\nthe textual knowledge in CLIP for quality assessment. Extensive experiments on\nseveral generated image quality assessment benchmarks, including AGIQA-3K and\nAIGCIQA2023, demonstrate that CLIP-AGIQA outperforms existing IQA models,\nachieving excellent results in evaluating the quality of generated images.\n","authors":["Zhenchen Tang","Zichuan Wang","Bo Peng","Jing Dong"],"pdf_url":"https://arxiv.org/pdf/2408.15098v1.pdf","comment":"accepted by ICPR2024"},{"id":"http://arxiv.org/abs/2408.15094v1","updated":"2024-08-27T14:25:42Z","published":"2024-08-27T14:25:42Z","title":"Constrained Diffusion Models via Dual Training","summary":"  Diffusion models have attained prominence for their ability to synthesize a\nprobability distribution for a given dataset via a diffusion process, enabling\nthe generation of new data points with high fidelity. However, diffusion\nprocesses are prone to generating biased data based on the training dataset. To\naddress this issue, we develop constrained diffusion models by imposing\ndiffusion constraints based on desired distributions that are informed by\nrequirements. Specifically, we cast the training of diffusion models under\nrequirements as a constrained distribution optimization problem that aims to\nreduce the distribution difference between original and generated data while\nobeying constraints on the distribution of generated data. We show that our\nconstrained diffusion models generate new data from a mixture data distribution\nthat achieves the optimal trade-off among objective and constraints. To train\nconstrained diffusion models, we develop a dual training algorithm and\ncharacterize the optimality of the trained constrained diffusion model. We\nempirically demonstrate the effectiveness of our constrained models in two\nconstrained generation tasks: (i) we consider a dataset with one or more\nunderrepresented classes where we train the model with constraints to ensure\nfairly sampling from all classes during inference; (ii) we fine-tune a\npre-trained diffusion model to sample from a new dataset while avoiding\noverfitting.\n","authors":["Shervin Khalafi","Dongsheng Ding","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2408.15094v1.pdf","comment":"41 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2307.10895v4","updated":"2024-08-27T14:23:51Z","published":"2023-07-20T14:18:44Z","title":"Variational Autoencoding of Dental Point Clouds","summary":"  Digital dentistry has made significant advancements, yet numerous challenges\nremain. This paper introduces the FDI 16 dataset, an extensive collection of\ntooth meshes and point clouds. Additionally, we present a novel approach:\nVariational FoldingNet (VF-Net), a fully probabilistic variational autoencoder\nfor point clouds. Notably, prior latent variable models for point clouds lack a\none-to-one correspondence between input and output points. Instead, they rely\non optimizing Chamfer distances, a metric that lacks a normalized\ndistributional counterpart, rendering it unsuitable for probabilistic modeling.\nWe replace the explicit minimization of Chamfer distances with a suitable\nencoder, increasing computational efficiency while simplifying the\nprobabilistic extension. This allows for straightforward application in various\ntasks, including mesh generation, shape completion, and representation\nlearning. Empirically, we provide evidence of lower reconstruction error in\ndental reconstruction and interpolation, showcasing state-of-the-art\nperformance in dental sample generation while identifying valuable latent\nrepresentations\n","authors":["Johan Ziruo Ye","Thomas Ãrkild","Peter Lempel SÃ¸ndergaard","SÃ¸ren Hauberg"],"pdf_url":"https://arxiv.org/pdf/2307.10895v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.01870v2","updated":"2024-08-27T14:16:06Z","published":"2022-09-05T10:06:03Z","title":"Unsupervised Domain Adaptation via Style-Aware Self-intermediate Domain","summary":"  Unsupervised domain adaptation (UDA) has attracted considerable attention,\nwhich transfers knowledge from a label-rich source domain to a related but\nunlabeled target domain. Reducing inter-domain differences has always been a\ncrucial factor to improve performance in UDA, especially for tasks where there\nis a large gap between source and target domains. To this end, we propose a\nnovel style-aware feature fusion method (SAFF) to bridge the large domain gap\nand transfer knowledge while alleviating the loss of class-discriminative\ninformation. Inspired by the human transitive inference and learning ability, a\nnovel style-aware self-intermediate domain (SSID) is investigated to link two\nseemingly unrelated concepts through a series of intermediate auxiliary\nsynthesized concepts. Specifically, we propose a novel learning strategy of\nSSID, which selects samples from both source and target domains as anchors, and\nthen randomly fuses the object and style features of these anchors to generate\nlabeled and style-rich intermediate auxiliary features for knowledge transfer.\nMoreover, we design an external memory bank to store and update specified\nlabeled features to obtain stable class features and class-wise style features.\nBased on the proposed memory bank, the intra- and inter-domain loss functions\nare designed to improve the class recognition ability and feature\ncompatibility, respectively. Meanwhile, we simulate the rich latent feature\nspace of SSID by infinite sampling and the convergence of the loss function by\nmathematical theory. Finally, we conduct comprehensive experiments on commonly\nused domain adaptive benchmarks to evaluate the proposed SAFF, and the\nexperimental results show that the proposed SAFF can be easily combined with\ndifferent backbone networks and obtain better performance as a plug-in-plug-out\nmodule.\n","authors":["Lianyu Wang","Meng Wang","Daoqiang Zhang","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2209.01870v2.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.13627v2","updated":"2024-08-27T14:14:51Z","published":"2024-08-24T16:48:25Z","title":"Recent Event Camera Innovations: A Survey","summary":"  Event-based vision, inspired by the human visual system, offers\ntransformative capabilities such as low latency, high dynamic range, and\nreduced power consumption. This paper presents a comprehensive survey of event\ncameras, tracing their evolution over time. It introduces the fundamental\nprinciples of event cameras, compares them with traditional frame cameras, and\nhighlights their unique characteristics and operational differences. The survey\ncovers various event camera models from leading manufacturers, key\ntechnological milestones, and influential research contributions. It explores\ndiverse application areas across different domains and discusses essential\nreal-world and synthetic datasets for research advancement. Additionally, the\nrole of event camera simulators in testing and development is discussed. This\nsurvey aims to consolidate the current state of event cameras and inspire\nfurther innovation in this rapidly evolving field. To support the research\ncommunity, a GitHub page\n(https://github.com/chakravarthi589/Event-based-Vision_Resources) categorizes\npast and future research articles and consolidates valuable resources.\n","authors":["Bharatesh Chakravarthi","Aayush Atul Verma","Kostas Daniilidis","Cornelia Fermuller","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2408.13627v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15077v1","updated":"2024-08-27T14:05:48Z","published":"2024-08-27T14:05:48Z","title":"MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of\n  Children with Autism Spectrum Disorder","summary":"  Autism spectrum disorder (ASD) is characterized by significant challenges in\nsocial interaction and comprehending communication signals. Recently,\ntherapeutic interventions for ASD have increasingly utilized Deep learning\npowered-computer vision techniques to monitor individual progress over time.\nThese models are trained on private, non-public datasets from the autism\ncommunity, creating challenges in comparing results across different models due\nto privacy-preserving data-sharing issues. This work introduces MMASD+. MMASD+\nconsists of diverse data modalities, including 3D-Skeleton, 3D Body Mesh, and\nOptical Flow data. It integrates the capabilities of Yolov8 and Deep SORT\nalgorithms to distinguish between the therapist and children, addressing a\nsignificant barrier in the original dataset. Additionally, a Multimodal\nTransformer framework is proposed to predict 11 action types and the presence\nof ASD. This framework achieves an accuracy of 95.03% for predicting action\ntypes and 96.42% for predicting ASD presence, demonstrating over a 10%\nimprovement compared to models trained on single data modalities. These\nfindings highlight the advantages of integrating multiple data modalities\nwithin the Multimodal Transformer framework.\n","authors":["Pavan Uttej Ravva","Behdokht Kiafar","Pinar Kullu","Jicheng Li","Anjana Bhat","Roghayeh Leila Barmaki"],"pdf_url":"https://arxiv.org/pdf/2408.15077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15069v1","updated":"2024-08-27T13:56:48Z","published":"2024-08-27T13:56:48Z","title":"Geometric Artifact Correction for Symmetric Multi-Linear Trajectory CT:\n  Theory, Method, and Generalization","summary":"  For extending CT field-of-view to perform non-destructive testing, the\nSymmetric Multi-Linear trajectory Computed Tomography (SMLCT) has been\ndeveloped as a successful example of non-standard CT scanning modes. However,\ninevitable geometric errors can cause severe artifacts in the reconstructed\nimages. The existing calibration method for SMLCT is both crude and\ninefficient. It involves reconstructing hundreds of images by exhaustively\nsubstituting each potential error, and then manually identifying the images\nwith the fewest geometric artifacts to estimate the final geometric errors for\ncalibration. In this paper, we comprehensively and efficiently address the\nchallenging geometric artifacts in SMLCT, , and the corresponding works mainly\ninvolve theory, method, and generalization. In particular, after identifying\nsensitive parameters and conducting some theory analysis of geometric\nartifacts, we summarize several key properties between sensitive geometric\nparameters and artifact characteristics. Then, we further construct\nmathematical relationships that relate sensitive geometric errors to the pixel\noffsets of reconstruction images with artifact characteristics. To accurately\nextract pixel bias, we innovatively adapt the Generalized Cross-Correlation\nwith Phase Transform (GCC-PHAT) algorithm, commonly used in sound processing,\nfor our image registration task for each paired symmetric LCT. This adaptation\nleads to the design of a highly efficient rigid translation registration\nmethod. Simulation and physical experiments have validated the excellent\nperformance of this work. Additionally, our results demonstrate significant\ngeneralization to common rotated CT and a variant of SMLCT.\n","authors":["Zhisheng Wang","Yanxu Sun","Shangyu Li","Legeng Lin","Shunli Wang","Junning Cui"],"pdf_url":"https://arxiv.org/pdf/2408.15069v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2312.11470v2","updated":"2024-08-27T13:55:17Z","published":"2023-11-14T11:36:20Z","title":"An Improved Anomaly Detection Model for Automated Inspection of Power\n  Line Insulators","summary":"  Inspection of insulators is important to ensure reliable operation of the\npower system. Deep learning is being increasingly exploited to automate the\ninspection process by leveraging object detection models to analyse aerial\nimages captured by drones. A purely object detection-based approach, however,\nsuffers from class imbalance-induced poor performance, which can be accentuated\nfor infrequent and hard-to-detect incipient faults. This article proposes the\nuse of anomaly detection along with object detection in a two-stage approach\nfor incipient fault detection in a data-efficient manner. An explainable\nconvolutional one-class classifier is adopted for anomaly detection. The\none-class formulation reduces the reliance on plentifully available images of\nfaulty insulators, while the explainability of the model is expected to promote\nadoption by the industry. A modified loss function is developed that addresses\ncomputational and interpretability issues with the existing model, also\nallowing for the integration of other losses. The superiority of the novel loss\nfunction is demonstrated with MVTec-AD dataset. The models are trained for\ninsulator inspection with two datasets -- representing data-abundant and\ndata-scarce scenarios -- in unsupervised and semi-supervised settings. The\nresults suggest that including as few as five real anomalies in the training\ndataset significantly improves the model's performance and enables reliable\ndetection of rarely occurring incipient faults in insulators.\n","authors":["Laya Das","Blazhe Gjorgiev","Giovanni Sansavini"],"pdf_url":"https://arxiv.org/pdf/2312.11470v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15063v1","updated":"2024-08-27T13:47:31Z","published":"2024-08-27T13:47:31Z","title":"Adapting Segment Anything Model to Multi-modal Salient Object Detection\n  with Semantic Feature Fusion Guidance","summary":"  Although most existing multi-modal salient object detection (SOD) methods\ndemonstrate effectiveness through training models from scratch, the limited\nmulti-modal data hinders these methods from reaching optimality. In this paper,\nwe propose a novel framework to explore and exploit the powerful feature\nrepresentation and zero-shot generalization ability of the pre-trained Segment\nAnything Model (SAM) for multi-modal SOD. Despite serving as a recent vision\nfundamental model, driving the class-agnostic SAM to comprehend and detect\nsalient objects accurately is non-trivial, especially in challenging scenes. To\nthis end, we develop \\underline{SAM} with se\\underline{m}antic\nf\\underline{e}ature fu\\underline{s}ion guidanc\\underline{e} (Sammese), which\nincorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to\nmulti-modal SOD tasks. However, it is difficult for SAM trained on single-modal\ndata to directly mine the complementary benefits of multi-modal inputs and\ncomprehensively utilize them to achieve accurate saliency prediction.To address\nthese issues, we first design a multi-modal complementary fusion module to\nextract robust multi-modal semantic features by integrating information from\nvisible and thermal or depth image pairs. Then, we feed the extracted\nmulti-modal semantic features into both the SAM image encoder and mask decoder\nfor fine-tuning and prompting, respectively. Specifically, in the image\nencoder, a multi-modal adapter is proposed to adapt the single-modal SAM to\nmulti-modal information. In the mask decoder, a semantic-geometric prompt\ngeneration strategy is proposed to produce corresponding embeddings with\nvarious saliency cues. Extensive experiments on both RGB-D and RGB-T SOD\nbenchmarks show the effectiveness of the proposed framework.\n","authors":["Kunpeng Wang","Keke Chen","Chenglong Li","Zhengzheng Tu","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2408.15063v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.04833v3","updated":"2024-08-27T13:45:49Z","published":"2024-07-05T19:38:10Z","title":"3D Adaptive Structural Convolution Network for Domain-Invariant Point\n  Cloud Recognition","summary":"  Adapting deep learning networks for point cloud data recognition in\nself-driving vehicles faces challenges due to the variability in datasets and\nsensor technologies, emphasizing the need for adaptive techniques to maintain\naccuracy across different conditions. In this paper, we introduce the 3D\nAdaptive Structural Convolution Network (3D-ASCN), a cutting-edge framework for\n3D point cloud recognition. It combines 3D convolution kernels, a structural\ntree structure, and adaptive neighborhood sampling for effective geometric\nfeature extraction. This method obtains domain-invariant features and\ndemonstrates robust, adaptable performance on a variety of point cloud\ndatasets, ensuring compatibility across diverse sensor configurations without\nthe need for parameter adjustments. This highlights its potential to\nsignificantly enhance the reliability and efficiency of self-driving vehicle\ntechnology.\n","authors":["Younggun Kim","Beomsik Cho","Seonghoon Ryoo","Soomok Lee"],"pdf_url":"https://arxiv.org/pdf/2407.04833v3.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2209.11200v3","updated":"2024-08-27T13:44:11Z","published":"2022-09-22T17:42:44Z","title":"Attention is All They Need: Exploring the Media Archaeology of the\n  Computer Vision Research Paper","summary":"  Research papers, in addition to textual documents, are a designed interface\nthrough which researchers communicate. Recently, rapid growth has transformed\nthat interface in many fields of computing. In this work, we examine the\neffects of this growth from a media archaeology perspective, through the\nchanges to figures and tables in research papers. Specifically, we study these\nchanges in computer vision over the past decade, as the deep learning\nrevolution has driven unprecedented growth in the discipline. We ground our\ninvestigation through interviews with veteran researchers spanning computer\nvision, graphics, and visualization. Our analysis focuses on the research\nattention economy: how research paper elements contribute towards advertising,\nmeasuring, and disseminating an increasingly commodified \"contribution.\"\nThrough this work, we seek to motivate future discussion surrounding the design\nof both the research paper itself as well as the larger sociotechnical research\npublishing system, including tools for finding, reading, and writing research\npapers.\n","authors":["Samuel Goree","Gabriel Appleby","David Crandall","Norman Su"],"pdf_url":"https://arxiv.org/pdf/2209.11200v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14745v3","updated":"2024-08-27T13:36:12Z","published":"2024-04-23T04:54:32Z","title":"TAAT: Think and Act from Arbitrary Texts in Text2Motion","summary":"  Text to Motion aims to generate human motions from texts. Existing settings\nassume that texts include action labels, which limits flexibility in practical\nscenarios. This paper extends this task with a more realistic assumption that\nthe texts are arbitrary. Specifically, in our setting, arbitrary texts include\nexisting action texts composed of action labels and introduce scene texts\nwithout explicit action labels. To address this practical issue, we extend the\naction texts in the HUMANML3D dataset by incorporating additional scene texts,\nthereby creating a new dataset, HUMANML3D++. Concurrently, we propose a simple\nframework that extracts action representations from arbitrary texts using a\nLarge Language Model (LLM) and subsequently generates motions. Furthermore, we\nenhance the existing evaluation methodologies to address their inadequacies.\nExtensive experiments are conducted under different application scenarios to\nvalidate the effectiveness of the proposed framework on existing and proposed\ndatasets. The results indicate that Text to Motion in this realistic setting is\nvery challenging, fostering new research in this practical direction. Our\ndataset and code will be released.\n","authors":["Runqi Wang","Caoyuan Ma","Guopeng Li","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.14745v3.pdf","comment":"Updated errors in author information"},{"id":"http://arxiv.org/abs/2408.15045v1","updated":"2024-08-27T13:13:38Z","published":"2024-08-27T13:13:38Z","title":"DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding","summary":"  Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors.\n","authors":["Wenhui Liao","Jiapeng Wang","Hongliang Li","Chengyu Wang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2408.15045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15038v1","updated":"2024-08-27T13:07:09Z","published":"2024-08-27T13:07:09Z","title":"Interactive Occlusion Boundary Estimation through Exploitation of\n  Synthetic Data","summary":"  Occlusion boundaries (OBs) geometrically localize the occlusion events in a\n2D image, and contain useful information for addressing various scene\nunderstanding problems. To advance their study, we have led the investigation\nin the following three aspects. Firstly, we have studied interactive estimation\nof OBs, which is the first in the literature, and proposed an efficient\ndeep-network-based method using multiple-scribble intervention, named DNMMSI,\nwhich significantly improves the performance over the state-of-the-art\nfully-automatic methods. Secondly, we propose to exploit the synthetic\nbenchmark for the training process, thanks to the particularity that OBs are\ndetermined geometrically and unambiguously from the 3D scene. To this end, we\nhave developed an efficient tool, named Mesh2OB, for the automatic generation\nof 2D images together with their ground-truth OBs, using which we have\nconstructed a synthetic benchmark, named OB-FUTURE. Abundant experimental\nresults demonstrate that leveraging such a synthetic benchmark for training\nachieves promising performance, even without the use of domain adaptation\ntechniques. Finally, to achieve a more compelling and robust evaluation in\nOB-related research, we have created a real benchmark, named OB-LabName,\nconsisting of 120 high-resolution images together with their ground-truth OBs,\nwith precision surpassing that of previous benchmarks. We will release DNMMSI\nwith pre-trained parameters, Mesh2OB, OB-FUTURE, and OB-LabName to support\nfurther research.\n","authors":["Lintao Xu","Chaohui Wang"],"pdf_url":"https://arxiv.org/pdf/2408.15038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20710v2","updated":"2024-08-27T13:05:27Z","published":"2023-10-31T17:59:58Z","title":"FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance\n  Fields by Analyzing and Enhancing Fourier PlenOctrees","summary":"  Fourier PlenOctrees have shown to be an efficient representation for\nreal-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many\nadvantages, this method suffers from artifacts introduced by the involved\ncompression when combining it with recent state-of-the-art techniques for\ntraining the static per-frame NeRF models. In this paper, we perform an\nin-depth analysis of these artifacts and leverage the resulting insights to\npropose an improved representation. In particular, we present a novel density\nencoding that adapts the Fourier-based compression to the characteristics of\nthe transfer function used by the underlying volume rendering procedure and\nleads to a substantial reduction of artifacts in the dynamic model.\nFurthermore, we show an augmentation of the training data that relaxes the\nperiodicity assumption of the compression. We demonstrate the effectiveness of\nour enhanced Fourier PlenOctrees in the scope of quantitative and qualitative\nevaluations on synthetic and real-world scenes.\n","authors":["Saskia Rabich","Patrick Stotko","Reinhard Klein"],"pdf_url":"https://arxiv.org/pdf/2310.20710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15032v1","updated":"2024-08-27T13:01:19Z","published":"2024-08-27T13:01:19Z","title":"Mamba2MIL: State Space Duality Based Multiple Instance Learning for\n  Computational Pathology","summary":"  Computational pathology (CPath) has significantly advanced the clinical\npractice of pathology. Despite the progress made, Multiple Instance Learning\n(MIL), a promising paradigm within CPath, continues to face challenges,\nparticularly related to incomplete information utilization. Existing\nframeworks, such as those based on Convolutional Neural Networks (CNNs),\nattention, and selective scan space state sequential model (SSM), lack\nsufficient flexibility and scalability in fusing diverse features, and cannot\neffectively fuse diverse features. Additionally, current approaches do not\nadequately exploit order-related and order-independent features, resulting in\nsuboptimal utilization of sequence information. To address these limitations,\nwe propose a novel MIL framework called Mamba2MIL. Our framework utilizes the\nstate space duality model (SSD) to model long sequences of patches of whole\nslide images (WSIs), which, combined with weighted feature selection, supports\nthe fusion processing of more branching features and can be extended according\nto specific application needs. Moreover, we introduce a sequence transformation\nmethod tailored to varying WSI sizes, which enhances sequence-independent\nfeatures while preserving local sequence information, thereby improving\nsequence information utilization. Extensive experiments demonstrate that\nMamba2MIL surpasses state-of-the-art MIL methods. We conducted extensive\nexperiments across multiple datasets, achieving improvements in nearly all\nperformance metrics. Specifically, on the NSCLC dataset, Mamba2MIL achieves a\nbinary tumor classification AUC of 0.9533 and an accuracy of 0.8794. On the\nBRACS dataset, it achieves a multiclass classification AUC of 0.7986 and an\naccuracy of 0.4981. The code is available at\nhttps://github.com/YuqiZhang-Buaa/Mamba2MIL.\n","authors":["Yuqi Zhang","Xiaoqian Zhang","Jiakai Wang","Yuancheng Yang","Taiying Peng","Chao Tong"],"pdf_url":"https://arxiv.org/pdf/2408.15032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15026v1","updated":"2024-08-27T12:55:54Z","published":"2024-08-27T12:55:54Z","title":"Sequence-aware Pre-training for Echocardiography Probe Guidance","summary":"  Cardiac ultrasound probe guidance aims to help novices adjust the 6-DOF probe\npose to obtain high-quality sectional images. Cardiac ultrasound faces two\nmajor challenges: (1) the inherently complex structure of the heart, and (2)\nsignificant individual variations. Previous works have only learned the\npopulation-averaged 2D and 3D structures of the heart rather than personalized\ncardiac structural features, leading to a performance bottleneck. Clinically,\nwe observed that sonographers adjust their understanding of a patient's cardiac\nstructure based on prior scanning sequences, thereby modifying their scanning\nstrategies. Inspired by this, we propose a sequence-aware self-supervised\npre-training method. Specifically, our approach learns personalized 2D and 3D\ncardiac structural features by predicting the masked-out images and actions in\na scanning sequence. We hypothesize that if the model can predict the missing\ncontent it has acquired a good understanding of the personalized cardiac\nstructure. In the downstream probe guidance task, we also introduced a sequence\nmodeling approach that models individual cardiac structural information based\non the images and actions from historical scan data, enabling more accurate\nnavigation decisions. Experiments on a large-scale dataset with 1.36 million\nsamples demonstrated that our proposed sequence-aware paradigm can\nsignificantly reduce navigation errors, with translation errors decreasing by\n15.90% to 36.87% and rotation errors decreasing by 11.13% to 20.77%, compared\nto state-of-the-art methods.\n","authors":["Haojun Jiang","Zhenguo Sun","Yu Sun","Ning Jia","Meng Li","Shaqi Luo","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2408.15026v1.pdf","comment":"Tech Report"},{"id":"http://arxiv.org/abs/2408.15020v1","updated":"2024-08-27T12:53:25Z","published":"2024-08-27T12:53:25Z","title":"Hierarchical Graph Interaction Transformer with Dynamic Token Clustering\n  for Camouflaged Object Detection","summary":"  Camouflaged object detection (COD) aims to identify the objects that\nseamlessly blend into the surrounding backgrounds. Due to the intrinsic\nsimilarity between the camouflaged objects and the background region, it is\nextremely challenging to precisely distinguish the camouflaged objects by\nexisting approaches. In this paper, we propose a hierarchical graph interaction\nnetwork termed HGINet for camouflaged object detection, which is capable of\ndiscovering imperceptible objects via effective graph interaction among the\nhierarchical tokenized features. Specifically, we first design a region-aware\ntoken focusing attention (RTFA) with dynamic token clustering to excavate the\npotentially distinguishable tokens in the local region. Afterwards, a\nhierarchical graph interaction transformer (HGIT) is proposed to construct\nbi-directional aligned communication between hierarchical features in the\nlatent interaction space for visual semantics enhancement. Furthermore, we\npropose a decoder network with confidence aggregated feature fusion (CAFF)\nmodules, which progressively fuses the hierarchical interacted features to\nrefine the local detail in ambiguous regions. Extensive experiments conducted\non the prevalent datasets, i.e. COD10K, CAMO, NC4K and CHAMELEON demonstrate\nthe superior performance of HGINet compared to existing state-of-the-art\nmethods. Our code is available at https://github.com/Garyson1204/HGINet.\n","authors":["Siyuan Yao","Hao Sun","Tian-Zhu Xiang","Xiao Wang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2408.15020v1.pdf","comment":"Submitted to IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2408.15015v1","updated":"2024-08-27T12:50:12Z","published":"2024-08-27T12:50:12Z","title":"Alternating Minimization Schemes for Computing\n  Rate-Distortion-Perception Functions with $f$-Divergence Perception\n  Constraints","summary":"  We study the computation of the rate-distortion-perception function (RDPF)\nfor discrete memoryless sources subject to a single-letter average distortion\nconstraint and a perception constraint that belongs to the family of\n$f$-divergences. In this setting, the RDPF forms a convex programming problem\nfor which we characterize the optimal parametric solutions. We employ the\ndeveloped solutions in an alternating minimization scheme, namely Optimal\nAlternating Minimization (OAM), for which we provide convergence guarantees.\nNevertheless, the OAM scheme does not lead to a direct implementation of a\ngeneralized Blahut-Arimoto (BA) type of algorithm due to the presence of\nimplicit equations in the structure of the iteration. To overcome this\ndifficulty, we propose two alternative minimization approaches whose\napplicability depends on the smoothness of the used perception metric: a\nNewton-based Alternating Minimization (NAM) scheme, relying on Newton's\nroot-finding method for the approximation of the optimal iteration solution,\nand a Relaxed Alternating Minimization (RAM) scheme, based on a relaxation of\nthe OAM iterates. Both schemes are shown, via the derivation of necessary and\nsufficient conditions, to guarantee convergence to a globally optimal solution.\nWe also provide sufficient conditions on the distortion and the perception\nconstraints which guarantee that the proposed algorithms converge exponentially\nfast in the number of iteration steps. We corroborate our theoretical results\nwith numerical simulations and draw connections with existing results.\n","authors":["Giuseppe Serra","Photios A. Stavrou","Marios Kountouris"],"pdf_url":"https://arxiv.org/pdf/2408.15015v1.pdf","comment":"This work has been submitted for possible publication"},{"id":"http://arxiv.org/abs/2408.15011v1","updated":"2024-08-27T12:48:46Z","published":"2024-08-27T12:48:46Z","title":"Pre-training Everywhere: Parameter-Efficient Fine-Tuning for Medical\n  Image Analysis via Target Parameter Pre-training","summary":"  Parameter-efficient fine-tuning (PEFT) techniques have emerged to address\nissues of overfitting and high computational costs associated with fully\nfine-tuning in the paradigm of self-supervised learning. Mainstream methods\nbased on PEFT involve adding a few trainable parameters while keeping the\npre-trained parameters of the backbone fixed. These methods achieve\ncomparative, and often superior, performance to fully fine-tuning,\ndemonstrating the powerful representation ability of the pre-trained backbone.\nDespite its success, these methods typically ignore the initialization of the\nnew parameters, often relying solely on random initialization. We argue that if\npre-training is significantly beneficial, it should be applied to all\nparameters requiring representational capacity. Motivated by this insight, we\npropose a simple yet effective fine-tuning framework based on Target Parameter\nPre-training (TPP). The target parameters refer to the new parameters\nintroduced during fine-tuning. TPP includes an additional stage before PEFT to\npre-train these target parameters. During this stage, the pre-trained backbone\nparameters are frozen, and only the target parameters are trainable. A defined\npre-text task is used to encourage the target parameters to learn specific\nrepresentations of downstream data. When PEFT is subsequently employed, the\npre-trained target parameters are loaded to enhance fine-tuning efficiency. The\nproposed TPP framework is versatile, allowing for the integration of various\npretext tasks for pre-training and supporting different PEFT methods as\nbackbones. We evaluated the fine-tining performance of our method using five\npublic datasets, including three modalities and two task types. The results\ndemonstrate that the proposed TPP can be easily integrated into existing PEFT\nmethods, significantly improving performance.\n","authors":["Xingliang Lei","Yiwen Ye","Ziyang Chen","Minglei Shu","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2408.15011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03209v2","updated":"2024-08-27T12:39:18Z","published":"2024-08-06T14:08:22Z","title":"IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning\n  using Instruct Prompts","summary":"  Diffusion models continuously push the boundary of state-of-the-art image\ngeneration, but the process is hard to control with any nuance: practice proves\nthat textual prompts are inadequate for accurately describing image style or\nfine structural details (such as faces). ControlNet and IPAdapter address this\nshortcoming by conditioning the generative process on imagery instead, but each\nindividual instance is limited to modeling a single conditional posterior: for\npractical use-cases, where multiple different posteriors are desired within the\nsame workflow, training and using multiple adapters is cumbersome. We propose\nIPAdapter-Instruct, which combines natural-image conditioning with ``Instruct''\nprompts to swap between interpretations for the same conditioning image: style\ntransfer, object extraction, both, or something else still? IPAdapterInstruct\nefficiently learns multiple tasks with minimal loss in quality compared to\ndedicated per-task models.\n","authors":["Ciara Rowles","Shimon Vainer","Dante De Nigris","Slava Elizarov","Konstantin Kutsy","Simon DonnÃ©"],"pdf_url":"https://arxiv.org/pdf/2408.03209v2.pdf","comment":"17 pages, 10 figures, Project page:\n  https://unity-research.github.io/IP-Adapter-Instruct.github.io/"},{"id":"http://arxiv.org/abs/2408.15002v1","updated":"2024-08-27T12:34:41Z","published":"2024-08-27T12:34:41Z","title":"Knowledge Discovery in Optical Music Recognition: Enhancing Information\n  Retrieval with Instance Segmentation","summary":"  Optical Music Recognition (OMR) automates the transcription of musical\nnotation from images into machine-readable formats like MusicXML, MEI, or MIDI,\nsignificantly reducing the costs and time of manual transcription. This study\nexplores knowledge discovery in OMR by applying instance segmentation using\nMask R-CNN to enhance the detection and delineation of musical symbols in sheet\nmusic. Unlike Optical Character Recognition (OCR), OMR must handle the\nintricate semantics of Common Western Music Notation (CWMN), where symbol\nmeanings depend on shape, position, and context. Our approach leverages\ninstance segmentation to manage the density and overlap of musical symbols,\nfacilitating more precise information retrieval from music scores. Evaluations\non the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with\nour method achieving a mean Average Precision (mAP) of up to 59.70\\% in dense\nsymbol environments, achieving comparable results to object detection.\nFurthermore, using traditional computer vision techniques, we add a parallel\nstep for staff detection to infer the pitch for the recognised symbols. This\nstudy emphasises the role of pixel-wise segmentation in advancing accurate\nmusic symbol recognition, contributing to knowledge discovery in OMR. Our\nfindings indicate that instance segmentation provides more precise\nrepresentations of musical symbols, particularly in densely populated scores,\nadvancing OMR technology. We make our implementation, pre-processing scripts,\ntrained models, and evaluation results publicly available to support further\nresearch and development.\n","authors":["Elona Shatri","George Fazekas"],"pdf_url":"https://arxiv.org/pdf/2408.15002v1.pdf","comment":"8 pages content and one references, accepted version at the\n  International Conference on Knowledge Discovery and Information Retrieval\n  2024, Porto, Portugal"},{"id":"http://arxiv.org/abs/2408.14998v1","updated":"2024-08-27T12:28:41Z","published":"2024-08-27T12:28:41Z","title":"FastTextSpotter: A High-Efficiency Transformer for Multilingual Scene\n  Text Spotting","summary":"  The proliferation of scene text in both structured and unstructured\nenvironments presents significant challenges in optical character recognition\n(OCR), necessitating more efficient and robust text spotting solutions. This\npaper presents FastTextSpotter, a framework that integrates a Swin Transformer\nvisual backbone with a Transformer Encoder-Decoder architecture, enhanced by a\nnovel, faster self-attention unit, SAC2, to improve processing speeds while\nmaintaining accuracy. FastTextSpotter has been validated across multiple\ndatasets, including ICDAR2015 for regular texts and CTW1500 and TotalText for\narbitrary-shaped texts, benchmarking against current state-of-the-art models.\nOur results indicate that FastTextSpotter not only achieves superior accuracy\nin detecting and recognizing multilingual scene text (English and Vietnamese)\nbut also improves model efficiency, thereby setting new benchmarks in the\nfield. This study underscores the potential of advanced transformer\narchitectures in improving the adaptability and speed of text spotting\napplications in diverse real-world settings. The dataset, code, and pre-trained\nmodels have been released in our Github.\n","authors":["Alloy Das","Sanket Biswas","Umapada Pal","Josep LladÃ³s","Saumik Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2408.14998v1.pdf","comment":"Accepted in ICPR 2024"},{"id":"http://arxiv.org/abs/2408.14997v1","updated":"2024-08-27T12:25:12Z","published":"2024-08-27T12:25:12Z","title":"Depth Restoration of Hand-Held Transparent Objects for Human-to-Robot\n  Handover","summary":"  Transparent objects are common in daily life, while their unique optical\nproperties pose challenges for RGB-D cameras, which struggle to capture\naccurate depth information. For assistant robots, accurately perceiving\ntransparent objects held by humans is essential for effective human-robot\ninteraction. This paper presents a Hand-Aware Depth Restoration (HADR) method\nfor hand-held transparent objects based on creating an implicit neural\nrepresentation function from a single RGB-D image. The proposed method\nintroduces the hand posture as an important guidance to leverage semantic and\ngeometric information. To train and evaluate the proposed method, we create a\nhigh-fidelity synthetic dataset called TransHand-14K with a real-to-sim data\ngeneration scheme. Experiments show that our method has a better performance\nand generalization ability compared with existing methods. We further develop a\nreal-world human-to-robot handover system based on the proposed depth\nrestoration method, demonstrating its application value in human-robot\ninteraction.\n","authors":["Ran Yu","Haixin Yu","Huang Yan","Ziwu Song","Shoujie Li","Wenbo Ding"],"pdf_url":"https://arxiv.org/pdf/2408.14997v1.pdf","comment":"7 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2407.21687v2","updated":"2024-08-27T12:03:00Z","published":"2024-07-31T15:29:34Z","title":"Dynamic Object Queries for Transformer-based Incremental Object\n  Detection","summary":"  Incremental object detection (IOD) aims to sequentially learn new classes,\nwhile maintaining the capability to locate and identify old ones. As the\ntraining data arrives with annotations only with new classes, IOD suffers from\ncatastrophic forgetting. Prior methodologies mainly tackle the forgetting issue\nthrough knowledge distillation and exemplar replay, ignoring the conflict\nbetween limited model capacity and increasing knowledge. In this paper, we\nexplore \\textit{dynamic object queries} for incremental object detection built\non Transformer architecture. We propose the \\textbf{Dy}namic object\n\\textbf{Q}uery-based \\textbf{DE}tection \\textbf{TR}ansformer (DyQ-DETR), which\nincrementally expands the model representation ability to achieve\nstability-plasticity tradeoff. First, a new set of learnable object queries are\nfed into the decoder to represent new classes. These new object queries are\naggregated with those from previous phases to adapt both old and new knowledge\nwell. Second, we propose the isolated bipartite matching for object queries in\ndifferent phases, based on disentangled self-attention. The interaction among\nthe object queries at different phases is eliminated to reduce inter-class\nconfusion. Thanks to the separate supervision and computation over object\nqueries, we further present the risk-balanced partial calibration for effective\nexemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly\nsurpasses the state-of-the-art methods, with limited parameter overhead. Code\nwill be made publicly available.\n","authors":["Jichuan Zhang","Wei Li","Shuang Cheng","Ya-Li Li","Shengjin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.21687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07257v2","updated":"2024-08-27T11:44:56Z","published":"2024-05-12T11:41:44Z","title":"Listen, Disentangle, and Control: Controllable Speech-Driven Talking\n  Head Generation","summary":"  Most earlier investigations on talking face generation have focused on the\nsynchronization of lip motion and speech content. However, human head pose and\nfacial emotions are equally important characteristics of natural human faces.\nWhile audio-driven talking face generation has seen notable advancements,\nexisting methods either overlook facial emotions or are limited to specific\nindividuals and cannot be applied to arbitrary subjects. In this paper, we\npropose a one-shot Talking Head Generation framework (SPEAK) that distinguishes\nitself from general Talking Face Generation by enabling emotional and postural\ncontrol. Specifically, we introduce the Inter-Reconstructed Feature\nDisentanglement (IRFD) method to decouple human facial features into three\nlatent spaces. We then design a face editing module that modifies speech\ncontent and facial latent codes into a single latent space. Subsequently, we\npresent a novel generator that employs modified latent codes derived from the\nediting module to regulate emotional expression, head poses, and speech content\nin synthesizing facial animations. Extensive trials demonstrate that our method\ncan generate realistic talking head with coordinated lip motions, authentic\nfacial emotions, and smooth head movements. The demo video is available at the\nanonymous link: https://anonymous.4open.science/r/SPEAK-F56E\n","authors":["Changpeng Cai","Guinan Guo","Jiao Li","Junhao Su","Chenghao He","Jing Xiao","Yuanxu Chen","Lei Dai","Feiyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2405.07257v2.pdf","comment":"Due to our negligence, there are factual errors in the experimental\n  results, so we are considering resubmitting the paper after an overhaul"},{"id":"http://arxiv.org/abs/2408.14977v1","updated":"2024-08-27T11:40:23Z","published":"2024-08-27T11:40:23Z","title":"LN-Gen: Rectal Lymph Nodes Generation via Anatomical Features","summary":"  Accurate segmentation of rectal lymph nodes is crucial for the staging and\ntreatment planning of rectal cancer. However, the complexity of the surrounding\nanatomical structures and the scarcity of annotated data pose significant\nchallenges. This study introduces a novel lymph node synthesis technique aimed\nat generating diverse and realistic synthetic rectal lymph node samples to\nmitigate the reliance on manual annotation. Unlike direct diffusion methods,\nwhich often produce masks that are discontinuous and of suboptimal quality, our\napproach leverages an implicit SDF-based method for mask generation, ensuring\nthe production of continuous, stable, and morphologically diverse masks.\nExperimental results demonstrate that our synthetic data significantly improves\nsegmentation performance. Our work highlights the potential of diffusion model\nfor accurately synthesizing structurally complex lesions, such as lymph nodes\nin rectal cancer, alleviating the challenge of limited annotated data in this\nfield and aiding in advancements in rectal cancer diagnosis and treatment.\n","authors":["Weidong Guo","Hantao Zhang","Shouhong Wan","Bingbing Zou","Wanqin Wang","Peiquan Jin"],"pdf_url":"https://arxiv.org/pdf/2408.14977v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2408.14976v1","updated":"2024-08-27T11:38:01Z","published":"2024-08-27T11:38:01Z","title":"Prior-free Balanced Replay: Uncertainty-guided Reservoir Sampling for\n  Long-Tailed Continual Learning","summary":"  Even in the era of large models, one of the well-known issues in continual\nlearning (CL) is catastrophic forgetting, which is significantly challenging\nwhen the continual data stream exhibits a long-tailed distribution, termed as\nLong-Tailed Continual Learning (LTCL). Existing LTCL solutions generally\nrequire the label distribution of the data stream to achieve re-balance\ntraining. However, obtaining such prior information is often infeasible in real\nscenarios since the model should learn without pre-identifying the majority and\nminority classes. To this end, we propose a novel Prior-free Balanced Replay\n(PBR) framework to learn from long-tailed data stream with less forgetting.\nConcretely, motivated by our experimental finding that the minority classes are\nmore likely to be forgotten due to the higher uncertainty, we newly design an\nuncertainty-guided reservoir sampling strategy to prioritize rehearsing\nminority data without using any prior information, which is based on the mutual\ndependence between the model and samples. Additionally, we incorporate two\nprior-free components to further reduce the forgetting issue: (1) Boundary\nconstraint is to preserve uncertain boundary supporting samples for continually\nre-estimating task boundaries. (2) Prototype constraint is to maintain the\nconsistency of learned class prototypes along with training. Our approach is\nevaluated on three standard long-tailed benchmarks, demonstrating superior\nperformance to existing CL methods and previous SOTA LTCL approach in both\ntask- and class-incremental learning settings, as well as ordered- and\nshuffled-LTCL settings.\n","authors":["Lei Liu","Li Liu","Yawen Cui"],"pdf_url":"https://arxiv.org/pdf/2408.14976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14975v1","updated":"2024-08-27T11:31:47Z","published":"2024-08-27T11:31:47Z","title":"MegActor-$Î£$: Unlocking Flexible Mixed-Modal Control in Portrait\n  Animation with Diffusion Transformer","summary":"  Diffusion models have demonstrated superior performance in the field of\nportrait animation. However, current approaches relied on either visual or\naudio modality to control character movements, failing to exploit the potential\nof mixed-modal control. This challenge arises from the difficulty in balancing\nthe weak control strength of audio modality and the strong control strength of\nvisual modality. To address this issue, we introduce MegActor-$\\Sigma$: a\nmixed-modal conditional diffusion transformer (DiT), which can flexibly inject\naudio and visual modality control signals into portrait animation.\nSpecifically, we make substantial advancements over its predecessor, MegActor,\nby leveraging the promising model structure of DiT and integrating audio and\nvisual conditions through advanced modules within the DiT framework. To further\nachieve flexible combinations of mixed-modal control signals, we propose a\n``Modality Decoupling Control\" training strategy to balance the control\nstrength between visual and audio modalities, along with the ``Amplitude\nAdjustment\" inference strategy to freely regulate the motion amplitude of each\nmodality. Finally, to facilitate extensive studies in this field, we design\nseveral dataset evaluation metrics to filter out public datasets and solely use\nthis filtered dataset to train MegActor-$\\Sigma$. Extensive experiments\ndemonstrate the superiority of our approach in generating vivid portrait\nanimations, outperforming previous methods trained on private dataset.\n","authors":["Shurong Yang","Huadong Li","Juhao Wu","Minhao Jing","Linze Li","Renhe Ji","Jiajun Liang","Haoqiang Fan","Jin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14962v1","updated":"2024-08-27T11:09:34Z","published":"2024-08-27T11:09:34Z","title":"Deep Learning-based Average Shear Wave Velocity Prediction using\n  Accelerometer Records","summary":"  Assessing seismic hazards and thereby designing earthquake-resilient\nstructures or evaluating structural damage that has been incurred after an\nearthquake are important objectives in earthquake engineering. Both tasks\nrequire critical evaluation of strong ground motion records, and the knowledge\nof site conditions at the earthquake stations plays a major role in achieving\nthe aforementioned objectives. Site conditions are generally represented by the\ntime-averaged shear wave velocity in the upper 30 meters of the geological\nmaterials (Vs30). Several strong motion stations lack Vs30 measurements\nresulting in potentially inaccurate assessment of seismic hazards and\nevaluation of ground motion records. In this study, we present a deep\nlearning-based approach for predicting Vs30 at strong motion station locations\nusing three-channel earthquake records. For this purpose, Convolutional Neural\nNetworks (CNNs) with dilated and causal convolutional layers are used to\nextract deep features from accelerometer records collected from over 700\nstations located in Turkey. In order to overcome the limited availability of\nlabeled data, we propose a two-phase training approach. In the first phase, a\nCNN is trained to estimate the epicenters, for which ground truth is available\nfor all records. After the CNN is trained, the pre-trained encoder is\nfine-tuned based on the Vs30 ground truth. The performance of the proposed\nmethod is compared with machine learning models that utilize hand-crafted\nfeatures. The results demonstrate that the deep convolutional encoder based\nVs30 prediction model outperforms the machine learning models that rely on\nhand-crafted features.\n","authors":["BarÄ±Å YÄ±lmaz","Melek TÃ¼rkmen","Sanem Meral","Erdem AkagÃ¼ndÃ¼z","Salih Tileylioglu"],"pdf_url":"https://arxiv.org/pdf/2408.14962v1.pdf","comment":"12 pages, 14 figures, Accepted by 18th World Conference on Earthquake\n  Engineering WCEE2024"},{"id":"http://arxiv.org/abs/2408.14961v1","updated":"2024-08-27T11:07:19Z","published":"2024-08-27T11:07:19Z","title":"CVPT: Cross-Attention help Visual Prompt Tuning adapt visual task","summary":"  In recent years, the rapid expansion of model sizes has led to large-scale\npre-trained models demonstrating remarkable capabilities. Consequently, there\nhas been a trend towards increasing the scale of models. However, this trend\nintroduces significant challenges, including substantial computational costs of\ntraining and transfer to downstream tasks. To address these issues,\nParameter-Efficient Fine-Tuning (PEFT) methods have been introduced. These\nmethods optimize large-scale pre-trained models for specific tasks by\nfine-tuning a select group of parameters. Among these PEFT methods,\nadapter-based and prompt-based methods are the primary techniques.\nSpecifically, in the field of visual fine-tuning, adapters gain prominence over\nprompts because of the latter's relatively weaker performance and efficiency.\nUnder the circumstances, we refine the widely-used Visual Prompt Tuning (VPT)\nmethod, proposing Cross Visual Prompt Tuning (CVPT). CVPT calculates\ncross-attention between the prompt tokens and the embedded tokens, which allows\nus to compute the semantic relationship between them and conduct the\nfine-tuning of models exactly to adapt visual tasks better. Furthermore, we\nintroduce the weight-sharing mechanism to initialize the parameters of\ncross-attention, which avoids massive learnable parameters from cross-attention\nand enhances the representative capability of cross-attention. We conduct\ncomprehensive testing across 25 datasets and the result indicates that CVPT\nsignificantly improves VPT's performance and efficiency in visual tasks. For\nexample, on the VTAB-1K benchmark, CVPT outperforms VPT over 4% in average\naccuracy, rivaling the advanced adapter-based methods in performance and\nefficiency. Our experiments confirm that prompt-based methods can achieve\nexceptional results in visual fine-tuning.\n","authors":["Lingyun Huang","Jianxu Mao","Yaonan Wang","Junfei Yi","Ziming Tao"],"pdf_url":"https://arxiv.org/pdf/2408.14961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14957v1","updated":"2024-08-27T11:04:53Z","published":"2024-08-27T11:04:53Z","title":"Applying ViT in Generalized Few-shot Semantic Segmentation","summary":"  This paper explores the capability of ViT-based models under the generalized\nfew-shot semantic segmentation (GFSS) framework. We conduct experiments with\nvarious combinations of backbone models, including ResNets and pretrained\nVision Transformer (ViT)-based models, along with decoders featuring a linear\nclassifier, UPerNet, and Mask Transformer. The structure made of DINOv2 and\nlinear classifier takes the lead on popular few-shot segmentation bench mark\nPASCAL-$5^i$, substantially outperforming the best of ResNet structure by 116%\nin one-shot scenario. We demonstrate the great potential of large pretrained\nViT-based model on GFSS task, and expect further improvement on testing\nbenchmarks. However, a potential caveat is that when applying pure ViT-based\nmodel and large scale ViT decoder, the model is easy to overfit.\n","authors":["Liyuan Geng","Jinhong Xia","Yuanhe Guo"],"pdf_url":"https://arxiv.org/pdf/2408.14957v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.17640v2","updated":"2024-08-27T11:00:47Z","published":"2024-06-25T15:24:06Z","title":"BayTTA: Uncertainty-aware medical image classification with optimized\n  test-time augmentation using Bayesian model averaging","summary":"  Test-time augmentation (TTA) is a well-known technique employed during the\ntesting phase of computer vision tasks. It involves aggregating multiple\naugmented versions of input data. Combining predictions using a simple average\nformulation is a common and straightforward approach after performing TTA. This\npaper introduces a novel framework for optimizing TTA, called BayTTA\n(Bayesian-based TTA), which is based on Bayesian Model Averaging (BMA). First,\nwe generate a prediction list associated with different variations of the input\ndata created through TTA. Then, we use BMA to combine predictions weighted by\nthe respective posterior probabilities. Such an approach allows one to take\ninto account model uncertainty, and thus to enhance the predictive performance\nof the related machine learning or deep learning model. We evaluate the\nperformance of BayTTA on various public data, including three medical image\ndatasets comprising skin cancer, breast cancer, and chest X-ray images and two\nwell-known gene editing datasets, CRISPOR and GUIDE-seq. Our experimental\nresults indicate that BayTTA can be effectively integrated into\nstate-of-the-art deep learning models used in medical image analysis as well as\ninto some popular pre-trained CNN models such as VGG-16, MobileNetV2,\nDenseNet201, ResNet152V2, and InceptionRes-NetV2, leading to the enhancement in\ntheir accuracy and robustness performance. The source code of the proposed\nBayTTA method is freely available at: \\underline\n{https://github.com/Z-Sherkat/BayTTA}.\n","authors":["Zeinab Sherkatghanad","Moloud Abdar","Mohammadreza Bakhtyari","Pawel Plawiak","Vladimir Makarenkov"],"pdf_url":"https://arxiv.org/pdf/2406.17640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14950v1","updated":"2024-08-27T10:54:37Z","published":"2024-08-27T10:54:37Z","title":"NeuralOOD: Improving Out-of-Distribution Generalization Performance with\n  Brain-machine Fusion Learning Framework","summary":"  Deep Neural Networks (DNNs) have demonstrated exceptional recognition\ncapabilities in traditional computer vision (CV) tasks. However, existing CV\nmodels often suffer a significant decrease in accuracy when confronted with\nout-of-distribution (OOD) data. In contrast to these DNN models, human can\nmaintain a consistently low error rate when facing OOD scenes, partly\nattributed to the rich prior cognitive knowledge stored in the human brain.\nPrevious OOD generalization researches only focus on the single modal,\noverlooking the advantages of multimodal learning method. In this paper, we\nutilize the multimodal learning method to improve the OOD generalization and\npropose a novel Brain-machine Fusion Learning (BMFL) framework. We adopt the\ncross-attention mechanism to fuse the visual knowledge from CV model and prior\ncognitive knowledge from the human brain. Specially, we employ a pre-trained\nvisual neural encoding model to predict the functional Magnetic Resonance\nImaging (fMRI) from visual features which eliminates the need for the fMRI data\ncollection and pre-processing, effectively reduces the workload associated with\nconventional BMFL methods. Furthermore, we construct a brain transformer to\nfacilitate the extraction of knowledge inside the fMRI data. Moreover, we\nintroduce the Pearson correlation coefficient maximization regularization\nmethod into the training process, which improves the fusion capability with\nbetter constrains. Our model outperforms the DINOv2 and baseline models on the\nImageNet-1k validation dataset as well as six curated OOD datasets, showcasing\nits superior performance in diverse scenarios.\n","authors":["Shuangchen Zhao","Changde Du","Hui Li","Huiguang He"],"pdf_url":"https://arxiv.org/pdf/2408.14950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19513v2","updated":"2024-08-27T10:50:13Z","published":"2024-04-30T12:45:41Z","title":"A Smartphone-Based Method for Assessing Tomato Nutrient Status through\n  Trichome Density Measurement","summary":"  Early detection of fertilizer-induced stress in tomato plants is crucial for\ntimely crop management interventions and yield optimization. Conventional\noptical methods detect fertilizer stress in young leaves with difficulty. This\nstudy proposes a novel, noninvasive technique for quantifying the density of\ntrichomes-elongated hair-like structures found on plant surfaces-on young\nleaves using a smartphone. This method exhibits superior detection latency,\nenabling earlier and more accurate identification of fertilizer stress in\ntomato plants. Our approach combines augmented reality technology and image\nprocessing algorithms to analyze smartphone images of a specialized measurement\npaper. This measurement paper is applied to a tomato leaf to transfer trichomes\nonto its adhesive surface. The captured images are then processed through a\npipeline involving region of interest extraction, perspective transformation,\nand illumination correction. Trichome detection and spatial distribution\nanalysis of these preprocessed images yield a robust density metric. We\nvalidated our method through experiments on hydroponically grown tomatoes under\nvarying fertilizer concentrations. Using leave-one-out cross-validation\n(LOOCV), our model achieves a mean area under the precision-recall curve of\n0.824 and a receiver operating characteristic curve of 0.641 for predicting\nadditional fertilization needs. Based on LOOCV, quantitative analysis revealed\na strong relationship between trichome density and explanatory variables,\nincluding nitrate ion concentration, explaining 62.48% of the variation ($R^2 =\n0.625$). The predicted and actual trichome densities were strongly correlated\n($r = 0.794$). This straightforward and cost-effective method overcomes the\nlimitations of traditional techniques, demonstrating the potential of using\nsmartphones for practical plant nutrition diagnosis.\n","authors":["Sho Ueda","Xujun Ye"],"pdf_url":"https://arxiv.org/pdf/2404.19513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14253v2","updated":"2024-08-27T10:50:13Z","published":"2024-08-26T13:16:03Z","title":"Text3DAug -- Prompted Instance Augmentation for LiDAR Perception","summary":"  LiDAR data of urban scenarios poses unique challenges, such as heterogeneous\ncharacteristics and inherent class imbalance. Therefore, large-scale datasets\nare necessary to apply deep learning methods. Instance augmentation has emerged\nas an efficient method to increase dataset diversity. However, current methods\nrequire the time-consuming curation of 3D models or costly manual data\nannotation. To overcome these limitations, we propose Text3DAug, a novel\napproach leveraging generative models for instance augmentation. Text3DAug does\nnot depend on labeled data and is the first of its kind to generate instances\nand annotations from text. This allows for a fully automated pipeline,\neliminating the need for manual effort in practical applications. Additionally,\nText3DAug is sensor agnostic and can be applied regardless of the LiDAR sensor\nused. Comprehensive experimental analysis on LiDAR segmentation, detection and\nnovel class discovery demonstrates that Text3DAug is effective in supplementing\nexisting methods or as a standalone method, performing on par or better than\nestablished methods, however while overcoming their specific drawbacks. The\ncode is publicly available.\n","authors":["Laurenz Reichardt","Luca Uhr","Oliver WasenmÃ¼ller"],"pdf_url":"https://arxiv.org/pdf/2408.14253v2.pdf","comment":"Accepted at the 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2308.13997v2","updated":"2024-08-27T10:47:47Z","published":"2023-08-27T03:54:55Z","title":"Adaptive Fusion of Radiomics and Deep Features for Lung Adenocarcinoma\n  Subtype Recognition","summary":"  The most common type of lung cancer, lung adenocarcinoma (LUAD), has been\nincreasingly detected since the advent of low-dose computed tomography\nscreening technology. In clinical practice, pre-invasive LUAD (Pre-IAs) should\nonly require regular follow-up care, while invasive LUAD (IAs) should receive\nimmediate treatment with appropriate lung cancer resection, based on the cancer\nsubtype. However, prior research on diagnosing LUAD has mainly focused on\nclassifying Pre-IAs/IAs, as techniques for distinguishing different subtypes of\nIAs have been lacking. In this study, we proposed a multi-head attentional\nfeature fusion (MHA-FF) model for not only distinguishing IAs from Pre-IAs, but\nalso for distinguishing the different subtypes of IAs. To predict the subtype\nof each nodule accurately, we leveraged both radiomics and deep features\nextracted from computed tomography images. Furthermore, those features were\naggregated through an adaptive fusion module that can learn attention-based\ndiscriminative features. The utility of our proposed method is demonstrated\nhere by means of real-world data collected from a multi-center cohort.\n","authors":["Jing Zhou","Xiaotong Fu","Xirong Li","Ying Ji"],"pdf_url":"https://arxiv.org/pdf/2308.13997v2.pdf","comment":"7 pages, 5 figures and 4 tables"},{"id":"http://arxiv.org/abs/2408.14947v1","updated":"2024-08-27T10:44:34Z","published":"2024-08-27T10:44:34Z","title":"ERX: A Fast Real-Time Anomaly Detection Algorithm for Hyperspectral\n  Line-Scanning","summary":"  Detecting unexpected objects (anomalies) in real-time has great potential for\nmonitoring, managing, and protecting the environment. Hyperspectral line-scan\ncameras are a low-cost solution that enhance confidence in anomaly detection\nover RGB and multispectral imagery. However, real-time algorithms for these\ncameras must be fast when using small computers (e.g., those onboard a drone or\nsmall satellite), scalable to high dimensions, adaptable to changing scenery,\nand robust against geometric and radiometric distortions. This paper introduces\nthe Exponentially moving RX algorithm (ERX) and compares it to existing\nRX-based anomaly detection methods for real-time line-scanning. ERX was tested\nusing a Jetson Xavier NX compute module, achieving the best combination of\nspeed and detection across three novel datasets compared to the other\nalgorithms. This research paves the way for future studies in grouping and\nlocating anomalous objects, adaptive and automatic threshold selection, and\nreal-time field tests. The Python code for the algorithms and experiments is\navailable at https://github.com/WiseGamgee/HyperAD.\n","authors":["Samuel Garske","Bradley Evans","Christopher Artlett","KC Wong"],"pdf_url":"https://arxiv.org/pdf/2408.14947v1.pdf","comment":"10 pages, 9 figures, 3 tables, code and datasets accessible at\n  https://github.com/WiseGamgee/HyperAD"},{"id":"http://arxiv.org/abs/2408.14941v1","updated":"2024-08-27T10:26:05Z","published":"2024-08-27T10:26:05Z","title":"BOX3D: Lightweight Camera-LiDAR Fusion for 3D Object Detection and\n  Localization","summary":"  Object detection and global localization play a crucial role in robotics,\nspanning across a great spectrum of applications from autonomous cars to\nmulti-layered 3D Scene Graphs for semantic scene understanding. This article\nproposes BOX3D, a novel multi-modal and lightweight scheme for localizing\nobjects of interest by fusing the information from RGB camera and 3D LiDAR.\nBOX3D is structured around a three-layered architecture, building up from the\nlocal perception of the incoming sequential sensor data to the global\nperception refinement that covers for outliers and the general consistency of\neach object's observation. More specifically, the first layer handles the\nlow-level fusion of camera and LiDAR data for initial 3D bounding box\nextraction. The second layer converts each LiDAR's scan 3D bounding boxes to\nthe world coordinate frame and applies a spatial pairing and merging mechanism\nto maintain the uniqueness of objects observed from different viewpoints.\nFinally, BOX3D integrates the third layer that supervises the consistency of\nthe results on the global map iteratively, using a point-to-voxel comparison\nfor identifying all points in the global map that belong to the object.\nBenchmarking results of the proposed novel architecture are showcased in\nmultiple experimental trials on public state-of-the-art large-scale dataset of\nurban environments.\n","authors":["Mario A. V. Saucedo","Nikolaos Stathoulopoulos","Vidya Sumathy","Christoforos Kanellakis","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2408.14941v1.pdf","comment":"Presented in MED 2024"},{"id":"http://arxiv.org/abs/2408.14930v1","updated":"2024-08-27T10:09:17Z","published":"2024-08-27T10:09:17Z","title":"Cross-Modal Temporal Alignment for Event-guided Video Deblurring","summary":"  Video deblurring aims to enhance the quality of restored results in\nmotion-blurred videos by effectively gathering information from adjacent video\nframes to compensate for the insufficient data in a single blurred frame.\nHowever, when faced with consecutively severe motion blur situations,\nframe-based video deblurring methods often fail to find accurate temporal\ncorrespondence among neighboring video frames, leading to diminished\nperformance. To address this limitation, we aim to solve the video deblurring\ntask by leveraging an event camera with micro-second temporal resolution. To\nfully exploit the dense temporal resolution of the event camera, we propose two\nmodules: 1) Intra-frame feature enhancement operates within the exposure time\nof a single blurred frame, iteratively enhancing cross-modality features in a\nrecurrent manner to better utilize the rich temporal information of events, 2)\nInter-frame temporal feature alignment gathers valuable long-range temporal\ninformation to target frames, aggregating sharp features leveraging the\nadvantages of the events. In addition, we present a novel dataset composed of\nreal-world blurred RGB videos, corresponding sharp videos, and event data. This\ndataset serves as a valuable resource for evaluating event-guided deblurring\nmethods. We demonstrate that our proposed methods outperform state-of-the-art\nframe-based and event-based motion deblurring methods through extensive\nexperiments conducted on both synthetic and real-world deblurring datasets. The\ncode and dataset are available at https://github.com/intelpro/CMTA.\n","authors":["Taewoo Kim","Hoonhee Cho","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2408.14930v1.pdf","comment":"Accepted in ECCV2024"},{"id":"http://arxiv.org/abs/2408.14927v1","updated":"2024-08-27T10:01:58Z","published":"2024-08-27T10:01:58Z","title":"Automatic Detection of COVID-19 from Chest X-ray Images Using Deep\n  Learning Model","summary":"  The infectious disease caused by novel corona virus (2019-nCoV) has been\nwidely spreading since last year and has shaken the entire world. It has caused\nan unprecedented effect on daily life, global economy and public health. Hence\nthis disease detection has life-saving importance for both patients as well as\ndoctors. Due to limited test kits, it is also a daunting task to test every\npatient with severe respiratory problems using conventional techniques\n(RT-PCR). Thus implementing an automatic diagnosis system is urgently required\nto overcome the scarcity problem of Covid-19 test kits at hospital, health care\nsystems. The diagnostic approach is mainly classified into two\ncategories-laboratory based and Chest radiography approach. In this paper, a\nnovel approach for computerized corona virus (2019-nCoV) detection from lung\nx-ray images is presented. Here, we propose models using deep learning to show\nthe effectiveness of diagnostic systems. In the experimental result, we\nevaluate proposed models on publicly available data-set which exhibit\nsatisfactory performance and promising results compared with other previous\nexisting methods.\n","authors":["Alloy Das","Rohit Agarwal","Rituparna Singh","Arindam Chowdhury","Debashis Nandi"],"pdf_url":"https://arxiv.org/pdf/2408.14927v1.pdf","comment":"Accepted in AIP Conference Proceedings (Vol. 2424, No. 1)"},{"id":"http://arxiv.org/abs/2311.12084v2","updated":"2024-08-27T09:55:37Z","published":"2023-11-20T11:08:06Z","title":"ODDR: Outlier Detection & Dimension Reduction Based Defense Against\n  Adversarial Patches","summary":"  Adversarial attacks present a significant challenge to the dependable\ndeployment of machine learning models, with patch-based attacks being\nparticularly potent. These attacks introduce adversarial perturbations in\nlocalized regions of an image, deceiving even well-trained models. In this\npaper, we propose Outlier Detection and Dimension Reduction (ODDR), a\ncomprehensive defense strategy engineered to counteract patch-based adversarial\nattacks through advanced statistical methodologies. Our approach is based on\nthe observation that input features corresponding to adversarial\npatches-whether naturalistic or synthetic-deviate from the intrinsic\ndistribution of the remaining image data and can thus be identified as\noutliers. ODDR operates through a robust three-stage pipeline: Fragmentation,\nSegregation, and Neutralization. This model-agnostic framework is versatile,\noffering protection across various tasks, including image classification,\nobject detection, and depth estimation, and is proved effective in both\nCNN-based and Transformer-based architectures. In the Fragmentation stage,\nimage samples are divided into smaller segments, preparing them for the\nSegregation stage, where advanced outlier detection techniques isolate\nanomalous features linked to adversarial perturbations. The Neutralization\nstage then applies dimension reduction techniques to these outliers,\neffectively neutralizing the adversarial impact while preserving critical\ninformation for the machine learning task. Extensive evaluation on benchmark\ndatasets against state-of-the-art adversarial patches underscores the efficacy\nof ODDR. Our method enhances model accuracy from 39.26% to 79.1% under the\nGoogleAp attack, outperforming leading defenses such as LGS (53.86%), Jujutsu\n(60%), and Jedi (64.34%).\n","authors":["Nandish Chattopadhyay","Amira Guesmi","Muhammad Abdullah Hanif","Bassem Ouni","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2311.12084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14916v1","updated":"2024-08-27T09:44:54Z","published":"2024-08-27T09:44:54Z","title":"Towards Real-world Event-guided Low-light Video Enhancement and\n  Deblurring","summary":"  In low-light conditions, capturing videos with frame-based cameras often\nrequires long exposure times, resulting in motion blur and reduced visibility.\nWhile frame-based motion deblurring and low-light enhancement have been\nstudied, they still pose significant challenges. Event cameras have emerged as\na promising solution for improving image quality in low-light environments and\naddressing motion blur. They provide two key advantages: capturing scene\ndetails well even in low light due to their high dynamic range, and effectively\ncapturing motion information during long exposures due to their high temporal\nresolution. Despite efforts to tackle low-light enhancement and motion\ndeblurring using event cameras separately, previous work has not addressed both\nsimultaneously. To explore the joint task, we first establish real-world\ndatasets for event-guided low-light enhancement and deblurring using a hybrid\ncamera system based on beam splitters. Subsequently, we introduce an end-to-end\nframework to effectively handle these tasks. Our framework incorporates a\nmodule to efficiently leverage temporal information from events and frames.\nFurthermore, we propose a module to utilize cross-modal feature information to\nemploy a low-pass filter for noise suppression while enhancing the main\nstructural information. Our proposed method significantly outperforms existing\napproaches in addressing the joint task. Our project pages are available at\nhttps://github.com/intelpro/ELEDNet.\n","authors":["Taewoo Kim","Jaeseok Jeong","Hoonhee Cho","Yuhwan Jeong","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2408.14916v1.pdf","comment":"Accepted in ECCV2024"},{"id":"http://arxiv.org/abs/2408.14899v1","updated":"2024-08-27T09:23:18Z","published":"2024-08-27T09:23:18Z","title":"MeshUp: Multi-Target Mesh Deformation via Blended Score Distillation","summary":"  We propose MeshUp, a technique that deforms a 3D mesh towards multiple target\nconcepts, and intuitively controls the region where each concept is expressed.\nConveniently, the concepts can be defined as either text queries, e.g., \"a dog\"\nand \"a turtle,\" or inspirational images, and the local regions can be selected\nas any number of vertices on the mesh. We can effectively control the influence\nof the concepts and mix them together using a novel score distillation\napproach, referred to as the Blended Score Distillation (BSD). BSD operates on\neach attention layer of the denoising U-Net of a diffusion model as it extracts\nand injects the per-objective activations into a unified denoising pipeline\nfrom which the deformation gradients are calculated. To localize the expression\nof these activations, we create a probabilistic Region of Interest (ROI) map on\nthe surface of the mesh, and turn it into 3D-consistent masks that we use to\ncontrol the expression of these activations. We demonstrate the effectiveness\nof BSD empirically and show that it can deform various meshes towards multiple\nobjectives.\n","authors":["Hyunwoo Kim","Itai Lang","Noam Aigerman","Thibault Groueix","Vladimir G. Kim","Rana Hanocka"],"pdf_url":"https://arxiv.org/pdf/2408.14899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14895v1","updated":"2024-08-27T09:18:57Z","published":"2024-08-27T09:18:57Z","title":"VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view\n  Videos of Daily Activities","summary":"  Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data\n(e.g., images and videos) into symbols, have attracted attention as resources\nenabling knowledge processing and machine learning across modalities. However,\nthe construction of MMKGs for videos consisting of multiple events, such as\ndaily activities, is still in the early stages. In this paper, we construct an\nMMKG based on synchronized multi-view simulated videos of daily activities.\nBesides representing the content of daily life videos as event-centric\nknowledge, our MMKG also includes frame-by-frame fine-grained changes, such as\nbounding boxes within video frames. In addition, we provide support tools for\nquerying our MMKG. As an application example, we demonstrate that our MMKG\nfacilitates benchmarking vision-language models by providing the necessary\nvision-language datasets for a tailored task.\n","authors":["Shusaku Egami","Takahiro Ugai","Ken Fukuda"],"pdf_url":"https://arxiv.org/pdf/2408.14895v1.pdf","comment":"5 pages,4 figures, accepted by CIKM2024 Resource Track"},{"id":"http://arxiv.org/abs/2408.00591v2","updated":"2024-08-27T09:09:18Z","published":"2024-08-01T14:20:47Z","title":"Regional quality estimation for echocardiography using deep learning","summary":"  Automatic estimation of cardiac ultrasound image quality can be beneficial\nfor guiding operators and ensuring the accuracy of clinical measurements.\nPrevious work often fails to distinguish the view correctness of the\nechocardiogram from the image quality. Additionally, previous studies only\nprovide a global image quality value, which limits their practical utility. In\nthis work, we developed and compared three methods to estimate image quality:\n1) classic pixel-based metrics like the generalized contrast-to-noise ratio\n(gCNR) on myocardial segments as region of interest and left ventricle lumen as\nbackground, obtained using a U-Net segmentation 2) local image coherence\nderived from a U-Net model that predicts coherence from B-Mode images 3) a deep\nconvolutional network that predicts the quality of each region directly in an\nend-to-end fashion. We evaluate each method against manual regional image\nquality annotations by three experienced cardiologists. The results indicate\npoor performance of the gCNR metric, with Spearman correlation to the\nannotations of rho = 0.24. The end-to-end learning model obtains the best\nresult, rho = 0.69, comparable to the inter-observer correlation, rho = 0.63.\nFinally, the coherence-based method, with rho = 0.58, outperformed the\nclassical metrics and is more generic than the end-to-end approach.\n","authors":["Gilles Van De Vyver","Svein-Erik MÃ¥sÃ¸y","HÃ¥vard Dalen","BjÃ¸rnar Leangen Grenne","Espen Holte","Sindre Hellum Olaisen","John Nyberg","Andreas Ãstvik","Lasse LÃ¸vstakken","Erik Smistad"],"pdf_url":"https://arxiv.org/pdf/2408.00591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14879v1","updated":"2024-08-27T08:48:21Z","published":"2024-08-27T08:48:21Z","title":"Adversarial Manhole: Challenging Monocular Depth Estimation and Semantic\n  Segmentation Models with Patch Attack","summary":"  Monocular depth estimation (MDE) and semantic segmentation (SS) are crucial\nfor the navigation and environmental interpretation of many autonomous driving\nsystems. However, their vulnerability to practical adversarial attacks is a\nsignificant concern. This paper presents a novel adversarial attack using\npractical patches that mimic manhole covers to deceive MDE and SS models. The\ngoal is to cause these systems to misinterpret scenes, leading to false\ndetections of near obstacles or non-passable objects. We use Depth Planar\nMapping to precisely position these patches on road surfaces, enhancing the\nattack's effectiveness. Our experiments show that these adversarial patches\ncause a 43% relative error in MDE and achieve a 96% attack success rate in SS.\nThese patches create affected error regions over twice their size in MDE and\napproximately equal to their size in SS. Our studies also confirm the patch's\neffectiveness in physical simulations, the adaptability of the patches across\ndifferent target models, and the effectiveness of our proposed modules,\nhighlighting their practical implications.\n","authors":["Naufal Suryanto","Andro Aprila Adiputra","Ahmada Yusril Kadiptya","Yongsu Kim","Howon Kim"],"pdf_url":"https://arxiv.org/pdf/2408.14879v1.pdf","comment":"Accepted for WISA 2024. Code and dataset:\n  https://github.com/naufalso/adversarial-manhole"},{"id":"http://arxiv.org/abs/2408.14868v1","updated":"2024-08-27T08:39:47Z","published":"2024-08-27T08:39:47Z","title":"ZeroMamba: Exploring Visual State Space Model for Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) aims to recognize unseen classes by transferring\nsemantic knowledge from seen classes to unseen ones, guided by semantic\ninformation. To this end, existing works have demonstrated remarkable\nperformance by utilizing global visual features from Convolutional Neural\nNetworks (CNNs) or Vision Transformers (ViTs) for visual-semantic interactions.\nDue to the limited receptive fields of CNNs and the quadratic complexity of\nViTs, however, these visual backbones achieve suboptimal visual-semantic\ninteractions. In this paper, motivated by the visual state space model (i.e.,\nVision Mamba), which is capable of capturing long-range dependencies and\nmodeling complex visual dynamics, we propose a parameter-efficient ZSL\nframework called ZeroMamba to advance ZSL. Our ZeroMamba comprises three key\ncomponents: Semantic-aware Local Projection (SLP), Global Representation\nLearning (GRL), and Semantic Fusion (SeF). Specifically, SLP integrates\nsemantic embeddings to map visual features to local semantic-related\nrepresentations, while GRL encourages the model to learn global semantic\nrepresentations. SeF combines these two semantic representations to enhance the\ndiscriminability of semantic features. We incorporate these designs into Vision\nMamba, forming an end-to-end ZSL framework. As a result, the learned semantic\nrepresentations are better suited for classification. Through extensive\nexperiments on four prominent ZSL benchmarks, ZeroMamba demonstrates superior\nperformance, significantly outperforming the state-of-the-art (i.e., CNN-based\nand ViT-based) methods under both conventional ZSL (CZSL) and generalized ZSL\n(GZSL) settings. Code is available at:\nhttps://anonymous.4open.science/r/ZeroMamba.\n","authors":["Wenjin Hou","Dingjie Fu","Kun Li","Shiming Chen","Hehe Fan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2408.14868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00737v2","updated":"2024-08-27T08:33:31Z","published":"2024-06-30T15:50:32Z","title":"LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image\n  Generation","summary":"  Diffusion models have exhibited substantial success in text-to-image\ngeneration. However, they often encounter challenges when dealing with complex\nand dense prompts involving multiple objects, attribute binding, and long\ndescriptions. In this paper, we propose a novel framework called\n\\textbf{LLM4GEN}, which enhances the semantic understanding of text-to-image\ndiffusion models by leveraging the representation of Large Language Models\n(LLMs). It can be seamlessly incorporated into various diffusion models as a\nplug-and-play component. A specially designed Cross-Adapter Module (CAM)\nintegrates the original text features of text-to-image models with LLM\nfeatures, thereby enhancing text-to-image generation. Additionally, to\nfacilitate and correct entity-attribute relationships in text prompts, we\ndevelop an entity-guided regularization loss to further improve generation\nperformance. We also introduce DensePrompts, which contains $7,000$ dense\nprompts to provide a comprehensive evaluation for the text-to-image generation\ntask. Experiments indicate that LLM4GEN significantly improves the semantic\nalignment of SD1.5 and SDXL, demonstrating increases of 9.69\\% and 12.90\\% in\ncolor on T2I-CompBench, respectively. Moreover, it surpasses existing models in\nterms of sample quality, image-text alignment, and human evaluation.\n","authors":["Mushui Liu","Yuhang Ma","Yang Zhen","Jun Dan","Yunlong Yu","Zeng Zhao","Zhipeng Hu","Bai Liu","Changjie Fan"],"pdf_url":"https://arxiv.org/pdf/2407.00737v2.pdf","comment":"11 pages, 13 figures"},{"id":"http://arxiv.org/abs/2408.14860v1","updated":"2024-08-27T08:28:01Z","published":"2024-08-27T08:28:01Z","title":"DiffSurf: A Transformer-based Diffusion Model for Generating and\n  Reconstructing 3D Surfaces in Pose","summary":"  This paper presents DiffSurf, a transformer-based denoising diffusion model\nfor generating and reconstructing 3D surfaces. Specifically, we design a\ndiffusion transformer architecture that predicts noise from noisy 3D surface\nvertices and normals. With this architecture, DiffSurf is able to generate 3D\nsurfaces in various poses and shapes, such as human bodies, hands, animals and\nman-made objects. Further, DiffSurf is versatile in that it can address various\n3D downstream tasks including morphing, body shape variation and 3D human mesh\nfitting to 2D keypoints. Experimental results on 3D human model benchmarks\ndemonstrate that DiffSurf can generate shapes with greater diversity and higher\nquality than previous generative models. Furthermore, when applied to the task\nof single-image 3D human mesh recovery, DiffSurf achieves accuracy comparable\nto prior techniques at a near real-time rate.\n","authors":["Yusuke Yoshiyasu","Leyuan Sun"],"pdf_url":"https://arxiv.org/pdf/2408.14860v1.pdf","comment":"Accepted at ECCV2024"},{"id":"http://arxiv.org/abs/2405.18911v2","updated":"2024-08-27T08:22:54Z","published":"2024-05-29T09:13:30Z","title":"Exploring Human-in-the-Loop Test-Time Adaptation by Synergizing Active\n  Learning and Model Selection","summary":"  Existing test-time adaptation (TTA) approaches often adapt models with the\nunlabeled testing data stream. A recent attempt relaxed the assumption by\nintroducing limited human annotation, referred to as Human-In-the-Loop\nTest-Time Adaptation (HILTTA) in this study. The focus of existing HILTTA\nstudies lies in selecting the most informative samples to label, a.k.a. active\nlearning. In this work, we are motivated by a pitfall of TTA, i.e. sensitivity\nto hyper-parameters, and propose to approach HILTTA by synergizing active\nlearning and model selection. Specifically, we first select samples for human\nannotation (active learning) and then use the labeled data to select optimal\nhyper-parameters (model selection). To prevent the model selection process from\noverfitting to local distributions, multiple regularization techniques are\nemployed to complement the validation objective. A sample selection strategy is\nfurther tailored by considering the balance between active learning and model\nselection purposes. We demonstrate on 5 TTA datasets that the proposed HILTTA\napproach is compatible with off-the-shelf TTA methods and such combinations\nsubstantially outperform the state-of-the-art HILTTA methods. Importantly, our\nproposed method can always prevent choosing the worst hyper-parameters on all\noff-the-shelf TTA methods. The source code will be released upon publication.\n","authors":["Yushu Li","Yongyi Su","Xulei Yang","Kui Jia","Xun Xu"],"pdf_url":"https://arxiv.org/pdf/2405.18911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09126v2","updated":"2024-08-27T08:19:22Z","published":"2024-08-17T07:27:14Z","title":"Barbie: Text to Barbie-Style 3D Avatars","summary":"  Recent advances in text-guided 3D avatar generation have made substantial\nprogress by distilling knowledge from diffusion models. Despite the plausible\ngenerated appearance, existing methods cannot achieve fine-grained\ndisentanglement or high-fidelity modeling between inner body and outfit. In\nthis paper, we propose Barbie, a novel framework for generating 3D avatars that\ncan be dressed in diverse and high-quality Barbie-like garments and\naccessories. Instead of relying on a holistic model, Barbie achieves\nfine-grained disentanglement on avatars by semantic-aligned separated models\nfor human body and outfits. These disentangled 3D representations are then\noptimized by different expert models to guarantee the domain-specific fidelity.\nTo balance geometry diversity and reasonableness, we propose a series of losses\nfor template-preserving and human-prior evolving. The final avatar is enhanced\nby unified texture refinement for superior texture consistency. Extensive\nexperiments demonstrate that Barbie outperforms existing methods in both\ndressed human and outfit generation, supporting flexible apparel combination\nand animation. The code will be released for research purposes. Our project\npage is: https://xiaokunsun.github.io/Barbie.github.io/.\n","authors":["Xiaokun Sun","Zhenyu Zhang","Ying Tai","Qian Wang","Hao Tang","Zili Yi","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2408.09126v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.21705v2","updated":"2024-08-27T08:14:16Z","published":"2024-07-31T15:53:20Z","title":"Tora: Trajectory-oriented Diffusion Transformer for Video Generation","summary":"  Recent advancements in Diffusion Transformer (DiT) have demonstrated\nremarkable proficiency in producing high-quality video content. Nonetheless,\nthe potential of transformer-based diffusion models for effectively generating\nvideos with controllable motion remains an area of limited exploration. This\npaper introduces Tora, the first trajectory-oriented DiT framework that\nconcurrently integrates textual, visual, and trajectory conditions, thereby\nenabling scalable video generation with effective motion guidance.\nSpecifically, Tora consists of a Trajectory Extractor(TE), a Spatial-Temporal\nDiT, and a Motion-guidance Fuser(MGF). The TE encodes arbitrary trajectories\ninto hierarchical spacetime motion patches with a 3D video compression network.\nThe MGF integrates the motion patches into the DiT blocks to generate\nconsistent videos that accurately follow designated trajectories. Our design\naligns seamlessly with DiT's scalability, allowing precise control of video\ncontent's dynamics with diverse durations, aspect ratios, and resolutions.\nExtensive experiments demonstrate Tora's excellence in achieving high motion\nfidelity, while also meticulously simulating the intricate movement of the\nphysical world.\n","authors":["Zhenghao Zhang","Junchao Liao","Menghao Li","Zuozhuo Dai","Bingxue Qiu","Siyu Zhu","Long Qin","Weizhi Wang"],"pdf_url":"https://arxiv.org/pdf/2407.21705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16580v4","updated":"2024-08-27T08:13:01Z","published":"2023-05-26T02:09:48Z","title":"TFDet: Target-Aware Fusion for RGB-T Pedestrian Detection","summary":"  Pedestrian detection plays a critical role in computer vision as it\ncontributes to ensuring traffic safety. Existing methods that rely solely on\nRGB images suffer from performance degradation under low-light conditions due\nto the lack of useful information. To address this issue, recent multispectral\ndetection approaches have combined thermal images to provide complementary\ninformation and have obtained enhanced performances. Nevertheless, few\napproaches focus on the negative effects of false positives caused by noisy\nfused feature maps. Different from them, we comprehensively analyze the impacts\nof false positives on the detection performance and find that enhancing feature\ncontrast can significantly reduce these false positives. In this paper, we\npropose a novel target-aware fusion strategy for multispectral pedestrian\ndetection, named TFDet. TFDet achieves state-of-the-art performance on two\nmultispectral pedestrian benchmarks, KAIST and LLVIP. TFDet can easily extend\nto multi-class object detection scenarios. It outperforms the previous best\napproaches on two multispectral object detection benchmarks, FLIR and M3FD.\nImportantly, TFDet has comparable inference efficiency to the previous\napproaches, and has remarkably good detection performance even under low-light\nconditions, which is a significant advancement for ensuring road safety.\n","authors":["Xue Zhang","Xiaohan Zhang","Jiangtao Wang","Jiacheng Ying","Zehua Sheng","Heng Yu","Chunguang Li","Hui-Liang Shen"],"pdf_url":"https://arxiv.org/pdf/2305.16580v4.pdf","comment":"This paper has been accepted by IEEE T-NNLS journal. Please jump to\n  External DOI to view the official version"},{"id":"http://arxiv.org/abs/2408.13766v2","updated":"2024-08-27T08:07:20Z","published":"2024-08-25T08:23:06Z","title":"Enhancing Robustness of Human Detection Algorithms in Maritime SAR\n  through Augmented Aerial Images to Simulate Weather Conditions","summary":"  7,651 cases of Search and Rescue Missions (SAR) were reported by the United\nStates Coast Guard in 2024, with over 1322 SAR helicopters deployed in the 6\nfirst months alone. Through the utilizations of YOLO, we were able to run\ndifferent weather conditions and lighting from our augmented dataset for\ntraining. YOLO then utilizes CNNs to apply a series of convolutions and pooling\nlayers to the input image, where the convolution layers are able to extract the\nmain features of the image. Through this, our YOLO model is able to learn to\ndifferentiate different objects which may considerably improve its accuracy,\npossibly enhancing the efficiency of SAR operations through enhanced detection\naccuracy. This paper aims to improve the model's accuracy of human detection in\nmaritime SAR by evaluating a robust datasets containing various elevations and\ngeological locations, as well as through data augmentation which simulates\ndifferent weather and lighting. We observed that models trained on augmented\ndatasets outperformed their non-augmented counterparts in which the human\nrecall scores ranged from 0.891 to 0.911 with an improvement rate of 3.4\\% on\nthe YOLOv5l model. Results showed that these models demonstrate greater\nrobustness to real-world conditions in varying of weather, brightness, tint,\nand contrast.\n","authors":["Miguel Tjia","Artem Kim","Elaine Wynette Wijaya","Hanna Tefara","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.13766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19698v2","updated":"2024-08-27T08:06:38Z","published":"2024-07-29T04:43:58Z","title":"Classification Matters: Improving Video Action Detection with\n  Class-Specific Attention","summary":"  Video action detection (VAD) aims to detect actors and classify their actions\nin a video. We figure that VAD suffers more from classification rather than\nlocalization of actors. Hence, we analyze how prevailing methods form features\nfor classification and find that they prioritize actor regions, yet often\noverlooking the essential contextual information necessary for accurate\nclassification. Accordingly, we propose to reduce the bias toward actor and\nencourage paying attention to the context that is relevant to each action\nclass. By assigning a class-dedicated query to each action class, our model can\ndynamically determine where to focus for effective classification. The proposed\nmodel demonstrates superior performance on three challenging benchmarks with\nsignificantly fewer parameters and less computation.\n","authors":["Jinsung Lee","Taeoh Kim","Inwoong Lee","Minho Shim","Dongyoon Wee","Minsu Cho","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.19698v2.pdf","comment":"31 pages, accepted to ECCV 2024 (oral)"},{"id":"http://arxiv.org/abs/2408.14847v1","updated":"2024-08-27T07:58:08Z","published":"2024-08-27T07:58:08Z","title":"Intraoperative Glioma Segmentation with YOLO + SAM for Improved Accuracy\n  in Tumor Resection","summary":"  Gliomas, a common type of malignant brain tumor, present significant surgical\nchallenges due to their similarity to healthy tissue. Preoperative Magnetic\nResonance Imaging (MRI) images are often ineffective during surgery due to\nfactors such as brain shift, which alters the position of brain structures and\ntumors. This makes real-time intraoperative MRI (ioMRI) crucial, as it provides\nupdated imaging that accounts for these shifts, ensuring more accurate tumor\nlocalization and safer resections. This paper presents a deep learning pipeline\ncombining You Only Look Once Version 8 (YOLOv8) and Segment Anything Model\nVision Transformer-base (SAM ViT-b) to enhance glioma detection and\nsegmentation during ioMRI. Our model was trained using the Brain Tumor\nSegmentation 2021 (BraTS 2021) dataset, which includes standard magnetic\nresonance imaging (MRI) images, and noise-augmented MRI images that simulate\nioMRI images. Noised MRI images are harder for a deep learning pipeline to\nsegment, but they are more representative of surgical conditions. Achieving a\nDice Similarity Coefficient (DICE) score of 0.79, our model performs comparably\nto state-of-the-art segmentation models tested on noiseless data. This\nperformance demonstrates the model's potential to assist surgeons in maximizing\ntumor resection and improving surgical outcomes.\n","authors":["Samir Kassam","Angelo Markham","Katie Vo","Yashas Revanakara","Michael Lam","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.14847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14846v1","updated":"2024-08-27T07:57:58Z","published":"2024-08-27T07:57:58Z","title":"Diffusion-Occ: 3D Point Cloud Completion via Occupancy Diffusion","summary":"  Point clouds are crucial for capturing three-dimensional data but often\nsuffer from incompleteness due to limitations such as resolution and occlusion.\nTraditional methods typically rely on point-based approaches within\ndiscriminative frameworks for point cloud completion. In this paper, we\nintroduce \\textbf{Diffusion-Occ}, a novel framework for Diffusion Point Cloud\nCompletion. Diffusion-Occ utilizes a two-stage coarse-to-fine approach. In the\nfirst stage, the Coarse Density Voxel Prediction Network (CDNet) processes\npartial points to predict coarse density voxels, streamlining global feature\nextraction through voxel classification, as opposed to previous\nregression-based methods. In the second stage, we introduce the Occupancy\nGeneration Network (OccGen), a conditional occupancy diffusion model based on a\ntransformer architecture and enhanced by our Point-Voxel Fuse (PVF) block. This\nblock integrates coarse density voxels with partial points to leverage both\nglobal and local features for comprehensive completion. By thresholding the\noccupancy field, we convert it into a complete point cloud. Additionally, our\nmethod employs diverse training mixtures and efficient diffusion\nparameterization to enable effective one-step sampling during both training and\ninference. Experimental results demonstrate that Diffusion-Occ outperforms\nexisting discriminative and generative methods.\n","authors":["Guoqing Zhang","Jian Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14842v1","updated":"2024-08-27T07:54:01Z","published":"2024-08-27T07:54:01Z","title":"From Bias to Balance: Detecting Facial Expression Recognition Biases in\n  Large Multimodal Foundation Models","summary":"  This study addresses the racial biases in facial expression recognition (FER)\nsystems within Large Multimodal Foundation Models (LMFMs). Despite advances in\ndeep learning and the availability of diverse datasets, FER systems often\nexhibit higher error rates for individuals with darker skin tones. Existing\nresearch predominantly focuses on traditional FER models (CNNs, RNNs, ViTs),\nleaving a gap in understanding racial biases in LMFMs. We benchmark four\nleading LMFMs: GPT-4o, PaliGemma, Gemini, and CLIP to assess their performance\nin facial emotion detection across different racial demographics. A linear\nclassifier trained on CLIP embeddings obtains accuracies of 95.9\\% for RADIATE,\n90.3\\% for Tarr, and 99.5\\% for Chicago Face. Furthermore, we identify that\nAnger is misclassified as Disgust 2.1 times more often in Black Females than\nWhite Females. This study highlights the need for fairer FER systems and\nestablishes a foundation for developing unbiased, accurate FER technologies.\nVisit https://kvjvhub.github.io/FERRacialBias/ for further information\nregarding the biases within facial expression recognition.\n","authors":["Kaylee Chhua","Zhoujinyi Wen","Vedant Hathalia","Kevin Zhu","Sean O'Brien"],"pdf_url":"https://arxiv.org/pdf/2408.14842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14841v1","updated":"2024-08-27T07:52:44Z","published":"2024-08-27T07:52:44Z","title":"Diffusion based Semantic Outlier Generation via Nuisance Awareness for\n  Out-of-Distribution Detection","summary":"  Out-of-distribution (OOD) detection, which determines whether a given sample\nis part of the in-distribution (ID), has recently shown promising results\nthrough training with synthetic OOD datasets. Nonetheless, existing methods\noften produce outliers that are considerably distant from the ID, showing\nlimited efficacy for capturing subtle distinctions between ID and OOD. To\naddress these issues, we propose a novel framework, Semantic Outlier generation\nvia Nuisance Awareness (SONA), which notably produces challenging outliers by\ndirectly leveraging pixel-space ID samples through diffusion models. Our\napproach incorporates SONA guidance, providing separate control over semantic\nand nuisance regions of ID samples. Thereby, the generated outliers achieve two\ncrucial properties: (i) they present explicit semantic-discrepant information,\nwhile (ii) maintaining various levels of nuisance resemblance with ID.\nFurthermore, the improved OOD detector training with SONA outliers facilitates\nlearning with a focus on semantic distinctions. Extensive experiments\ndemonstrate the effectiveness of our framework, achieving an impressive AUROC\nof 88% on near-OOD datasets, which surpasses the performance of baseline\nmethods by a significant margin of approximately 6%.\n","authors":["Suhee Yoon","Sanghyu Yoon","Hankook Lee","Ye Seul Sim","Sungik Choi","Kyungeun Lee","Hye-Seung Cho","Woohyung Lim"],"pdf_url":"https://arxiv.org/pdf/2408.14841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14837v1","updated":"2024-08-27T07:46:07Z","published":"2024-08-27T07:46:07Z","title":"Diffusion Models Are Real-Time Game Engines","summary":"  We present GameNGen, the first game engine powered entirely by a neural model\nthat enables real-time interaction with a complex environment over long\ntrajectories at high quality. GameNGen can interactively simulate the classic\ngame DOOM at over 20 frames per second on a single TPU. Next frame prediction\nachieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are\nonly slightly better than random chance at distinguishing short clips of the\ngame from clips of the simulation. GameNGen is trained in two phases: (1) an\nRL-agent learns to play the game and the training sessions are recorded, and\n(2) a diffusion model is trained to produce the next frame, conditioned on the\nsequence of past frames and actions. Conditioning augmentations enable stable\nauto-regressive generation over long trajectories.\n","authors":["Dani Valevski","Yaniv Leviathan","Moab Arar","Shlomi Fruchter"],"pdf_url":"https://arxiv.org/pdf/2408.14837v1.pdf","comment":"Project page: https://gamengen.github.io/"},{"id":"http://arxiv.org/abs/2407.16232v2","updated":"2024-08-27T07:31:37Z","published":"2024-07-23T07:17:10Z","title":"Channel-Partitioned Windowed Attention And Frequency Learning for Single\n  Image Super-Resolution","summary":"  Recently, window-based attention methods have shown great potential for\ncomputer vision tasks, particularly in Single Image Super-Resolution (SISR).\nHowever, it may fall short in capturing long-range dependencies and\nrelationships between distant tokens. Additionally, we find that learning on\nspatial domain does not convey the frequency content of the image, which is a\ncrucial aspect in SISR. To tackle these issues, we propose a new\nChannel-Partitioned Attention Transformer (CPAT) to better capture long-range\ndependencies by sequentially expanding windows along the height and width of\nfeature maps. In addition, we propose a novel Spatial-Frequency Interaction\nModule (SFIM), which incorporates information from spatial and frequency\ndomains to provide a more comprehensive information from feature maps. This\nincludes information about the frequency content and enhances the receptive\nfield across the entire image. Experimental findings show the effectiveness of\nour proposed modules and architecture. In particular, CPAT surpasses current\nstate-of-the-art methods by up to 0.31dB at x2 SR on Urban100.\n","authors":["Dinh Phu Tran","Dao Duy Hung","Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2407.16232v2.pdf","comment":"Camera ready version, BMVC 2024"},{"id":"http://arxiv.org/abs/2408.14829v1","updated":"2024-08-27T07:26:10Z","published":"2024-08-27T07:26:10Z","title":"Time-Aware Face Anti-Spoofing with Rotation Invariant Local Binary\n  Patterns and Deep Learning","summary":"  Facial recognition systems have become an integral part of the modern world.\nThese methods accomplish the task of human identification in an automatic,\nfast, and non-interfering way. Past research has uncovered high vulnerability\nto simple imitation attacks that could lead to erroneous identification and\nsubsequent authentication of attackers. Similar to face recognition, imitation\nattacks can also be detected with Machine Learning. Attack detection systems\nuse a variety of facial features and advanced machine learning models for\nuncovering the presence of attacks. In this work, we assess existing work on\nliveness detection and propose a novel approach that promises high\nclassification accuracy by combining previously unused features with time-aware\ndeep learning strategies.\n","authors":["Moritz Finke","Alexandra Dmitrienko"],"pdf_url":"https://arxiv.org/pdf/2408.14829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12017v3","updated":"2024-08-27T07:23:22Z","published":"2023-08-23T09:20:05Z","title":"Distribution-Aware Calibration for Object Detection with Noisy Bounding\n  Boxes","summary":"  Large-scale well-annotated datasets are of great importance for training an\neffective object detector. However, obtaining accurate bounding box annotations\nis laborious and demanding. Unfortunately, the resultant noisy bounding boxes\ncould cause corrupt supervision signals and thus diminish detection\nperformance. Motivated by the observation that the real ground-truth is usually\nsituated in the aggregation region of the proposals assigned to a noisy\nground-truth, we propose DIStribution-aware CalibratiOn (DISCO) to model the\nspatial distribution of proposals for calibrating supervision signals. In\nDISCO, spatial distribution modeling is performed to statistically extract the\npotential locations of objects. Based on the modeled distribution, three\ndistribution-aware techniques, i.e., distribution-aware proposal augmentation\n(DA-Aug), distribution-aware box refinement (DA-Ref), and distribution-aware\nconfidence estimation (DA-Est), are developed to improve classification,\nlocalization, and interpretability, respectively. Extensive experiments on\nlarge-scale noisy image datasets (i.e., Pascal VOC and MS-COCO) demonstrate\nthat DISCO can achieve state-of-the-art detection performance, especially at\nhigh noise levels. Code is available at https://github.com/Correr-Zhou/DISCO.\n","authors":["Donghao Zhou","Jialin Li","Jinpeng Li","Jiancheng Huang","Qiang Nie","Yong Liu","Bin-Bin Gao","Qiong Wang","Pheng-Ann Heng","Guangyong Chen"],"pdf_url":"https://arxiv.org/pdf/2308.12017v3.pdf","comment":"Accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2408.11413v2","updated":"2024-08-27T07:21:02Z","published":"2024-08-21T08:19:12Z","title":"Pano2Room: Novel View Synthesis from a Single Indoor Panorama","summary":"  Recent single-view 3D generative methods have made significant advancements\nby leveraging knowledge distilled from extensive 3D object datasets. However,\nchallenges persist in the synthesis of 3D scenes from a single view, primarily\ndue to the complexity of real-world environments and the limited availability\nof high-quality prior resources. In this paper, we introduce a novel approach\ncalled Pano2Room, designed to automatically reconstruct high-quality 3D indoor\nscenes from a single panoramic image. These panoramic images can be easily\ngenerated using a panoramic RGBD inpainter from captures at a single location\nwith any camera. The key idea is to initially construct a preliminary mesh from\nthe input panorama, and iteratively refine this mesh using a panoramic RGBD\ninpainter while collecting photo-realistic 3D-consistent pseudo novel views.\nFinally, the refined mesh is converted into a 3D Gaussian Splatting field and\ntrained with the collected pseudo novel views. This pipeline enables the\nreconstruction of real-world 3D scenes, even in the presence of large\nocclusions, and facilitates the synthesis of photo-realistic novel views with\ndetailed geometry. Extensive qualitative and quantitative experiments have been\nconducted to validate the superiority of our method in single-panorama indoor\nnovel synthesis compared to the state-of-the-art. Our code and data are\navailable at \\url{https://github.com/TrickyGo/Pano2Room}.\n","authors":["Guo Pu","Yiming Zhao","Zhouhui Lian"],"pdf_url":"https://arxiv.org/pdf/2408.11413v2.pdf","comment":"SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers '24),\n  December 3--6, 2024, Tokyo, Japan"},{"id":"http://arxiv.org/abs/2408.14826v1","updated":"2024-08-27T07:13:44Z","published":"2024-08-27T07:13:44Z","title":"Alfie: Democratising RGBA Image Generation With No $$$","summary":"  Designs and artworks are ubiquitous across various creative fields, requiring\ngraphic design skills and dedicated software to create compositions that\ninclude many graphical elements, such as logos, icons, symbols, and art scenes,\nwhich are integral to visual storytelling. Automating the generation of such\nvisual elements improves graphic designers' productivity, democratizes and\ninnovates the creative industry, and helps generate more realistic synthetic\ndata for related tasks. These illustration elements are mostly RGBA images with\nirregular shapes and cutouts, facilitating blending and scene composition.\nHowever, most image generation models are incapable of generating such images\nand achieving this capability requires expensive computational resources,\nspecific training recipes, or post-processing solutions. In this work, we\npropose a fully-automated approach for obtaining RGBA illustrations by\nmodifying the inference-time behavior of a pre-trained Diffusion Transformer\nmodel, exploiting the prompt-guided controllability and visual quality offered\nby such models with no additional computational cost. We force the generation\nof entire subjects without sharp croppings, whose background is easily removed\nfor seamless integration into design projects or artistic scenes. We show with\na user study that, in most cases, users prefer our solution over generating and\nthen matting an image, and we show that our generated illustrations yield good\nresults when used as inputs for composite scene generation pipelines. We\nrelease the code at https://github.com/aimagelab/Alfie.\n","authors":["Fabio Quattrini","Vittorio Pippi","Silvia Cascianelli","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2408.14826v1.pdf","comment":"Accepted at ECCV AI for Visual Arts Workshop and Challenges"},{"id":"http://arxiv.org/abs/2408.13423v2","updated":"2024-08-27T07:12:52Z","published":"2024-08-24T01:33:28Z","title":"Training-free Long Video Generation with Chain of Diffusion Model\n  Experts","summary":"  Video generation models hold substantial potential in areas such as\nfilmmaking. However, current video diffusion models need high computational\ncosts and produce suboptimal results due to high complexity of video generation\ntask. In this paper, we propose \\textbf{ConFiner}, an efficient high-quality\nvideo generation framework that decouples video generation into easier\nsubtasks: structure \\textbf{con}trol and spatial-temporal re\\textbf{fine}ment.\nIt can generate high-quality videos with chain of off-the-shelf diffusion model\nexperts, each expert responsible for a decoupled subtask. During the\nrefinement, we introduce coordinated denoising, which can merge multiple\ndiffusion experts' capabilities into a single sampling. Furthermore, we design\nConFiner-Long framework, which can generate long coherent video with three\nconstraint strategies on ConFiner. Experimental results indicate that with only\n10\\% of the inference cost, our ConFiner surpasses representative models like\nLavie and Modelscope across all objective and subjective metrics. And\nConFiner-Long can generate high-quality and coherent videos with up to 600\nframes.\n","authors":["Wenhao Li","Yichao Cao","Xiu Su","Xi Lin","Shan You","Mingkai Zheng","Yi Chen","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2408.13423v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14825v1","updated":"2024-08-27T07:11:45Z","published":"2024-08-27T07:11:45Z","title":"From Rule-Based Models to Deep Learning Transformers Architectures for\n  Natural Language Processing and Sign Language Translation Systems: Survey,\n  Taxonomy and Performance Evaluation","summary":"  With the growing Deaf and Hard of Hearing population worldwide and the\npersistent shortage of certified sign language interpreters, there is a\npressing need for an efficient, signs-driven, integrated end-to-end translation\nsystem, from sign to gloss to text and vice-versa. There has been a wealth of\nresearch on machine translations and related reviews. However, there are few\nworks on sign language machine translation considering the particularity of the\nlanguage being continuous and dynamic. This paper aims to address this void,\nproviding a retrospective analysis of the temporal evolution of sign language\nmachine translation algorithms and a taxonomy of the Transformers\narchitectures, the most used approach in language translation. We also present\nthe requirements of a real-time Quality-of-Service sign language ma-chine\ntranslation system underpinned by accurate deep learning algorithms. We propose\nfuture research directions for sign language translation systems.\n","authors":["Nada Shahin","Leila Ismail"],"pdf_url":"https://arxiv.org/pdf/2408.14825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14823v1","updated":"2024-08-27T07:06:49Z","published":"2024-08-27T07:06:49Z","title":"LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive\n  Streaming","summary":"  The rise of Extended Reality (XR) requires efficient streaming of 3D online\nworlds, challenging current 3DGS representations to adapt to\nbandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS\nthat supports adaptive streaming and progressive rendering. Our method\nconstructs a layered structure for cumulative representation, incorporates\ndynamic opacity optimization to maintain visual fidelity, and utilizes\noccupancy maps to efficiently manage Gaussian splats. This proposed model\noffers a progressive representation supporting a continuous rendering quality\nadapted for bandwidth-aware streaming. Extensive experiments validate the\neffectiveness of our approach in balancing visual fidelity with the compactness\nof the model, with up to 50.71% improvement in SSIM, 286.53% improvement in\nLPIPS, and 318.41% reduction in model size, and shows its potential for\nbandwidth-adapted 3D streaming and rendering applications.\n","authors":["Yuang Shi","Simone Gasparini","GÃ©raldine Morin","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2408.14823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09193v2","updated":"2024-08-27T07:02:07Z","published":"2024-04-14T09:01:26Z","title":"FaceCat: Enhancing Face Recognition Security with a Unified Diffusion\n  Model","summary":"  Face anti-spoofing (FAS) and adversarial detection (FAD) have been regarded\nas critical technologies to ensure the safety of face recognition systems.\nHowever, due to limited practicality, complex deployment, and the additional\ncomputational overhead, it is necessary to implement both detection techniques\nwithin a unified framework. This paper aims to achieve this goal by breaking\nthrough two primary obstacles: 1) the suboptimal face feature representation\nand 2) the scarcity of training data. To address the limited performance caused\nby existing feature representations, motivated by the rich structural and\ndetailed features of face diffusion models, we propose FaceCat, the first\napproach leveraging the diffusion model to simultaneously enhance the\nperformance of FAS and FAD. Specifically, FaceCat elaborately designs a\nhierarchical fusion mechanism to capture rich face semantic features of the\ndiffusion model. These features then serve as a robust foundation for a\nlightweight head, designed to execute FAS and FAD simultaneously. Due to the\nlimitations in feature representation that arise from relying solely on\nsingle-modality image data, we further propose a novel text-guided multi-modal\nalignment strategy that utilizes text prompts to enrich feature representation,\nthereby enhancing performance. To combat data scarcity, we build a\ncomprehensive dataset with a wide range of 28 attack types, offering greater\npotential for a unified framework in facial security. Extensive experiments\nvalidate the effectiveness of FaceCat generalizes significantly better and\nobtains excellent robustness against common input transformations.\n","authors":["Jiawei Chen","Xiao Yang","Yinpeng Dong","Hang Su","Zhaoxia Yin"],"pdf_url":"https://arxiv.org/pdf/2404.09193v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2408.14819v1","updated":"2024-08-27T07:01:56Z","published":"2024-08-27T07:01:56Z","title":"Build-A-Scene: Interactive 3D Layout Control for Diffusion-Based Image\n  Generation","summary":"  We propose a diffusion-based approach for Text-to-Image (T2I) generation with\ninteractive 3D layout control. Layout control has been widely studied to\nalleviate the shortcomings of T2I diffusion models in understanding objects'\nplacement and relationships from text descriptions. Nevertheless, existing\napproaches for layout control are limited to 2D layouts, require the user to\nprovide a static layout beforehand, and fail to preserve generated images under\nlayout changes. This makes these approaches unsuitable for applications that\nrequire 3D object-wise control and iterative refinements, e.g., interior design\nand complex scene generation. To this end, we leverage the recent advancements\nin depth-conditioned T2I models and propose a novel approach for interactive 3D\nlayout control. We replace the traditional 2D boxes used in layout control with\n3D boxes. Furthermore, we revamp the T2I task as a multi-stage generation\nprocess, where at each stage, the user can insert, change, and move an object\nin 3D while preserving objects from earlier stages. We achieve this through our\nproposed Dynamic Self-Attention (DSA) module and the consistent 3D object\ntranslation strategy. Experiments show that our approach can generate\ncomplicated scenes based on 3D layouts, boosting the object generation success\nrate over the standard depth-conditioned T2I methods by 2x. Moreover, it\noutperforms other methods in comparison in preserving objects under layout\nchanges. Project Page: \\url{https://abdo-eldesokey.github.io/build-a-scene/}\n","authors":["Abdelrahman Eldesokey","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2408.14819v1.pdf","comment":"Project Page: https://abdo-eldesokey.github.io/build-a-scene/"},{"id":"http://arxiv.org/abs/2408.14812v1","updated":"2024-08-27T06:50:28Z","published":"2024-08-27T06:50:28Z","title":"HPT++: Hierarchically Prompting Vision-Language Models with\n  Multi-Granularity Knowledge Generation and Improved Structure Modeling","summary":"  Prompt learning has become a prevalent strategy for adapting vision-language\nfoundation models (VLMs) such as CLIP to downstream tasks. With the emergence\nof large language models (LLMs), recent studies have explored the potential of\nusing category-related descriptions to enhance prompt effectiveness. However,\nconventional descriptions lack explicit structured information necessary to\nrepresent the interconnections among key elements like entities or attributes\nwith relation to a particular category. Since existing prompt tuning methods\ngive little consideration to managing structured knowledge, this paper\nadvocates leveraging LLMs to construct a graph for each description to\nprioritize such structured knowledge. Consequently, we propose a novel approach\ncalled Hierarchical Prompt Tuning (HPT), enabling simultaneous modeling of both\nstructured and conventional linguistic knowledge. Specifically, we introduce a\nrelationship-guided attention module to capture pair-wise associations among\nentities and attributes for low-level prompt learning. In addition, by\nincorporating high-level and global-level prompts modeling overall semantics,\nthe proposed hierarchical structure forges cross-level interlinks and empowers\nthe model to handle more complex and long-term relationships. Finally, by\nenhancing multi-granularity knowledge generation, redesigning the\nrelationship-driven attention re-weighting module, and incorporating consistent\nconstraints on the hierarchical text encoder, we propose HPT++, which further\nimproves the performance of HPT. Our experiments are conducted across a wide\nrange of evaluation settings, including base-to-new generalization,\ncross-dataset evaluation, and domain generalization. Extensive results and\nablation studies demonstrate the effectiveness of our methods, which\nconsistently outperform existing SOTA methods.\n","authors":["Yubin Wang","Xinyang Jiang","De Cheng","Wenli Sun","Dongsheng Li","Cairong Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.14812v1.pdf","comment":"19 pages, 7 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2312.06323"},{"id":"http://arxiv.org/abs/2408.14810v1","updated":"2024-08-27T06:49:21Z","published":"2024-08-27T06:49:21Z","title":"Generalist Segmentation Algorithm for Photoreceptors Analysis in\n  Adaptive Optics Imaging","summary":"  Analyzing the cone photoreceptor pattern in images obtained from the living\nhuman retina using quantitative methods can be crucial for the early detection\nand management of various eye conditions. Confocal adaptive optics scanning\nlight ophthalmoscope (AOSLO) imaging enables visualization of the cones from\nreflections of waveguiding cone photoreceptors. While there have been\nsignificant improvements in automated algorithms for segmenting cones in\nconfocal AOSLO images, the process of labelling data remains labor-intensive\nand manual. This paper introduces a method based on deep learning (DL) for\ndetecting and segmenting cones in AOSLO images. The models were trained on a\nsemi-automatically labelled dataset of 20 AOSLO batches of images of 18\nparticipants for 0$^{\\circ}$, 1$^{\\circ}$, and 2$^{\\circ}$ from the foveal\ncenter. F1 scores were 0.968, 0.958, and 0.954 for 0$^{\\circ}$, 1$^{\\circ}$,\nand 2$^{\\circ}$, respectively, which is better than previously reported DL\napproaches. Our method minimizes the need for labelled data by only\nnecessitating a fraction of labelled cones, which is especially beneficial in\nthe field of ophthalmology, where labelled data can often be limited.\n","authors":["Mikhail Kulyabin","Aline Sindel","Hilde Pedersen","Stuart Gilson","Rigmor Baraas","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2408.14810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19271v2","updated":"2024-08-27T06:34:00Z","published":"2024-07-27T14:45:34Z","title":"Sewer Image Super-Resolution with Depth Priors and Its Lightweight\n  Network","summary":"  The Quick-view (QV) technique serves as a primary method for detecting\ndefects within sewerage systems. However, the effectiveness of QV is impeded by\nthe limited visual range of its hardware, resulting in suboptimal image quality\nfor distant portions of the sewer network. Image super-resolution is an\neffective way to improve image quality and has been applied in a variety of\nscenes. However, research on super-resolution for sewer images remains\nconsiderably unexplored. In response, this study leverages the inherent depth\nrelationships present within QV images and introduces a novel Depth-guided,\nReference-based Super-Resolution framework denoted as DSRNet. It comprises two\ncore components: a depth extraction module and a depth information matching\nmodule (DMM). DSRNet utilizes the adjacent frames of the low-resolution image\nas reference images and helps them recover texture information based on the\ncorrelation. By combining these modules, the integration of depth priors\nsignificantly enhances both visual quality and performance benchmarks. Besides,\nin pursuit of computational efficiency and compactness, a super-resolution\nknowledge distillation model based on an attention mechanism is introduced.\nThis mechanism facilitates the acquisition of feature similarity between a more\ncomplex teacher model and a streamlined student model, with the latter being a\nlightweight version of DSRNet. Experimental results demonstrate that DSRNet\nsignificantly improves PSNR and SSIM compared with other methods. This study\nalso conducts experiments on sewer defect semantic segmentation, object\ndetection, and classification on the Pipe dataset and Sewer-ML dataset.\nExperiments show that the method can improve the performance of low-resolution\nsewer images in these tasks.\n","authors":["Gang Pan","Chen Wang","Zhijie Sui","Shuai Guo","Yaozhi Lv","Honglie Li","Di Sun","Zixia Xia"],"pdf_url":"https://arxiv.org/pdf/2407.19271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14805v1","updated":"2024-08-27T06:24:51Z","published":"2024-08-27T06:24:51Z","title":"Platypus: A Generalized Specialist Model for Reading Text in Various\n  Forms","summary":"  Reading text from images (either natural scenes or documents) has been a\nlong-standing research topic for decades, due to the high technical challenge\nand wide application range. Previously, individual specialist models are\ndeveloped to tackle the sub-tasks of text reading (e.g., scene text\nrecognition, handwritten text recognition and mathematical expression\nrecognition). However, such specialist models usually cannot effectively\ngeneralize across different sub-tasks. Recently, generalist models (such as\nGPT-4V), trained on tremendous data in a unified way, have shown enormous\npotential in reading text in various scenarios, but with the drawbacks of\nlimited accuracy and low efficiency. In this work, we propose Platypus, a\ngeneralized specialist model for text reading. Specifically, Platypus combines\nthe best of both worlds: being able to recognize text of various forms with a\nsingle unified architecture, while achieving excellent accuracy and high\nefficiency. To better exploit the advantage of Platypus, we also construct a\ntext reading dataset (called Worms), the images of which are curated from\nprevious datasets and partially re-labeled. Experiments on standard benchmarks\ndemonstrate the effectiveness and superiority of the proposed Platypus model.\nModel and data will be made publicly available at\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/Platypus.\n","authors":["Peng Wang","Zhaohai Li","Jun Tang","Humen Zhong","Fei Huang","Zhibo Yang","Cong Yao"],"pdf_url":"https://arxiv.org/pdf/2408.14805v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.14802v1","updated":"2024-08-27T06:14:54Z","published":"2024-08-27T06:14:54Z","title":"RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images","summary":"  sRGB images are now the predominant choice for pre-training visual models in\ncomputer vision research, owing to their ease of acquisition and efficient\nstorage. Meanwhile, the advantage of RAW images lies in their rich physical\ninformation under variable real-world challenging lighting conditions. For\ncomputer vision tasks directly based on camera RAW data, most existing studies\nadopt methods of integrating image signal processor (ISP) with backend\nnetworks, yet often overlook the interaction capabilities between the ISP\nstages and subsequent networks. Drawing inspiration from ongoing adapter\nresearch in NLP and CV areas, we introduce RAW-Adapter, a novel approach aimed\nat adapting sRGB pre-trained models to camera RAW data. RAW-Adapter comprises\ninput-level adapters that employ learnable ISP stages to adjust RAW inputs, as\nwell as model-level adapters to build connections between ISP stages and\nsubsequent high-level networks. Additionally, RAW-Adapter is a general\nframework that could be used in various computer vision frameworks. Abundant\nexperiments under different lighting conditions have shown our algorithm's\nstate-of-the-art (SOTA) performance, demonstrating its effectiveness and\nefficiency across a range of real-world and synthetic datasets.\n","authors":["Ziteng Cui","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2408.14802v1.pdf","comment":"ECCV 2024, code link: https://github.com/cuiziteng/ECCV_RAW_Adapter"},{"id":"http://arxiv.org/abs/2408.14131v2","updated":"2024-08-27T05:54:42Z","published":"2024-08-26T09:26:08Z","title":"GenFormer -- Generated Images are All You Need to Improve Robustness of\n  Transformers on Small Datasets","summary":"  Recent studies showcase the competitive accuracy of Vision Transformers\n(ViTs) in relation to Convolutional Neural Networks (CNNs), along with their\nremarkable robustness. However, ViTs demand a large amount of data to achieve\nadequate performance, which makes their application to small datasets\nchallenging, falling behind CNNs. To overcome this, we propose GenFormer, a\ndata augmentation strategy utilizing generated images, thereby improving\ntransformer accuracy and robustness on small-scale image classification tasks.\nIn our comprehensive evaluation we propose Tiny ImageNetV2, -R, and -A as new\ntest set variants of Tiny ImageNet by transferring established ImageNet\ngeneralization and robustness benchmarks to the small-scale data domain.\nSimilarly, we introduce MedMNIST-C and EuroSAT-C as corrupted test set variants\nof established fine-grained datasets in the medical and aerial domain. Through\na series of experiments conducted on small datasets of various domains,\nincluding Tiny ImageNet, CIFAR, EuroSAT and MedMNIST datasets, we demonstrate\nthe synergistic power of our method, in particular when combined with common\ntrain and test time augmentations, knowledge distillation, and architectural\ndesign choices. Additionally, we prove the effectiveness of our approach under\nchallenging conditions with limited training data, demonstrating significant\nimprovements in both accuracy and robustness, bridging the gap between CNNs and\nViTs in the small-scale dataset domain.\n","authors":["Sven Oehri","Nikolas Ebert","Ahmed Abdullah","Didier Stricker","Oliver WasenmÃ¼ller"],"pdf_url":"https://arxiv.org/pdf/2408.14131v2.pdf","comment":"This paper has been accepted at International Conference on Pattern\n  Recognition (ICPR), 2024"},{"id":"http://arxiv.org/abs/2406.18459v5","updated":"2024-08-27T05:46:06Z","published":"2024-06-26T16:10:31Z","title":"DiffuseHigh: Training-free Progressive High-Resolution Image Synthesis\n  through Structure Guidance","summary":"  Large-scale generative models, such as text-to-image diffusion models, have\ngarnered widespread attention across diverse domains due to their creative and\nhigh-fidelity image generation. Nonetheless, existing large-scale diffusion\nmodels are confined to generating images of up to 1K resolution, which is far\nfrom meeting the demands of contemporary commercial applications. Directly\nsampling higher-resolution images often yields results marred by artifacts such\nas object repetition and distorted shapes. Addressing the aforementioned issues\ntypically necessitates training or fine-tuning models on higher-resolution\ndatasets. However, this poses a formidable challenge due to the difficulty in\ncollecting large-scale high-resolution images and substantial computational\nresources. While several preceding works have proposed alternatives to bypass\nthe cumbersome training process, they often fail to produce convincing results.\nIn this work, we probe the generative ability of diffusion models at higher\nresolution beyond their original capability and propose a novel progressive\napproach that fully utilizes generated low-resolution images to guide the\ngeneration of higher-resolution images. Our method obviates the need for\nadditional training or fine-tuning which significantly lowers the burden of\ncomputational costs. Extensive experiments and results validate the efficiency\nand efficacy of our method. Project page:\nhttps://yhyun225.github.io/DiffuseHigh/\n","authors":["Younghyun Kim","Geunmin Hwang","Junyu Zhang","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2406.18459v5.pdf","comment":"Project page: https://yhyun225.github.io/DiffuseHigh/"},{"id":"http://arxiv.org/abs/2408.14789v1","updated":"2024-08-27T05:31:30Z","published":"2024-08-27T05:31:30Z","title":"Revisiting Surgical Instrument Segmentation Without Human Intervention:\n  A Graph Partitioning View","summary":"  Surgical instrument segmentation (SIS) on endoscopic images stands as a\nlong-standing and essential task in the context of computer-assisted\ninterventions for boosting minimally invasive surgery. Given the recent surge\nof deep learning methodologies and their data-hungry nature, training a neural\npredictive model based on massive expert-curated annotations has been\ndominating and served as an off-the-shelf approach in the field, which could,\nhowever, impose prohibitive burden to clinicians for preparing fine-grained\npixel-wise labels corresponding to the collected surgical video frames. In this\nwork, we propose an unsupervised method by reframing the video frame\nsegmentation as a graph partitioning problem and regarding image pixels as\ngraph nodes, which is significantly different from the previous efforts. A\nself-supervised pre-trained model is firstly leveraged as a feature extractor\nto capture high-level semantic features. Then, Laplacian matrixs are computed\nfrom the features and are eigendecomposed for graph partitioning. On the \"deep\"\neigenvectors, a surgical video frame is meaningfully segmented into different\nmodules such as tools and tissues, providing distinguishable semantic\ninformation like locations, classes, and relations. The segmentation problem\ncan then be naturally tackled by applying clustering or threshold on the\neigenvectors. Extensive experiments are conducted on various datasets (e.g.,\nEndoVis2017, EndoVis2018, UCL, etc.) for different clinical endpoints. Across\nall the challenging scenarios, our method demonstrates outstanding performance\nand robustness higher than unsupervised state-of-the-art (SOTA) methods. The\ncode is released at https://github.com/MingyuShengSMY/GraphClusteringSIS.git.\n","authors":["Mingyu Sheng","Jianan Fan","Dongnan Liu","Ron Kikinis","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2408.14789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08345v2","updated":"2024-08-27T05:08:00Z","published":"2024-08-15T17:58:10Z","title":"5%>100%: Breaking Performance Shackles of Full Fine-Tuning on Visual\n  Recognition Tasks","summary":"  Pre-training & fine-tuning can enhance the transferring efficiency and\nperformance in visual tasks. Recent delta-tuning methods provide more options\nfor visual classification tasks. Despite their success, existing visual\ndelta-tuning art fails to exceed the upper limit of full fine-tuning on\nchallenging tasks like object detection and segmentation. To find a competitive\nalternative to full fine-tuning, we propose the Multi-cognitive Visual Adapter\n(Mona) tuning, a novel adapter-based tuning method. First, we introduce\nmultiple vision-friendly filters into the adapter to enhance its ability to\nprocess visual signals, while previous methods mainly rely on language-friendly\nlinear filters. Second, we add the scaled normalization layer in the adapter to\nregulate the distribution of input features for visual filters. To fully\ndemonstrate the practicality and generality of Mona, we conduct experiments on\nmultiple representative visual tasks, including instance segmentation on COCO,\nsemantic segmentation on ADE20K, object detection on Pascal VOC, oriented\nobject detection on DOTA/STAR, and image classification on three common\ndatasets. Exciting results illustrate that Mona surpasses full fine-tuning on\nall these tasks, and is the only delta-tuning method outperforming full\nfine-tuning on the above various tasks. For example, Mona achieves 1%\nperformance gain on the COCO dataset compared to full fine-tuning.\nComprehensive results suggest that Mona-tuning is more suitable for retaining\nand utilizing the capabilities of pre-trained models than full fine-tuning. The\ncode will be released at https://github.com/Leiyi-Hu/mona.\n","authors":["Dongshuo Yin","Leiyi Hu","Bin Li","Youqun Zhang","Xue Yang"],"pdf_url":"https://arxiv.org/pdf/2408.08345v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2311.15010"},{"id":"http://arxiv.org/abs/2408.14176v2","updated":"2024-08-27T04:59:58Z","published":"2024-08-26T10:42:53Z","title":"SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its\n  Teacher","summary":"  In this paper, we aim to enhance the performance of SwiftBrush, a prominent\none-step text-to-image diffusion model, to be competitive with its multi-step\nStable Diffusion counterpart. Initially, we explore the quality-diversity\ntrade-off between SwiftBrush and SD Turbo: the former excels in image\ndiversity, while the latter excels in image quality. This observation motivates\nour proposed modifications in the training methodology, including better weight\ninitialization and efficient LoRA training. Moreover, our introduction of a\nnovel clamped CLIP loss enhances image-text alignment and results in improved\nimage quality. Remarkably, by combining the weights of models trained with\nefficient LoRA and full training, we achieve a new state-of-the-art one-step\ndiffusion model, achieving an FID of 8.14 and surpassing all GAN-based and\nmulti-step Stable Diffusion models. The project page is available at\nhttps://swiftbrushv2.github.io.\n","authors":["Trung Dao","Thuan Hoang Nguyen","Thanh Le","Duc Vu","Khoi Nguyen","Cuong Pham","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2408.14176v2.pdf","comment":"Accepted to ECCV'24"},{"id":"http://arxiv.org/abs/2408.14776v1","updated":"2024-08-27T04:45:53Z","published":"2024-08-27T04:45:53Z","title":"MROVSeg: Breaking the Resolution Curse of Vision-Language Models in\n  Open-Vocabulary Semantic Segmentation","summary":"  Open-vocabulary semantic segmentation aims to segment and recognize\nsemantically meaningful regions based on text-based descriptions during\ninference. A typical solution to address this task is to leverage powerful\nvision-language models (VLMs), such as CLIP, to bridge the gap between open-\nand close-vocabulary recognition. As VLMs are usually pretrained with\nlow-resolution images (e.g. $224\\times224$), most previous methods operate only\non downscaled images. We question this design as low resolution features often\nfail to preserve fine details. Although employing additional image backbones\nfor high-resolution inputs can mitigate this issue, it may also introduce\nsignificant computation overhead. Therefore, we propose MROVSeg, a\nmulti-resolution training framework for open-vocabulary semantic segmentation\nwith a single pretrained CLIP backbone, that uses sliding windows to slice the\nhigh-resolution input into uniform patches, each matching the input size of the\nwell-trained image encoder. Its key components include a Multi-Res Adapter,\nwhich restores the spatial geometry and grasps local-global correspondences\nacross patches by learnable convolutional and scale attention layers. To\nachieve accurate segmentation, we introduce Multi-grained Masked Attention\nscheme to aggregate multi-grained semantics by performing cross-attention\nbetween object queries and multi-resolution CLIP features within the region of\ninterests. Through comprehensive experiments, we demonstrate the superiority of\nMROVSeg on well-established open-vocabulary semantic segmentation benchmarks,\nparticularly for high-resolution inputs, establishing new standards for\nopen-vocabulary semantic segmentation.\n","authors":["Yuanbing Zhu","Bingke Zhu","Zhen Chen","Huan Xu","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14776v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2407.15773v2","updated":"2024-08-27T04:41:40Z","published":"2024-07-22T16:25:41Z","title":"STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay","summary":"  Test-time adaptation (TTA) aims to address the distribution shift between the\ntraining and test data with only unlabeled data at test time. Existing TTA\nmethods often focus on improving recognition performance specifically for test\ndata associated with classes in the training set. However, during the\nopen-world inference process, there are inevitably test data instances from\nunknown classes, commonly referred to as outliers. This paper pays attention to\nthe problem that conducts both sample recognition and outlier rejection during\ninference while outliers exist. To address this problem, we propose a new\napproach called STAble Memory rePlay (STAMP), which performs optimization over\na stable memory bank instead of the risky mini-batch. In particular, the memory\nbank is dynamically updated by selecting low-entropy and label-consistent\nsamples in a class-balanced manner. In addition, we develop a self-weighted\nentropy minimization strategy that assigns higher weight to low-entropy\nsamples. Extensive results demonstrate that STAMP outperforms existing TTA\nmethods in terms of both recognition and outlier detection performance. The\ncode is released at https://github.com/yuyongcan/STAMP.\n","authors":["Yongcan Yu","Lijun Sheng","Ran He","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2407.15773v2.pdf","comment":"Accepted by ECCV 2024; Fixed a bug in calculating OOD score of STAMP\n  and updated the results"},{"id":"http://arxiv.org/abs/2408.14770v1","updated":"2024-08-27T04:18:18Z","published":"2024-08-27T04:18:18Z","title":"Text-guided Foundation Model Adaptation for Long-Tailed Medical Image\n  Classification","summary":"  In medical contexts, the imbalanced data distribution in long-tailed\ndatasets, due to scarce labels for rare diseases, greatly impairs the\ndiagnostic accuracy of deep learning models. Recent multimodal text-image\nsupervised foundation models offer new solutions to data scarcity through\neffective representation learning. However, their limited medical-specific\npretraining hinders their performance in medical image classification relative\nto natural images. To address this issue, we propose a novel Text-guided\nFoundation model Adaptation for Long-Tailed medical image classification\n(TFA-LT). We adopt a two-stage training strategy, integrating representations\nfrom the foundation model using just two linear adapters and a single ensembler\nfor balanced outcomes. Experimental results on two long-tailed medical image\ndatasets validate the simplicity, lightweight and efficiency of our approach:\nrequiring only 6.1% GPU memory usage of the current best-performing algorithm,\nour method achieves an accuracy improvement of up to 27.1%, highlighting the\nsubstantial potential of foundation model adaptation in this area.\n","authors":["Sirui Li","Li Lin","Yijin Huang","Pujin Cheng","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2408.14770v1.pdf","comment":"Accepted by IEEE ISBI 2024"},{"id":"http://arxiv.org/abs/2408.14080v2","updated":"2024-08-27T04:14:14Z","published":"2024-08-26T08:02:57Z","title":"SONICS: Synthetic Or Not -- Identifying Counterfeit Songs","summary":"  The recent surge in AI-generated songs presents exciting possibilities and\nchallenges. While these tools democratize music creation, they also necessitate\nthe ability to distinguish between human-composed and AI-generated songs for\nsafeguarding artistic integrity and content curation. Existing research and\ndatasets in fake song detection only focus on singing voice deepfake detection\n(SVDD), where the vocals are AI-generated but the instrumental music is sourced\nfrom real songs. However, this approach is inadequate for contemporary\nend-to-end AI-generated songs where all components (vocals, lyrics, music, and\nstyle) could be AI-generated. Additionally, existing datasets lack lyrics-music\ndiversity, long-duration songs, and open fake songs. To address these gaps, we\nintroduce SONICS, a novel dataset for end-to-end Synthetic Song Detection\n(SSD), comprising over 97k songs with over 49k synthetic songs from popular\nplatforms like Suno and Udio. Furthermore, we highlight the importance of\nmodeling long-range temporal dependencies in songs for effective authenticity\ndetection, an aspect overlooked in existing methods. To capture these patterns,\nwe propose a novel model, SpecTTTra, that is up to 3 times faster and 6 times\nmore memory efficient compared to popular CNN and Transformer-based models\nwhile maintaining competitive performance. Finally, we offer both AI-based and\nHuman evaluation benchmarks, addressing another deficiency in current research.\n","authors":["Md Awsafur Rahman","Zaber Ibn Abdul Hakim","Najibul Haque Sarker","Bishmoy Paul","Shaikh Anowarul Fattah"],"pdf_url":"https://arxiv.org/pdf/2408.14080v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17137v4","updated":"2024-08-27T04:02:58Z","published":"2024-05-27T12:54:09Z","title":"Jump-teaching: Ultra Efficient and Robust Learning with Noisy Label","summary":"  Sample selection is the most straightforward technique to combat label noise,\naiming to distinguish mislabeled samples during training and avoid the\ndegradation of the robustness of the model. In the workflow, $\\textit{selecting\npossibly clean data}$ and $\\textit{model update}$ are iterative. However, their\ninterplay and intrinsic characteristics hinder the robustness and efficiency of\nlearning with noisy labels: 1) The model chooses clean data with selection\nbias, leading to the accumulated error in the model update. 2) Most selection\nstrategies leverage partner networks or supplementary information to mitigate\nlabel corruption, albeit with increased computation resources and lower\nthroughput speed. Therefore, we employ only one network with the jump manner\nupdate to decouple the interplay and mine more semantic information from the\nloss for a more precise selection. Specifically, the selection of clean data\nfor each model update is based on one of the prior models, excluding the last\niteration. The strategy of model update exhibits a jump behavior in the form.\nMoreover, we map the outputs of the network and labels into the same semantic\nfeature space, respectively. In this space, a detailed and simple loss\ndistribution is generated to distinguish clean samples more effectively. Our\nproposed approach achieves almost up to $2.53\\times$ speedup, $0.46\\times$ peak\nmemory footprint, and superior robustness over state-of-the-art works with\nvarious noise settings.\n","authors":["Kangye Ji","Fei Cheng","Zeqing Wang","Bohu Huang"],"pdf_url":"https://arxiv.org/pdf/2405.17137v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19730v4","updated":"2024-08-27T03:45:18Z","published":"2024-05-30T06:21:34Z","title":"Research on the Spatial Data Intelligent Foundation Model","summary":"  This report focuses on spatial data intelligent large models, delving into\nthe principles, methods, and cutting-edge applications of these models. It\nprovides an in-depth discussion on the definition, development history, current\nstatus, and trends of spatial data intelligent large models, as well as the\nchallenges they face. The report systematically elucidates the key technologies\nof spatial data intelligent large models and their applications in urban\nenvironments, aerospace remote sensing, geography, transportation, and other\nscenarios. Additionally, it summarizes the latest application cases of spatial\ndata intelligent large models in themes such as urban development, multimodal\nsystems, remote sensing, smart transportation, and resource environments.\nFinally, the report concludes with an overview and outlook on the development\nprospects of spatial data intelligent large models.\n","authors":["Shaohua Wang","Xing Xie","Yong Li","Danhuai Guo","Zhi Cai","Yu Liu","Yang Yue","Xiao Pan","Feng Lu","Huayi Wu","Zhipeng Gui","Zhiming Ding","Bolong Zheng","Fuzheng Zhang","Jingyuan Wang","Zhengchao Chen","Hao Lu","Jiayi Li","Peng Yue","Wenhao Yu","Yao Yao","Leilei Sun","Yong Zhang","Longbiao Chen","Xiaoping Du","Xiang Li","Xueying Zhang","Kun Qin","Zhaoya Gong","Weihua Dong","Xiaofeng Meng"],"pdf_url":"https://arxiv.org/pdf/2405.19730v4.pdf","comment":"V1 and V2 are in Chinese language, other versions are in English"},{"id":"http://arxiv.org/abs/2408.14765v1","updated":"2024-08-27T03:41:44Z","published":"2024-08-27T03:41:44Z","title":"CrossViewDiff: A Cross-View Diffusion Model for Satellite-to-Street View\n  Synthesis","summary":"  Satellite-to-street view synthesis aims at generating a realistic street-view\nimage from its corresponding satellite-view image. Although stable diffusion\nmodels have exhibit remarkable performance in a variety of image generation\napplications, their reliance on similar-view inputs to control the generated\nstructure or texture restricts their application to the challenging cross-view\nsynthesis task. In this work, we propose CrossViewDiff, a cross-view diffusion\nmodel for satellite-to-street view synthesis. To address the challenges posed\nby the large discrepancy across views, we design the satellite scene structure\nestimation and cross-view texture mapping modules to construct the structural\nand textural controls for street-view image synthesis. We further design a\ncross-view control guided denoising process that incorporates the above\ncontrols via an enhanced cross-view attention module. To achieve a more\ncomprehensive evaluation of the synthesis results, we additionally design a\nGPT-based scoring method as a supplement to standard evaluation metrics. We\nalso explore the effect of different data sources (e.g., text, maps, building\nheights, and multi-temporal satellite imagery) on this task. Results on three\npublic cross-view datasets show that CrossViewDiff outperforms current\nstate-of-the-art on both standard and GPT-based evaluation metrics, generating\nhigh-quality street-view panoramas with more realistic structures and textures\nacross rural, suburban, and urban scenes. The code and models of this work will\nbe released at https://opendatalab.github.io/CrossViewDiff/.\n","authors":["Weijia Li","Jun He","Junyan Ye","Huaping Zhong","Zhimeng Zheng","Zilong Huang","Dahua Lin","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2408.14765v1.pdf","comment":"21 pages, 11 figures"},{"id":"http://arxiv.org/abs/2312.04822v2","updated":"2024-08-27T03:33:51Z","published":"2023-12-08T04:12:26Z","title":"SiCP: Simultaneous Individual and Cooperative Perception for 3D Object\n  Detection in Connected and Automated Vehicles","summary":"  Cooperative perception for connected and automated vehicles is traditionally\nachieved through the fusion of feature maps from two or more vehicles. However,\nthe absence of feature maps shared from other vehicles can lead to a\nsignificant decline in 3D object detection performance for cooperative\nperception models compared to standalone 3D detection models. This drawback\nimpedes the adoption of cooperative perception as vehicle resources are often\ninsufficient to concurrently employ two perception models. To tackle this\nissue, we present Simultaneous Individual and Cooperative Perception (SiCP), a\ngeneric framework that supports a wide range of the state-of-the-art standalone\nperception backbones and enhances them with a novel Dual-Perception Network\n(DP-Net) designed to facilitate both individual and cooperative perception. In\naddition to its lightweight nature with only 0.13M parameters, DP-Net is robust\nand retains crucial gradient information during feature map fusion. As\ndemonstrated in a comprehensive evaluation on the V2V4Real and OPV2V datasets,\nthanks to DP-Net, SiCP surpasses state-of-the-art cooperative perception\nsolutions while preserving the performance of standalone perception solutions.\n","authors":["Deyuan Qu","Qi Chen","Tianyu Bai","Hongsheng Lu","Heng Fan","Hao Zhang","Song Fu","Qing Yang"],"pdf_url":"https://arxiv.org/pdf/2312.04822v2.pdf","comment":"Accepted by IROS 2024"},{"id":"http://arxiv.org/abs/2408.14764v1","updated":"2024-08-27T03:31:24Z","published":"2024-08-27T03:31:24Z","title":"SynthDoc: Bilingual Documents Synthesis for Visual Document\n  Understanding","summary":"  This paper introduces SynthDoc, a novel synthetic document generation\npipeline designed to enhance Visual Document Understanding (VDU) by generating\nhigh-quality, diverse datasets that include text, images, tables, and charts.\nAddressing the challenges of data acquisition and the limitations of existing\ndatasets, SynthDoc leverages publicly available corpora and advanced rendering\ntools to create a comprehensive and versatile dataset. Our experiments,\nconducted using the Donut model, demonstrate that models trained with\nSynthDoc's data achieve superior performance in pre-training read tasks and\nmaintain robustness in downstream tasks, despite language inconsistencies. The\nrelease of a benchmark dataset comprising 5,000 image-text pairs not only\nshowcases the pipeline's capabilities but also provides a valuable resource for\nthe VDU community to advance research and development in document image\nrecognition. This work significantly contributes to the field by offering a\nscalable solution to data scarcity and by validating the efficacy of end-to-end\nmodels in parsing complex, real-world documents.\n","authors":["Chuanghao Ding","Xuejing Liu","Wei Tang","Juan Li","Xiaoliang Wang","Rui Zhao","Cam-Tu Nguyen","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2408.14764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14757v1","updated":"2024-08-27T03:17:52Z","published":"2024-08-27T03:17:52Z","title":"Learning effective pruning at initialization from iterative pruning","summary":"  Pruning at initialization (PaI) reduces training costs by removing weights\nbefore training, which becomes increasingly crucial with the growing network\nsize. However, current PaI methods still have a large accuracy gap with\niterative pruning, especially at high sparsity levels. This raises an\nintriguing question: can we get inspiration from iterative pruning to improve\nthe PaI performance? In the lottery ticket hypothesis, the iterative rewind\npruning (IRP) finds subnetworks retroactively by rewinding the parameter to the\noriginal initialization in every pruning iteration, which means all the\nsubnetworks are based on the initial state. Here, we hypothesise the surviving\nsubnetworks are more important and bridge the initial feature and their\nsurviving score as the PaI criterion. We employ an end-to-end neural network\n(\\textbf{AutoS}parse) to learn this correlation, input the model's initial\nfeatures, output their score and then prune the lowest score parameters before\ntraining. To validate the accuracy and generalization of our method, we\nperformed PaI across various models. Results show that our approach outperforms\nexisting methods in high-sparsity settings. Notably, as the underlying logic of\nmodel pruning is consistent in different models, only one-time IRP on one model\nis needed (e.g., once IRP on ResNet-18/CIFAR-10, AutoS can be generalized to\nVGG-16/CIFAR-10, ResNet-18/TinyImageNet, et al.). As the first neural\nnetwork-based PaI method, we conduct extensive experiments to validate the\nfactors influencing this approach. These results reveal the learning tendencies\nof neural networks and provide new insights into our understanding and research\nof PaI from a practical perspective. Our code is available at:\nhttps://github.com/ChengYaofeng/AutoSparse.git.\n","authors":["Shengkai Liu","Yaofeng Cheng","Fusheng Zha","Wei Guo","Lining Sun","Zhenshan Bing","Chenguang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.14757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14754v1","updated":"2024-08-27T03:09:39Z","published":"2024-08-27T03:09:39Z","title":"Sequential-Scanning Dual-Energy CT Imaging Using High Temporal\n  Resolution Image Reconstruction and Error-Compensated Material Basis Image\n  Generation","summary":"  Dual-energy computed tomography (DECT) has been widely used to obtain\nquantitative elemental composition of imaged subjects for personalized and\nprecise medical diagnosis. Compared with DECT leveraging advanced X-ray source\nand/or detector technologies, the use of the sequential-scanning data\nacquisition scheme to implement DECT may make a broader impact on clinical\npractice because this scheme requires no specialized hardware designs and can\nbe directly implemented into conventional CT systems. However, since the\nconcentration of iodinated contrast agent in the imaged subject varies over\ntime, sequentially scanned data sets acquired at two tube potentials are\ntemporally inconsistent. As existing material basis image reconstruction\napproaches assume that the data sets acquired at two tube potentials are\ntemporally consistent, the violation of this assumption results in inaccurate\nquantification of material concentration. In this work, we developed\nsequential-scanning DECT imaging using high temporal resolution image\nreconstruction and error-compensated material basis image generation,\nACCELERATION in short, to address the technical challenge induced by temporal\ninconsistency of sequentially scanned data sets and improve quantification\naccuracy of material concentration in sequential-scanning DECT. ACCELERATION\nhas been validated and evaluated using numerical simulation data sets generated\nfrom clinical human subject exams and experimental human subject studies.\nResults demonstrated the improvement of quantification accuracy and image\nquality using ACCELERATION.\n","authors":["Qiaoxin Li","Ruifeng Chen","Peng Wang","Guotao Quan","Yanfeng Du","Dong Liang","Yinsheng Li"],"pdf_url":"https://arxiv.org/pdf/2408.14754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12340v2","updated":"2024-08-27T02:53:37Z","published":"2024-08-22T12:36:10Z","title":"VTON-HandFit: Virtual Try-on for Arbitrary Hand Pose Guided by Hand\n  Priors Embedding","summary":"  Although diffusion-based image virtual try-on has made considerable progress,\nemerging approaches still struggle to effectively address the issue of hand\nocclusion (i.e., clothing regions occluded by the hand part), leading to a\nnotable degradation of the try-on performance. To tackle this issue widely\nexisting in real-world scenarios, we propose VTON-HandFit, leveraging the power\nof hand priors to reconstruct the appearance and structure for hand occlusion\ncases. Firstly, we tailor a Handpose Aggregation Net using the ControlNet-based\nstructure explicitly and adaptively encoding the global hand and pose priors.\nBesides, to fully exploit the hand-related structure and appearance\ninformation, we propose Hand-feature Disentanglement Embedding module to\ndisentangle the hand priors into the hand structure-parametric and\nvisual-appearance features, and customize a masked cross attention for further\ndecoupled feature embedding. Lastly, we customize a hand-canny constraint loss\nto better learn the structure edge knowledge from the hand template of model\nimage. VTON-HandFit outperforms the baselines in qualitative and quantitative\nevaluations on the public dataset and our self-collected hand-occlusion\nHandfit-3K dataset particularly for the arbitrary hand pose occlusion cases in\nreal-world scenarios. The Code and dataset will be available at\n\\url{https://github.com/VTON-HandFit/VTON-HandFit}.\n","authors":["Yujie Liang","Xiaobin Hu","Boyuan Jiang","Donghao Luo","Kai WU","Wenhui Han","Taisong Jin","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.12340v2.pdf","comment":"The project page is \\url{https://vton-handfit.github.io}"},{"id":"http://arxiv.org/abs/2305.10662v2","updated":"2024-08-27T02:46:38Z","published":"2023-05-18T02:51:17Z","title":"Private Gradient Estimation is Useful for Generative Modeling","summary":"  While generative models have proved successful in many domains, they may pose\na privacy leakage risk in practical deployment. To address this issue,\ndifferentially private generative model learning has emerged as a solution to\ntrain private generative models for different downstream tasks. However,\nexisting private generative modeling approaches face significant challenges in\ngenerating high-dimensional data due to the inherent complexity involved in\nmodeling such data. In this work, we present a new private generative modeling\napproach where samples are generated via Hamiltonian dynamics with gradients of\nthe private dataset estimated by a well-trained network. In the approach, we\nachieve differential privacy by perturbing the projection vectors in the\nestimation of gradients with sliced score matching. In addition, we enhance the\nreconstruction ability of the model by incorporating a residual enhancement\nmodule during the score matching. For sampling, we perform Hamiltonian dynamics\nwith gradients estimated by the well-trained network, allowing the sampled data\nclose to the private dataset's manifold step by step. In this way, our model is\nable to generate data with a resolution of 256x256. Extensive experiments and\nanalysis clearly demonstrate the effectiveness and rationality of the proposed\napproach.\n","authors":["Bochao Liu","Pengju Wang","Weijia Guo","Yong Li","Liansheng Zhuang","Weiping Wang","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2305.10662v2.pdf","comment":"accepted by ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2408.14744v1","updated":"2024-08-27T02:45:26Z","published":"2024-08-27T02:45:26Z","title":"RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models","summary":"  Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1 million RS images, each\naccompanied by multiple descriptive captions. Extensive experiments demonstrate\nthat RSTeller enhances the performance of multiple existing vision language\nmodels for RS scene understanding through continual pre-training. Our\nmethodology significantly reduces the manual effort and expertise needed for\nannotating remote sensing imagery while democratizing access to high-quality\nannotated data. This advancement fosters progress in visual language modeling\nand encourages broader participation in remote sensing research and\napplications. The RSTeller dataset is available at\nhttps://github.com/SlytherinGe/RSTeller.\n","authors":["Junyao Ge","Yang Zheng","Kaitai Guo","Jimin Liang"],"pdf_url":"https://arxiv.org/pdf/2408.14744v1.pdf","comment":"Submitted to ISPRS"},{"id":"http://arxiv.org/abs/2408.14743v1","updated":"2024-08-27T02:43:40Z","published":"2024-08-27T02:43:40Z","title":"Personalized Video Summarization using Text-Based Queries and\n  Conditional Modeling","summary":"  The proliferation of video content on platforms like YouTube and Vimeo\npresents significant challenges in efficiently locating relevant information.\nAutomatic video summarization aims to address this by extracting and presenting\nkey content in a condensed form. This thesis explores enhancing video\nsummarization by integrating text-based queries and conditional modeling to\ntailor summaries to user needs. Traditional methods often produce fixed\nsummaries that may not align with individual requirements. To overcome this, we\npropose a multi-modal deep learning approach that incorporates both textual\nqueries and visual information, fusing them at different levels of the model\narchitecture. Evaluation metrics such as accuracy and F1-score assess the\nquality of the generated summaries. The thesis also investigates improving\ntext-based query representations using contextualized word embeddings and\nspecialized attention networks. This enhances the semantic understanding of\nqueries, leading to better video summaries. To emulate human-like\nsummarization, which accounts for both visual coherence and abstract factors\nlike storyline consistency, we introduce a conditional modeling approach. This\nmethod uses multiple random variables and joint distributions to capture key\nsummarization components, resulting in more human-like and explainable\nsummaries. Addressing data scarcity in fully supervised learning, the thesis\nproposes a segment-level pseudo-labeling approach. This self-supervised method\ngenerates additional data, improving model performance even with limited\nhuman-labeled datasets. In summary, this research aims to enhance automatic\nvideo summarization by incorporating text-based queries, improving query\nrepresentations, introducing conditional modeling, and addressing data\nscarcity, thereby creating more effective and personalized video summaries.\n","authors":["Jia-Hong Huang"],"pdf_url":"https://arxiv.org/pdf/2408.14743v1.pdf","comment":"Ph.D. thesis, 137 pages"},{"id":"http://arxiv.org/abs/2408.12569v3","updated":"2024-08-27T02:31:42Z","published":"2024-08-22T17:37:27Z","title":"Sapiens: Foundation for Human Vision Models","summary":"  We present Sapiens, a family of models for four fundamental human-centric\nvision tasks -- 2D pose estimation, body-part segmentation, depth estimation,\nand surface normal prediction. Our models natively support 1K high-resolution\ninference and are extremely easy to adapt for individual tasks by simply\nfine-tuning models pretrained on over 300 million in-the-wild human images. We\nobserve that, given the same computational budget, self-supervised pretraining\non a curated dataset of human images significantly boosts the performance for a\ndiverse set of human-centric tasks. The resulting models exhibit remarkable\ngeneralization to in-the-wild data, even when labeled data is scarce or\nentirely synthetic. Our simple model design also brings scalability -- model\nperformance across tasks improves as we scale the number of parameters from 0.3\nto 2 billion. Sapiens consistently surpasses existing baselines across various\nhuman-centric benchmarks. We achieve significant improvements over the prior\nstate-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1\nmIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5%\nrelative angular error. Project page:\nhttps://about.meta.com/realitylabs/codecavatars/sapiens.\n","authors":["Rawal Khirodkar","Timur Bagautdinov","Julieta Martinez","Su Zhaoen","Austin James","Peter Selednik","Stuart Anderson","Shunsuke Saito"],"pdf_url":"https://arxiv.org/pdf/2408.12569v3.pdf","comment":"ECCV 2024 (Oral)"},{"id":"http://arxiv.org/abs/2408.13800v2","updated":"2024-08-27T02:30:47Z","published":"2024-08-25T10:42:07Z","title":"BCDNet: A Convolutional Neural Network For Breast Cancer Detection","summary":"  Previous research has established that breast cancer is a prevalent cancer\ntype, with Invasive Ductal Carcinoma (IDC) being the most common subtype. The\nincidence of this dangerous cancer continues to rise, making accurate and rapid\ndiagnosis, particularly in the early stages, critically important. While modern\nComputer-Aided Diagnosis (CAD) systems can address most cases, medical\nprofessionals still face challenges in using them in the field without powerful\ncomputing resources. In this paper, we propose a novel CNN model called BCDNet,\nwhich effectively detects IDC in histopathological images with an accuracy of\nup to 89.5% and reduces training time effectively.\n","authors":["Yujia Lin","Aiwei Lian","Mingyu Liao","Yipeng Liu"],"pdf_url":"https://arxiv.org/pdf/2408.13800v2.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.14738v1","updated":"2024-08-27T02:29:29Z","published":"2024-08-27T02:29:29Z","title":"Learning Differentially Private Diffusion Models via Stochastic\n  Adversarial Distillation","summary":"  While the success of deep learning relies on large amounts of training\ndatasets, data is often limited in privacy-sensitive domains. To address this\nchallenge, generative model learning with differential privacy has emerged as a\nsolution to train private generative models for desensitized data generation.\nHowever, the quality of the images generated by existing methods is limited due\nto the complexity of modeling data distribution. We build on the success of\ndiffusion models and introduce DP-SAD, which trains a private diffusion model\nby a stochastic adversarial distillation method. Specifically, we first train a\ndiffusion model as a teacher and then train a student by distillation, in which\nwe achieve differential privacy by adding noise to the gradients from other\nmodels to the student. For better generation quality, we introduce a\ndiscriminator to distinguish whether an image is from the teacher or the\nstudent, which forms the adversarial training. Extensive experiments and\nanalysis clearly demonstrate the effectiveness of our proposed method.\n","authors":["Bochao Liu","Pengju Wang","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2408.14738v1.pdf","comment":"accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.13623v2","updated":"2024-08-27T01:59:59Z","published":"2024-08-24T16:33:26Z","title":"Prompt-Softbox-Prompt: A free-text Embedding Control for Image Editing","summary":"  Text-driven diffusion models have achieved remarkable success in image\nediting, but a crucial component in these models-text embeddings-has not been\nfully explored. The entanglement and opacity of text embeddings present\nsignificant challenges to achieving precise image editing. In this paper, we\nprovide a comprehensive and in-depth analysis of text embeddings in Stable\nDiffusion XL, offering three key insights. First, while the 'aug_embedding'\ncaptures the full semantic content of the text, its contribution to the final\nimage generation is relatively minor. Second, 'BOS' and 'Padding_embedding' do\nnot contain any semantic information. Lastly, the 'EOS' holds the semantic\ninformation of all words and contains the most style features. Each word\nembedding plays a unique role without interfering with one another. Based on\nthese insights, we propose a novel approach for controllable image editing\nusing a free-text embedding control method called PSP (Prompt-Softbox-Prompt).\nPSP enables precise image editing by inserting or adding text embeddings within\nthe cross-attention layers and using Softbox to define and control the specific\narea for semantic injection. This technique allows for obejct additions and\nreplacements while preserving other areas of the image. Additionally, PSP can\nachieve style transfer by simply replacing text embeddings. Extensive\nexperimental results show that PSP achieves significant results in tasks such\nas object replacement, object addition, and style transfer.\n","authors":["Yitong Yang","Yinglin Wang","Jing Wang","Tian Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.13623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14732v1","updated":"2024-08-27T01:55:40Z","published":"2024-08-27T01:55:40Z","title":"OctFusion: Octree-based Diffusion Models for 3D Shape Generation","summary":"  Diffusion models have emerged as a popular method for 3D generation. However,\nit is still challenging for diffusion models to efficiently generate diverse\nand high-quality 3D shapes. In this paper, we introduce OctFusion, which can\ngenerate 3D shapes with arbitrary resolutions in 2.5 seconds on a single Nvidia\n4090 GPU, and the extracted meshes are guaranteed to be continuous and\nmanifold. The key components of OctFusion are the octree-based latent\nrepresentation and the accompanying diffusion models. The representation\ncombines the benefits of both implicit neural representations and explicit\nspatial octrees and is learned with an octree-based variational autoencoder.\nThe proposed diffusion model is a unified multi-scale U-Net that enables\nweights and computation sharing across different octree levels and avoids the\ncomplexity of widely used cascaded diffusion schemes. We verify the\neffectiveness of OctFusion on the ShapeNet and Objaverse datasets and achieve\nstate-of-the-art performances on shape generation tasks. We demonstrate that\nOctFusion is extendable and flexible by generating high-quality color fields\nfor textured mesh generation and high-quality 3D shapes conditioned on text\nprompts, sketches, or category labels. Our code and pre-trained models are\navailable at \\url{https://github.com/octree-nn/octfusion}.\n","authors":["Bojun Xiong","Si-Tong Wei","Xin-Yang Zheng","Yan-Pei Cao","Zhouhui Lian","Peng-Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14732v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2408.14724v1","updated":"2024-08-27T01:28:15Z","published":"2024-08-27T01:28:15Z","title":"GeoTransfer : Generalizable Few-Shot Multi-View Reconstruction via\n  Transfer Learning","summary":"  This paper presents a novel approach for sparse 3D reconstruction by\nleveraging the expressive power of Neural Radiance Fields (NeRFs) and fast\ntransfer of their features to learn accurate occupancy fields. Existing 3D\nreconstruction methods from sparse inputs still struggle with capturing\nintricate geometric details and can suffer from limitations in handling\noccluded regions. On the other hand, NeRFs excel in modeling complex scenes but\ndo not offer means to extract meaningful geometry. Our proposed method offers\nthe best of both worlds by transferring the information encoded in NeRF\nfeatures to derive an accurate occupancy field representation. We utilize a\npre-trained, generalizable state-of-the-art NeRF network to capture detailed\nscene radiance information, and rapidly transfer this knowledge to train a\ngeneralizable implicit occupancy network. This process helps in leveraging the\nknowledge of the scene geometry encoded in the generalizable NeRF prior and\nrefining it to learn occupancy fields, facilitating a more precise\ngeneralizable representation of 3D space. The transfer learning approach leads\nto a dramatic reduction in training time, by orders of magnitude (i.e. from\nseveral days to 3.5 hrs), obviating the need to train generalizable sparse\nsurface reconstruction methods from scratch. Additionally, we introduce a novel\nloss on volumetric rendering weights that helps in the learning of accurate\noccupancy fields, along with a normal loss that helps in global smoothing of\nthe occupancy fields. We evaluate our approach on the DTU dataset and\ndemonstrate state-of-the-art performance in terms of reconstruction accuracy,\nespecially in challenging scenarios with sparse input data and occluded\nregions. We furthermore demonstrate the generalization capabilities of our\nmethod by showing qualitative results on the Blended MVS dataset without any\nretraining.\n","authors":["Shubhendu Jena","Franck Multon","Adnane Boukhayma"],"pdf_url":"https://arxiv.org/pdf/2408.14724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13621v5","updated":"2024-08-27T01:23:50Z","published":"2024-04-21T11:21:27Z","title":"Attack on Scene Flow using Point Clouds","summary":"  Deep neural networks have made significant advancements in accurately\nestimating scene flow using point clouds, which is vital for many applications\nlike video analysis, action recognition, and navigation. The robustness of\nthese techniques, however, remains a concern, particularly in the face of\nadversarial attacks that have been proven to deceive state-of-the-art deep\nneural networks in many domains. Surprisingly, the robustness of scene flow\nnetworks against such attacks has not been thoroughly investigated. To address\nthis problem, the proposed approach aims to bridge this gap by introducing\nadversarial white-box attacks specifically tailored for scene flow networks.\nExperimental results show that the generated adversarial examples obtain up to\n33.7 relative degradation in average end-point error on the KITTI and\nFlyingThings3D datasets. The study also reveals the significant impact that\nattacks targeting point clouds in only one dimension or color channel have on\naverage end-point error. Analyzing the success and failure of these attacks on\nthe scene flow networks and their 2D optical flow network variants shows a\nhigher vulnerability for the optical flow networks. Code is available at\nhttps://github.com/aheldis/Attack-on-Scene-Flow-using-Point-Clouds.git.\n","authors":["Haniyeh Ehsani Oskouie","Mohammad-Shahram Moin","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2404.13621v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14723v1","updated":"2024-08-27T01:23:49Z","published":"2024-08-27T01:23:49Z","title":"Snap and Diagnose: An Advanced Multimodal Retrieval System for\n  Identifying Plant Diseases in the Wild","summary":"  Plant disease recognition is a critical task that ensures crop health and\nmitigates the damage caused by diseases. A handy tool that enables farmers to\nreceive a diagnosis based on query pictures or the text description of\nsuspicious plants is in high demand for initiating treatment before potential\ndiseases spread further. In this paper, we develop a multimodal plant disease\nimage retrieval system to support disease search based on either image or text\nprompts. Specifically, we utilize the largest in-the-wild plant disease dataset\nPlantWild, which includes over 18,000 images across 89 categories, to provide a\ncomprehensive view of potential diseases relating to the query. Furthermore,\ncross-modal retrieval is achieved in the developed system, facilitated by a\nnovel CLIP-based vision-language model that encodes both disease descriptions\nand disease images into the same latent space. Built on top of the retriever,\nour retrieval system allows users to upload either plant disease images or\ndisease descriptions to retrieve the corresponding images with similar\ncharacteristics from the disease dataset to suggest candidate diseases for end\nusers' consideration.\n","authors":["Tianqi Wei","Zhi Chen","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2408.14723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06889v3","updated":"2024-08-27T00:03:31Z","published":"2024-07-09T14:18:35Z","title":"A Neurosymbolic Approach to Adaptive Feature Extraction in SLAM","summary":"  Autonomous robots, autonomous vehicles, and humans wearing mixed-reality\nheadsets require accurate and reliable tracking services for safety-critical\napplications in dynamically changing real-world environments. However, the\nexisting tracking approaches, such as Simultaneous Localization and Mapping\n(SLAM), do not adapt well to environmental changes and boundary conditions\ndespite extensive manual tuning. On the other hand, while deep learning-based\napproaches can better adapt to environmental changes, they typically demand\nsubstantial data for training and often lack flexibility in adapting to new\ndomains. To solve this problem, we propose leveraging the neurosymbolic program\nsynthesis approach to construct adaptable SLAM pipelines that integrate the\ndomain knowledge from traditional SLAM approaches while leveraging data to\nlearn complex relationships. While the approach can synthesize end-to-end SLAM\npipelines, we focus on synthesizing the feature extraction module. We first\ndevise a domain-specific language (DSL) that can encapsulate domain knowledge\non the important attributes for feature extraction and the real-world\nperformance of various feature extractors. Our neurosymbolic architecture then\nundertakes adaptive feature extraction, optimizing parameters via learning\nwhile employing symbolic reasoning to select the most suitable feature\nextractor. Our evaluations demonstrate that our approach, neurosymbolic Feature\nEXtraction (nFEX), yields higher-quality features. It also reduces the pose\nerror observed for the state-of-the-art baseline feature extractors ORB and\nSIFT by up to 90% and up to 66%, respectively, thereby enhancing the system's\nefficiency and adaptability to novel environments.\n","authors":["Yasra Chandio","Momin A. Khan","Khotso Selialia","Luis Garcia","Joseph DeGol","Fatima M. Anwar"],"pdf_url":"https://arxiv.org/pdf/2407.06889v3.pdf","comment":"8 pages, 6 figures, and 5 tables. Published at the 2024 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS).\n  Corresponding author: Yasra Chandio (ychandio@umass.edu)"},{"id":"http://arxiv.org/abs/2408.15447v1","updated":"2024-08-27T23:53:52Z","published":"2024-08-27T23:53:52Z","title":"Fine-grained length controllable video captioning with ordinal\n  embeddings","summary":"  This paper proposes a method for video captioning that controls the length of\ngenerated captions. Previous work on length control often had few levels for\nexpressing length. In this study, we propose two methods of length embedding\nfor fine-grained length control. A traditional embedding method is linear,\nusing a one-hot vector and an embedding matrix. In this study, we propose\nmethods that represent length in multi-hot vectors. One is bit embedding that\nexpresses length in bit representation, and the other is ordinal embedding that\nuses the binary representation often used in ordinal regression. These length\nrepresentations of multi-hot vectors are converted into length embedding by a\nnonlinear MLP. This method allows for not only the length control of caption\nsentences but also the control of the time when reading the caption.\nExperiments using ActivityNet Captions and Spoken Moments in Time show that the\nproposed method effectively controls the length of the generated captions.\nAnalysis of the embedding vectors with ICA shows that length and semantics were\nlearned separately, demonstrating the effectiveness of the proposed embedding\nmethods.\n","authors":["Tomoya Nitta","Takumi Fukuzawa","Toru Tamaki"],"pdf_url":"https://arxiv.org/pdf/2408.15447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15891v4","updated":"2024-08-27T22:09:19Z","published":"2024-04-24T14:29:26Z","title":"OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian\n  Segmentation","summary":"  Recent advancements in 3D reconstruction technologies have paved the way for\nhigh-quality and real-time rendering of complex 3D scenes. Despite these\nachievements, a notable challenge persists: it is difficult to precisely\nreconstruct specific objects from large scenes. Current scene reconstruction\ntechniques frequently result in the loss of object detail textures and are\nunable to reconstruct object portions that are occluded or unseen in views. To\naddress this challenge, we delve into the meticulous 3D reconstruction of\nspecific objects within large scenes and propose a framework termed OMEGAS:\nObject Mesh Extraction from Large Scenes Guided by Gaussian Segmentation.\nSpecifically, we proposed a novel 3D target segmentation technique based on 2D\nGaussian Splatting, which segments 3D consistent target masks in multi-view\nscene images and generates a preliminary target model. Moreover, to reconstruct\nthe unseen portions of the target, we propose a novel target replenishment\ntechnique driven by large-scale generative diffusion priors. We demonstrate\nthat our method can accurately reconstruct specific targets from large scenes,\nboth quantitatively and qualitatively. Our experiments show that OMEGAS\nsignificantly outperforms existing reconstruction methods across various\nscenarios. Our project page is at: https://github.com/CrystalWlz/OMEGAS\n","authors":["Lizhi Wang","Feng Zhou","Bo yu","Pu Cao","Jianqin Yin"],"pdf_url":"https://arxiv.org/pdf/2404.15891v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15428v1","updated":"2024-08-27T22:05:44Z","published":"2024-08-27T22:05:44Z","title":"HEAD: A Bandwidth-Efficient Cooperative Perception Approach for\n  Heterogeneous Connected and Autonomous Vehicles","summary":"  In cooperative perception studies, there is often a trade-off between\ncommunication bandwidth and perception performance. While current feature\nfusion solutions are known for their excellent object detection performance,\ntransmitting the entire sets of intermediate feature maps requires substantial\nbandwidth. Furthermore, these fusion approaches are typically limited to\nvehicles that use identical detection models. Our goal is to develop a solution\nthat supports cooperative perception across vehicles equipped with different\nmodalities of sensors. This method aims to deliver improved perception\nperformance compared to late fusion techniques, while achieving precision\nsimilar to the state-of-art intermediate fusion, but requires an order of\nmagnitude less bandwidth. We propose HEAD, a method that fuses features from\nthe classification and regression heads in 3D object detection networks. Our\nmethod is compatible with heterogeneous detection networks such as LiDAR\nPointPillars, SECOND, VoxelNet, and camera Bird's-eye View (BEV) Encoder. Given\nthe naturally smaller feature size in the detection heads, we design a\nself-attention mechanism to fuse the classification head and a complementary\nfeature fusion layer to fuse the regression head. Our experiments,\ncomprehensively evaluated on the V2V4Real and OPV2V datasets, demonstrate that\nHEAD is a fusion method that effectively balances communication bandwidth and\nperception performance.\n","authors":["Deyuan Qu","Qi Chen","Yongqi Zhu","Yihao Zhu","Sergei S. Avedisov","Song Fu","Qing Yang"],"pdf_url":"https://arxiv.org/pdf/2408.15428v1.pdf","comment":"Accepted by ECCV 2024 Workshop"},{"id":"http://arxiv.org/abs/2307.11986v2","updated":"2024-08-27T21:25:39Z","published":"2023-07-22T05:34:18Z","title":"Expert Knowledge-Aware Image Difference Graph Representation Learning\n  for Difference-Aware Medical Visual Question Answering","summary":"  To contribute to automating the medical vision-language model, we propose a\nnovel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair\nof main and reference images, this task attempts to answer several questions on\nboth diseases and, more importantly, the differences between them. This is\nconsistent with the radiologist's diagnosis practice that compares the current\nimage with the reference before concluding the report. We collect a new\ndataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs\nof main and reference images. Compared to existing medical VQA datasets, our\nquestions are tailored to the Assessment-Diagnosis-Intervention-Evaluation\ntreatment procedure used by clinical professionals. Meanwhile, we also propose\na novel expert knowledge-aware graph representation learning model to address\nthis task. The proposed baseline model leverages expert knowledge such as\nanatomical structure prior, semantic, and spatial knowledge to construct a\nmulti-relationship graph, representing the image differences between two images\nfor the image difference VQA task. The dataset and code can be found at\nhttps://github.com/Holipori/MIMIC-Diff-VQA. We believe this work would further\npush forward the medical vision language model.\n","authors":["Xinyue Hu","Lin Gu","Qiyuan An","Mengliang Zhang","Liangchen Liu","Kazuma Kobayashi","Tatsuya Harada","Ronald M. Summers","Yingying Zhu"],"pdf_url":"https://arxiv.org/pdf/2307.11986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01949v2","updated":"2024-08-27T21:05:09Z","published":"2023-09-05T04:55:10Z","title":"Variational Bayesian Imaging with an Efficient Surrogate Score-based\n  Prior","summary":"  We propose a surrogate function for efficient yet principled use of\nscore-based priors in Bayesian imaging. We consider ill-posed inverse imaging\nproblems in which one aims for a clean image posterior given incomplete or\nnoisy measurements. Since the measurements do not uniquely determine a true\nimage, a prior is needed to constrain the solution space. Recent work turned\nscore-based diffusion models into principled priors for solving ill-posed\nimaging problems by appealing to an ODE-based log-probability function.\nHowever, evaluating the ODE is computationally inefficient and inhibits\nposterior estimation of high-dimensional images. Our proposed surrogate prior\nis based on the evidence lower bound of a score-based diffusion model. We\ndemonstrate the surrogate prior on variational inference for efficient\napproximate posterior sampling of large images. Compared to the exact prior in\nprevious work, our surrogate accelerates optimization of the variational image\ndistribution by at least two orders of magnitude. We also find that our\nprincipled approach gives more accurate posterior estimation than\nnon-variational diffusion-based approaches that involve hyperparameter-tuning\nat inference. Our work establishes a practical path forward for using\nscore-based diffusion models as general-purpose image priors.\n","authors":["Berthy T. Feng","Katherine L. Bouman"],"pdf_url":"https://arxiv.org/pdf/2309.01949v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR) August\n  2024"},{"id":"http://arxiv.org/abs/2302.11552v5","updated":"2024-08-27T20:56:53Z","published":"2023-02-22T18:48:46Z","title":"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based\n  Diffusion Models and MCMC","summary":"  Since their introduction, diffusion models have quickly become the prevailing\napproach to generative modeling in many domains. They can be interpreted as\nlearning the gradients of a time-varying sequence of log-probability density\nfunctions. This interpretation has motivated classifier-based and\nclassifier-free guidance as methods for post-hoc control of diffusion models.\nIn this work, we build upon these ideas using the score-based interpretation of\ndiffusion models, and explore alternative ways to condition, modify, and reuse\ndiffusion models for tasks involving compositional generation and guidance. In\nparticular, we investigate why certain types of composition fail using current\ntechniques and present a number of solutions. We conclude that the sampler (not\nthe model) is responsible for this failure and propose new samplers, inspired\nby MCMC, which enable successful compositional generation. Further, we propose\nan energy-based parameterization of diffusion models which enables the use of\nnew compositional operators and more sophisticated, Metropolis-corrected\nsamplers. Intriguingly we find these samplers lead to notable improvements in\ncompositional generation across a wide set of problems such as\nclassifier-guided ImageNet modeling and compositional text-to-image generation.\n","authors":["Yilun Du","Conor Durkan","Robin Strudel","Joshua B. Tenenbaum","Sander Dieleman","Rob Fergus","Jascha Sohl-Dickstein","Arnaud Doucet","Will Grathwohl"],"pdf_url":"https://arxiv.org/pdf/2302.11552v5.pdf","comment":"ICML 2023, Project Webpage:\n  https://energy-based-model.github.io/reduce-reuse-recycle/"},{"id":"http://arxiv.org/abs/2408.15398v1","updated":"2024-08-27T20:49:11Z","published":"2024-08-27T20:49:11Z","title":"Evaluating Pre-Training Bias on Severe Acute Respiratory Syndrome\n  Dataset","summary":"  Machine learning (ML) is a growing field of computer science that has found\nmany practical applications in several domains, including Health. However, as\ndata grows in size and availability, and the number of models that aim to aid\nor replace human decisions, it raises the concern that these models can be\nsusceptible to bias, which can lead to harm to specific individuals by basing\nits decisions on protected attributes such as gender, religion, sexual\norientation, ethnicity, and others. Visualization techniques might generate\ninsights and help summarize large datasets, enabling data scientists to\nunderstand the data better before training a model by evaluating pre-training\nmetrics applied to the datasets before training, which might contribute to\nidentifying potential harm before any effort is put into training and deploying\nthe models. This work uses the severe acute respiratory syndrome dataset from\nOpenDataSUS to visualize three pre-training bias metrics and their distribution\nacross different regions in Brazil. A random forest model is trained in each\nregion and applied to the others. The aim is to compare the bias for the\ndifferent regions, focusing on their protected attributes and comparing the\nmodel's performance with the metric values.\n","authors":["Diego Dimer Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2408.15398v1.pdf","comment":"short paper for eurovis, 5 pages"},{"id":"http://arxiv.org/abs/2408.15388v1","updated":"2024-08-27T20:14:42Z","published":"2024-08-27T20:14:42Z","title":"Panoptic Perception for Autonomous Driving: A Survey","summary":"  Panoptic perception represents a forefront advancement in autonomous driving\ntechnology, unifying multiple perception tasks into a singular, cohesive\nframework to facilitate a thorough understanding of the vehicle's surroundings.\nThis survey reviews typical panoptic perception models for their unique inputs\nand architectures and compares them to performance, responsiveness, and\nresource utilization. It also delves into the prevailing challenges faced in\npanoptic perception and explores potential trajectories for future research.\nOur goal is to furnish researchers in autonomous driving with a detailed\nsynopsis of panoptic perception, positioning this survey as a pivotal reference\nin the ever-evolving landscape of autonomous driving technologies.\n","authors":["Yunge Li","Lanyu Xu"],"pdf_url":"https://arxiv.org/pdf/2408.15388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15386v1","updated":"2024-08-27T20:08:33Z","published":"2024-08-27T20:08:33Z","title":"Multi-Feature Aggregation in Diffusion Models for Enhanced Face\n  Super-Resolution","summary":"  Super-resolution algorithms often struggle with images from surveillance\nenvironments due to adverse conditions such as unknown degradation, variations\nin pose, irregular illumination, and occlusions. However, acquiring multiple\nimages, even of low quality, is possible with surveillance cameras. In this\nwork, we develop an algorithm based on diffusion models that utilize a\nlow-resolution image combined with features extracted from multiple low-quality\nimages to generate a super-resolved image while minimizing distortions in the\nindividual's identity. Unlike other algorithms, our approach recovers facial\nfeatures without explicitly providing attribute information or without the need\nto calculate a gradient of a function during the reconstruction process. To the\nbest of our knowledge, this is the first time multi-features combined with\nlow-resolution images are used as conditioners to generate more reliable\nsuper-resolution images using stochastic differential equations. The FFHQ\ndataset was employed for training, resulting in state-of-the-art performance in\nfacial recognition and verification metrics when evaluated on the CelebA and\nQuis-Campi datasets. Our code is publicly available at\nhttps://github.com/marcelowds/fasr\n","authors":["Marcelo dos Santos","Rayson Laroca","Rafael O. Ribeiro","JoÃ£o C. Neves","David Menotti"],"pdf_url":"https://arxiv.org/pdf/2408.15386v1.pdf","comment":"Accepted for presentation at the Conference on Graphics, Patterns and\n  Images (SIBGRAPI) 2024"},{"id":"http://arxiv.org/abs/2408.01959v2","updated":"2024-08-27T19:57:45Z","published":"2024-08-04T08:26:58Z","title":"Dataset Scale and Societal Consistency Mediate Facial Impression Bias in\n  Vision-Language AI","summary":"  Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting.\n","authors":["Robert Wolfe","Aayushi Dangol","Alexis Hiniker","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.01959v2.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2312.06731v6","updated":"2024-08-27T19:51:13Z","published":"2023-12-11T09:44:41Z","title":"Genixer: Empowering Multimodal Large Language Models as a Powerful Data\n  Generator","summary":"  Multimodal Large Language Models (MLLMs) demonstrate exceptional\nproblem-solving capabilities, but few research studies aim to gauge the ability\nto generate visual instruction tuning data. This paper proposes to explore the\npotential of empowering MLLMs to generate data independently without relying on\nGPT-4. We introduce Genixer, a comprehensive data generation pipeline\nconsisting of four key steps: (i) instruction data collection, (ii) instruction\ntemplate design, (iii) empowering MLLMs, and (iv) data generation and\nfiltering. Additionally, we outline two modes of data generation: task-agnostic\nand task-specific, enabling controllable output. We demonstrate that a\nsynthetic VQA-like dataset trained with LLaVA1.5 enhances performance on 10 out\nof 12 multimodal benchmarks. Additionally, the grounding MLLM Shikra, when\ntrained with a REC-like synthetic dataset, shows improvements on 7 out of 8 REC\ndatasets. Through experiments and synthetic data analysis, our findings are:\n(1) current MLLMs can serve as robust data generators without assistance from\nGPT-4V; (2) MLLMs trained with task-specific datasets can surpass GPT-4V in\ngenerating complex instruction tuning data; (3) synthetic datasets enhance\nperformance across various multimodal benchmarks and help mitigate model\nhallucinations. The data, code, and models can be found at\nhttps://github.com/zhaohengyuan1/Genixer.\n","authors":["Henry Hengyuan Zhao","Pan Zhou","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2312.06731v6.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.15374v1","updated":"2024-08-27T19:22:06Z","published":"2024-08-27T19:22:06Z","title":"CycleGAN with Better Cycles","summary":"  CycleGAN provides a framework to train image-to-image translation with\nunpaired datasets using cycle consistency loss [4]. While results are great in\nmany applications, the pixel level cycle consistency can potentially be\nproblematic and causes unrealistic images in certain cases. In this project, we\npropose three simple modifications to cycle consistency, and show that such an\napproach achieves better results with fewer artifacts.\n","authors":["Tongzhou Wang","Yihan Lin"],"pdf_url":"https://arxiv.org/pdf/2408.15374v1.pdf","comment":"Technical Report 2018"},{"id":"http://arxiv.org/abs/2408.15373v1","updated":"2024-08-27T19:13:15Z","published":"2024-08-27T19:13:15Z","title":"Handling Geometric Domain Shifts in Semantic Segmentation of Surgical\n  RGB and Hyperspectral Images","summary":"  Robust semantic segmentation of intraoperative image data holds promise for\nenabling automatic surgical scene understanding and autonomous robotic surgery.\nWhile model development and validation are primarily conducted on idealistic\nscenes, geometric domain shifts, such as occlusions of the situs, are common in\nreal-world open surgeries. To close this gap, we (1) present the first analysis\nof state-of-the-art (SOA) semantic segmentation models when faced with\ngeometric out-of-distribution (OOD) data, and (2) propose an augmentation\ntechnique called \"Organ Transplantation\", to enhance generalizability. Our\ncomprehensive validation on six different OOD datasets, comprising 600 RGB and\nhyperspectral imaging (HSI) cubes from 33 pigs, each annotated with 19 classes,\nreveals a large performance drop in SOA organ segmentation models on geometric\nOOD data. This performance decline is observed not only in conventional RGB\ndata (with a dice similarity coefficient (DSC) drop of 46 %) but also in HSI\ndata (with a DSC drop of 45 %), despite the richer spectral information\ncontent. The performance decline increases with the spatial granularity of the\ninput data. Our augmentation technique improves SOA model performance by up to\n67 % for RGB data and 90 % for HSI data, achieving performance at the level of\nin-distribution performance on real OOD test data. Given the simplicity and\neffectiveness of our augmentation method, it is a valuable tool for addressing\ngeometric domain shifts in surgical scene segmentation, regardless of the\nunderlying model. Our code and pre-trained models are publicly available at\nhttps://github.com/IMSY-DKFZ/htc.\n","authors":["Silvia Seidlitz","Jan Sellner","Alexander Studier-Fischer","Alessandro Motta","Berkin Ãzdemir","Beat P. MÃ¼ller-Stich","Felix Nickel","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2408.15373v1.pdf","comment":"Silvia Seidlitz and Jan Sellner contributed equally"},{"id":"http://arxiv.org/abs/2408.13912v2","updated":"2024-08-27T19:06:57Z","published":"2024-08-25T18:27:20Z","title":"Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs","summary":"  In this paper, we introduce Splatt3R, a pose-free, feed-forward method for\nin-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given\nuncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without\nrequiring any camera parameters or depth information. For generalizability, we\nbuild Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R,\nby extending it to deal with both 3D structure and appearance. Specifically,\nunlike the original MASt3R which reconstructs only 3D point clouds, we predict\nthe additional Gaussian attributes required to construct a Gaussian primitive\nfor each point. Hence, unlike other novel view synthesis methods, Splatt3R is\nfirst trained by optimizing the 3D point cloud's geometry loss, and then a\nnovel view synthesis objective. By doing this, we avoid the local minima\npresent in training 3D Gaussian Splats from stereo views. We also propose a\nnovel loss masking strategy that we empirically find is critical for strong\nperformance on extrapolated viewpoints. We train Splatt3R on the ScanNet++\ndataset and demonstrate excellent generalisation to uncalibrated, in-the-wild\nimages. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and\nthe resultant splats can be rendered in real-time.\n","authors":["Brandon Smart","Chuanxia Zheng","Iro Laina","Victor Adrian Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2408.13912v2.pdf","comment":"Our project page can be found at: https://splatt3r.active.vision/"},{"id":"http://arxiv.org/abs/2301.06267v5","updated":"2024-08-27T19:00:47Z","published":"2023-01-16T05:40:42Z","title":"Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with\n  Multimodal Models","summary":"  The ability to quickly learn a new task with minimal instruction - known as\nfew-shot learning - is a central aspect of intelligent agents. Classical\nfew-shot benchmarks make use of few-shot samples from a single modality, but\nsuch samples may not be sufficient to characterize an entire concept class. In\ncontrast, humans use cross-modal information to learn new concepts efficiently.\nIn this work, we demonstrate that one can indeed build a better ${\\bf visual}$\ndog classifier by ${\\bf read}$ing about dogs and ${\\bf listen}$ing to them\nbark. To do so, we exploit the fact that recent multimodal foundation models\nsuch as CLIP learn cross-modal encoders that map different modalities to the\nsame representation space. Specifically, we propose a simple strategy for ${\\bf\ncross-modal}$ ${\\bf adaptation}$: we treat examples from different modalities\nas additional few-shot examples. For example, by simply repurposing class names\nas an additional training sample, we trivially turn any n-shot learning problem\ninto a (n+1)-shot problem. This allows us to produce SOTA results with\nembarrassingly simple linear classifiers. We show that our approach can be\ncombined with existing methods such as prefix tuning, adapters, and classifier\nensembling. Finally, to explore other modalities beyond vision and language, we\nconstruct the first (to our knowledge) audiovisual few-shot benchmark and use\ncross-modal training to improve the performance of both image and audio\nclassification.\n","authors":["Zhiqiu Lin","Samuel Yu","Zhiyi Kuang","Deepak Pathak","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2301.06267v5.pdf","comment":"Published at CVPR 2023. Project site:\n  https://linzhiqiu.github.io/papers/cross_modal/"},{"id":"http://arxiv.org/abs/2406.14568v2","updated":"2024-08-27T18:42:09Z","published":"2024-04-29T23:53:42Z","title":"Policy Gradient-Driven Noise Mask","summary":"  Deep learning classifiers face significant challenges when dealing with\nheterogeneous multi-modal and multi-organ biomedical datasets. The low-level\nfeature distinguishability limited to imaging-modality hinders the classifiers'\nability to learn high-level semantic relationships, resulting in sub-optimal\nperformance. To address this issue, image augmentation strategies are employed\nas regularization techniques. While additive noise input during network\ntraining is a well-established augmentation as regularization method, modern\npipelines often favor more robust techniques such as dropout and weight decay.\nThis preference stems from the observation that combining these established\ntechniques with noise input can adversely affect model performance.\n  In this study, we propose a novel pretraining pipeline that learns to\ngenerate conditional noise mask specifically tailored to improve performance on\nmulti-modal and multi-organ datasets. As a reinforcement learning algorithm,\nour approach employs a dual-component system comprising a very light-weight\npolicy network that learns to sample conditional noise using a differentiable\nbeta distribution as well as a classifier network. The policy network is\ntrained using the reinforce algorithm to generate image-specific noise masks\nthat regularize the classifier during pretraining. A key aspect is that the\npolicy network's role is limited to obtaining an intermediate (or heated) model\nbefore fine-tuning. During inference, the policy network is omitted, allowing\ndirect comparison between the baseline and noise-regularized models.\n  We conducted experiments and related analyses on RadImageNet datasets.\nResults demonstrate that fine-tuning the intermediate models consistently\noutperforms conventional training algorithms on both classification and\ngeneralization to unseen concept tasks.\n","authors":["Mehmet Can Yavuz","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.14568v2.pdf","comment":"13 pages; 8 figures; 5 tables"},{"id":"http://arxiv.org/abs/2403.10170v2","updated":"2024-08-27T18:36:12Z","published":"2024-03-15T10:26:52Z","title":"Computer User Interface Understanding. A New Dataset and a Learning\n  Framework","summary":"  User Interface (UI) understanding has been an increasingly popular topic over\nthe last few years. So far, there has been a vast focus solely on web and\nmobile applications. In this paper, we introduce the harder task of computer UI\nunderstanding. With the goal of enabling research in this field, we have\ngenerated a dataset with a set of videos where a user is performing a sequence\nof actions and each image shows the desktop contents at that time point. We\nalso present a framework that is composed of a synthetic sample generation\npipeline to augment the dataset with relevant characteristics, and a\ncontrastive learning method to classify images in the videos. We take advantage\nof the natural conditional, tree-like, relationship of the images'\ncharacteristics to regularize the learning of the representations by dealing\nwith multiple partial tasks simultaneously. Experimental results show that the\nproposed framework outperforms previously proposed hierarchical multi-label\ncontrastive losses in fine-grain UI classification.\n","authors":["AndrÃ©s MuÃ±oz","Daniel Borrajo"],"pdf_url":"https://arxiv.org/pdf/2403.10170v2.pdf","comment":"14 pages main paper, 6 pages appendix"},{"id":"http://arxiv.org/abs/2408.15355v1","updated":"2024-08-27T18:27:47Z","published":"2024-08-27T18:27:47Z","title":"Optimizing Lung Cancer Detection in CT Imaging: A Wavelet Multi-Layer\n  Perceptron (WMLP) Approach Enhanced by Dragonfly Algorithm (DA)","summary":"  Lung cancer stands as the preeminent cause of cancer-related mortality\nglobally. Prompt and precise diagnosis, coupled with effective treatment, is\nimperative to reduce the fatality rates associated with this formidable\ndisease. This study introduces a cutting-edge deep learning framework for the\nclassification of lung cancer from CT scan imagery. The research encompasses a\nsuite of image pre-processing strategies, notably Canny edge detection, and\nwavelet transformations, which precede the extraction of salient features and\nsubsequent classification via a Multi-Layer Perceptron (MLP). The optimization\nprocess is further refined using the Dragonfly Algorithm (DA). The methodology\nput forth has attained an impressive training and testing accuracy of 99.82\\%,\nunderscoring its efficacy and reliability in the accurate diagnosis of lung\ncancer.\n","authors":["Bitasadat Jamshidi","Nastaran Ghorbani","Mohsen Rostamy-Malkhalifeh"],"pdf_url":"https://arxiv.org/pdf/2408.15355v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.15232v1","updated":"2024-08-27T17:50:03Z","published":"2024-08-27T17:50:03Z","title":"Into the Unknown Unknowns: Engaged Human Learning through Participation\n  in Language Model Agent Conversations","summary":"  While language model (LM)-powered chatbots and generative search engines\nexcel at answering concrete queries, discovering information in the terrain of\nunknown unknowns remains challenging for users. To emulate the common\neducational scenario where children/students learn by listening to and\nparticipating in conversations of their parents/teachers, we create\nCollaborative STORM (Co-STORM). Unlike QA systems that require users to ask all\nthe questions, Co-STORM lets users observe and occasionally steer the discourse\namong several LM agents. The agents ask questions on the user's behalf,\nallowing the user to discover unknown unknowns serendipitously. To facilitate\nuser interaction, Co-STORM assists users in tracking the discourse by\norganizing the uncovered information into a dynamic mind map, ultimately\ngenerating a comprehensive report as takeaways. For automatic evaluation, we\nconstruct the WildSeek dataset by collecting real information-seeking records\nwith user goals. Co-STORM outperforms baseline methods on both discourse trace\nand report quality. In a further human evaluation, 70% of participants prefer\nCo-STORM over a search engine, and 78% favor it over a RAG chatbot.\n","authors":["Yucheng Jiang","Yijia Shao","Dekun Ma","Sina J. Semnani","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2408.15232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15172v1","updated":"2024-08-27T16:10:21Z","published":"2024-08-27T16:10:21Z","title":"X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation","summary":"  Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been\nshown to enhance the effectiveness of enriching item descriptions, thereby\nimproving the accuracy of recommendation systems. However, most existing\napproaches either rely on text-only prompting or employ basic multimodal\nstrategies that do not fully exploit the complementary information available\nfrom both textual and visual modalities. This paper introduces a novel\nframework, Cross-Reflection Prompting, termed X-Reflect, designed to address\nthese limitations by prompting LMMs to explicitly identify and reconcile\nsupportive and conflicting information between text and images. By capturing\nnuanced insights from both modalities, this approach generates more\ncomprehensive and contextually richer item representations. Extensive\nexperiments conducted on two widely used benchmarks demonstrate that our method\noutperforms existing prompting baselines in downstream recommendation accuracy.\nAdditionally, we evaluate the generalizability of our framework across\ndifferent LMM backbones and the robustness of the prompting strategies,\noffering insights for optimization. This work underscores the importance of\nintegrating multimodal information and presents a novel solution for improving\nitem understanding in multimodal recommendation systems.\n","authors":["Hanjia Lyu","Ryan Rossi","Xiang Chen","Md Mehrab Tanjim","Stefano Petrangeli","Somdeb Sarkhel","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2408.15172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16828v2","updated":"2024-08-27T15:07:28Z","published":"2024-07-23T20:38:23Z","title":"Pareto Front Approximation for Multi-Objective Session-Based Recommender\n  Systems","summary":"  This work introduces MultiTRON, an approach that adapts Pareto front\napproximation techniques to multi-objective session-based recommender systems\nusing a transformer neural network. Our approach optimizes trade-offs between\nkey metrics such as click-through and conversion rates by training on sampled\npreference vectors. A significant advantage is that after training, a single\nmodel can access the entire Pareto front, allowing it to be tailored to meet\nthe specific requirements of different stakeholders by adjusting an additional\ninput vector that weights the objectives. We validate the model's performance\nthrough extensive offline and online evaluation. For broader application and\nresearch, the source code is made available at\nhttps://github.com/otto-de/MultiTRON. The results confirm the model's ability\nto manage multiple recommendation objectives effectively, offering a flexible\ntool for diverse business needs.\n","authors":["Timo Wilm","Philipp Normann","Felix Stepprath"],"pdf_url":"https://arxiv.org/pdf/2407.16828v2.pdf","comment":"Accepted at the Eighteenth ACM Conference on Recommender Systems\n  (RecSys '24)"},{"id":"http://arxiv.org/abs/2402.09766v2","updated":"2024-08-27T13:01:56Z","published":"2024-02-15T07:35:52Z","title":"From Variability to Stability: Advancing RecSys Benchmarking Practices","summary":"  In the rapidly evolving domain of Recommender Systems (RecSys), new\nalgorithms frequently claim state-of-the-art performance based on evaluations\nover a limited set of arbitrarily selected datasets. However, this approach may\nfail to holistically reflect their effectiveness due to the significant impact\nof dataset characteristics on algorithm performance. Addressing this\ndeficiency, this paper introduces a novel benchmarking methodology to\nfacilitate a fair and robust comparison of RecSys algorithms, thereby advancing\nevaluation practices. By utilizing a diverse set of $30$ open datasets,\nincluding two introduced in this work, and evaluating $11$ collaborative\nfiltering algorithms across $9$ metrics, we critically examine the influence of\ndataset characteristics on algorithm performance. We further investigate the\nfeasibility of aggregating outcomes from multiple datasets into a unified\nranking. Through rigorous experimental analysis, we validate the reliability of\nour methodology under the variability of datasets, offering a benchmarking\nstrategy that balances quality and computational demands. This methodology\nenables a fair yet effective means of evaluating RecSys algorithms, providing\nvaluable guidance for future research endeavors.\n","authors":["Valeriy Shevchenko","Nikita Belousov","Alexey Vasilev","Vladimir Zholobov","Artyom Sosedka","Natalia Semenova","Anna Volodkevich","Andrey Savchenko","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2402.09766v2.pdf","comment":"8 pages with 11 figures"},{"id":"http://arxiv.org/abs/2408.15004v1","updated":"2024-08-27T12:41:37Z","published":"2024-08-27T12:41:37Z","title":"Measuring publication relatedness using controlled vocabularies","summary":"  Measuring the relatedness between scientific publications has important\napplications in many areas of bibliometrics and science policy. Controlled\nvocabularies provide a promising basis for measuring relatedness because they\naddress issues that arise when using citation or textual similarity to measure\nrelatedness. While several controlled-vocabulary-based relatedness measures\nhave been developed, there exists no comprehensive and direct test of their\naccuracy and suitability for different types of research questions. This paper\nreviews existing measures, develops a new measure, and benchmarks the measures\nusing TREC Genomics data as a ground truth of topics. The benchmark test show\nthat the new measure and the measure proposed by Ahlgren et al. (2020) have\ndiffering strengths and weaknesses. These results inform a discussion of which\nmethod to choose when studying interdisciplinarity, information retrieval,\nclustering of science, and researcher topic switching.\n","authors":["Emil Dolmer Alnor"],"pdf_url":"https://arxiv.org/pdf/2408.15004v1.pdf","comment":"Accepted for presentation at the 28th International Conference on\n  Science, Technology and Innovation Indicators, 2024"},{"id":"http://arxiv.org/abs/2408.15002v1","updated":"2024-08-27T12:34:41Z","published":"2024-08-27T12:34:41Z","title":"Knowledge Discovery in Optical Music Recognition: Enhancing Information\n  Retrieval with Instance Segmentation","summary":"  Optical Music Recognition (OMR) automates the transcription of musical\nnotation from images into machine-readable formats like MusicXML, MEI, or MIDI,\nsignificantly reducing the costs and time of manual transcription. This study\nexplores knowledge discovery in OMR by applying instance segmentation using\nMask R-CNN to enhance the detection and delineation of musical symbols in sheet\nmusic. Unlike Optical Character Recognition (OCR), OMR must handle the\nintricate semantics of Common Western Music Notation (CWMN), where symbol\nmeanings depend on shape, position, and context. Our approach leverages\ninstance segmentation to manage the density and overlap of musical symbols,\nfacilitating more precise information retrieval from music scores. Evaluations\non the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with\nour method achieving a mean Average Precision (mAP) of up to 59.70\\% in dense\nsymbol environments, achieving comparable results to object detection.\nFurthermore, using traditional computer vision techniques, we add a parallel\nstep for staff detection to infer the pitch for the recognised symbols. This\nstudy emphasises the role of pixel-wise segmentation in advancing accurate\nmusic symbol recognition, contributing to knowledge discovery in OMR. Our\nfindings indicate that instance segmentation provides more precise\nrepresentations of musical symbols, particularly in densely populated scores,\nadvancing OMR technology. We make our implementation, pre-processing scripts,\ntrained models, and evaluation results publicly available to support further\nresearch and development.\n","authors":["Elona Shatri","George Fazekas"],"pdf_url":"https://arxiv.org/pdf/2408.15002v1.pdf","comment":"8 pages content and one references, accepted version at the\n  International Conference on Knowledge Discovery and Information Retrieval\n  2024, Porto, Portugal"},{"id":"http://arxiv.org/abs/2408.14968v1","updated":"2024-08-27T11:21:19Z","published":"2024-08-27T11:21:19Z","title":"MRSE: An Efficient Multi-modality Retrieval System for Large Scale\n  E-commerce","summary":"  Providing high-quality item recall for text queries is crucial in large-scale\ne-commerce search systems. Current Embedding-based Retrieval Systems (ERS)\nembed queries and items into a shared low-dimensional space, but uni-modality\nERS rely too heavily on textual features, making them unreliable in complex\ncontexts. While multi-modality ERS incorporate various data sources, they often\noverlook individual preferences for different modalities, leading to suboptimal\nresults. To address these issues, we propose MRSE, a Multi-modality Retrieval\nSystem that integrates text, item images, and user preferences through\nlightweight mixture-of-expert (LMoE) modules to better align features across\nand within modalities. MRSE also builds user profiles at a multi-modality level\nand introduces a novel hybrid loss function that enhances consistency and\nrobustness using hard negative sampling. Experiments on a large-scale dataset\nfrom Shopee and online A/B testing show that MRSE achieves an 18.9% improvement\nin offline relevance and a 3.7% gain in online core metrics compared to\nShopee's state-of-the-art uni-modality system.\n","authors":["Hao Jiang","Haoxiang Zhang","Qingshan Hou","Chaofeng Chen","Weisi Lin","Jingchang Zhang","Annan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14908v1","updated":"2024-08-27T09:35:13Z","published":"2024-08-27T09:35:13Z","title":"TriplÃ¨toile: Extraction of Knowledge from Microblogging Text","summary":"  Numerous methods and pipelines have recently emerged for the automatic\nextraction of knowledge graphs from documents such as scientific publications\nand patents. However, adapting these methods to incorporate alternative text\nsources like micro-blogging posts and news has proven challenging as they\nstruggle to model open-domain entities and relations, typically found in these\nsources. In this paper, we propose an enhanced information extraction pipeline\ntailored to the extraction of a knowledge graph comprising open-domain entities\nfrom micro-blogging posts on social media platforms. Our pipeline leverages\ndependency parsing and classifies entity relations in an unsupervised manner\nthrough hierarchical clustering over word embeddings. We provide a use case on\nextracting semantic triples from a corpus of 100 thousand tweets about digital\ntransformation and publicly release the generated knowledge graph. On the same\ndataset, we conduct two experimental evaluations, showing that the system\nproduces triples with precision over 95% and outperforms similar pipelines of\naround 5% in terms of precision, while generating a comparatively higher number\nof triples.\n","authors":["Vanni Zavarella","Sergio Consoli","Diego Reforgiato Recupero","Gianni Fenu","Simone Angioni","Davide Buscaldi","Danilo DessÃ¬","Francesco Osborne"],"pdf_url":"https://arxiv.org/pdf/2408.14908v1.pdf","comment":"42 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.14906v1","updated":"2024-08-27T09:34:38Z","published":"2024-08-27T09:34:38Z","title":"Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval","summary":"  In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.\n","authors":["Melisa Russak","Umar Jamil","Christopher Bryant","Kiran Kamble","Axel Magnuson","Mateusz Russak","Waseem AlShikh"],"pdf_url":"https://arxiv.org/pdf/2408.14906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14851v1","updated":"2024-08-27T08:08:05Z","published":"2024-08-27T08:08:05Z","title":"Graph and Sequential Neural Networks in Session-based Recommendation: A\n  Survey","summary":"  Recent years have witnessed the remarkable success of recommendation systems\n(RSs) in alleviating the information overload problem. As a new paradigm of\nRSs, session-based recommendation (SR) specializes in users' short-term\npreference capture and aims to provide a more dynamic and timely recommendation\nbased on the ongoing interacted actions. In this survey, we will give a\ncomprehensive overview of the recent works on SR. First, we clarify the\ndefinitions of various SR tasks and introduce the characteristics of\nsession-based recommendation against other recommendation tasks. Then, we\nsummarize the existing methods in two categories: sequential neural network\nbased methods and graph neural network (GNN) based methods. The standard\nframeworks and technical are also introduced. Finally, we discuss the\nchallenges of SR and new research directions in this area.\n","authors":["Zihao Li","Chao Yang","Yakun Chen","Xianzhi Wang","Hongxu Chen","Guandong Xu","Lina Yao","Quan Z. Sheng"],"pdf_url":"https://arxiv.org/pdf/2408.14851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14043v2","updated":"2024-08-27T06:18:05Z","published":"2024-06-20T07:06:58Z","title":"Taxonomy-Guided Zero-Shot Recommendations with LLMs","summary":"  With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec.\n","authors":["Yueqing Liang","Liangwei Yang","Chen Wang","Xiongxiao Xu","Philip S. Yu","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2406.14043v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01262v3","updated":"2024-08-27T03:13:50Z","published":"2024-08-02T13:35:11Z","title":"RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework","summary":"  Retrieval-Augmented Generation (RAG) systems have demonstrated their\nadvantages in alleviating the hallucination of Large Language Models (LLMs).\nExisting RAG benchmarks mainly focus on evaluating whether LLMs can correctly\nanswer the general knowledge. However, they are unable to evaluate the\neffectiveness of the RAG system in dealing with the data from different\nvertical domains. This paper introduces RAGEval, a framework for automatically\ngenerating evaluation datasets to evaluate the knowledge usage ability of\ndifferent LLMs in different scenarios. Specifically, RAGEval summarizes a\nschema from seed documents, applies the configurations to generate diverse\ndocuments, and constructs question-answering pairs according to both articles\nand configurations. We propose three novel metrics, Completeness,\nHallucination, and Irrelevance, to carefully evaluate the responses generated\nby LLMs. By benchmarking RAG models in vertical domains, RAGEval has the\nability to better evaluate the knowledge usage ability of LLMs, which avoids\nthe confusion regarding the source of knowledge in answering question in\nexisting QA datasets--whether it comes from parameterized memory or retrieval.\nThe code and dataset will be released.\n","authors":["Kunlun Zhu","Yifan Luo","Dingling Xu","Ruobing Wang","Shi Yu","Shuo Wang","Yukun Yan","Zhenghao Liu","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01262v3.pdf","comment":"add github repo"},{"id":"http://arxiv.org/abs/2408.14743v1","updated":"2024-08-27T02:43:40Z","published":"2024-08-27T02:43:40Z","title":"Personalized Video Summarization using Text-Based Queries and\n  Conditional Modeling","summary":"  The proliferation of video content on platforms like YouTube and Vimeo\npresents significant challenges in efficiently locating relevant information.\nAutomatic video summarization aims to address this by extracting and presenting\nkey content in a condensed form. This thesis explores enhancing video\nsummarization by integrating text-based queries and conditional modeling to\ntailor summaries to user needs. Traditional methods often produce fixed\nsummaries that may not align with individual requirements. To overcome this, we\npropose a multi-modal deep learning approach that incorporates both textual\nqueries and visual information, fusing them at different levels of the model\narchitecture. Evaluation metrics such as accuracy and F1-score assess the\nquality of the generated summaries. The thesis also investigates improving\ntext-based query representations using contextualized word embeddings and\nspecialized attention networks. This enhances the semantic understanding of\nqueries, leading to better video summaries. To emulate human-like\nsummarization, which accounts for both visual coherence and abstract factors\nlike storyline consistency, we introduce a conditional modeling approach. This\nmethod uses multiple random variables and joint distributions to capture key\nsummarization components, resulting in more human-like and explainable\nsummaries. Addressing data scarcity in fully supervised learning, the thesis\nproposes a segment-level pseudo-labeling approach. This self-supervised method\ngenerates additional data, improving model performance even with limited\nhuman-labeled datasets. In summary, this research aims to enhance automatic\nvideo summarization by incorporating text-based queries, improving query\nrepresentations, introducing conditional modeling, and addressing data\nscarcity, thereby creating more effective and personalized video summaries.\n","authors":["Jia-Hong Huang"],"pdf_url":"https://arxiv.org/pdf/2408.14743v1.pdf","comment":"Ph.D. thesis, 137 pages"},{"id":"http://arxiv.org/abs/2408.14723v1","updated":"2024-08-27T01:23:49Z","published":"2024-08-27T01:23:49Z","title":"Snap and Diagnose: An Advanced Multimodal Retrieval System for\n  Identifying Plant Diseases in the Wild","summary":"  Plant disease recognition is a critical task that ensures crop health and\nmitigates the damage caused by diseases. A handy tool that enables farmers to\nreceive a diagnosis based on query pictures or the text description of\nsuspicious plants is in high demand for initiating treatment before potential\ndiseases spread further. In this paper, we develop a multimodal plant disease\nimage retrieval system to support disease search based on either image or text\nprompts. Specifically, we utilize the largest in-the-wild plant disease dataset\nPlantWild, which includes over 18,000 images across 89 categories, to provide a\ncomprehensive view of potential diseases relating to the query. Furthermore,\ncross-modal retrieval is achieved in the developed system, facilitated by a\nnovel CLIP-based vision-language model that encodes both disease descriptions\nand disease images into the same latent space. Built on top of the retriever,\nour retrieval system allows users to upload either plant disease images or\ndisease descriptions to retrieve the corresponding images with similar\ncharacteristics from the disease dataset to suggest candidate diseases for end\nusers' consideration.\n","authors":["Tianqi Wei","Zhi Chen","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2408.14723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15371v1","updated":"2024-08-27T19:10:21Z","published":"2024-08-27T19:10:21Z","title":"Temporal Graph Neural Network-Powered Paper Recommendation on Dynamic\n  Citation Networks","summary":"  Due to the rapid growth of scientific publications, identifying all related\nreference articles in the literature has become increasingly challenging yet\nhighly demanding. Existing methods primarily assess candidate publications from\na static perspective, focusing on the content of articles and their structural\ninformation, such as citation relationships. There is a lack of research\nregarding how to account for the evolving impact among papers on their\nembeddings. Toward this goal, this paper introduces a temporal dimension to\npaper recommendation strategies. The core idea is to continuously update a\npaper's embedding when new citation relationships appear, enhancing its\nrelevance for future recommendations. Whenever a citation relationship is added\nto the literature upon the publication of a paper, the embeddings of the two\nrelated papers are updated through a Temporal Graph Neural Network (TGN). A\nlearnable memory update module based on a Recurrent Neural Network (RNN) is\nutilized to study the evolution of the embedding of a paper in order to predict\nits reference impact in a future timestamp. Such a TGN-based model learns a\npattern of how people's views of the paper may evolve, aiming to guide paper\nrecommendations more precisely. Extensive experiments on an open citation\nnetwork dataset, including 313,278 articles from\nhttps://paperswithcode.com/about PaperWithCode, have demonstrated the\neffectiveness of the proposed approach.\n","authors":["Junhao Shen","Mohammad Ausaf Ali Haqqani","Beichen Hu","Cheng Huang","Xihao Xie","Tsengdar Lee","Jia Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15371v1.pdf","comment":"10 pages, 4 figures, accepted by SDU@AAAI-2024. The AAAI Workshop on\n  Scientific Document Understanding (2024)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.15240v1","updated":"2024-08-27T17:57:45Z","published":"2024-08-27T17:57:45Z","title":"Generative Verifiers: Reward Modeling as Next-Token Prediction","summary":"  Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional inference-time compute via majority voting for better verification.\nWe demonstrate that when using Gemma-based verifiers on algorithmic and\ngrade-school math reasoning tasks, GenRM outperforms discriminative verifiers\nand LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems\nsolved with Best-of-N. Furthermore, we show that GenRM scales favorably across\ndataset size, model capacity, and inference-time compute.\n","authors":["Lunjun Zhang","Arian Hosseini","Hritik Bansal","Mehran Kazemi","Aviral Kumar","Rishabh Agarwal"],"pdf_url":"https://arxiv.org/pdf/2408.15240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15237v1","updated":"2024-08-27T17:56:11Z","published":"2024-08-27T17:56:11Z","title":"The Mamba in the Llama: Distilling and Accelerating Hybrid Models","summary":"  Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.\n","authors":["Junxiong Wang","Daniele Paliotta","Avner May","Alexander M. Rush","Tri Dao"],"pdf_url":"https://arxiv.org/pdf/2408.15237v1.pdf","comment":"Code is open-sourced at https://github.com/jxiw/MambaInLlama"},{"id":"http://arxiv.org/abs/2408.15231v1","updated":"2024-08-27T17:48:29Z","published":"2024-08-27T17:48:29Z","title":"DCT-CryptoNets: Scaling Private Inference in the Frequency Domain","summary":"  The convergence of fully homomorphic encryption (FHE) and machine learning\noffers unprecedented opportunities for private inference of sensitive data. FHE\nenables computation directly on encrypted data, safeguarding the entire machine\nlearning pipeline, including data and model confidentiality. However, existing\nFHE-based implementations for deep neural networks face significant challenges\nin computational cost, latency, and scalability, limiting their practical\ndeployment. This paper introduces DCT-CryptoNets, a novel approach that\nleverages frequency-domain learning to tackle these issues. Our method operates\ndirectly in the frequency domain, utilizing the discrete cosine transform (DCT)\ncommonly employed in JPEG compression. This approach is inherently compatible\nwith remote computing services, where images are usually transmitted and stored\nin compressed formats. DCT-CryptoNets reduces the computational burden of\nhomomorphic operations by focusing on perceptually relevant low-frequency\ncomponents. This is demonstrated by substantial latency reduction of up to\n5.3$\\times$ compared to prior work on image classification tasks, including a\nnovel demonstration of ImageNet inference within 2.5 hours, down from 12.5\nhours compared to prior work on equivalent compute resources. Moreover,\nDCT-CryptoNets improves the reliability of encrypted accuracy by reducing\nvariability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet). This study\ndemonstrates a promising avenue for achieving efficient and practical\nprivacy-preserving deep learning on high resolution images seen in real-world\napplications.\n","authors":["Arjun Roy","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2408.15231v1.pdf","comment":"Under Review; 10 pages content, 3 pages appendix, 4 figures, 8\n  tables; Code TBD"},{"id":"http://arxiv.org/abs/2408.15221v1","updated":"2024-08-27T17:33:30Z","published":"2024-08-27T17:33:30Z","title":"LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet","summary":"  Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses.\n","authors":["Nathaniel Li","Ziwen Han","Ian Steneker","Willow Primack","Riley Goodside","Hugh Zhang","Zifan Wang","Cristina Menghini","Summer Yue"],"pdf_url":"https://arxiv.org/pdf/2408.15221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13643v2","updated":"2024-08-27T17:32:39Z","published":"2022-09-27T19:16:26Z","title":"MPC-Pipe: an Efficient Pipeline Scheme for Secure Multi-party Machine\n  Learning Inference","summary":"  Multi-party computing (MPC) has been gaining popularity as a secure computing\nmodel over the past few years. However, prior works have demonstrated that MPC\nprotocols still pay substantial performance penalties compared to plaintext,\nparticularly when applied to ML algorithms. The overhead is due to added\ncomputation and communication costs. Prior studies, as well as our own\nanalysis, found that most MPC protocols today sequentially perform\ncommunication and computation. The participating parties must compute on their\nshares first and then perform data communication to allow the distribution of\nnew secret shares before proceeding to the next computation step. In this work,\nwe show that serialization is unnecessary, particularly in the context of ML\ncomputations (both in Convolutional neural networks and in Transformer-based\nmodels). We demonstrate that it is possible to carefully orchestrate the\ncomputation and communication steps to overlap.\n  We propose MPC-Pipe, an efficient MPC system for both training and inference\nof ML workloads, which pipelines computations and communications in an MPC\nprotocol during the online phase. MPC-Pipe proposes three pipeline schemes to\noptimize the online phase of ML in the semi-honest majority adversary setting.\nWe implement MPC-Pipe by augmenting a modified version of CrypTen, which\nseparates online and offline phases. We evaluate the end-to-end system\nperformance benefits of the online phase of MPC using deep neural networks\n(VGG16, ResNet50) and Transformers using different network settings. We show\nthat MPC-Pipe can improve the throughput and latency of ML workloads.\n","authors":["Yongqin Wang","Rachit Rajat","Murali Annavaram"],"pdf_url":"https://arxiv.org/pdf/2209.13643v2.pdf","comment":"To be appeared in ASPLOS'25"},{"id":"http://arxiv.org/abs/2209.04042v3","updated":"2024-08-27T17:24:51Z","published":"2022-09-08T21:46:12Z","title":"Assessing Lower Limb Strength using Internet-of-Things Enabled Chair","summary":"  This project describes the application of the technologies of Machine\nLearning and Internet-of-Things to assess the lower limb strength of\nindividuals undergoing rehabilitation or therapy. Specifically, it seeks to\nmeasure and assess the progress of individuals by sensors attached to chairs\nand processing the data through Google GPU Tensorflow CoLab. Pressure sensors\nare attached to various locations on a chair, including but not limited to the\nseating area, backrest, hand rests, and legs. Sensor data from the individual\nperforming both sit-to-stand transition and stand-to-sit transition provides a\ntime series dataset regarding the pressure distribution and vibratory motion on\nthe chair. The dataset and timing information can then be fed into a machine\nlearning model to estimate the relative strength and weakness during various\nphases of the movement.\n","authors":["Chelsea Yeh","Hanna Kaitlin Dy","Phillip Schodinger","Hudson Kaleb Dy"],"pdf_url":"https://arxiv.org/pdf/2209.04042v3.pdf","comment":"12 Pages"},{"id":"http://arxiv.org/abs/2406.14507v2","updated":"2024-08-27T17:19:20Z","published":"2024-06-20T17:12:20Z","title":"On Newton's Method to Unlearn Neural Networks","summary":"  With the widespread applications of neural networks (NNs) trained on personal\ndata, machine unlearning has become increasingly important for enabling\nindividuals to exercise their personal data ownership, particularly the \"right\nto be forgotten\" from trained NNs. Since retraining is computationally\nexpensive, we seek approximate unlearning algorithms for NNs that return\nidentical models to the retrained oracle. While Newton's method has been\nsuccessfully used to approximately unlearn linear models, we observe that\nadapting it for NN is challenging due to degenerate Hessians that make\ncomputing Newton's update impossible. Additionally, we show that when coupled\nwith popular techniques to resolve the degeneracy, Newton's method often incurs\noffensively large norm updates and empirically degrades model performance\npost-unlearning. To address these challenges, we propose CureNewton's method, a\nprinciple approach that leverages cubic regularization to handle the Hessian\ndegeneracy effectively. The added regularizer eliminates the need for manual\nfinetuning and affords a natural interpretation within the unlearning context.\nExperiments across different models and datasets show that our method can\nachieve competitive unlearning performance to the state-of-the-art algorithm in\npractical unlearning settings, while being theoretically justified and\nefficient in running time.\n","authors":["Nhung Bui","Xinyang Lu","Rachael Hwee Ling Sim","See-Kiong Ng","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2406.14507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05921v2","updated":"2024-08-27T17:14:16Z","published":"2024-07-08T13:28:47Z","title":"TAPVid-3D: A Benchmark for Tracking Any Point in 3D","summary":"  We introduce a new benchmark, TAPVid-3D, for evaluating the task of\nlong-range Tracking Any Point in 3D (TAP-3D). While point tracking in two\ndimensions (TAP) has many benchmarks measuring performance on real-world\nvideos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To\nthis end, leveraging existing footage, we build a new benchmark for 3D point\ntracking featuring 4,000+ real-world videos, composed of three different data\nsources spanning a variety of object types, motion patterns, and indoor and\noutdoor environments. To measure performance on the TAP-3D task, we formulate a\ncollection of metrics that extend the Jaccard-based metric used in TAP to\nhandle the complexities of ambiguous depth scales across models, occlusions,\nand multi-track spatio-temporal smoothness. We manually verify a large sample\nof trajectories to ensure correct video annotations, and assess the current\nstate of the TAP-3D task by constructing competitive baselines using existing\ntracking models. We anticipate this benchmark will serve as a guidepost to\nimprove our ability to understand precise 3D motion and surface deformation\nfrom monocular video. Code for dataset download, generation, and model\nevaluation is available at https://tapvid3d.github.io\n","authors":["Skanda Koppula","Ignacio Rocco","Yi Yang","Joe Heyward","JoÃ£o Carreira","Andrew Zisserman","Gabriel Brostow","Carl Doersch"],"pdf_url":"https://arxiv.org/pdf/2407.05921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14053v5","updated":"2024-08-27T17:03:12Z","published":"2023-09-25T11:35:10Z","title":"Revisiting LARS for Large Batch Training Generalization of Neural\n  Networks","summary":"  This paper explores Large Batch Training techniques using layer-wise adaptive\nscaling ratio (LARS) across diverse settings, uncovering insights. LARS\nalgorithms with warm-up tend to be trapped in sharp minimizers early on due to\nredundant ratio scaling. Additionally, a fixed steep decline in the latter\nphase restricts deep neural networks from effectively navigating early-phase\nsharp minimizers. Building on these findings, we propose Time Varying LARS\n(TVLARS), a novel algorithm that replaces warm-up with a configurable\nsigmoid-like function for robust training in the initial phase. TVLARS promotes\ngradient exploration early on, surpassing sharp optimizers and gradually\ntransitioning to LARS for robustness in later phases. Extensive experiments\ndemonstrate that TVLARS consistently outperforms LARS and LAMB in most cases,\nwith up to 2\\% improvement in classification scenarios. Notably, in all\nself-supervised learning cases, TVLARS dominates LARS and LAMB with performance\nimprovements of up to 10\\%.\n","authors":["Khoi Do","Duong Nguyen","Hoa Nguyen","Long Tran-Thanh","Nguyen-Hoang Tran","Quoc-Viet Pham"],"pdf_url":"https://arxiv.org/pdf/2309.14053v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15198v1","updated":"2024-08-27T16:58:23Z","published":"2024-08-27T16:58:23Z","title":"Automatic 8-tissue Segmentation for 6-month Infant Brains","summary":"  Numerous studies have highlighted that atypical brain development,\nparticularly during infancy and toddlerhood, is linked to an increased\nlikelihood of being diagnosed with a neurodevelopmental condition, such as\nautism. Accurate brain tissue segmentations for morphological analysis are\nessential in numerous infant studies. However, due to ongoing white matter (WM)\nmyelination changing tissue contrast in T1- and T2-weighted images, automatic\ntissue segmentation in 6-month infants is particularly difficult. On the other\nhand, manual labelling by experts is time-consuming and labor-intensive. In\nthis study, we propose the first 8-tissue segmentation pipeline for\nsix-month-old infant brains. This pipeline utilizes domain adaptation (DA)\ntechniques to leverage our longitudinal data, including neonatal images\nsegmented with the neonatal Developing Human Connectome Project structural\npipeline. Our pipeline takes raw 6-month images as inputs and generates the\n8-tissue segmentation as outputs, forming an end-to-end segmentation pipeline.\nThe segmented tissues include WM, gray matter (GM), cerebrospinal fluid (CSF),\nventricles, cerebellum, basal ganglia, brainstem, and hippocampus/amygdala.\nCycle-Consistent Generative Adversarial Network (CycleGAN) and Attention U-Net\nwere employed to achieve the image contrast transformation between neonatal and\n6-month images and perform tissue segmentation on the synthesized 6-month\nimages (neonatal images with 6-month intensity contrast), respectively.\nMoreover, we incorporated the segmentation outputs from Infant Brain Extraction\nand Analysis Toolbox (iBEAT) and another Attention U-Net to further enhance the\nperformance and construct the end-to-end segmentation pipeline. Our evaluation\nwith real 6-month images achieved a DICE score of 0.92, an HD95 of 1.6, and an\nASSD of 0.42.\n","authors":["Yilan Dong","Vanessa Kyriakopoulou","Irina Grigorescu","Grainne McAlonan","Dafnis Batalle","Maria Deprez"],"pdf_url":"https://arxiv.org/pdf/2408.15198v1.pdf","comment":"11 pages, 4 figures, to be published in MICCAI PIPPI workshop"},{"id":"http://arxiv.org/abs/2408.14461v2","updated":"2024-08-27T16:43:52Z","published":"2024-08-26T17:50:47Z","title":"A domain decomposition-based autoregressive deep learning model for\n  unsteady and nonlinear partial differential equations","summary":"  In this paper, we propose a domain-decomposition-based deep learning (DL)\nframework, named transient-CoMLSim, for accurately modeling unsteady and\nnonlinear partial differential equations (PDEs). The framework consists of two\nkey components: (a) a convolutional neural network (CNN)-based autoencoder\narchitecture and (b) an autoregressive model composed of fully connected\nlayers. Unlike existing state-of-the-art methods that operate on the entire\ncomputational domain, our CNN-based autoencoder computes a lower-dimensional\nbasis for solution and condition fields represented on subdomains. Timestepping\nis performed entirely in the latent space, generating embeddings of the\nsolution variables from the time history of embeddings of solution and\ncondition variables. This approach not only reduces computational complexity\nbut also enhances scalability, making it well-suited for large-scale\nsimulations. Furthermore, to improve the stability of our rollouts, we employ a\ncurriculum learning (CL) approach during the training of the autoregressive\nmodel. The domain-decomposition strategy enables scaling to out-of-distribution\ndomain sizes while maintaining the accuracy of predictions -- a feature not\neasily integrated into popular DL-based approaches for physics simulations. We\nbenchmark our model against two widely-used DL architectures, Fourier Neural\nOperator (FNO) and U-Net, and demonstrate that our framework outperforms them\nin terms of accuracy, extrapolation to unseen timesteps, and stability for a\nwide range of use cases.\n","authors":["Sheel Nidhan","Haoliang Jiang","Lalit Ghule","Clancy Umphrey","Rishikesh Ranade","Jay Pathak"],"pdf_url":"https://arxiv.org/pdf/2408.14461v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2401.13054v3","updated":"2024-08-27T16:42:26Z","published":"2024-01-23T19:26:24Z","title":"Frustrated Random Walks: A Fast Method to Compute Node Distances on\n  Hypergraphs","summary":"  A hypergraph is a generalization of a graph that arises naturally when\nattribute-sharing among entities is considered. Compared to graphs, hypergraphs\nhave the distinct advantage that they contain explicit communities and are more\nconvenient to manipulate. An open problem in hypergraph research is how to\naccurately and efficiently calculate node distances on hypergraphs. Estimating\nnode distances enables us to find a node's nearest neighbors, which has\nimportant applications in such areas as recommender system, targeted\nadvertising, etc. In this paper, we propose using expected hitting times of\nrandom walks to compute hypergraph node distances. We note that simple random\nwalks (SRW) cannot accurately compute node distances on highly complex\nreal-world hypergraphs, which motivates us to introduce frustrated random walks\n(FRW) for this task. We further benchmark our method against DeepWalk, and show\nthat while the latter can achieve comparable results, FRW has a distinct\ncomputational advantage in cases where the number of targets is fairly small.\nFor such cases, we show that FRW runs in significantly shorter time than\nDeepWalk. Finally, we analyze the time complexity of our method, and show that\nfor large and sparse hypergraphs, the complexity is approximately linear,\nrendering it superior to the DeepWalk alternative.\n","authors":["Enzhi Li","Scott Nickleach","Bilal Fadlallah"],"pdf_url":"https://arxiv.org/pdf/2401.13054v3.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.15183v1","updated":"2024-08-27T16:35:06Z","published":"2024-08-27T16:35:06Z","title":"On latent dynamics learning in nonlinear reduced order modeling","summary":"  In this work, we present the novel mathematical framework of latent dynamics\nmodels (LDMs) for reduced order modeling of parameterized nonlinear\ntime-dependent PDEs. Our framework casts this latter task as a nonlinear\ndimensionality reduction problem, while constraining the latent state to evolve\naccordingly to an (unknown) dynamical system. A time-continuous setting is\nemployed to derive error and stability estimates for the LDM approximation of\nthe full order model (FOM) solution. We analyze the impact of using an explicit\nRunge-Kutta scheme in the time-discrete setting, resulting in the\n$\\Delta\\text{LDM}$ formulation, and further explore the learnable setting,\n$\\Delta\\text{LDM}_\\theta$, where deep neural networks approximate the discrete\nLDM components, while providing a bounded approximation error with respect to\nthe FOM. Moreover, we extend the concept of parameterized Neural ODE - recently\nproposed as a possible way to build data-driven dynamical systems with varying\ninput parameters - to be a convolutional architecture, where the input\nparameters information is injected by means of an affine modulation mechanism,\nwhile designing a convolutional autoencoder neural network able to retain\nspatial-coherence, thus enhancing interpretability at the latent level.\nNumerical experiments, including the Burgers' and the\nadvection-reaction-diffusion equations, demonstrate the framework's ability to\nobtain, in a multi-query context, a time-continuous approximation of the FOM\nsolution, thus being able to query the LDM approximation at any given time\ninstance while retaining a prescribed level of accuracy. Our findings highlight\nthe remarkable potential of the proposed LDMs, representing a mathematically\nrigorous framework to enhance the accuracy and approximation capabilities of\nreduced order modeling for time-dependent parameterized PDEs.\n","authors":["Nicola Farenga","Stefania Fresca","Simone Brivio","Andrea Manzoni"],"pdf_url":"https://arxiv.org/pdf/2408.15183v1.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2408.15173v1","updated":"2024-08-27T16:11:20Z","published":"2024-08-27T16:11:20Z","title":"Exploiting Approximate Symmetry for Efficient Multi-Agent Reinforcement\n  Learning","summary":"  Mean-field games (MFG) have become significant tools for solving large-scale\nmulti-agent reinforcement learning problems under symmetry. However, the\nassumption of exact symmetry limits the applicability of MFGs, as real-world\nscenarios often feature inherent heterogeneity. Furthermore, most works on MFG\nassume access to a known MFG model, which might not be readily available for\nreal-world finite-agent games. In this work, we broaden the applicability of\nMFGs by providing a methodology to extend any finite-player, possibly\nasymmetric, game to an \"induced MFG\". First, we prove that $N$-player dynamic\ngames can be symmetrized and smoothly extended to the infinite-player continuum\nvia explicit Kirszbraun extensions. Next, we propose the notion of\n$\\alpha,\\beta$-symmetric games, a new class of dynamic population games that\nincorporate approximate permutation invariance. For $\\alpha,\\beta$-symmetric\ngames, we establish explicit approximation bounds, demonstrating that a Nash\npolicy of the induced MFG is an approximate Nash of the $N$-player dynamic\ngame. We show that TD learning converges up to a small bias using trajectories\nof the $N$-player game with finite-sample guarantees, permitting symmetrized\nlearning without building an explicit MFG model. Finally, for certain games\nsatisfying monotonicity, we prove a sample complexity of\n$\\widetilde{\\mathcal{O}}(\\varepsilon^{-6})$ for the $N$-agent game to learn an\n$\\varepsilon$-Nash up to symmetrization bias. Our theory is supported by\nevaluations on MARL benchmarks with thousands of agents.\n","authors":["Batuhan Yardim","Niao He"],"pdf_url":"https://arxiv.org/pdf/2408.15173v1.pdf","comment":"5 figures"},{"id":"http://arxiv.org/abs/2408.15165v1","updated":"2024-08-27T16:03:18Z","published":"2024-08-27T16:03:18Z","title":"Latent Ewald summation for machine learning of long-range interactions","summary":"  Machine learning interatomic potentials (MLIPs) often neglect long-range\ninteractions, such as electrostatic and dispersion forces. In this work, we\nintroduce a straightforward and efficient method to account for long-range\ninteractions by learning a latent variable from local atomic descriptors and\napplying an Ewald summation to this variable. We demonstrate that in systems\nincluding charged, polar, or apolar molecular dimers, bulk water, and\nwater-vapor interface, standard short-ranged MLIPs can lead to unphysical\npredictions even when employing message passing. The long-range models\neffectively eliminate these artifacts, with only about twice the computational\ncost of short-range MLIPs.\n","authors":["Bingqing Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.15165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15158v1","updated":"2024-08-27T15:52:52Z","published":"2024-08-27T15:52:52Z","title":"Delay as Payoff in MAB","summary":"  In this paper, we investigate a variant of the classical stochastic\nMulti-armed Bandit (MAB) problem, where the payoff received by an agent (either\ncost or reward) is both delayed, and directly corresponds to the magnitude of\nthe delay. This setting models faithfully many real world scenarios such as the\ntime it takes for a data packet to traverse a network given a choice of route\n(where delay serves as the agent's cost); or a user's time spent on a web page\ngiven a choice of content (where delay serves as the agent's reward).\n  Our main contributions are tight upper and lower bounds for both the cost and\nreward settings. For the case that delays serve as costs, which we are the\nfirst to consider, we prove optimal regret that scales as $\\sum_{i:\\Delta_i >\n0}\\frac{\\log T}{\\Delta_i} + d^*$, where $T$ is the maximal number of steps,\n$\\Delta_i$ are the sub-optimality gaps and $d^*$ is the minimal expected delay\namongst arms. For the case that delays serves as rewards, we show optimal\nregret of $\\sum_{i:\\Delta_i > 0}\\frac{\\log T}{\\Delta_i} + \\bar{d}$, where $\\bar\nd$ is the second maximal expected delay. These improve over the regret in the\ngeneral delay-dependent payoff setting, which scales as $\\sum_{i:\\Delta_i >\n0}\\frac{\\log T}{\\Delta_i} + D$, where $D$ is the maximum possible delay. Our\nregret bounds highlight the difference between the cost and reward scenarios,\nshowing that the improvement in the cost scenario is more significant than for\nthe reward. Finally, we accompany our theoretical results with an empirical\nevaluation.\n","authors":["Ofir Schlisselberg","Ido Cohen","Tal Lancewicki","Yishay Mansour"],"pdf_url":"https://arxiv.org/pdf/2408.15158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11075v4","updated":"2024-08-27T15:39:53Z","published":"2024-07-13T04:29:36Z","title":"A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)","summary":"  Through this comprehensive survey of Kolmogorov-Arnold Networks(KAN), we have\ngained a thorough understanding of its theoretical foundation, architectural\ndesign, application scenarios, and current research progress. KAN, with its\nunique architecture and flexible activation functions, excels in handling\ncomplex data patterns and nonlinear relationships, demonstrating wide-ranging\napplication potential. While challenges remain, KAN is poised to pave the way\nfor innovative solutions in various fields, potentially revolutionizing how we\napproach complex computational problems.\n","authors":["Yuntian Hou","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.11075v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08669v2","updated":"2024-08-27T15:36:59Z","published":"2024-01-08T21:13:07Z","title":"Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems\n  with Multi-Leg Demand Routes","summary":"  Deep reinforcement learning (RL) has been shown to be effective in producing\napproximate solutions to some vehicle routing problems (VRPs), especially when\nusing policies generated by encoder-decoder attention mechanisms. While these\ntechniques have been quite successful for relatively simple problem instances,\nthere are still under-researched and highly complex VRP variants for which no\neffective RL method has been demonstrated. In this work we focus on one such\nVRP variant, which contains multiple trucks and multi-leg routing requirements.\nIn these problems, demand is required to move along sequences of nodes, instead\nof just from a start node to an end node. With the goal of making deep RL a\nviable strategy for real-world industrial-scale supply chain logistics, we\ndevelop new extensions to existing encoder-decoder attention models which allow\nthem to handle multiple trucks and multi-leg routing requirements. Our models\nhave the advantage that they can be trained for a small number of trucks and\nnodes, and then embedded into a large supply chain to yield solutions for\nlarger numbers of trucks and nodes. We test our approach on a real supply chain\nenvironment arising in the operations of Japanese automotive parts manufacturer\nAisin Corporation, and find that our algorithm outperforms Aisin's previous\nbest solution.\n","authors":["Joshua Levin","Randall Correll","Takanori Ide","Takafumi Suzuki","Takaho Saito","Alan Arai"],"pdf_url":"https://arxiv.org/pdf/2401.08669v2.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.10280v2","updated":"2024-08-27T15:34:49Z","published":"2024-08-18T12:18:56Z","title":"NoRA: Nested Low-Rank Adaptation for Efficient Fine-Tuning Large Models","summary":"  In this paper, we introduce Nested Low-Rank Adaptation (NoRA), a novel\napproach to parameter-efficient fine-tuning that extends the capabilities of\nLow-Rank Adaptation (LoRA) techniques. Vanilla LoRA overlooks pre-trained\nweight inheritance and still requires fine-tuning numerous parameters. To\naddresses these issues, our NoRA adopts a dual-layer nested structure with\nSingular Value Decomposition (SVD), effectively leveraging original matrix\nknowledge while reducing tunable parameters. Specifically, NoRA freezes the\nouter LoRA weights and utilizes an inner LoRA design, providing enhanced\ncontrol over model optimization. This approach allows the model to more\nprecisely adapt to specific tasks while maintaining a compact parameter space.\nBy freezing outer LoRA weights and using an inner LoRA design, NoRA enables\nprecise task adaptation with a compact parameter space. Evaluations on tasks\nincluding commonsense reasoning with large language models, fine-tuning\nvision-language models, and subject-driven generation demonstrate NoRA's\nsuperiority over LoRA and its variants. Code will be released upon acceptance.\n","authors":["Cheng Lin","Lujun Li","Dezhi Li","Jie Zou","Wei Xue","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2408.10280v2.pdf","comment":"Work in progress, revisions ongoing"},{"id":"http://arxiv.org/abs/2311.07596v2","updated":"2024-08-27T15:34:43Z","published":"2023-11-10T11:40:24Z","title":"Graph GOSPA metric: a metric to measure the discrepancy between graphs\n  of different sizes","summary":"  This paper proposes a metric to measure the dissimilarity between graphs that\nmay have a different number of nodes. The proposed metric extends the\ngeneralised optimal subpattern assignment (GOSPA) metric, which is a metric for\nsets, to graphs. The proposed graph GOSPA metric includes costs associated with\nnode attribute errors for properly assigned nodes, missed and false nodes and\nedge mismatches between graphs. The computation of this metric is based on\nfinding the optimal assignments between nodes in the two graphs, with the\npossibility of leaving some of the nodes unassigned. We also propose a lower\nbound for the metric, which is also a metric for graphs and is computable in\npolynomial time using linear programming. The metric is first derived for\nundirected unweighted graphs and it is then extended to directed and weighted\ngraphs. The properties of the metric are demonstrated via simulated and\nempirical datasets.\n","authors":["Jinhao Gu","Ãngel F. GarcÃ­a-FernÃ¡ndez","Robert E. Firth","Lennart Svensson"],"pdf_url":"https://arxiv.org/pdf/2311.07596v2.pdf","comment":"Accepted in IEEE Transactions on Signal Processing. The code is\n  available at https://github.com/JinhaoGu/The-graph-GOSPA-metric"},{"id":"http://arxiv.org/abs/2405.14848v2","updated":"2024-08-27T15:28:33Z","published":"2024-05-23T17:56:38Z","title":"Local Causal Discovery for Structural Evidence of Direct Discrimination","summary":"  Identifying the causal pathways of unfairness is a critical objective in\nimproving policy design and algorithmic decision-making. Prior work in causal\nfairness analysis often requires knowledge of the causal graph, hindering\npractical applications in complex or low-knowledge domains. Moreover, global\ndiscovery methods that learn causal structure from data can result in unstable\nperformance with finite samples, potentially leading to contradictory fairness\nconclusions. To mitigate these issues, we introduce local discovery for direct\ndiscrimination (LD3): a method that uncovers structural evidence of direct\ndiscrimination by identifying the causal parents of an outcome variable. LD3\nperforms a linear number of conditional independence tests relative to variable\nset size, and allows for latent confounding under the sufficient condition that\nno parent of the outcome is latent. We show that LD3 returns a valid adjustment\nset (VAS) under a new graphical criterion for the weighted controlled direct\neffect, a qualitative indicator of direct discrimination. LD3 limits\nunnecessary adjustment, providing interpretable VAS for assessing unfairness.\nWe use LD3 to analyze causal fairness in two complex decision systems: criminal\nrecidivism prediction and liver transplant allocation. LD3 was more\ntime-efficient and returned more plausible results on real-world data than\nbaselines, which took 46x to 5870x longer to execute.\n","authors":["Jacqueline Maasch","Kyra Gan","Violet Chen","Agni Orfanoudaki","Nil-Jana Akpinar","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2405.14848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15138v1","updated":"2024-08-27T15:23:09Z","published":"2024-08-27T15:23:09Z","title":"How transformers learn structured data: insights from hierarchical\n  filtering","summary":"  We introduce a hierarchical filtering procedure for generative models of\nsequences on trees, enabling control over the range of positional correlations\nin the data. Leveraging this controlled setting, we provide evidence that\nvanilla encoder-only transformer architectures can implement the optimal Belief\nPropagation algorithm on both root classification and masked language modeling\ntasks. Correlations at larger distances corresponding to increasing layers of\nthe hierarchy are sequentially included as the network is trained. We analyze\nhow the transformer layers succeed by focusing on attention maps from models\ntrained with varying degrees of filtering. These attention maps show clear\nevidence for iterative hierarchical reconstruction of correlations, and we can\nrelate these observations to a plausible implementation of the exact inference\nalgorithm for the network sizes considered.\n","authors":["Jerome Garnier-Brun","Marc MÃ©zard","Emanuele Moscato","Luca Saglietti"],"pdf_url":"https://arxiv.org/pdf/2408.15138v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.15136v1","updated":"2024-08-27T15:19:07Z","published":"2024-08-27T15:19:07Z","title":"Low-Budget Simulation-Based Inference with Bayesian Neural Networks","summary":"  Simulation-based inference methods have been shown to be inaccurate in the\ndata-poor regime, when training simulations are limited or expensive. Under\nthese circumstances, the inference network is particularly prone to\noverfitting, and using it without accounting for the computational uncertainty\narising from the lack of identifiability of the network weights can lead to\nunreliable results. To address this issue, we propose using Bayesian neural\nnetworks in low-budget simulation-based inference, thereby explicitly\naccounting for the computational uncertainty of the posterior approximation. We\ndesign a family of Bayesian neural network priors that are tailored for\ninference and show that they lead to well-calibrated posteriors on tested\nbenchmarks, even when as few as $O(10)$ simulations are available. This opens\nup the possibility of performing reliable simulation-based inference using very\nexpensive simulators, as we demonstrate on a problem from the field of\ncosmology where single simulations are computationally expensive. We show that\nBayesian neural networks produce informative and well-calibrated posterior\nestimates with only a few hundred simulations.\n","authors":["Arnaud Delaunoy","Maxence de la Brassinne Bonardeaux","Siddharth Mishra-Sharma","Gilles Louppe"],"pdf_url":"https://arxiv.org/pdf/2408.15136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07531v2","updated":"2024-08-27T15:16:06Z","published":"2024-08-14T13:03:41Z","title":"Development of a Large Language Model-based Multi-Agent Clinical\n  Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based\n  Triage and Treatment Planning in Emergency Departments","summary":"  Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation.\n","authors":["Seungjun Han","Wongyung Choi"],"pdf_url":"https://arxiv.org/pdf/2408.07531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15133v1","updated":"2024-08-27T15:13:06Z","published":"2024-08-27T15:13:06Z","title":"Using LLMs for Explaining Sets of Counterfactual Examples to Final Users","summary":"  Causality is vital for understanding true cause-and-effect relationships\nbetween variables within predictive models, rather than relying on mere\ncorrelations, making it highly relevant in the field of Explainable AI. In an\nautomated decision-making scenario, causal inference methods can analyze the\nunderlying data-generation process, enabling explanations of a model's decision\nby manipulating features and creating counterfactual examples. These\ncounterfactuals explore hypothetical scenarios where a minimal number of\nfactors are altered, providing end-users with valuable information on how to\nchange their situation. However, interpreting a set of multiple counterfactuals\ncan be challenging for end-users who are not used to analyzing raw data\nrecords. In our work, we propose a novel multi-step pipeline that uses\ncounterfactuals to generate natural language explanations of actions that will\nlead to a change in outcome in classifiers of tabular data using LLMs. This\npipeline is designed to guide the LLM through smaller tasks that mimic human\nreasoning when explaining a decision based on counterfactual cases. We\nconducted various experiments using a public dataset and proposed a method of\nclosed-loop evaluation to assess the coherence of the final explanation with\nthe counterfactuals, as well as the quality of the content. Results are\npromising, although further experiments with other datasets and human\nevaluations should be carried out.\n","authors":["Arturo Fredes","Jordi Vitria"],"pdf_url":"https://arxiv.org/pdf/2408.15133v1.pdf","comment":"Presented as a poster in the 2nd Workshop on Causal Inference and\n  Machine Learning in Practice at KDD 2024"},{"id":"http://arxiv.org/abs/2408.15128v1","updated":"2024-08-27T15:08:06Z","published":"2024-08-27T15:08:06Z","title":"Evaluating the Energy Consumption of Machine Learning: Systematic\n  Literature Review and Experiments","summary":"  Monitoring, understanding, and optimizing the energy consumption of Machine\nLearning (ML) are various reasons why it is necessary to evaluate the energy\nusage of ML. However, there exists no universal tool that can answer this\nquestion for all use cases, and there may even be disagreement on how to\nevaluate energy consumption for a specific use case. Tools and methods are\nbased on different approaches, each with their own advantages and drawbacks,\nand they need to be mapped out and explained in order to select the most\nsuitable one for a given situation. We address this challenge through two\napproaches. First, we conduct a systematic literature review of all tools and\nmethods that permit to evaluate the energy consumption of ML (both at training\nand at inference), irrespective of whether they were originally designed for\nmachine learning or general software. Second, we develop and use an\nexperimental protocol to compare a selection of these tools and methods. The\ncomparison is both qualitative and quantitative on a range of ML tasks of\ndifferent nature (vision, language) and computational complexity. The\nsystematic literature review serves as a comprehensive guide for understanding\nthe array of tools and methods used in evaluating energy consumption of ML, for\nvarious use cases going from basic energy monitoring to consumption\noptimization. Two open-source repositories are provided for further\nexploration. The first one contains tools that can be used to replicate this\nwork or extend the current review. The second repository houses the\nexperimental protocol, allowing users to augment the protocol with new ML\ncomputing tasks and additional energy evaluation tools.\n","authors":["Charlotte Rodriguez","Laura Degioanni","Laetitia Kameni","Richard Vidal","Giovanni Neglia"],"pdf_url":"https://arxiv.org/pdf/2408.15128v1.pdf","comment":"52 pages,"},{"id":"http://arxiv.org/abs/2407.16828v2","updated":"2024-08-27T15:07:28Z","published":"2024-07-23T20:38:23Z","title":"Pareto Front Approximation for Multi-Objective Session-Based Recommender\n  Systems","summary":"  This work introduces MultiTRON, an approach that adapts Pareto front\napproximation techniques to multi-objective session-based recommender systems\nusing a transformer neural network. Our approach optimizes trade-offs between\nkey metrics such as click-through and conversion rates by training on sampled\npreference vectors. A significant advantage is that after training, a single\nmodel can access the entire Pareto front, allowing it to be tailored to meet\nthe specific requirements of different stakeholders by adjusting an additional\ninput vector that weights the objectives. We validate the model's performance\nthrough extensive offline and online evaluation. For broader application and\nresearch, the source code is made available at\nhttps://github.com/otto-de/MultiTRON. The results confirm the model's ability\nto manage multiple recommendation objectives effectively, offering a flexible\ntool for diverse business needs.\n","authors":["Timo Wilm","Philipp Normann","Felix Stepprath"],"pdf_url":"https://arxiv.org/pdf/2407.16828v2.pdf","comment":"Accepted at the Eighteenth ACM Conference on Recommender Systems\n  (RecSys '24)"},{"id":"http://arxiv.org/abs/2408.15126v1","updated":"2024-08-27T15:07:27Z","published":"2024-08-27T15:07:27Z","title":"Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of\n  Peptides","summary":"  Molecular Dynamics (MD) simulations are irreplaceable and ubiquitous in\nfields of materials science, chemistry, pharmacology just to name a few.\nConventional MD simulations are plagued by numerical stability as well as long\nequilibration time issues, which limits broader applications of MD simulations.\nRecently, a surge of deep learning approaches have been devised for\ntime-coarsened dynamics, which learns the state transition mechanism over much\nlarger time scales to overcome these limitations. However, only a few methods\ntarget the underlying Boltzmann distribution by resampling techniques, where\nproposals are rarely accepted as new states with low efficiency. In this work,\nwe propose a force-guided bridge matching model, FBM, a novel framework that\nfirst incorporates physical priors into bridge matching for full-atom\ntime-coarsened dynamics. With the guidance of our well-designed intermediate\nforce field, FBM is feasible to target the Boltzmann-like distribution by\ndirect inference without extra steps. Experiments on small peptides verify our\nsuperiority in terms of comprehensive metrics and demonstrate transferability\nto unseen peptide systems.\n","authors":["Ziyang Yu","Wenbing Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13960v2","updated":"2024-08-27T15:06:17Z","published":"2024-08-25T23:48:11Z","title":"Time Series Analysis for Education: Methods, Applications, and Future\n  Directions","summary":"  Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage.\n","authors":["Shengzhong Mao","Chaoli Zhang","Yichi Song","Jindong Wang","Xiao-Jun Zeng","Zenglin Xu","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2408.13960v2.pdf","comment":"24 pages, 3 figures, 6 tables, project page: see\n  https://github.com/ai-for-edu/time-series-analysis-for-education"},{"id":"http://arxiv.org/abs/2408.05892v3","updated":"2024-08-27T15:00:53Z","published":"2024-08-12T02:10:18Z","title":"Polyp SAM 2: Advancing Zero shot Polyp Segmentation in Colorectal Cancer\n  Detection","summary":"  Polyp segmentation plays a crucial role in the early detection and diagnosis\nof colorectal cancer. However, obtaining accurate segmentations often requires\nlabor-intensive annotations and specialized models. Recently, Meta AI Research\nreleased a general Segment Anything Model 2 (SAM 2), which has demonstrated\npromising performance in several segmentation tasks. In this manuscript, we\nevaluate the performance of SAM 2 in segmenting polyps under various prompted\nsettings. We hope this report will provide insights to advance the field of\npolyp segmentation and promote more interesting work in the future. This\nproject is publicly available at https://github.com/ sajjad-sh33/Polyp-SAM-2.\n","authors":["Mobina Mansoori","Sajjad Shahabodini","Jamshid Abouei","Konstantinos N. Plataniotis","Arash Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2408.05892v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12446v2","updated":"2024-08-27T14:59:25Z","published":"2024-08-22T14:41:49Z","title":"EX-DRL: Hedging Against Heavy Losses with EXtreme Distributional\n  Reinforcement Learning","summary":"  Recent advancements in Distributional Reinforcement Learning (DRL) for\nmodeling loss distributions have shown promise in developing hedging strategies\nin derivatives markets. A common approach in DRL involves learning the\nquantiles of loss distributions at specified levels using Quantile Regression\n(QR). This method is particularly effective in option hedging due to its direct\nquantile-based risk assessment, such as Value at Risk (VaR) and Conditional\nValue at Risk (CVaR). However, these risk measures depend on the accurate\nestimation of extreme quantiles in the loss distribution's tail, which can be\nimprecise in QR-based DRL due to the rarity and extremity of tail data, as\nhighlighted in the literature. To address this issue, we propose EXtreme DRL\n(EX-DRL), which enhances extreme quantile prediction by modeling the tail of\nthe loss distribution with a Generalized Pareto Distribution (GPD). This method\nintroduces supplementary data to mitigate the scarcity of extreme quantile\nobservations, thereby improving estimation accuracy through QR. Comprehensive\nexperiments on gamma hedging options demonstrate that EX-DRL improves existing\nQR-based models by providing more precise estimates of extreme quantiles,\nthereby improving the computation and reliability of risk metrics for complex\nfinancial risk management.\n","authors":["Parvin Malekzadeh","Zissis Poulos","Jacky Chen","Zeyu Wang","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2408.12446v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.15114v1","updated":"2024-08-27T14:54:33Z","published":"2024-08-27T14:54:33Z","title":"Few-Shot Unsupervised Implicit Neural Shape Representation Learning with\n  Spatial Adversaries","summary":"  Implicit Neural Representations have gained prominence as a powerful\nframework for capturing complex data modalities, encompassing a wide range from\n3D shapes to images and audio. Within the realm of 3D shape representation,\nNeural Signed Distance Functions (SDF) have demonstrated remarkable potential\nin faithfully encoding intricate shape geometry. However, learning SDFs from\nsparse 3D point clouds in the absence of ground truth supervision remains a\nvery challenging task. While recent methods rely on smoothness priors to\nregularize the learning, our method introduces a regularization term that\nleverages adversarial samples around the shape to improve the learned SDFs.\nThrough extensive experiments and evaluations, we illustrate the efficacy of\nour proposed method, highlighting its capacity to improve SDF learning with\nrespect to baselines and the state-of-the-art using synthetic and real data.\n","authors":["Amine Ouasfi","Adnane Boukhayma"],"pdf_url":"https://arxiv.org/pdf/2408.15114v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2403.20324v3","updated":"2024-08-27T14:53:41Z","published":"2024-03-29T17:51:50Z","title":"Localising the Seizure Onset Zone from Single-Pulse Electrical\n  Stimulation Responses with a CNN Transformer","summary":"  Epilepsy is one of the most common neurological disorders, often requiring\nsurgical intervention when medication fails to control seizures. For effective\nsurgical outcomes, precise localisation of the epileptogenic focus - often\napproximated through the Seizure Onset Zone (SOZ) - is critical yet remains a\nchallenge. Active probing through electrical stimulation is already standard\nclinical practice for identifying epileptogenic areas. Our study advances the\napplication of deep learning for SOZ localisation using Single-Pulse Electrical\nStimulation (SPES) responses, with two key contributions. Firstly, we implement\nan existing deep learning model to compare two SPES analysis paradigms:\ndivergent and convergent. These paradigms evaluate outward and inward effective\nconnections, respectively. We assess the generalisability of these models to\nunseen patients and electrode placements using held-out test sets. Our findings\nreveal a notable improvement in moving from a divergent (AUROC: 0.574) to a\nconvergent approach (AUROC: 0.666), marking the first application of the latter\nin this context. Secondly, we demonstrate the efficacy of CNN Transformers with\ncross-channel attention in handling heterogeneous electrode placements,\nincreasing the AUROC to 0.730. These findings represent a significant step in\nmodelling patient-specific intracranial EEG electrode placements in SPES.\nFuture work will explore integrating these models into clinical decision-making\nprocesses to bridge the gap between deep learning research and practical\nhealthcare applications.\n","authors":["Jamie Norris","Aswin Chari","Dorien van Blooijs","Gerald Cooray","Karl Friston","Martin Tisdall","Richard Rosch"],"pdf_url":"https://arxiv.org/pdf/2403.20324v3.pdf","comment":"21 pages, 6 figures, accepted at Machine Learning for Healthcare 2024"},{"id":"http://arxiv.org/abs/2311.07537v2","updated":"2024-08-27T14:34:26Z","published":"2023-11-13T18:23:46Z","title":"Estimating optical vegetation indices and biophysical variables for\n  temperate forests with Sentinel-1 SAR data using machine learning techniques:\n  A case study for Czechia","summary":"  Current optical vegetation indices (VIs) for monitoring forest ecosystems are\nwell established and widely used in various applications, but can be limited by\natmospheric effects such as clouds. In contrast, synthetic aperture radar (SAR)\ndata can offer insightful and systematic forest monitoring with complete time\nseries (TS) due to signal penetration through clouds and day and night image\nacquisitions. This study aims to address the limitations of optical satellite\ndata by using SAR data as an alternative for estimating optical VIs for forests\nthrough machine learning (ML). While this approach is less direct and likely\nonly feasible through the power of ML, it raises the scientific question of\nwhether enough relevant information is contained in the SAR signal to\naccurately estimate VIs. This work covers the estimation of TS of four VIs\n(LAI, FAPAR, EVI and NDVI) using multitemporal Sentinel-1 SAR and ancillary\ndata. The study focused on both healthy and disturbed temperate forest areas in\nCzechia for the year 2021, while ground truth labels generated from Sentinel-2\nmultispectral data. This was enabled by creating a paired multi-modal TS\ndataset in Google Earth Engine (GEE), including temporally and spatially\naligned Sentinel-1, Sentinel-2, DEM, weather and land cover datasets. The\ninclusion of DEM-derived auxiliary features and additional meteorological\ninformation, further improved the results. In the comparison of ML models, the\ntraditional ML algorithms, RFR and XGBoost slightly outperformed the AutoML\napproach, auto-sklearn, for all VIs, achieving high accuracies ($R^2$ between\n70-86%) and low errors (0.055-0.29 of MAE). In general, up to 240 measurements\nper year and a spatial resolution of 20 m can be achieved using estimated\nSAR-based VIs with high accuracy. A great advantage of the SAR-based VI is the\nability to detect abrupt forest changes with sub-weekly temporal accuracy.\n","authors":["Daniel Paluba","Bertrand Le Saux","PÅemysl Stych"],"pdf_url":"https://arxiv.org/pdf/2311.07537v2.pdf","comment":"Revised version of the preprint, based on comments from the\n  reviewers. Full research article. 23 pages, 10 figures, 7 tables"},{"id":"http://arxiv.org/abs/2408.15099v1","updated":"2024-08-27T14:31:54Z","published":"2024-08-27T14:31:54Z","title":"No Regrets: Investigating and Improving Regret Approximations for\n  Curriculum Discovery","summary":"  What data or environments to use for training to improve downstream\nperformance is a longstanding and very topical question in reinforcement\nlearning. In particular, Unsupervised Environment Design (UED) methods have\ngained recent attention as their adaptive curricula enable agents to be robust\nto in- and out-of-distribution tasks. We ask to what extent these methods are\nthemselves robust when applied to a novel setting, closely inspired by a\nreal-world robotics problem. Surprisingly, we find that the state-of-the-art\nUED methods either do not improve upon the na\\\"{i}ve baseline of Domain\nRandomisation (DR), or require substantial hyperparameter tuning to do so. Our\nanalysis shows that this is due to their underlying scoring functions failing\nto predict intuitive measures of ``learnability'', i.e., in finding the\nsettings that the agent sometimes solves, but not always. Based on this, we\ninstead directly train on levels with high learnability and find that this\nsimple and intuitive approach outperforms UED methods and DR in several\nbinary-outcome environments, including on our domain and the standard UED\ndomain of Minigrid. We further introduce a new adversarial evaluation procedure\nfor directly measuring robustness, closely mirroring the conditional value at\nrisk (CVaR). We open-source all our code and present visualisations of final\npolicies here: https://github.com/amacrutherford/sampling-for-learnability.\n","authors":["Alexander Rutherford","Michael Beukman","Timon Willi","Bruno Lacerda","Nick Hawes","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2408.15099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15097v1","updated":"2024-08-27T14:30:06Z","published":"2024-08-27T14:30:06Z","title":"Data-Driven Nonlinear Deformation Design of 3D-Printable Shells","summary":"  Designing and fabricating structures with specific mechanical properties\nrequires understanding the intricate relationship between design parameters and\nperformance. Understanding the design-performance relationship becomes\nincreasingly complicated for nonlinear deformations. Though successful at\nmodeling elastic deformations, simulation-based techniques struggle to model\nlarge elastoplastic deformations exhibiting plasticity and densification. We\npropose a neural network trained on experimental data to learn the\ndesign-performance relationship between 3D-printable shells and their\ncompressive force-displacement behavior. Trained on thousands of physical\nexperiments, our network aids in both forward and inverse design to generate\nshells exhibiting desired elastoplastic and hyperelastic deformations. We\nvalidate a subset of generated designs through fabrication and testing.\nFurthermore, we demonstrate the network's inverse design efficacy in generating\ncustom shells for several applications.\n","authors":["Samuel Silverman","Kelsey L. Snapp","Keith A. Brown","Emily Whiting"],"pdf_url":"https://arxiv.org/pdf/2408.15097v1.pdf","comment":"Submitted to 3D Printing and Additive Manufacturing"},{"id":"http://arxiv.org/abs/2408.15096v1","updated":"2024-08-27T14:26:56Z","published":"2024-08-27T14:26:56Z","title":"Post-processing fairness with minimal changes","summary":"  In this paper, we introduce a novel post-processing algorithm that is both\nmodel-agnostic and does not require the sensitive attribute at test time. In\naddition, our algorithm is explicitly designed to enforce minimal changes\nbetween biased and debiased predictions; a property that, while highly\ndesirable, is rarely prioritized as an explicit objective in fairness\nliterature. Our approach leverages a multiplicative factor applied to the logit\nvalue of probability scores produced by a black-box classifier. We demonstrate\nthe efficacy of our method through empirical evaluations, comparing its\nperformance against other four debiasing algorithms on two widely used datasets\nin fairness research.\n","authors":["Federico Di Gennaro","Thibault Laugel","Vincent Grari","Xavier Renard","Marcin Detyniecki"],"pdf_url":"https://arxiv.org/pdf/2408.15096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15094v1","updated":"2024-08-27T14:25:42Z","published":"2024-08-27T14:25:42Z","title":"Constrained Diffusion Models via Dual Training","summary":"  Diffusion models have attained prominence for their ability to synthesize a\nprobability distribution for a given dataset via a diffusion process, enabling\nthe generation of new data points with high fidelity. However, diffusion\nprocesses are prone to generating biased data based on the training dataset. To\naddress this issue, we develop constrained diffusion models by imposing\ndiffusion constraints based on desired distributions that are informed by\nrequirements. Specifically, we cast the training of diffusion models under\nrequirements as a constrained distribution optimization problem that aims to\nreduce the distribution difference between original and generated data while\nobeying constraints on the distribution of generated data. We show that our\nconstrained diffusion models generate new data from a mixture data distribution\nthat achieves the optimal trade-off among objective and constraints. To train\nconstrained diffusion models, we develop a dual training algorithm and\ncharacterize the optimality of the trained constrained diffusion model. We\nempirically demonstrate the effectiveness of our constrained models in two\nconstrained generation tasks: (i) we consider a dataset with one or more\nunderrepresented classes where we train the model with constraints to ensure\nfairly sampling from all classes during inference; (ii) we fine-tune a\npre-trained diffusion model to sample from a new dataset while avoiding\noverfitting.\n","authors":["Shervin Khalafi","Dongsheng Ding","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2408.15094v1.pdf","comment":"41 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.13843v2","updated":"2024-08-27T14:24:52Z","published":"2024-08-25T14:17:43Z","title":"Consistent machine learning for topology optimization with\n  microstructure-dependent neural network material models","summary":"  Additive manufacturing methods together with topology optimization have\nenabled the creation of multiscale structures with controlled spatially-varying\nmaterial microstructure. However, topology optimization or inverse design of\nsuch structures in the presence of nonlinearities remains a challenge due to\nthe expense of computational homogenization methods and the complexity of\ndifferentiably parameterizing the microstructural response. A solution to this\nchallenge lies in machine learning techniques that offer efficient,\ndifferentiable mappings between the material response and its microstructural\ndescriptors. This work presents a framework for designing multiscale\nheterogeneous structures with spatially varying microstructures by merging a\nhomogenization-based topology optimization strategy with a consistent machine\nlearning approach grounded in hyperelasticity theory. We leverage neural\narchitectures that adhere to critical physical principles such as\npolyconvexity, objectivity, material symmetry, and thermodynamic consistency to\nsupply the framework with a reliable constitutive model that is dependent on\nmaterial microstructural descriptors. Our findings highlight the potential of\nintegrating consistent machine learning models with density-based topology\noptimization for enhancing design optimization of heterogeneous hyperelastic\nstructures under finite deformations.\n","authors":["Harikrishnan Vijayakumaran","Jonathan B. Russ","Glaucio H. Paulino","Miguel A. Bessa"],"pdf_url":"https://arxiv.org/pdf/2408.13843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10895v4","updated":"2024-08-27T14:23:51Z","published":"2023-07-20T14:18:44Z","title":"Variational Autoencoding of Dental Point Clouds","summary":"  Digital dentistry has made significant advancements, yet numerous challenges\nremain. This paper introduces the FDI 16 dataset, an extensive collection of\ntooth meshes and point clouds. Additionally, we present a novel approach:\nVariational FoldingNet (VF-Net), a fully probabilistic variational autoencoder\nfor point clouds. Notably, prior latent variable models for point clouds lack a\none-to-one correspondence between input and output points. Instead, they rely\non optimizing Chamfer distances, a metric that lacks a normalized\ndistributional counterpart, rendering it unsuitable for probabilistic modeling.\nWe replace the explicit minimization of Chamfer distances with a suitable\nencoder, increasing computational efficiency while simplifying the\nprobabilistic extension. This allows for straightforward application in various\ntasks, including mesh generation, shape completion, and representation\nlearning. Empirically, we provide evidence of lower reconstruction error in\ndental reconstruction and interpolation, showcasing state-of-the-art\nperformance in dental sample generation while identifying valuable latent\nrepresentations\n","authors":["Johan Ziruo Ye","Thomas Ãrkild","Peter Lempel SÃ¸ndergaard","SÃ¸ren Hauberg"],"pdf_url":"https://arxiv.org/pdf/2307.10895v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15089v1","updated":"2024-08-27T14:20:21Z","published":"2024-08-27T14:20:21Z","title":"SiHGNN: Leveraging Properties of Semantic Graphs for Efficient HGNN\n  Acceleration","summary":"  Heterogeneous Graph Neural Networks (HGNNs) have expanded graph\nrepresentation learning to heterogeneous graph fields. Recent studies have\ndemonstrated their superior performance across various applications, including\nmedical analysis and recommendation systems, often surpassing existing methods.\nHowever, GPUs often experience inefficiencies when executing HGNNs due to their\nunique and complex execution patterns. Compared to traditional Graph Neural\nNetworks, these patterns further exacerbate irregularities in memory access. To\ntackle these challenges, recent studies have focused on developing\ndomain-specific accelerators for HGNNs. Nonetheless, most of these efforts have\nconcentrated on optimizing the datapath or scheduling data accesses, while\nlargely overlooking the potential benefits that could be gained from leveraging\nthe inherent properties of the semantic graph, such as its topology, layout,\nand generation.\n  In this work, we focus on leveraging the properties of semantic graphs to\nenhance HGNN performance. First, we analyze the Semantic Graph Build (SGB)\nstage and identify significant opportunities for data reuse during semantic\ngraph generation. Next, we uncover the phenomenon of buffer thrashing during\nthe Graph Feature Processing (GFP) stage, revealing potential optimization\nopportunities in semantic graph layout. Furthermore, we propose a lightweight\nhardware accelerator frontend for HGNNs, called SiHGNN. This accelerator\nfrontend incorporates a tree-based Semantic Graph Builder for efficient\nsemantic graph generation and features a novel Graph Restructurer for\noptimizing semantic graph layouts. Experimental results show that SiHGNN\nenables the state-of-the-art HGNN accelerator to achieve an average performance\nimprovement of 2.95$\\times$.\n","authors":["Runzhen Xue","Mingyu Yan","Dengke Han","Zhimin Tang","Xiaochun Ye","Dongrui Fan"],"pdf_url":"https://arxiv.org/pdf/2408.15089v1.pdf","comment":"12 pages, 18 figures. arXiv admin note: text overlap with\n  arXiv:2404.04792"},{"id":"http://arxiv.org/abs/2408.14340v2","updated":"2024-08-27T14:09:44Z","published":"2024-08-26T15:13:14Z","title":"Foundation Models for Music: A Survey","summary":"  In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.\n","authors":["Yinghao Ma","Anders Ãland","Anton Ragni","Bleiz MacSen Del Sette","Charalampos Saitis","Chris Donahue","Chenghua Lin","Christos Plachouras","Emmanouil Benetos","Elio Quinton","Elona Shatri","Fabio Morreale","Ge Zhang","GyÃ¶rgy Fazekas","Gus Xia","Huan Zhang","Ilaria Manco","Jiawen Huang","Julien Guinot","Liwei Lin","Luca Marinelli","Max W. Y. Lam","Megha Sharma","Qiuqiang Kong","Roger B. Dannenberg","Ruibin Yuan","Shangda Wu","Shih-Lun Wu","Shuqi Dai","Shun Lei","Shiyin Kang","Simon Dixon","Wenhu Chen","Wenhao Huang","Xingjian Du","Xingwei Qu","Xu Tan","Yizhi Li","Zeyue Tian","Zhiyong Wu","Zhizheng Wu","Ziyang Ma","Ziyu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.14340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15077v1","updated":"2024-08-27T14:05:48Z","published":"2024-08-27T14:05:48Z","title":"MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of\n  Children with Autism Spectrum Disorder","summary":"  Autism spectrum disorder (ASD) is characterized by significant challenges in\nsocial interaction and comprehending communication signals. Recently,\ntherapeutic interventions for ASD have increasingly utilized Deep learning\npowered-computer vision techniques to monitor individual progress over time.\nThese models are trained on private, non-public datasets from the autism\ncommunity, creating challenges in comparing results across different models due\nto privacy-preserving data-sharing issues. This work introduces MMASD+. MMASD+\nconsists of diverse data modalities, including 3D-Skeleton, 3D Body Mesh, and\nOptical Flow data. It integrates the capabilities of Yolov8 and Deep SORT\nalgorithms to distinguish between the therapist and children, addressing a\nsignificant barrier in the original dataset. Additionally, a Multimodal\nTransformer framework is proposed to predict 11 action types and the presence\nof ASD. This framework achieves an accuracy of 95.03% for predicting action\ntypes and 96.42% for predicting ASD presence, demonstrating over a 10%\nimprovement compared to models trained on single data modalities. These\nfindings highlight the advantages of integrating multiple data modalities\nwithin the Multimodal Transformer framework.\n","authors":["Pavan Uttej Ravva","Behdokht Kiafar","Pinar Kullu","Jicheng Li","Anjana Bhat","Roghayeh Leila Barmaki"],"pdf_url":"https://arxiv.org/pdf/2408.15077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04580v2","updated":"2024-08-27T14:05:38Z","published":"2024-02-07T04:43:41Z","title":"A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied\n  Agents","summary":"  The burgeoning fields of robot learning and embodied AI have triggered an\nincreasing demand for large quantities of data. However, collecting sufficient\nunbiased data from the target domain remains a challenge due to costly data\ncollection processes and stringent safety requirements. Consequently,\nresearchers often resort to data from easily accessible source domains, such as\nsimulation and laboratory environments, for cost-effective data acquisition and\nrapid model iteration. Nevertheless, the environments and embodiments of these\nsource domains can be quite different from their target domain counterparts,\nunderscoring the need for effective cross-domain policy transfer approaches. In\nthis paper, we conduct a systematic review of existing cross-domain policy\ntransfer methods. Through a nuanced categorization of domain gaps, we\nencapsulate the overarching insights and design considerations of each problem\nsetting. We also provide a high-level discussion about the key methodologies\nused in cross-domain policy transfer problems. Lastly, we summarize the open\nchallenges that lie beyond the capabilities of current paradigms and discuss\npotential future directions in this field.\n","authors":["Haoyi Niu","Jianming Hu","Guyue Zhou","Xianyuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2402.04580v2.pdf","comment":"IJCAI 2024"},{"id":"http://arxiv.org/abs/2408.15076v1","updated":"2024-08-27T14:04:04Z","published":"2024-08-27T14:04:04Z","title":"MiWaves Reinforcement Learning Algorithm","summary":"  The escalating prevalence of cannabis use poses a significant public health\nchallenge globally. In the U.S., cannabis use is more prevalent among emerging\nadults (EAs) (ages 18-25) than any other age group, with legalization in the\nmultiple states contributing to a public perception that cannabis is less risky\nthan in prior decades. To address this growing concern, we developed MiWaves, a\nreinforcement learning (RL) algorithm designed to optimize the delivery of\npersonalized intervention prompts to reduce cannabis use among EAs. MiWaves\nleverages domain expertise and prior data to tailor the likelihood of delivery\nof intervention messages. This paper presents a comprehensive overview of the\nalgorithm's design, including key decisions and experimental outcomes. The\nfinalized MiWaves RL algorithm was deployed in a clinical trial from March to\nMay 2024.\n","authors":["Susobhan Ghosh","Yongyi Guo","Pei-Yao Hung","Lara Coughlin","Erin Bonar","Inbal Nahum-Shani","Maureen Walton","Susan Murphy"],"pdf_url":"https://arxiv.org/pdf/2408.15076v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.17739"},{"id":"http://arxiv.org/abs/2408.06425v5","updated":"2024-08-27T14:03:15Z","published":"2024-08-12T18:04:59Z","title":"Bayesian Learning in a Nonlinear Multiscale State-Space Model","summary":"  The ubiquity of multiscale interactions in complex systems is\nwell-recognized, with development and heredity serving as a prime example of\nhow processes at different temporal scales influence one another. This work\nintroduces a novel multiscale state-space model to explore the dynamic\ninterplay between systems interacting across different time scales, with\nfeedback between each scale. We propose a Bayesian learning framework to\nestimate unknown states by learning the unknown process noise covariances\nwithin this multiscale model. We develop a Particle Gibbs with Ancestor\nSampling (PGAS) algorithm for inference and demonstrate through simulations the\nefficacy of our approach.\n","authors":["Nayely VÃ©lez-Cruz","Manfred D. Laubichler"],"pdf_url":"https://arxiv.org/pdf/2408.06425v5.pdf","comment":"Corrected a typo"},{"id":"http://arxiv.org/abs/2408.15073v1","updated":"2024-08-27T14:02:21Z","published":"2024-08-27T14:02:21Z","title":"Interactive dense pixel visualizations for time series and model\n  attribution explanations","summary":"  The field of Explainable Artificial Intelligence (XAI) for Deep Neural\nNetwork models has developed significantly, offering numerous techniques to\nextract explanations from models. However, evaluating explanations is often not\ntrivial, and differences in applied metrics can be subtle, especially with\nnon-intelligible data. Thus, there is a need for visualizations tailored to\nexplore explanations for domains with such data, e.g., time series. We propose\nDAVOTS, an interactive visual analytics approach to explore raw time series\ndata, activations of neural networks, and attributions in a dense-pixel\nvisualization to gain insights into the data, models' decisions, and\nexplanations. To further support users in exploring large datasets, we apply\nclustering approaches to the visualized data domains to highlight groups and\npresent ordering strategies for individual and combined data exploration to\nfacilitate finding patterns. We visualize a CNN trained on the FordA dataset to\ndemonstrate the approach.\n","authors":["Udo Schlegel","Daniel A. Keim"],"pdf_url":"https://arxiv.org/pdf/2408.15073v1.pdf","comment":"5 pages, 2 figures, accepted at MLVIS 2023"},{"id":"http://arxiv.org/abs/2408.15065v1","updated":"2024-08-27T13:48:15Z","published":"2024-08-27T13:48:15Z","title":"The Benefits of Balance: From Information Projections to Variance\n  Reduction","summary":"  Data balancing across multiple modalities/sources appears in various forms in\nseveral foundation models (e.g., CLIP and DINO) achieving universal\nrepresentation learning. We show that this iterative algorithm, usually used to\navoid representation collapse, enjoys an unsuspected benefit: reducing the\nvariance of estimators that are functionals of the empirical distribution over\nthese sources. We provide non-asymptotic bounds quantifying this variance\nreduction effect and relate them to the eigendecays of appropriately defined\nMarkov operators. We explain how various forms of data balancing in contrastive\nmultimodal learning and self-supervised clustering can be interpreted as\ninstances of this variance reduction scheme.\n","authors":["Lang Liu","Ronak Mehta","Soumik Pal","Zaid Harchaoui"],"pdf_url":"https://arxiv.org/pdf/2408.15065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15057v1","updated":"2024-08-27T13:40:15Z","published":"2024-08-27T13:40:15Z","title":"Subgroup Analysis via Model-based Rule Forest","summary":"  Machine learning models are often criticized for their black-box nature,\nraising concerns about their applicability in critical decision-making\nscenarios. Consequently, there is a growing demand for interpretable models in\nsuch contexts. In this study, we introduce Model-based Deep Rule Forests\n(mobDRF), an interpretable representation learning algorithm designed to\nextract transparent models from data. By leveraging IF-THEN rules with\nmulti-level logic expressions, mobDRF enhances the interpretability of existing\nmodels without compromising accuracy. We apply mobDRF to identify key risk\nfactors for cognitive decline in an elderly population, demonstrating its\neffectiveness in subgroup analysis and local model optimization. Our method\noffers a promising solution for developing trustworthy and interpretable\nmachine learning models, particularly valuable in fields like healthcare, where\nunderstanding differential effects across patient subgroups can lead to more\npersonalized and effective treatments.\n","authors":["I-Ling Cheng","Chan Hsu","Chantung Ku","Pei-Ju Lee","Yihuang Kang"],"pdf_url":"https://arxiv.org/pdf/2408.15057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09212v4","updated":"2024-08-27T13:40:13Z","published":"2024-07-12T12:20:39Z","title":"Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding\n  Learning","summary":"  Query embedding approaches answer complex logical queries over incomplete\nknowledge graphs (KGs) by computing and operating on low-dimensional vector\nrepresentations of entities, relations, and queries. However, current query\nembedding models heavily rely on excessively parameterized neural networks and\ncannot explain the knowledge learned from the graph. We propose a novel query\nembedding method, AConE, which explains the knowledge learned from the graph in\nthe form of $SROI^-$ description logic axioms while being more\nparameter-efficient than most existing approaches. AConE associates queries to\na $SROI^-$ description logic concept. Every $SROI^-$ concept is embedded as a\ncone in complex vector space, and each $SROI^-$ relation is embedded as a\ntransformation that rotates and scales cones. We show theoretically that AConE\ncan learn $SROI^-$ axioms, and defines an algebra whose operations correspond\none to one to $SROI^-$ description logic concept constructs. Our empirical\nstudy on multiple query datasets shows that AConE achieves superior results\nover previous baselines with fewer parameters. Notably on the WN18RR dataset,\nAConE achieves significant improvement over baseline models. We provide\ncomprehensive analyses showing that the capability to represent axioms\npositively impacts the results of query answering.\n","authors":["Yunjie He","Daniel Hernandez","Mojtaba Nayyeri","Bo Xiong","Yuqicheng Zhu","Evgeny Kharlamov","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2407.09212v4.pdf","comment":"Accepted by ECAI 2024"},{"id":"http://arxiv.org/abs/2408.15055v1","updated":"2024-08-27T13:32:31Z","published":"2024-08-27T13:32:31Z","title":"Causal Rule Forest: Toward Interpretable and Precise Treatment Effect\n  Estimation","summary":"  Understanding and inferencing Heterogeneous Treatment Effects (HTE) and\nConditional Average Treatment Effects (CATE) are vital for developing\npersonalized treatment recommendations. Many state-of-the-art approaches\nachieve inspiring performance in estimating HTE on benchmark datasets or\nsimulation studies. However, the indirect predicting manner and complex model\narchitecture reduce the interpretability of these approaches. To mitigate the\ngap between predictive performance and heterogeneity interpretability, we\nintroduce the Causal Rule Forest (CRF), a novel approach to learning hidden\npatterns from data and transforming the patterns into interpretable multi-level\nBoolean rules. By training the other interpretable causal inference models with\ndata representation learned by CRF, we can reduce the predictive errors of\nthese models in estimating HTE and CATE, while keeping their interpretability\nfor identifying subgroups that a treatment is more effective. Our experiments\nunderscore the potential of CRF to advance personalized interventions and\npolicies, paving the way for future research to enhance its scalability and\napplication across complex causal inference challenges.\n","authors":["Chan Hsu","Jun-Ting Wu","Yihuang Kang"],"pdf_url":"https://arxiv.org/pdf/2408.15055v1.pdf","comment":"The 25th IEEE International Conference on Information Reuse and\n  Integration for Data Science (IRI 2024)"},{"id":"http://arxiv.org/abs/2301.01188v4","updated":"2024-08-27T13:28:52Z","published":"2022-12-29T01:07:19Z","title":"Deep R Programming","summary":"  Deep R Programming is a comprehensive and in-depth introductory course on one\nof the most popular languages for data science. It equips ambitious students,\nprofessionals, and researchers with the knowledge and skills to become\nindependent users of this potent environment so that they can tackle any\nproblem related to data wrangling and analytics, numerical computing,\nstatistics, and machine learning. This textbook is a non-profit project. Its\nonline and PDF versions are freely available at\n<https://deepr.gagolewski.com/>.\n","authors":["Marek Gagolewski"],"pdf_url":"https://arxiv.org/pdf/2301.01188v4.pdf","comment":"v1.0.1 (2024-08-27)"},{"id":"http://arxiv.org/abs/2403.00381v2","updated":"2024-08-27T13:13:54Z","published":"2024-03-01T09:09:37Z","title":"Structured Deep Neural Networks-Based Backstepping Trajectory Tracking\n  Control for Lagrangian Systems","summary":"  Deep neural networks (DNN) are increasingly being used to learn controllers\ndue to their excellent approximation capabilities. However, their black-box\nnature poses significant challenges to closed-loop stability guarantees and\nperformance analysis. In this paper, we introduce a structured DNN-based\ncontroller for the trajectory tracking control of Lagrangian systems using\nbacking techniques. By properly designing neural network structures, the\nproposed controller can ensure closed-loop stability for any compatible neural\nnetwork parameters. In addition, improved control performance can be achieved\nby further optimizing neural network parameters. Besides, we provide explicit\nupper bounds on tracking errors in terms of controller parameters, which allows\nus to achieve the desired tracking performance by properly selecting the\ncontroller parameters. Furthermore, when system models are unknown, we propose\nan improved Lagrangian neural network (LNN) structure to learn the system\ndynamics and design the controller. We show that in the presence of model\napproximation errors and external disturbances, the closed-loop stability and\ntracking control performance can still be guaranteed. The effectiveness of the\nproposed approach is demonstrated through simulations.\n","authors":["Jiajun Qian","Liang Xu","Xiaoqiang Ren","Xiaofan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.00381v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15041v1","updated":"2024-08-27T13:10:26Z","published":"2024-08-27T13:10:26Z","title":"Earth Observation Satellite Scheduling with Graph Neural Networks","summary":"  The Earth Observation Satellite Planning (EOSP) is a difficult optimization\nproblem with considerable practical interest. A set of requested observations\nmust be scheduled on an agile Earth observation satellite while respecting\nconstraints on their visibility window, as well as maneuver constraints that\nimpose varying delays between successive observations. In addition, the problem\nis largely oversubscribed: there are much more candidate observations than what\ncan possibly be achieved. Therefore, one must select the set of observations\nthat will be performed while maximizing their weighted cumulative benefit, and\npropose a feasible schedule for these observations. As previous work mostly\nfocused on heuristic and iterative search algorithms, this paper presents a new\ntechnique for selecting and scheduling observations based on Graph Neural\nNetworks (GNNs) and Deep Reinforcement Learning (DRL). GNNs are used to extract\nrelevant information from the graphs representing instances of the EOSP, and\nDRL drives the search for optimal schedules. Our simulations show that it is\nable to learn on small problem instances and generalize to larger real-world\ninstances, with very competitive performance compared to traditional\napproaches.\n","authors":["Antoine Jacquet","Guillaume Infantes","Nicolas Meuleau","Emmanuel Benazera","StÃ©phanie Roussel","Vincent Baudoui","Jonathan Guerra"],"pdf_url":"https://arxiv.org/pdf/2408.15041v1.pdf","comment":"Accepted at 17th European Workshop on Reinforcement Learning (EWRL\n  2024)"},{"id":"http://arxiv.org/abs/2405.17035v3","updated":"2024-08-27T13:05:33Z","published":"2024-05-27T10:42:13Z","title":"Glauber Generative Model: Discrete Diffusion Models via Binary\n  Classification","summary":"  We introduce the Glauber Generative Model (GGM), a new class of discrete\ndiffusion models, to obtain new samples from a distribution given samples from\na discrete space. GGM deploys a discrete Markov chain called the heat bath\ndynamics (or the Glauber dynamics) to denoise a sequence of noisy tokens to a\nsample from a joint distribution of discrete tokens. Our novel conceptual\nframework provides an exact reduction of the task of learning the denoising\nMarkov chain to solving a class of binary classification tasks. More\nspecifically, the model learns to classify a given token in a noisy sequence as\nsignal or noise. In contrast, prior works on discrete diffusion models either\nsolve regression problems to learn importance ratios, or minimize loss\nfunctions given by variational approximations. We apply GGM to language\nmodeling and image generation, where images are discretized using image\ntokenizers like VQGANs. We show that it outperforms existing discrete diffusion\nmodels in language generation, and demonstrates strong performance for image\ngeneration without using dataset-specific image tokenizers. We also show that\nour model is capable of performing well in zero-shot control settings like text\nand image infilling.\n","authors":["Harshit Varma","Dheeraj Nagaraj","Karthikeyan Shanmugam"],"pdf_url":"https://arxiv.org/pdf/2405.17035v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09766v2","updated":"2024-08-27T13:01:56Z","published":"2024-02-15T07:35:52Z","title":"From Variability to Stability: Advancing RecSys Benchmarking Practices","summary":"  In the rapidly evolving domain of Recommender Systems (RecSys), new\nalgorithms frequently claim state-of-the-art performance based on evaluations\nover a limited set of arbitrarily selected datasets. However, this approach may\nfail to holistically reflect their effectiveness due to the significant impact\nof dataset characteristics on algorithm performance. Addressing this\ndeficiency, this paper introduces a novel benchmarking methodology to\nfacilitate a fair and robust comparison of RecSys algorithms, thereby advancing\nevaluation practices. By utilizing a diverse set of $30$ open datasets,\nincluding two introduced in this work, and evaluating $11$ collaborative\nfiltering algorithms across $9$ metrics, we critically examine the influence of\ndataset characteristics on algorithm performance. We further investigate the\nfeasibility of aggregating outcomes from multiple datasets into a unified\nranking. Through rigorous experimental analysis, we validate the reliability of\nour methodology under the variability of datasets, offering a benchmarking\nstrategy that balances quality and computational demands. This methodology\nenables a fair yet effective means of evaluating RecSys algorithms, providing\nvaluable guidance for future research endeavors.\n","authors":["Valeriy Shevchenko","Nikita Belousov","Alexey Vasilev","Vladimir Zholobov","Artyom Sosedka","Natalia Semenova","Anna Volodkevich","Andrey Savchenko","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2402.09766v2.pdf","comment":"8 pages with 11 figures"},{"id":"http://arxiv.org/abs/2408.13628v2","updated":"2024-08-27T12:53:22Z","published":"2024-08-24T17:10:59Z","title":"Enhancing Uplift Modeling in Multi-Treatment Marketing Campaigns:\n  Leveraging Score Ranking and Calibration Techniques","summary":"  Uplift modeling is essential for optimizing marketing strategies by selecting\nindividuals likely to respond positively to specific marketing campaigns. This\nimportance escalates in multi-treatment marketing campaigns, where diverse\ntreatment is available and we may want to assign the customers to treatment\nthat can make the most impact. While there are existing approaches with\nconvenient frameworks like Causalml, there are potential spaces to enhance the\neffect of uplift modeling in multi treatment cases. This paper introduces a\nnovel approach to uplift modeling in multi-treatment campaigns, leveraging\nscore ranking and calibration techniques to improve overall performance of the\nmarketing campaign. We review existing uplift models, including Meta Learner\nframeworks (S, T, X), and their application in real-world scenarios.\nAdditionally, we delve into insights from multi-treatment studies to highlight\nthe complexities and potential advancements in the field. Our methodology\nincorporates Meta-Learner calibration and a scoring rank-based offer selection\nstrategy. Extensive experiment results with real-world datasets demonstrate the\npractical benefits and superior performance of our approach. The findings\nunderscore the critical role of integrating score ranking and calibration\ntechniques in refining the performance and reliability of uplift predictions,\nthereby advancing predictive modeling in marketing analytics and providing\nactionable insights for practitioners seeking to optimize their campaign\nstrategies.\n","authors":["Yoon Tae Park","Ting Xu","Mohamed Anany"],"pdf_url":"https://arxiv.org/pdf/2408.13628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10635v5","updated":"2024-08-27T12:12:42Z","published":"2024-03-26T15:36:47Z","title":"Compressed Federated Reinforcement Learning with a Generative Model","summary":"  Reinforcement learning has recently gained unprecedented popularity, yet it\nstill grapples with sample inefficiency. Addressing this challenge, federated\nreinforcement learning (FedRL) has emerged, wherein agents collaboratively\nlearn a single policy by aggregating local estimations. However, this\naggregation step incurs significant communication costs. In this paper, we\npropose CompFedRL, a communication-efficient FedRL approach incorporating both\n\\textit{periodic aggregation} and (direct/error-feedback) compression\nmechanisms. Specifically, we consider compressed federated $Q$-learning with a\ngenerative model setup, where a central server learns an optimal $Q$-function\nby periodically aggregating compressed $Q$-estimates from local agents. For the\nfirst time, we characterize the impact of these two mechanisms (which have\nremained elusive) by providing a finite-time analysis of our algorithm,\ndemonstrating strong convergence behaviors when utilizing either direct or\nerror-feedback compression. Our bounds indicate improved solution accuracy\nconcerning the number of agents and other federated hyperparameters while\nsimultaneously reducing communication costs. To corroborate our theory, we also\nconduct in-depth numerical experiments to verify our findings, considering\nTop-$K$ and Sparsified-$K$ sparsification operators.\n","authors":["Ali Beikmohammadi","Sarit Khirirat","Sindri MagnÃºsson"],"pdf_url":"https://arxiv.org/pdf/2404.10635v5.pdf","comment":"European Conference on Machine Learning and Principles and Practice\n  of Knowledge Discovery in Databases (ECML-PKDD 2024)"},{"id":"http://arxiv.org/abs/2111.10847v3","updated":"2024-08-27T12:09:32Z","published":"2021-11-21T15:58:01Z","title":"Diffusion Tensor Estimation with Uncertainty Calibration","summary":"  It is highly desirable to know how uncertain a model's predictions are,\nespecially for models that are complex and hard to understand as in deep\nlearning. Although there has been a growing interest in using deep learning\nmethods in diffusion-weighted MRI, prior works have not addressed the issue of\nmodel uncertainty. Here, we propose a deep learning method to estimate the\ndiffusion tensor and compute the estimation uncertainty. Data-dependent\nuncertainty is computed directly by the network and learned via loss\nattenuation. Model uncertainty is computed using Monte Carlo dropout. We also\npropose a new method for evaluating the quality of predicted uncertainties. We\ncompare the new method with the standard least-squares tensor estimation and\nbootstrap-based uncertainty computation techniques. Our experiments show that\nwhen the number of measurements is small the deep learning method is more\naccurate and its uncertainty predictions are better calibrated than the\nstandard methods. We show that the estimation uncertainties computed by the new\nmethod can highlight the model's biases, detect domain shift, and reflect the\nstrength of noise in the measurements. Our study shows the importance and\npractical value of modeling prediction uncertainties in deep learning-based\ndiffusion MRI analysis.\n","authors":["Davood Karimi","Simon K. Warfield","Ali Gholipour"],"pdf_url":"https://arxiv.org/pdf/2111.10847v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09096v2","updated":"2024-08-27T11:50:50Z","published":"2024-07-12T08:48:16Z","title":"STD-PLM: Understanding Both Spatial and Temporal Properties of\n  Spatial-Temporal Data with PLM","summary":"  Spatial-temporal forecasting and imputation are important for real-world\nintelligent systems. Most existing methods are tailored for individual\nforecasting or imputation tasks but are not designed for both. Additionally,\nthey are less effective for zero-shot and few-shot learning. While pre-trained\nlanguage model (PLM) have exhibited strong pattern recognition and reasoning\nabilities across various tasks, including few-shot and zero-shot learning,\ntheir applications in spatial-temporal data understanding has been constrained\nby insufficient modeling of complex correlations such as the temporal\ncorrelations, spatial connectivity, non-pairwise and high-order\nspatial-temporal correlations within data. In this paper, we propose STD-PLM\nfor understanding both spatial and temporal properties of\n\\underline{S}patial-\\underline{T}emporal \\underline{D}ata with \\underline{PLM},\nwhich is capable of implementing both spatial-temporal forecasting and\nimputation tasks. STD-PLM understands spatial-temporal correlations via\nexplicitly designed spatial and temporal tokenizers. Topology-aware node\nembeddings are designed for PLM to comprehend and exploit the topology\nstructure of data in inductive manner. Furthermore, to mitigate the efficiency\nissues introduced by the PLM, we design a sandglass attention module (SGA)\ncombined with a specific constrained loss function, which significantly\nimproves the model's efficiency while ensuring performance. Extensive\nexperiments demonstrate that STD-PLM exhibits competitive performance and\ngeneralization capabilities across the forecasting and imputation tasks on\nvarious datasets. Moreover, STD-PLM achieves promising results on both few-shot\nand zero-shot tasks.\n","authors":["YiHeng Huang","Xiaowei Mao","Shengnan Guo","Yubin Chen","Junfeng Shen","Tiankuo Li","Youfang Lin","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2407.09096v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.16263v4","updated":"2024-08-27T11:48:49Z","published":"2022-03-30T12:48:22Z","title":"Does Audio Deepfake Detection Generalize?","summary":"  Current text-to-speech algorithms produce realistic fakes of human voices,\nmaking deepfake detection a much-needed area of research. While researchers\nhave presented various techniques for detecting audio spoofs, it is often\nunclear exactly why these architectures are successful: Preprocessing steps,\nhyperparameter settings, and the degree of fine-tuning are not consistent\nacross related work. Which factors contribute to success, and which are\naccidental? In this work, we address this problem: We systematize audio\nspoofing detection by re-implementing and uniformly evaluating architectures\nfrom related work. We identify overarching features for successful audio\ndeepfake detection, such as using cqtspec or logspec features instead of\nmelspec features, which improves performance by 37% EER on average, all other\nfactors constant. Additionally, we evaluate generalization capabilities: We\ncollect and publish a new dataset consisting of 37.9 hours of found audio\nrecordings of celebrities and politicians, of which 17.2 hours are deepfakes.\nWe find that related work performs poorly on such real-world data (performance\ndegradation of up to one thousand percent). This may suggest that the community\nhas tailored its solutions too closely to the prevailing ASVSpoof benchmark and\nthat deepfakes are much harder to detect outside the lab than previously\nthought.\n","authors":["Nicolas M. MÃ¼ller","Pavel Czempin","Franziska Dieckmann","Adam Froghyar","Konstantin BÃ¶ttinger"],"pdf_url":"https://arxiv.org/pdf/2203.16263v4.pdf","comment":"Interspeech 2022"},{"id":"http://arxiv.org/abs/2408.14976v1","updated":"2024-08-27T11:38:01Z","published":"2024-08-27T11:38:01Z","title":"Prior-free Balanced Replay: Uncertainty-guided Reservoir Sampling for\n  Long-Tailed Continual Learning","summary":"  Even in the era of large models, one of the well-known issues in continual\nlearning (CL) is catastrophic forgetting, which is significantly challenging\nwhen the continual data stream exhibits a long-tailed distribution, termed as\nLong-Tailed Continual Learning (LTCL). Existing LTCL solutions generally\nrequire the label distribution of the data stream to achieve re-balance\ntraining. However, obtaining such prior information is often infeasible in real\nscenarios since the model should learn without pre-identifying the majority and\nminority classes. To this end, we propose a novel Prior-free Balanced Replay\n(PBR) framework to learn from long-tailed data stream with less forgetting.\nConcretely, motivated by our experimental finding that the minority classes are\nmore likely to be forgotten due to the higher uncertainty, we newly design an\nuncertainty-guided reservoir sampling strategy to prioritize rehearsing\nminority data without using any prior information, which is based on the mutual\ndependence between the model and samples. Additionally, we incorporate two\nprior-free components to further reduce the forgetting issue: (1) Boundary\nconstraint is to preserve uncertain boundary supporting samples for continually\nre-estimating task boundaries. (2) Prototype constraint is to maintain the\nconsistency of learned class prototypes along with training. Our approach is\nevaluated on three standard long-tailed benchmarks, demonstrating superior\nperformance to existing CL methods and previous SOTA LTCL approach in both\ntask- and class-incremental learning settings, as well as ordered- and\nshuffled-LTCL settings.\n","authors":["Lei Liu","Li Liu","Yawen Cui"],"pdf_url":"https://arxiv.org/pdf/2408.14976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10672v2","updated":"2024-08-27T11:13:43Z","published":"2024-03-15T20:48:41Z","title":"Riemannian Flow Matching Policy for Robot Motion Learning","summary":"  We introduce Riemannian Flow Matching Policies (RFMP), a novel model for\nlearning and synthesizing robot visuomotor policies. RFMP leverages the\nefficient training and inference capabilities of flow matching methods. By\ndesign, RFMP inherits the strengths of flow matching: the ability to encode\nhigh-dimensional multimodal distributions, commonly encountered in robotic\ntasks, and a very simple and fast inference process. We demonstrate the\napplicability of RFMP to both state-based and vision-conditioned robot motion\npolicies. Notably, as the robot state resides on a Riemannian manifold, RFMP\ninherently incorporates geometric awareness, which is crucial for realistic\nrobotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments,\ncomparing its performance against Diffusion Policies. Although both approaches\nsuccessfully learn the considered tasks, our results show that RFMP provides\nsmoother action trajectories with significantly lower inference times.\n","authors":["Max Braun","NoÃ©mie Jaquier","Leonel Rozo","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.10672v2.pdf","comment":"Accepted for publication at IROS'24. 8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.14964v1","updated":"2024-08-27T11:10:39Z","published":"2024-08-27T11:10:39Z","title":"Cross-Modal Learning for Chemistry Property Prediction: Large Language\n  Models Meet Graph Machine Learning","summary":"  In the field of chemistry, the objective is to create novel molecules with\ndesired properties, facilitating accurate property predictions for applications\nsuch as material design and drug screening. However, existing graph deep\nlearning methods face limitations that curb their expressive power. To address\nthis, we explore the integration of vast molecular domain knowledge from Large\nLanguage Models (LLMs) with the complementary strengths of Graph Neural\nNetworks (GNNs) to enhance performance in property prediction tasks. We\nintroduce a Multi-Modal Fusion (MMF) framework that synergistically harnesses\nthe analytical prowess of GNNs and the linguistic generative and predictive\nabilities of LLMs, thereby improving accuracy and robustness in predicting\nmolecular properties. Our framework combines the effectiveness of GNNs in\nmodeling graph-structured data with the zero-shot and few-shot learning\ncapabilities of LLMs, enabling improved predictions while reducing the risk of\noverfitting. Furthermore, our approach effectively addresses distributional\nshifts, a common challenge in real-world applications, and showcases the\nefficacy of learning cross-modal representations, surpassing state-of-the-art\nbaselines on benchmark datasets for property prediction tasks.\n","authors":["Sakhinana Sagar Srinivas","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2408.14964v1.pdf","comment":"Paper Accepted at Workshop on Robustness of Few-shot and Zero-shot\n  Learning in Foundation Models at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2406.17640v2","updated":"2024-08-27T11:00:47Z","published":"2024-06-25T15:24:06Z","title":"BayTTA: Uncertainty-aware medical image classification with optimized\n  test-time augmentation using Bayesian model averaging","summary":"  Test-time augmentation (TTA) is a well-known technique employed during the\ntesting phase of computer vision tasks. It involves aggregating multiple\naugmented versions of input data. Combining predictions using a simple average\nformulation is a common and straightforward approach after performing TTA. This\npaper introduces a novel framework for optimizing TTA, called BayTTA\n(Bayesian-based TTA), which is based on Bayesian Model Averaging (BMA). First,\nwe generate a prediction list associated with different variations of the input\ndata created through TTA. Then, we use BMA to combine predictions weighted by\nthe respective posterior probabilities. Such an approach allows one to take\ninto account model uncertainty, and thus to enhance the predictive performance\nof the related machine learning or deep learning model. We evaluate the\nperformance of BayTTA on various public data, including three medical image\ndatasets comprising skin cancer, breast cancer, and chest X-ray images and two\nwell-known gene editing datasets, CRISPOR and GUIDE-seq. Our experimental\nresults indicate that BayTTA can be effectively integrated into\nstate-of-the-art deep learning models used in medical image analysis as well as\ninto some popular pre-trained CNN models such as VGG-16, MobileNetV2,\nDenseNet201, ResNet152V2, and InceptionRes-NetV2, leading to the enhancement in\ntheir accuracy and robustness performance. The source code of the proposed\nBayTTA method is freely available at: \\underline\n{https://github.com/Z-Sherkat/BayTTA}.\n","authors":["Zeinab Sherkatghanad","Moloud Abdar","Mohammadreza Bakhtyari","Pawel Plawiak","Vladimir Makarenkov"],"pdf_url":"https://arxiv.org/pdf/2406.17640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03729v2","updated":"2024-08-27T10:57:01Z","published":"2024-06-06T04:05:12Z","title":"Enhancing Sign Language Detection through Mediapipe and Convolutional\n  Neural Networks (CNN)","summary":"  This research combines MediaPipe and CNNs for the efficient and accurate\ninterpretation of ASL dataset for the real-time detection of sign language. The\nsystem presented here captures and processes hands' gestures in real time. the\nintended purpose was to create a very easy, accurate, and fast way of entering\ncommands without the necessity of touching something.MediaPipe supports one of\nthe powerful frameworks in real-time hand tracking capabilities for the ability\nto capture and preprocess hand movements, which increases the accuracy of the\ngesture recognition system. Actually, the integration of CNN with the MediaPipe\nresults in higher efficiency in using the model of real-time processing.The\naccuracy achieved by the model on ASL datasets is 99.12\\%.The model was tested\nusing American Sign Language (ASL) datasets. The results were then compared to\nthose of existing methods to evaluate how well it performed, using established\nevaluation techniques. The system will have applications in the communication,\neducation, and accessibility domains. Making systems such as described in this\npaper even better will assist people with hearing impairment and make things\naccessible to them. We tested the recognition and translation performance on an\nASL dataset and achieved better accuracy over previous models.It is meant to\nthe research is to identify the characters that American signs recognize using\nhand images taken from a web camera by based on mediapipe and CNNs\n","authors":["Aditya Raj Verma","Gagandeep Singh","Karnim Meghwal","Banawath Ramji","Praveen Kumar Dadheech"],"pdf_url":"https://arxiv.org/pdf/2406.03729v2.pdf","comment":"We have decided to withdraw our paper due to significant revisions\n  and improvements that need to be made based on new findings. After further\n  analysis, we believe these changes are necessary to ensure the accuracy and\n  completeness of our work. We plan to resubmit the revised version in the\n  future once the updates are complete"},{"id":"http://arxiv.org/abs/2408.14951v1","updated":"2024-08-27T10:54:51Z","published":"2024-08-27T10:54:51Z","title":"Domain-decoupled Physics-informed Neural Networks with Closed-form\n  Gradients for Fast Model Learning of Dynamical Systems","summary":"  Physics-informed neural networks (PINNs) are trained using physical equations\nand can also incorporate unmodeled effects by learning from data. PINNs for\ncontrol (PINCs) of dynamical systems are gaining interest due to their\nprediction speed compared to classical numerical integration methods for\nnonlinear state-space models, making them suitable for real-time control\napplications. We introduce the domain-decoupled physics-informed neural network\n(DD-PINN) to address current limitations of PINC in handling large and complex\nnonlinear dynamic systems. The time domain is decoupled from the feed-forward\nneural network to construct an Ansatz function, allowing for calculation of\ngradients in closed form. This approach significantly reduces training times,\nespecially for large dynamical systems, compared to PINC, which relies on\ngraph-based automatic differentiation. Additionally, the DD-PINN inherently\nfulfills the initial condition and supports higher-order excitation inputs,\nsimplifying the training process and enabling improved prediction accuracy.\nValidation on three systems - a nonlinear mass-spring-damper, a\nfive-mass-chain, and a two-link robot - demonstrates that the DD-PINN achieves\nsignificantly shorter training times. In cases where the PINC's prediction\ndiverges, the DD-PINN's prediction remains stable and accurate due to higher\nphysics loss reduction or use of a higher-order excitation input. The DD-PINN\nallows for fast and accurate learning of large dynamical systems previously out\nof reach for the PINC.\n","authors":["Henrik Krauss","Tim-Lukas Habich","Max Bartholdt","Thomas Seel","Moritz Schappler"],"pdf_url":"https://arxiv.org/pdf/2408.14951v1.pdf","comment":"Accepted to International Conference on Informatics in Control,\n  Automation and Robotics (ICINCO) 2024"},{"id":"http://arxiv.org/abs/2408.14935v1","updated":"2024-08-27T10:17:22Z","published":"2024-08-27T10:17:22Z","title":"Quotient Normalized Maximum Likelihood Criterion for Learning Bayesian\n  Network Structures","summary":"  We introduce an information theoretic criterion for Bayesian network\nstructure learning which we call quotient normalized maximum likelihood (qNML).\nIn contrast to the closely related factorized normalized maximum likelihood\ncriterion, qNML satisfies the property of score equivalence. It is also\ndecomposable and completely free of adjustable hyperparameters. For practical\ncomputations, we identify a remarkably accurate approximation proposed earlier\nby Szpankowski and Weinberger. Experiments on both simulated and real data\ndemonstrate that the new criterion leads to parsimonious models with good\npredictive accuracy.\n","authors":["Tomi Silander","Janne LeppÃ¤-aho","Elias JÃ¤Ã¤saari","Teemu Roos"],"pdf_url":"https://arxiv.org/pdf/2408.14935v1.pdf","comment":"Accepted to AISTATS 2018"},{"id":"http://arxiv.org/abs/2406.15504v2","updated":"2024-08-27T10:07:27Z","published":"2024-06-19T16:43:56Z","title":"Dr.E Bridges Graphs with Large Language Models through Words","summary":"  Significant efforts have been dedicated to integrating the powerful Large\nLanguage Models (LLMs) with diverse modalities, particularly focusing on the\nfusion of language, vision and audio data. However, the graph-structured data,\nwhich is inherently rich in structural and domain-specific knowledge, has not\nyet been gracefully adapted to LLMs. Existing methods either describe the graph\nwith raw text, suffering the loss of graph structural information, or feed\nGraph Neural Network (GNN) embeddings into LLMs at the cost of losing\nexplainable prompt semantics. To bridge this gap, we introduce an end-to-end\nmodality-aligning framework for LLM-graph alignment: Dual-Residual Vector\nQuantized-Variational AutoEncoder, namely Dr.E. Our approach is purposefully\ndesigned to facilitate token-level alignment with LLMs, enabling an effective\ntranslation of the intrinsic `language' of graphs into comprehensible natural\nlanguage. We also manage to enhance LLMs' more robust structural understanding\nof graphs by incorporating multiple views of the central nodes based on their\nsurrounding nodes at various distances. Our experimental evaluations on\nstandard graph tasks demonstrate competitive performance against other\nstate-of-the-art (SOTA) approaches. Additionally, our framework ensures certain\nvisual interpretability, efficiency, and robustness, marking the promising\nsuccessful endeavor to achieve token-level alignment between LLMs and GNNs. Our\ncode is available at: https://anonymous.4open.science/r/dre-817.\n","authors":["Zipeng Liu","Likang Wu","Ming He","Zhong Guan","Hongke Zhao","Nan Feng"],"pdf_url":"https://arxiv.org/pdf/2406.15504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14928v1","updated":"2024-08-27T10:05:37Z","published":"2024-08-27T10:05:37Z","title":"Targetin the partition function of chemically disordered materials with\n  a generative approach based on inverse variational autoencoders","summary":"  Computing atomic-scale properties of chemically disordered materials requires\nan efficient exploration of their vast configuration space. Traditional\napproaches such as Monte Carlo or Special Quasirandom Structures either entail\nsampling an excessive amount of configurations or do not ensure that the\nconfiguration space has been properly covered. In this work, we propose a novel\napproach where generative machine learning is used to yield a representative\nset of configurations for accurate property evaluation and provide accurate\nestimations of atomic-scale properties with minimal computational cost. Our\nmethod employs a specific type of variational autoencoder with inverse roles\nfor the encoder and decoder, enabling the application of an unsupervised active\nlearning scheme that does not require any initial training database. The model\niteratively generates configuration batches, whose properties are computed with\nconventional atomic-scale methods. These results are then fed back into the\nmodel to estimate the partition function, repeating the process until\nconvergence. We illustrate our approach by computing point-defect formation\nenergies and concentrations in (U, Pu)O2 mixed-oxide fuels. In addition, the ML\nmodel provides valuable insights into the physical factors influencing the\ntarget property. Our method is generally applicable to explore other\nproperties, such as atomic-scale diffusion coefficients, in ideally or\nnon-ideally disordered materials like high-entropy alloys.\n","authors":["Maciej J. Karcz","Luca Messina","Eiji Kawasaki","Emeric Bourasseau"],"pdf_url":"https://arxiv.org/pdf/2408.14928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14915v1","updated":"2024-08-27T09:44:01Z","published":"2024-08-27T09:44:01Z","title":"Can Transformers Do Enumerative Geometry?","summary":"  How can Transformers model and learn enumerative geometry? What is a robust\nprocedure for using Transformers in abductive knowledge discovery within a\nmathematician-machine collaboration? In this work, we introduce a new paradigm\nin computational enumerative geometry in analyzing the $\\psi$-class\nintersection numbers on the moduli space of curves. By formulating the\nenumerative problem as a continuous optimization task, we develop a\nTransformer-based model for computing $\\psi$-class intersection numbers based\non the underlying quantum Airy structure. For a finite range of genera, our\nmodel is capable of regressing intersection numbers that span an extremely wide\nrange of values, from $10^{-45}$ to $10^{45}$. To provide a proper inductive\nbias for capturing the recursive behavior of intersection numbers, we propose a\nnew activation function, Dynamic Range Activator (DRA). Moreover, given the\nsevere heteroscedasticity of $\\psi$-class intersections and the required\nprecision, we quantify the uncertainty of the predictions using Conformal\nPrediction with a dynamic sliding window that is aware of the number of marked\npoints. Next, we go beyond merely computing intersection numbers and explore\nthe enumerative \"world-model\" of the Transformers. Through a series of causal\ninference and correlational interpretability analyses, we demonstrate that\nTransformers are actually modeling Virasoro constraints in a purely data-driven\nmanner. Additionally, we provide evidence for the comprehension of several\nvalues appearing in the large genus asymptotic of $\\psi$-class intersection\nnumbers through abductive hypothesis testing.\n","authors":["Baran Hashemi","Roderic G. Corominas","Alessandro Giacchetto"],"pdf_url":"https://arxiv.org/pdf/2408.14915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14909v1","updated":"2024-08-27T09:35:49Z","published":"2024-08-27T09:35:49Z","title":"SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models","summary":"  Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs.\n","authors":["Shuaijie Shen","Chao Wang","Renzhuo Huang","Yan Zhong","Qinghai Guo","Zhichao Lu","Jianguo Zhang","Luziwei Leng"],"pdf_url":"https://arxiv.org/pdf/2408.14909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10779v2","updated":"2024-08-27T09:28:35Z","published":"2024-05-17T13:40:59Z","title":"Baseline Results for Selected Nonlinear System Identification Benchmarks","summary":"  Nonlinear system identification remains an important open challenge across\nresearch and academia. Large numbers of novel approaches are seen published\neach year, each presenting improvements or extensions to existing methods. It\nis natural, therefore, to consider how one might choose between these competing\nmodels. Benchmark datasets provide one clear way to approach this question.\nHowever, to make meaningful inference based on benchmark performance it is\nimportant to understand how well a new method performs comparatively to results\navailable with well-established methods. This paper presents a set of ten\nbaseline techniques and their relative performances on five popular benchmarks.\nThe aim of this contribution is to stimulate thought and discussion regarding\nobjective comparison of identification methodologies.\n","authors":["Max D. Champneys","Gerben I. Beintema","Roland TÃ³th","Maarten Schoukens","Timothy J. Rogers"],"pdf_url":"https://arxiv.org/pdf/2405.10779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05655v2","updated":"2024-08-27T09:24:41Z","published":"2023-10-09T12:10:51Z","title":"Causal structure learning with momentum: Sampling distributions over\n  Markov Equivalence Classes of DAGs","summary":"  In the context of inferring a Bayesian network structure (directed acyclic\ngraph, DAG for short), we devise a non-reversible continuous time Markov chain,\nthe ``Causal Zig-Zag sampler'', that targets a probability distribution over\nclasses of observationally equivalent (Markov equivalent) DAGs. The classes are\nrepresented as completed partially directed acyclic graphs (CPDAGs). The\nnon-reversible Markov chain relies on the operators used in Chickering's Greedy\nEquivalence Search (GES) and is endowed with a momentum variable, which\nimproves mixing significantly as we show empirically. The possible target\ndistributions include posterior distributions based on a prior over DAGs and a\nMarkov equivalent likelihood. We offer an efficient implementation wherein we\ndevelop new algorithms for listing, counting, uniformly sampling, and applying\npossible moves of the GES operators, all of which significantly improve upon\nthe state-of-the-art run-time.\n","authors":["Moritz Schauer","Marcel WienÃ¶bst"],"pdf_url":"https://arxiv.org/pdf/2310.05655v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.06458v3","updated":"2024-08-27T09:09:05Z","published":"2021-09-14T05:54:29Z","title":"A Note on Knowledge Distillation Loss Function for Object Classification","summary":"  This research note provides a quick introduction to the knowledge\ndistillation loss function used in object classification. In particular, we\ndiscuss its connection to a previously proposed logits matching loss function.\nWe further treat knowledge distillation as a specific form of output\nregularization and demonstrate its connection to label smoothing and\nentropy-based regularization.\n","authors":["Defang Chen"],"pdf_url":"https://arxiv.org/pdf/2109.06458v3.pdf","comment":"Research Note, 4 pages"},{"id":"http://arxiv.org/abs/2408.14890v1","updated":"2024-08-27T09:06:29Z","published":"2024-08-27T09:06:29Z","title":"Development of Large Annotated Music Datasets using HMM-based Forced\n  Viterbi Alignment","summary":"  Datasets are essential for any machine learning task. Automatic Music\nTranscription (AMT) is one such task, where considerable amount of data is\nrequired depending on the way the solution is achieved. Considering the fact\nthat a music dataset, complete with audio and its time-aligned transcriptions\nwould require the effort of people with musical experience, it could be stated\nthat the task becomes even more challenging. Musical experience is required in\nplaying the musical instrument(s), and in annotating and verifying the\ntranscriptions. We propose a method that would help in streamlining this\nprocess, making the task of obtaining a dataset from a particular instrument\neasy and efficient. We use predefined guitar exercises and hidden Markov\nmodel(HMM) based forced viterbi alignment to accomplish this. The guitar\nexercises are designed to be simple. Since the note sequence are already\ndefined, HMM based forced viterbi alignment provides time-aligned\ntranscriptions of these audio files. The onsets of the transcriptions are\nmanually verified and the labels are accurate up to 10ms, averaging at 5ms. The\ncontributions of the proposed work is two fold, i) a well streamlined and\nefficient method for generating datasets for any instrument, especially\nmonophonic and, ii) an acoustic plectrum guitar dataset containing wave files\nand transcriptions in the form of label files. This method will aid as a\npreliminary step towards building concrete datasets for building AMT systems\nfor different instruments.\n","authors":["S. Johanan Joysingh","P. Vijayalakshmi","T. Nagarajan"],"pdf_url":"https://arxiv.org/pdf/2408.14890v1.pdf","comment":"submitted to TENCON 2019"},{"id":"http://arxiv.org/abs/2408.08448v3","updated":"2024-08-27T09:04:35Z","published":"2024-08-15T22:57:39Z","title":"Exploring Cross-model Neuronal Correlations in the Context of Predicting\n  Model Performance and Generalizability","summary":"  As Artificial Intelligence (AI) models are increasingly integrated into\ncritical systems, the need for a robust framework to establish the\ntrustworthiness of AI is increasingly paramount. While collaborative efforts\nhave established conceptual foundations for such a framework, there remains a\nsignificant gap in developing concrete, technically robust methods for\nassessing AI model quality and performance. A critical drawback in the\ntraditional methods for assessing the validity and generalizability of models\nis their dependence on internal developer datasets, rendering it challenging to\nindependently assess and verify their performance claims. This paper introduces\na novel approach for assessing a newly trained model's performance based on\nanother known model by calculating correlation between neural networks. The\nproposed method evaluates correlations by determining if, for each neuron in\none network, there exists a neuron in the other network that produces similar\noutput. This approach has implications for memory efficiency, allowing for the\nuse of smaller networks when high correlation exists between networks of\ndifferent sizes. Additionally, the method provides insights into robustness,\nsuggesting that if two highly correlated networks are compared and one\ndemonstrates robustness when operating in production environments, the other is\nlikely to exhibit similar robustness. This contribution advances the technical\ntoolkit for responsible AI, supporting more comprehensive and nuanced\nevaluations of AI models to ensure their safe and effective deployment. Code is\navailable at https://github.com/aheldis/Cross-model-correlation.git.\n","authors":["Haniyeh Ehsani Oskouie","Lionel Levine","Majid Sarrafzadeh"],"pdf_url":"https://arxiv.org/pdf/2408.08448v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14889v1","updated":"2024-08-27T09:04:08Z","published":"2024-08-27T09:04:08Z","title":"Towards turbine-location-aware multi-decadal wind power predictions with\n  CMIP6","summary":"  With the increasing amount of renewable energy in the grid, long-term wind\npower forecasting for multiple decades becomes more critical. In these\nlong-term forecasts, climate data is essential as it allows us to account for\nclimate change. Yet the resolution of climate models is often very coarse. In\nthis paper, we show that by including turbine locations when downscaling with\nGaussian Processes, we can generate valuable aggregate wind power predictions\ndespite the low resolution of the CMIP6 climate models. This work is a first\nstep towards multi-decadal turbine-location-aware wind power forecasting using\nglobal climate model output.\n","authors":["Nina Effenberger","Nicole Ludwig"],"pdf_url":"https://arxiv.org/pdf/2408.14889v1.pdf","comment":"4 pages, pre-print"},{"id":"http://arxiv.org/abs/2408.14887v1","updated":"2024-08-27T09:00:27Z","published":"2024-08-27T09:00:27Z","title":"Literary and Colloquial Dialect Identification for Tamil using Acoustic\n  Features","summary":"  The evolution and diversity of a language is evident from it's various\ndialects. If the various dialects are not addressed in technological\nadvancements like automatic speech recognition and speech synthesis, there is a\nchance that these dialects may disappear. Speech technology plays a role in\npreserving various dialects of a language from going extinct. In order to build\na full fledged automatic speech recognition system that addresses various\ndialects, an Automatic Dialect Identification (ADI) system acting as the front\nend is required. This is similar to how language identification systems act as\nfront ends to automatic speech recognition systems that handle multiple\nlanguages. The current work proposes a way to identify two popular and broadly\nclassified Tamil dialects, namely literary and colloquial Tamil. Acoustical\ncharacteristics rather than phonetics and phonotactics are used, alleviating\nthe requirement of language-dependant linguistic tools. Hence one major\nadvantage of the proposed method is that it does not require an annotated\ncorpus, hence it can be easily adapted to other languages. Gaussian Mixture\nModels (GMM) using Mel Frequency Cepstral Coefficient (MFCC) features are used\nto perform the classification task. The experiments yielded an error rate of\n12%. Vowel nasalization, as being the reason for this good performance, is\ndiscussed. The number of mixture models for the GMM is varied and the\nperformance is analysed.\n","authors":["M. Nanmalar","P. Vijayalakshmi","T. Nagarajan"],"pdf_url":"https://arxiv.org/pdf/2408.14887v1.pdf","comment":"submitted to TENCON 2019"},{"id":"http://arxiv.org/abs/2408.14875v1","updated":"2024-08-27T08:44:31Z","published":"2024-08-27T08:44:31Z","title":"Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting\n  for Smart and Connected Infrastructures","summary":"  The emergence of deep learning models has revolutionized various industries\nover the last decade, leading to a surge in connected devices and\ninfrastructures. However, these models can be tricked into making incorrect\npredictions with high confidence, leading to disastrous failures and security\nconcerns. To this end, we explore the impact of adversarial attacks on\nmultivariate time-series forecasting and investigate methods to counter them.\nSpecifically, we employ untargeted white-box attacks, namely the Fast Gradient\nSign Method (FGSM) and the Basic Iterative Method (BIM), to poison the inputs\nto the training process, effectively misleading the model. We also illustrate\nthe subtle modifications to the inputs after the attack, which makes detecting\nthe attack using the naked eye quite difficult. Having demonstrated the\nfeasibility of these attacks, we develop robust models through adversarial\ntraining and model hardening. We are among the first to showcase the\ntransferability of these attacks and defenses by extrapolating our work from\nthe benchmark electricity data to a larger, 10-year real-world data used for\npredicting the time-to-failure of hard disks. Our experimental results confirm\nthat the attacks and defenses achieve the desired security thresholds, leading\nto a 72.41% and 94.81% decrease in RMSE for the electricity and hard disk\ndatasets respectively after implementing the adversarial defenses.\n","authors":["Pooja Krishan","Rohan Mohapatra","Saptarshi Sengupta"],"pdf_url":"https://arxiv.org/pdf/2408.14875v1.pdf","comment":"17 pages, 32 figures"},{"id":"http://arxiv.org/abs/2405.07488v2","updated":"2024-08-27T08:44:20Z","published":"2024-05-13T06:04:26Z","title":"Predictive Modeling of Flexible EHD Pumps using Kolmogorov-Arnold\n  Networks","summary":"  We present a novel approach to predicting the pressure and flow rate of\nflexible electrohydrodynamic pumps using the Kolmogorov-Arnold Network.\nInspired by the Kolmogorov-Arnold representation theorem, KAN replaces fixed\nactivation functions with learnable spline-based activation functions, enabling\nit to approximate complex nonlinear functions more effectively than traditional\nmodels like Multi-Layer Perceptron and Random Forest. We evaluated KAN on a\ndataset of flexible EHD pump parameters and compared its performance against\nRF, and MLP models. KAN achieved superior predictive accuracy, with Mean\nSquared Errors of 12.186 and 0.001 for pressure and flow rate predictions,\nrespectively. The symbolic formulas extracted from KAN provided insights into\nthe nonlinear relationships between input parameters and pump performance.\nThese findings demonstrate that KAN offers exceptional accuracy and\ninterpretability, making it a promising alternative for predictive modeling in\nelectrohydrodynamic pumping.\n","authors":["Yanhong Peng","Yuxin Wang","Fangchao Hu","Miao He","Zebing Mao","Xia Huang","Jun Ding"],"pdf_url":"https://arxiv.org/pdf/2405.07488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14871v1","updated":"2024-08-27T08:41:42Z","published":"2024-08-27T08:41:42Z","title":"Learning Robust Reward Machines from Noisy Labels","summary":"  This paper presents PROB-IRM, an approach that learns robust reward machines\n(RMs) for reinforcement learning (RL) agents from noisy execution traces. The\nkey aspect of RM-driven RL is the exploitation of a finite-state machine that\ndecomposes the agent's task into different subtasks. PROB-IRM uses a\nstate-of-the-art inductive logic programming framework robust to noisy examples\nto learn RMs from noisy traces using the Bayesian posterior degree of beliefs,\nthus ensuring robustness against inconsistencies. Pivotal for the results is\nthe interleaving between RM learning and policy learning: a new RM is learned\nwhenever the RL agent generates a trace that is believed not to be accepted by\nthe current RM. To speed up the training of the RL agent, PROB-IRM employs a\nprobabilistic formulation of reward shaping that uses the posterior Bayesian\nbeliefs derived from the traces. Our experimental analysis shows that PROB-IRM\ncan learn (potentially imperfect) RMs from noisy traces and exploit them to\ntrain an RL agent to solve its tasks successfully. Despite the complexity of\nlearning the RM from noisy traces, agents trained with PROB-IRM perform\ncomparably to agents provided with handcrafted RMs.\n","authors":["Roko Parac","Lorenzo Nodari","Leo Ardon","Daniel Furelos-Blanco","Federico Cerutti","Alessandra Russo"],"pdf_url":"https://arxiv.org/pdf/2408.14871v1.pdf","comment":"Preprint accepted for publication to the 21st International\n  Conference on Principles of Knowledge Representation and Reasoning (KR 2024)"},{"id":"http://arxiv.org/abs/2308.16818v3","updated":"2024-08-27T08:39:38Z","published":"2023-08-31T15:49:21Z","title":"Irregular Traffic Time Series Forecasting Based on Asynchronous\n  Spatio-Temporal Graph Convolutional Network","summary":"  Accurate traffic forecasting is crucial for the development of Intelligent\nTransportation Systems (ITS), playing a pivotal role in modern urban traffic\nmanagement. Traditional forecasting methods, however, struggle with the\nirregular traffic time series resulting from adaptive traffic signal controls,\npresenting challenges in asynchronous spatial dependency, irregular temporal\ndependency, and predicting variable-length sequences. To this end, we propose\nan Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) tailored\nfor irregular traffic time series forecasting. Specifically, we first propose\nan Asynchronous Graph Diffusion Network to capture the spatial dependency\nbetween asynchronously measured traffic states regulated by adaptive traffic\nsignals. After that, to capture the temporal dependency within irregular\ntraffic state sequences, a personalized time encoding is devised to embed the\ncontinuous time signals. Then, we propose a Transformable Time-aware\nConvolution Network, which adapts meta-filters for time-aware convolution on\nthe sequences with inconsistent temporal flow. Additionally, a\nSemi-Autoregressive Prediction Network, comprising a state evolution unit and a\nsemi-autoregressive predictor, is designed to predict variable-length traffic\nsequences effectively and efficiently. Extensive experiments on a newly\nestablished benchmark demonstrate the superiority of ASeer compared with twelve\ncompetitive baselines across six metrics.\n","authors":["Weijia Zhang","Le Zhang","Jindong Han","Hao Liu","Yanjie Fu","Jingbo Zhou","Yu Mei","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2308.16818v3.pdf","comment":"This work is published in the research track of KDD 2024"},{"id":"http://arxiv.org/abs/2408.14866v1","updated":"2024-08-27T08:38:48Z","published":"2024-08-27T08:38:48Z","title":"Advancing Adversarial Suffix Transfer Learning on Aligned Large Language\n  Models","summary":"  Language Language Models (LLMs) face safety concerns due to potential misuse\nby malicious users. Recent red-teaming efforts have identified adversarial\nsuffixes capable of jailbreaking LLMs using the gradient-based search algorithm\nGreedy Coordinate Gradient (GCG). However, GCG struggles with computational\ninefficiency, limiting further investigations regarding suffix transferability\nand scalability across models and data. In this work, we bridge the connection\nbetween search efficiency and suffix transferability. We propose a two-stage\ntransfer learning framework, DeGCG, which decouples the search process into\nbehavior-agnostic pre-searching and behavior-relevant post-searching.\nSpecifically, we employ direct first target token optimization in pre-searching\nto facilitate the search process. We apply our approach to cross-model,\ncross-data, and self-transfer scenarios. Furthermore, we introduce an\ninterleaved variant of our approach, i-DeGCG, which iteratively leverages\nself-transferability to accelerate the search process. Experiments on HarmBench\ndemonstrate the efficiency of our approach across various models and domains.\nNotably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of\n$43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively.\nFurther analysis on cross-model transfer indicates the pivotal role of first\ntarget token optimization in leveraging suffix transferability for efficient\nsearching.\n","authors":["Hongfu Liu","Yuxi Xie","Ye Wang","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2408.14866v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.14865v1","updated":"2024-08-27T08:38:45Z","published":"2024-08-27T08:38:45Z","title":"Data downlink prioritization using image classification on-board a 6U\n  CubeSat","summary":"  Nanosatellites are proliferating as low-cost dedicated sensing systems with\nlean development cycles. Kyushu Institute of Technology and collaborators have\nlaunched a joint venture for a nanosatellite mission, VERTECS. The primary\nmission is to elucidate the formation history of stars by observing the\noptical-wavelength cosmic background radiation. The VERTECS satellite will be\nequipped with a small-aperture telescope and a high-precision attitude control\nsystem to capture the cosmic data for analysis on the ground. However,\nnanosatellites are limited by their onboard memory resources and downlink speed\ncapabilities. Additionally, due to a limited number of ground stations, the\nsatellite mission will face issues meeting the required data budget for mission\nsuccess. To alleviate this issue, we propose an on-orbit system to autonomously\nclassify and then compress desirable image data for data downlink\nprioritization and optimization. The system comprises a prototype Camera\nController Board (CCB) which carries a Raspberry Pi Compute Module 4 which is\nused for classification and compression. The system uses a lightweight\nConvolutional Neural Network (CNN) model to classify and determine the\ndesirability of captured image data. The model is designed to be lean and\nrobust to reduce the computational and memory load on the satellite. The model\nis trained and tested on a novel star field dataset consisting of data captured\nby the Sloan Digital Sky Survey (SDSS). The dataset is meant to simulate the\nexpected data produced by the 6U satellite. The compression step implements\nGZip, RICE or HCOMPRESS compression, which are standards for astronomical data.\nPreliminary testing on the proposed CNN model results in a classification\naccuracy of about 100\\% on the star field dataset, with compression ratios of\n3.99, 5.16 and 5.43 for GZip, RICE and HCOMPRESS that were achieved on tested\nFITS image data.\n","authors":["Keenan A. A. Chatar","Ezra Fielding","Kei Sano","Kentaro Kitamura"],"pdf_url":"https://arxiv.org/pdf/2408.14865v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.14864v1","updated":"2024-08-27T08:38:17Z","published":"2024-08-27T08:38:17Z","title":"Dynamic operator management in meta-heuristics using reinforcement\n  learning: an application to permutation flowshop scheduling problems","summary":"  This study develops a framework based on reinforcement learning to\ndynamically manage a large portfolio of search operators within\nmeta-heuristics. Using the idea of tabu search, the framework allows for\ncontinuous adaptation by temporarily excluding less efficient operators and\nupdating the portfolio composition during the search. A Q-learning-based\nadaptive operator selection mechanism is used to select the most suitable\noperator from the dynamically updated portfolio at each stage. Unlike\ntraditional approaches, the proposed framework requires no input from the\nexperts regarding the search operators, allowing domain-specific non-experts to\neffectively use the framework. The performance of the proposed framework is\nanalyzed through an application to the permutation flowshop scheduling problem.\nThe results demonstrate the superior performance of the proposed framework\nagainst state-of-the-art algorithms in terms of optimality gap and convergence\nspeed.\n","authors":["Maryam Karimi Mamaghan","Mehrdad Mohammadi","Wout Dullaert","Daniele Vigo","Amir Pirayesh"],"pdf_url":"https://arxiv.org/pdf/2408.14864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03870v2","updated":"2024-08-27T08:31:04Z","published":"2024-03-06T17:23:28Z","title":"Learning to Decode Collaboratively with Multiple Language Models","summary":"  We propose a method to teach multiple large language models (LLM) to\ncollaborate by interleaving their generations at the token level. We model the\ndecision of which LLM generates the next token as a latent variable. By\noptimizing the marginal likelihood of a training set under our latent variable\nmodel, the base LLM automatically learns when to generate itself and when to\ncall on one of the ``assistant'' language models to generate, all without\ndirect supervision. Token-level collaboration during decoding allows for a\nfusion of each model's expertise in a manner tailored to the specific task at\nhand. Our collaborative decoding is especially useful in cross-domain settings\nwhere a generalist base LLM learns to invoke domain expert models. On\ninstruction-following, domain-specific QA, and reasoning tasks, we show that\nthe performance of the joint system exceeds that of the individual models.\nThrough qualitative analysis of the learned latent decisions, we show models\ntrained with our method exhibit several interesting collaboration patterns,\ne.g., template-filling. Our code is available at\nhttps://github.com/clinicalml/co-llm.\n","authors":["Shannon Zejiang Shen","Hunter Lang","Bailin Wang","Yoon Kim","David Sontag"],"pdf_url":"https://arxiv.org/pdf/2403.03870v2.pdf","comment":"16 pages, 4 figures, 11 tables"},{"id":"http://arxiv.org/abs/2408.13766v2","updated":"2024-08-27T08:07:20Z","published":"2024-08-25T08:23:06Z","title":"Enhancing Robustness of Human Detection Algorithms in Maritime SAR\n  through Augmented Aerial Images to Simulate Weather Conditions","summary":"  7,651 cases of Search and Rescue Missions (SAR) were reported by the United\nStates Coast Guard in 2024, with over 1322 SAR helicopters deployed in the 6\nfirst months alone. Through the utilizations of YOLO, we were able to run\ndifferent weather conditions and lighting from our augmented dataset for\ntraining. YOLO then utilizes CNNs to apply a series of convolutions and pooling\nlayers to the input image, where the convolution layers are able to extract the\nmain features of the image. Through this, our YOLO model is able to learn to\ndifferentiate different objects which may considerably improve its accuracy,\npossibly enhancing the efficiency of SAR operations through enhanced detection\naccuracy. This paper aims to improve the model's accuracy of human detection in\nmaritime SAR by evaluating a robust datasets containing various elevations and\ngeological locations, as well as through data augmentation which simulates\ndifferent weather and lighting. We observed that models trained on augmented\ndatasets outperformed their non-augmented counterparts in which the human\nrecall scores ranged from 0.891 to 0.911 with an improvement rate of 3.4\\% on\nthe YOLOv5l model. Results showed that these models demonstrate greater\nrobustness to real-world conditions in varying of weather, brightness, tint,\nand contrast.\n","authors":["Miguel Tjia","Artem Kim","Elaine Wynette Wijaya","Hanna Tefara","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.13766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14010v2","updated":"2024-08-27T08:02:49Z","published":"2024-08-26T04:31:55Z","title":"Improving Water Quality Time-Series Prediction in Hong Kong using\n  Sentinel-2 MSI Data and Google Earth Engine Cloud Computing","summary":"  Effective water quality monitoring in coastal regions is crucial due to the\nprogressive deterioration caused by pollution and human activities. To address\nthis, this study develops time-series models to predict chlorophyll-a (Chl-a),\nsuspended solids (SS), and turbidity using Sentinel-2 satellite data and Google\nEarth Engine (GEE) in the coastal regions of Hong Kong. Leveraging Long\nShort-Term Memory (LSTM) Recurrent Neural Networks, the study incorporates\nextensive temporal datasets to enhance prediction accuracy. The models utilize\nspectral data from Sentinel-2, focusing on optically active components, and\ndemonstrate that selected variables closely align with the spectral\ncharacteristics of Chl-a and SS. The results indicate improved predictive\nperformance over previous methods, highlighting the potential for remote\nsensing technology in continuous and comprehensive water quality assessment.\n","authors":["Rohin Sood","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.14010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14847v1","updated":"2024-08-27T07:58:08Z","published":"2024-08-27T07:58:08Z","title":"Intraoperative Glioma Segmentation with YOLO + SAM for Improved Accuracy\n  in Tumor Resection","summary":"  Gliomas, a common type of malignant brain tumor, present significant surgical\nchallenges due to their similarity to healthy tissue. Preoperative Magnetic\nResonance Imaging (MRI) images are often ineffective during surgery due to\nfactors such as brain shift, which alters the position of brain structures and\ntumors. This makes real-time intraoperative MRI (ioMRI) crucial, as it provides\nupdated imaging that accounts for these shifts, ensuring more accurate tumor\nlocalization and safer resections. This paper presents a deep learning pipeline\ncombining You Only Look Once Version 8 (YOLOv8) and Segment Anything Model\nVision Transformer-base (SAM ViT-b) to enhance glioma detection and\nsegmentation during ioMRI. Our model was trained using the Brain Tumor\nSegmentation 2021 (BraTS 2021) dataset, which includes standard magnetic\nresonance imaging (MRI) images, and noise-augmented MRI images that simulate\nioMRI images. Noised MRI images are harder for a deep learning pipeline to\nsegment, but they are more representative of surgical conditions. Achieving a\nDice Similarity Coefficient (DICE) score of 0.79, our model performs comparably\nto state-of-the-art segmentation models tested on noiseless data. This\nperformance demonstrates the model's potential to assist surgeons in maximizing\ntumor resection and improving surgical outcomes.\n","authors":["Samir Kassam","Angelo Markham","Katie Vo","Yashas Revanakara","Michael Lam","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.14847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14843v1","updated":"2024-08-27T07:54:15Z","published":"2024-08-27T07:54:15Z","title":"Correntropy-Based Improper Likelihood Model for Robust\n  Electrophysiological Source Imaging","summary":"  Bayesian learning provides a unified skeleton to solve the\nelectrophysiological source imaging task. From this perspective, existing\nsource imaging algorithms utilize the Gaussian assumption for the observation\nnoise to build the likelihood function for Bayesian inference. However, the\nelectromagnetic measurements of brain activity are usually affected by\nmiscellaneous artifacts, leading to a potentially non-Gaussian distribution for\nthe observation noise. Hence the conventional Gaussian likelihood model is a\nsuboptimal choice for the real-world source imaging task. In this study, we aim\nto solve this problem by proposing a new likelihood model which is robust with\nrespect to non-Gaussian noises. Motivated by the robust maximum correntropy\ncriterion, we propose a new improper distribution model concerning the noise\nassumption. This new noise distribution is leveraged to structure a robust\nlikelihood function and integrated with hierarchical prior distributions to\nestimate source activities by variational inference. In particular, the score\nmatching is adopted to determine the hyperparameters for the improper\nlikelihood model. A comprehensive performance evaluation is performed to\ncompare the proposed noise assumption to the conventional Gaussian model.\nSimulation results show that, the proposed method can realize more precise\nsource reconstruction by designing known ground-truth. The real-world dataset\nalso demonstrates the superiority of our new method with the visual perception\ntask. This study provides a new backbone for Bayesian source imaging, which\nwould facilitate its application using real-world noisy brain signal.\n","authors":["Yuanhao Li","Badong Chen","Zhongxu Hu","Keita Suzuki","Wenjun Bai","Yasuharu Koike","Okito Yamashita"],"pdf_url":"https://arxiv.org/pdf/2408.14843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14842v1","updated":"2024-08-27T07:54:01Z","published":"2024-08-27T07:54:01Z","title":"From Bias to Balance: Detecting Facial Expression Recognition Biases in\n  Large Multimodal Foundation Models","summary":"  This study addresses the racial biases in facial expression recognition (FER)\nsystems within Large Multimodal Foundation Models (LMFMs). Despite advances in\ndeep learning and the availability of diverse datasets, FER systems often\nexhibit higher error rates for individuals with darker skin tones. Existing\nresearch predominantly focuses on traditional FER models (CNNs, RNNs, ViTs),\nleaving a gap in understanding racial biases in LMFMs. We benchmark four\nleading LMFMs: GPT-4o, PaliGemma, Gemini, and CLIP to assess their performance\nin facial emotion detection across different racial demographics. A linear\nclassifier trained on CLIP embeddings obtains accuracies of 95.9\\% for RADIATE,\n90.3\\% for Tarr, and 99.5\\% for Chicago Face. Furthermore, we identify that\nAnger is misclassified as Disgust 2.1 times more often in Black Females than\nWhite Females. This study highlights the need for fairer FER systems and\nestablishes a foundation for developing unbiased, accurate FER technologies.\nVisit https://kvjvhub.github.io/FERRacialBias/ for further information\nregarding the biases within facial expression recognition.\n","authors":["Kaylee Chhua","Zhoujinyi Wen","Vedant Hathalia","Kevin Zhu","Sean O'Brien"],"pdf_url":"https://arxiv.org/pdf/2408.14842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14840v1","updated":"2024-08-27T07:51:26Z","published":"2024-08-27T07:51:26Z","title":"CL4KGE: A Curriculum Learning Method for Knowledge Graph Embedding","summary":"  Knowledge graph embedding (KGE) constitutes a foundational task, directed\ntowards learning representations for entities and relations within knowledge\ngraphs (KGs), with the objective of crafting representations comprehensive\nenough to approximate the logical and symbolic interconnections among entities.\nIn this paper, we define a metric Z-counts to measure the difficulty of\ntraining each triple ($<$head entity, relation, tail entity$>$) in KGs with\ntheoretical analysis. Based on this metric, we propose \\textbf{CL4KGE}, an\nefficient \\textbf{C}urriculum \\textbf{L}earning based training strategy for\n\\textbf{KGE}. This method includes a difficulty measurer and a training\nscheduler that aids in the training of KGE models. Our approach possesses the\nflexibility to act as a plugin within a wide range of KGE models, with the\nadded advantage of adaptability to the majority of KGs in existence. The\nproposed method has been evaluated on popular KGE models, and the results\ndemonstrate that it enhances the state-of-the-art methods. The use of Z-counts\nas a metric has enabled the identification of challenging triples in KGs, which\nhelps in devising effective training strategies.\n","authors":["Yang Liu","Chuan Zhou","Peng Zhang","Yanan Cao","Yongchao Liu","Zhao Li","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2408.14840v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.14837v1","updated":"2024-08-27T07:46:07Z","published":"2024-08-27T07:46:07Z","title":"Diffusion Models Are Real-Time Game Engines","summary":"  We present GameNGen, the first game engine powered entirely by a neural model\nthat enables real-time interaction with a complex environment over long\ntrajectories at high quality. GameNGen can interactively simulate the classic\ngame DOOM at over 20 frames per second on a single TPU. Next frame prediction\nachieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are\nonly slightly better than random chance at distinguishing short clips of the\ngame from clips of the simulation. GameNGen is trained in two phases: (1) an\nRL-agent learns to play the game and the training sessions are recorded, and\n(2) a diffusion model is trained to produce the next frame, conditioned on the\nsequence of past frames and actions. Conditioning augmentations enable stable\nauto-regressive generation over long trajectories.\n","authors":["Dani Valevski","Yaniv Leviathan","Moab Arar","Shlomi Fruchter"],"pdf_url":"https://arxiv.org/pdf/2408.14837v1.pdf","comment":"Project page: https://gamengen.github.io/"},{"id":"http://arxiv.org/abs/2405.18723v3","updated":"2024-08-27T07:31:44Z","published":"2024-05-29T03:08:30Z","title":"Conformal Depression Prediction","summary":"  While existing depression prediction methods based on deep learning show\npromise, their practical application is hindered by the lack of\ntrustworthiness, as these deep models are often deployed as black box models,\nleaving us uncertain on the confidence of their predictions. For high-risk\nclinical applications like depression prediction, uncertainty quantification is\nessential in decision-making. In this paper, we introduce conformal depression\nprediction (CDP), a depression prediction method with uncertainty\nquantification based on conformal prediction (CP), giving valid confidence\nintervals with theoretical coverage guarantees for the model predictions. CDP\nis a plug-and-play module that requires neither model retraining nor an\nassumption about the depression data distribution. As CDP provides only an\naverage coverage guarantee across all inputs rather than per-input performance\nguarantee, we further propose CDP-ACC, an improved conformal prediction with\napproximate conditional coverage. CDP-ACC firstly estimates the prediction\ndistribution through neighborhood relaxation, and then introduces a conformal\nscore function by constructing nested sequences, so as to provide a tighter\nprediction interval adaptive to specific input. We empirically demonstrate the\napplication of CDP in uncertainty-aware facial depression prediction, as well\nas the effectiveness and superiority of CDP-ACC on the AVEC 2013 and AVEC 2014\ndatasets. Our code is publicly available at https://github.com/PushineLee/CDP.\n","authors":["Yonghong Li","Xiuzhuang Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.18723v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14831v1","updated":"2024-08-27T07:28:05Z","published":"2024-08-27T07:28:05Z","title":"DRL-Based Federated Self-Supervised Learning for Task Offloading and\n  Resource Allocation in ISAC-Enabled Vehicle Edge Computing","summary":"  Intelligent Transportation Systems (ITS) leverage Integrated Sensing and\nCommunications (ISAC) to enhance data exchange between vehicles and\ninfrastructure in the Internet of Vehicles (IoV). This integration inevitably\nincreases computing demands, risking real-time system stability. Vehicle Edge\nComputing (VEC) addresses this by offloading tasks to Road Side Unit (RSU),\nensuring timely services. Our previous work FLSimCo algorithm, which uses local\nresources for Federated Self-Supervised Learning (SSL), though vehicles often\ncan't complete all iterations task. Our improved algorithm offloads partial\ntask to RSU and optimizes energy consumption by adjusting transmission power,\nCPU frequency, and task assignment ratios, balancing local and RSU-based\ntraining. Meanwhile, setting an offloading threshold further prevents\ninefficiencies. Simulation results show that the enhanced algorithm reduces\nenergy consumption, improves offloading efficiency and the accuracy of\nFederated SSL.\n","authors":["Xueying Gu","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2408.14831v1.pdf","comment":"This paper has been submitted to Digital Communications and Networks.\n  The source code has been released at:\n  https://github.com/qiongwu86/Federated-SSL-task-offloading-and-resource-allocation"},{"id":"http://arxiv.org/abs/2408.13751v2","updated":"2024-08-27T07:26:20Z","published":"2024-08-25T07:32:58Z","title":"Improved identification of breakpoints in piecewise regression and its\n  applications","summary":"  Identifying breakpoints in piecewise regression is critical in enhancing the\nreliability and interpretability of data fitting. In this paper, we propose\nnovel algorithms based on the greedy algorithm to accurately and efficiently\nidentify breakpoints in piecewise polynomial regression. The algorithm updates\nthe breakpoints to minimize the error by exploring the neighborhood of each\nbreakpoint. It has a fast convergence rate and stability to find optimal\nbreakpoints. Moreover, it can determine the optimal number of breakpoints. The\ncomputational results for real and synthetic data show that its accuracy is\nbetter than any existing methods. The real-world datasets demonstrate that\nbreakpoints through the proposed algorithm provide valuable data information.\n","authors":["Taehyeong Kim","Hyungu Lee","Hayoung Choi"],"pdf_url":"https://arxiv.org/pdf/2408.13751v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.14825v1","updated":"2024-08-27T07:11:45Z","published":"2024-08-27T07:11:45Z","title":"From Rule-Based Models to Deep Learning Transformers Architectures for\n  Natural Language Processing and Sign Language Translation Systems: Survey,\n  Taxonomy and Performance Evaluation","summary":"  With the growing Deaf and Hard of Hearing population worldwide and the\npersistent shortage of certified sign language interpreters, there is a\npressing need for an efficient, signs-driven, integrated end-to-end translation\nsystem, from sign to gloss to text and vice-versa. There has been a wealth of\nresearch on machine translations and related reviews. However, there are few\nworks on sign language machine translation considering the particularity of the\nlanguage being continuous and dynamic. This paper aims to address this void,\nproviding a retrospective analysis of the temporal evolution of sign language\nmachine translation algorithms and a taxonomy of the Transformers\narchitectures, the most used approach in language translation. We also present\nthe requirements of a real-time Quality-of-Service sign language ma-chine\ntranslation system underpinned by accurate deep learning algorithms. We propose\nfuture research directions for sign language translation systems.\n","authors":["Nada Shahin","Leila Ismail"],"pdf_url":"https://arxiv.org/pdf/2408.14825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14821v1","updated":"2024-08-27T07:03:51Z","published":"2024-08-27T07:03:51Z","title":"Data-driven Effective Modeling of Multiscale Stochastic Dynamical\n  Systems","summary":"  We present a numerical method for learning the dynamics of slow components of\nunknown multiscale stochastic dynamical systems. While the governing equations\nof the systems are unknown, bursts of observation data of the slow variables\nare available. By utilizing the observation data, our proposed method is\ncapable of constructing a generative stochastic model that can accurately\ncapture the effective dynamics of the slow variables in distribution. We\npresent a comprehensive set of numerical examples to demonstrate the\nperformance of the proposed method.\n","authors":["Yuan Chen","Dongbin Xiu"],"pdf_url":"https://arxiv.org/pdf/2408.14821v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.15747"},{"id":"http://arxiv.org/abs/2408.14817v1","updated":"2024-08-27T06:58:52Z","published":"2024-08-27T06:58:52Z","title":"A Comprehensive Benchmark of Machine and Deep Learning Across Diverse\n  Tabular Datasets","summary":"  The analysis of tabular datasets is highly prevalent both in scientific\nresearch and real-world applications of Machine Learning (ML). Unlike many\nother ML tasks, Deep Learning (DL) models often do not outperform traditional\nmethods in this area. Previous comparative benchmarks have shown that DL\nperformance is frequently equivalent or even inferior to models such as\nGradient Boosting Machines (GBMs). In this study, we introduce a comprehensive\nbenchmark aimed at better characterizing the types of datasets where DL models\nexcel. Although several important benchmarks for tabular datasets already\nexist, our contribution lies in the variety and depth of our comparison: we\nevaluate 111 datasets with 20 different models, including both regression and\nclassification tasks. These datasets vary in scale and include both those with\nand without categorical variables. Importantly, our benchmark contains a\nsufficient number of datasets where DL models perform best, allowing for a\nthorough analysis of the conditions under which DL models excel. Building on\nthe results of this benchmark, we train a model that predicts scenarios where\nDL models outperform alternative methods with 86.1% accuracy (AUC 0.78). We\npresent insights derived from this characterization and compare these findings\nto previous benchmarks.\n","authors":["Assaf Shmuel","Oren Glickman","Teddy Lazebnik"],"pdf_url":"https://arxiv.org/pdf/2408.14817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14806v1","updated":"2024-08-27T06:28:35Z","published":"2024-08-27T06:28:35Z","title":"Poly2Vec: Polymorphic Encoding of Geospatial Objects for Spatial\n  Reasoning with Deep Neural Networks","summary":"  Encoding geospatial data is crucial for enabling machine learning (ML) models\nto perform tasks that require spatial reasoning, such as identifying the\ntopological relationships between two different geospatial objects. However,\nexisting encoding methods are limited as they are typically customized to\nhandle only specific types of spatial data, which impedes their applicability\nacross different downstream tasks where multiple data types coexist. To address\nthis, we introduce Poly2Vec, an encoding framework that unifies the modeling of\ndifferent geospatial objects, including 2D points, polylines, and polygons,\nirrespective of the downstream task. We leverage the power of the 2D Fourier\ntransform to encode useful spatial properties, such as shape and location, from\ngeospatial objects into fixed-length vectors. These vectors are then inputted\ninto neural network models for spatial reasoning tasks.This unified approach\neliminates the need to develop and train separate models for each distinct\nspatial type. We evaluate Poly2Vec on both synthetic and real datasets of mixed\ngeometry types and verify its consistent performance across several downstream\nspatial reasoning tasks.\n","authors":["Maria Despoina Siampou","Jialiang Li","John Krumm","Cyrus Shahabi","Hua Lu"],"pdf_url":"https://arxiv.org/pdf/2408.14806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14797v1","updated":"2024-08-27T06:07:18Z","published":"2024-08-27T06:07:18Z","title":"MaskCycleGAN-based Whisper to Normal Speech Conversion","summary":"  Whisper to normal speech conversion is an active area of research. Various\narchitectures based on generative adversarial networks have been proposed in\nthe recent past. Especially, recent study shows that MaskCycleGAN, which is a\nmask guided, and cyclic consistency keeping, generative adversarial network,\nperforms really well for voice conversion from spectrogram representations. In\nthe current work we present a MaskCycleGAN approach for the conversion of\nwhispered speech to normal speech. We find that tuning the mask parameters, and\npre-processing the signal with a voice activity detector provides superior\nperformance when compared to the existing approach. The wTIMIT dataset is used\nfor evaluation. Objective metrics such as PESQ and G-Loss are used to evaluate\nthe converted speech, along with subjective evaluation using mean opinion\nscore. The results show that the proposed approach offers considerable\nbenefits.\n","authors":["K. Rohith Gupta","K. Ramnath","S. Johanan Joysingh","P. Vijayalakshmi","T. Nagarajan"],"pdf_url":"https://arxiv.org/pdf/2408.14797v1.pdf","comment":"submitted to TENCON 2024"},{"id":"http://arxiv.org/abs/2408.14788v1","updated":"2024-08-27T05:28:52Z","published":"2024-08-27T05:28:52Z","title":"Learning from Complementary Features","summary":"  While precise data observation is essential for the learning processes of\npredictive models, it can be challenging owing to factors such as insufficient\nobservation accuracy, high collection costs, and privacy constraints. In this\npaper, we examines cases where some qualitative features are unavailable as\nprecise information indicating \"what it is,\" but rather as complementary\ninformation indicating \"what it is not.\" We refer to features defined by\nprecise information as ordinary features (OFs) and those defined by\ncomplementary information as complementary features (CFs). We then formulate a\nnew learning scenario termed Complementary Feature Learning (CFL), where\npredictive models are constructed using instances consisting of OFs and CFs.\nThe simplest formalization of CFL applies conventional supervised learning\ndirectly using the observed values of CFs. However, this approach does not\nresolve the ambiguity associated with CFs, making learning challenging and\ncomplicating the interpretation of the predictive model's specific predictions.\nTherefore, we derive an objective function from an information-theoretic\nperspective to estimate the OF values corresponding to CFs and to predict\noutput labels based on these estimations. Based on this objective function, we\npropose a theoretically guaranteed graph-based estimation method along with its\npractical approximation, for estimating OF values corresponding to CFs. The\nresults of numerical experiments conducted with real-world data demonstrate\nthat our proposed method effectively estimates OF values corresponding to CFs\nand predicts output labels.\n","authors":["Kosuke Sugiyama","Masato Uchida"],"pdf_url":"https://arxiv.org/pdf/2408.14788v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.02051v2","updated":"2024-08-27T05:26:14Z","published":"2024-02-03T06:01:21Z","title":"Nonlinear subspace clustering by functional link neural networks","summary":"  Nonlinear subspace clustering based on a feed-forward neural network has been\ndemonstrated to provide better clustering accuracy than some advanced subspace\nclustering algorithms. While this approach demonstrates impressive outcomes, it\ninvolves a balance between effectiveness and computational cost. In this study,\nwe employ a functional link neural network to transform data samples into a\nnonlinear domain. Subsequently, we acquire a self-representation matrix through\na learning mechanism that builds upon the mapped samples. As the functional\nlink neural network is a single-layer neural network, our proposed method\nachieves high computational efficiency while ensuring desirable clustering\nperformance. By incorporating the local similarity regularization to enhance\nthe grouping effect, our proposed method further improves the quality of the\nclustering results. Additionally, we introduce a convex combination subspace\nclustering scheme, which combining a linear subspace clustering method with the\nfunctional link neural network subspace clustering approach. This combination\napproach allows for a dynamic balance between linear and nonlinear\nrepresentations. Extensive experiments confirm the advancement of our methods.\nThe source code will be released on https://lshi91.github.io/ soon.\n","authors":["Long Shi","Lei Cao","Zhongpu Chen","Badong Chen","Yu Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.02051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14785v1","updated":"2024-08-27T05:23:45Z","published":"2024-08-27T05:23:45Z","title":"Unsupervised-to-Online Reinforcement Learning","summary":"  Offline-to-online reinforcement learning (RL), a framework that trains a\npolicy with offline RL and then further fine-tunes it with online RL, has been\nconsidered a promising recipe for data-driven decision-making. While sensible,\nthis framework has drawbacks: it requires domain-specific offline RL\npre-training for each task, and is often brittle in practice. In this work, we\npropose unsupervised-to-online RL (U2O RL), which replaces domain-specific\nsupervised offline RL with unsupervised offline RL, as a better alternative to\noffline-to-online RL. U2O RL not only enables reusing a single pre-trained\nmodel for multiple downstream tasks, but also learns better representations,\nwhich often result in even better performance and stability than supervised\noffline-to-online RL. To instantiate U2O RL in practice, we propose a general\nrecipe for U2O RL to bridge task-agnostic unsupervised offline skill-based\npolicy pre-training and supervised online fine-tuning. Throughout our\nexperiments in nine state-based and pixel-based environments, we empirically\ndemonstrate that U2O RL achieves strong performance that matches or even\noutperforms previous offline-to-online RL approaches, while being able to reuse\na single pre-trained model for a number of different downstream tasks.\n","authors":["Junsu Kim","Seohong Park","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2408.14785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14763v2","updated":"2024-08-27T05:09:09Z","published":"2023-12-22T15:28:55Z","title":"Enhanced Latent Multi-view Subspace Clustering","summary":"  Latent multi-view subspace clustering has been demonstrated to have desirable\nclustering performance. However, the original latent representation method\nvertically concatenates the data matrices from multiple views into a single\nmatrix along the direction of dimensionality to recover the latent\nrepresentation matrix, which may result in an incomplete information recovery.\nTo fully recover the latent space representation, we in this paper propose an\nEnhanced Latent Multi-view Subspace Clustering (ELMSC) method. The ELMSC method\ninvolves constructing an augmented data matrix that enhances the representation\nof multi-view data. Specifically, we stack the data matrices from various views\ninto the block-diagonal locations of the augmented matrix to exploit the\ncomplementary information. Meanwhile, the non-block-diagonal entries are\ncomposed based on the similarity between different views to capture the\nconsistent information. In addition, we enforce a sparse regularization for the\nnon-diagonal blocks of the augmented self-representation matrix to avoid\nredundant calculations of consistency information. Finally, a novel iterative\nalgorithm based on the framework of Alternating Direction Method of Multipliers\n(ADMM) is developed to solve the optimization problem for ELMSC. Extensive\nexperiments on real-world datasets demonstrate that our proposed ELMSC is able\nto achieve higher clustering performance than some state-of-art multi-view\nclustering methods.\n","authors":["Long Shi","Lei Cao","Jun Wang","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2312.14763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14780v1","updated":"2024-08-27T04:57:53Z","published":"2024-08-27T04:57:53Z","title":"GINN-KAN: Interpretability pipelining with applications in Physics\n  Informed Neural Networks","summary":"  Neural networks are powerful function approximators, yet their ``black-box\"\nnature often renders them opaque and difficult to interpret. While many\npost-hoc explanation methods exist, they typically fail to capture the\nunderlying reasoning processes of the networks. A truly interpretable neural\nnetwork would be trained similarly to conventional models using techniques such\nas backpropagation, but additionally provide insights into the learned\ninput-output relationships. In this work, we introduce the concept of\ninterpretability pipelineing, to incorporate multiple interpretability\ntechniques to outperform each individual technique. To this end, we first\nevaluate several architectures that promise such interpretability, with a\nparticular focus on two recent models selected for their potential to\nincorporate interpretability into standard neural network architectures while\nstill leveraging backpropagation: the Growing Interpretable Neural Network\n(GINN) and Kolmogorov Arnold Networks (KAN). We analyze the limitations and\nstrengths of each and introduce a novel interpretable neural network GINN-KAN\nthat synthesizes the advantages of both models. When tested on the Feynman\nsymbolic regression benchmark datasets, GINN-KAN outperforms both GINN and KAN.\nTo highlight the capabilities and the generalizability of this approach, we\nposition GINN-KAN as an alternative to conventional black-box networks in\nPhysics-Informed Neural Networks (PINNs). We expect this to have far-reaching\nimplications in the application of deep learning pipelines in the natural\nsciences. Our experiments with this interpretable PINN on 15 different partial\ndifferential equations demonstrate that GINN-KAN augmented PINNs outperform\nPINNs with black-box networks in solving differential equations and surpass the\ncapabilities of both GINN and KAN.\n","authors":["Nisal Ranasinghe","Yu Xia","Sachith Seneviratne","Saman Halgamuge"],"pdf_url":"https://arxiv.org/pdf/2408.14780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14778v1","updated":"2024-08-27T04:56:45Z","published":"2024-08-27T04:56:45Z","title":"GPU-Accelerated Counterfactual Regret Minimization","summary":"  Counterfactual regret minimization (CFR) is a family of algorithms of\nno-regret learning dynamics capable of solving large-scale imperfect\ninformation games. There has been a notable lack of work on making CFR more\ncomputationally efficient. We propose implementing this algorithm as a series\nof dense and sparse matrix and vector operations, thereby making it highly\nparallelizable for a graphical processing unit. Our experiments show that our\nimplementation performs up to about 352.5 times faster than OpenSpiel's Python\nimplementation and up to about 22.2 times faster than OpenSpiel's C++\nimplementation and the speedup becomes more pronounced as the size of the game\nbeing solved grows.\n","authors":["Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2408.14778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14777v1","updated":"2024-08-27T04:56:22Z","published":"2024-08-27T04:56:22Z","title":"Quartered Chirp Spectral Envelope for Whispered vs Normal Speech\n  Classification","summary":"  Whispered speech as an acceptable form of human-computer interaction is\ngaining traction. Systems that address multiple modes of speech require a\nrobust front-end speech classifier. Performance of whispered vs normal speech\nclassification drops in the presence of additive white Gaussian noise, since\nnormal speech takes on some of the characteristics of whispered speech. In this\nwork, we propose a new feature named the quartered chirp spectral envelope, a\ncombination of the chirp spectrum and the quartered spectral envelope, to\nclassify whispered and normal speech. The chirp spectrum can be fine-tuned to\nobtain customized features for a given task, and the quartered spectral\nenvelope has been proven to work especially well for the current task. The\nfeature is trained on a one dimensional convolutional neural network, that\ncaptures the trends in the spectral envelope. The proposed system performs\nbetter than the state of the art, in the presence of white noise.\n","authors":["S. Johanan Joysingh","P. Vijayalakshmi","T. Nagarajan"],"pdf_url":"https://arxiv.org/pdf/2408.14777v1.pdf","comment":"submitted to TENCON 2024"},{"id":"http://arxiv.org/abs/2408.13609v2","updated":"2024-08-27T04:49:46Z","published":"2024-08-24T15:43:02Z","title":"GNN: Graph Neural Network and Large Language Model for Data Discovery","summary":"  Our algorithm GNN: Graph Neural Network and Large Language Model for Data\nDiscovery inherit the benefits of \\cite{hoang2024plod} (PLOD: Predictive\nLearning Optimal Data Discovery), \\cite{Hoang2024BODBO} (BOD: Blindly Optimal\nData Discovery) in terms of overcoming the challenges of having to predefine\nutility function and the human input for attribute ranking, which helps prevent\nthe time-consuming loop process. In addition to these previous works, our\nalgorithm GNN leverages the advantages of graph neural networks and large\nlanguage models to understand text type values that cannot be understood by\nPLOD and MOD, thus making the task of predicting outcomes more reliable. GNN\ncould be seen as an extension of PLOD in terms of understanding the text type\nvalue and the user's preferences, not only numerical values but also text\nvalues, making the promise of data science and analytics purposes.\n","authors":["Thomas Hoang"],"pdf_url":"https://arxiv.org/pdf/2408.13609v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04814v4","updated":"2024-08-27T04:43:10Z","published":"2024-04-07T05:47:41Z","title":"Inference-Time Rule Eraser: Fair Recognition via Distilling and Removing\n  Biased Rules","summary":"  Machine learning models often make predictions based on biased features such\nas gender, race, and other social attributes, posing significant fairness\nrisks, especially in societal applications, such as hiring, banking, and\ncriminal justice. Traditional approaches to addressing this issue involve\nretraining or fine-tuning neural networks with fairness-aware optimization\nobjectives. However, these methods can be impractical due to significant\ncomputational resources, complex industrial tests, and the associated CO2\nfootprint. Additionally, regular users often fail to fine-tune models because\nthey lack access to model parameters In this paper, we introduce the\nInference-Time Rule Eraser (Eraser), a novel method designed to address\nfairness concerns by removing biased decision-making rules from deployed models\nduring inference without altering model weights. We begin by establishing a\ntheoretical foundation for modifying model outputs to eliminate biased rules\nthrough Bayesian analysis. Next, we present a specific implementation of Eraser\nthat involves two stages: (1) distilling the biased rules from the deployed\nmodel into an additional patch model, and (2) removing these biased rules from\nthe output of the deployed model during inference. Extensive experiments\nvalidate the effectiveness of our approach, showcasing its superior performance\nin addressing fairness concerns in AI systems.\n","authors":["Yi Zhang","Dongyuan Lu","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2404.04814v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15773v2","updated":"2024-08-27T04:41:40Z","published":"2024-07-22T16:25:41Z","title":"STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay","summary":"  Test-time adaptation (TTA) aims to address the distribution shift between the\ntraining and test data with only unlabeled data at test time. Existing TTA\nmethods often focus on improving recognition performance specifically for test\ndata associated with classes in the training set. However, during the\nopen-world inference process, there are inevitably test data instances from\nunknown classes, commonly referred to as outliers. This paper pays attention to\nthe problem that conducts both sample recognition and outlier rejection during\ninference while outliers exist. To address this problem, we propose a new\napproach called STAble Memory rePlay (STAMP), which performs optimization over\na stable memory bank instead of the risky mini-batch. In particular, the memory\nbank is dynamically updated by selecting low-entropy and label-consistent\nsamples in a class-balanced manner. In addition, we develop a self-weighted\nentropy minimization strategy that assigns higher weight to low-entropy\nsamples. Extensive results demonstrate that STAMP outperforms existing TTA\nmethods in terms of both recognition and outlier detection performance. The\ncode is released at https://github.com/yuyongcan/STAMP.\n","authors":["Yongcan Yu","Lijun Sheng","Ran He","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2407.15773v2.pdf","comment":"Accepted by ECCV 2024; Fixed a bug in calculating OOD score of STAMP\n  and updated the results"},{"id":"http://arxiv.org/abs/2408.14025v2","updated":"2024-08-27T04:36:52Z","published":"2024-08-26T05:31:46Z","title":"An Item Response Theory-based R Module for Algorithm Portfolio Analysis","summary":"  Experimental evaluation is crucial in AI research, especially for assessing\nalgorithms across diverse tasks. Many studies often evaluate a limited set of\nalgorithms, failing to fully understand their strengths and weaknesses within a\ncomprehensive portfolio. This paper introduces an Item Response Theory (IRT)\nbased analysis tool for algorithm portfolio evaluation called AIRT-Module.\nTraditionally used in educational psychometrics, IRT models test question\ndifficulty and student ability using responses to test questions. Adapting IRT\nto algorithm evaluation, the AIRT-Module contains a Shiny web application and\nthe R package airt. AIRT-Module uses algorithm performance measures to compute\nanomalousness, consistency, and difficulty limits for an algorithm and the\ndifficulty of test instances. The strengths and weaknesses of algorithms are\nvisualised using the difficulty spectrum of the test instances. AIRT-Module\noffers a detailed understanding of algorithm capabilities across varied test\ninstances, thus enhancing comprehensive AI method assessment. It is available\nat https://sevvandi.shinyapps.io/AIRT/ .\n","authors":["Brodie Oldfield","Sevvandi Kandanaarachchi","Ziqi Xu","Mario AndrÃ©s MuÃ±oz"],"pdf_url":"https://arxiv.org/pdf/2408.14025v2.pdf","comment":"10 Pages, 6 Figures. Submitted to SoftwareX"},{"id":"http://arxiv.org/abs/2408.14774v1","updated":"2024-08-27T04:31:58Z","published":"2024-08-27T04:31:58Z","title":"Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning","summary":"  We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.\n","authors":["Simran Kaur","Simon Park","Anirudh Goyal","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2408.14774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14080v2","updated":"2024-08-27T04:14:14Z","published":"2024-08-26T08:02:57Z","title":"SONICS: Synthetic Or Not -- Identifying Counterfeit Songs","summary":"  The recent surge in AI-generated songs presents exciting possibilities and\nchallenges. While these tools democratize music creation, they also necessitate\nthe ability to distinguish between human-composed and AI-generated songs for\nsafeguarding artistic integrity and content curation. Existing research and\ndatasets in fake song detection only focus on singing voice deepfake detection\n(SVDD), where the vocals are AI-generated but the instrumental music is sourced\nfrom real songs. However, this approach is inadequate for contemporary\nend-to-end AI-generated songs where all components (vocals, lyrics, music, and\nstyle) could be AI-generated. Additionally, existing datasets lack lyrics-music\ndiversity, long-duration songs, and open fake songs. To address these gaps, we\nintroduce SONICS, a novel dataset for end-to-end Synthetic Song Detection\n(SSD), comprising over 97k songs with over 49k synthetic songs from popular\nplatforms like Suno and Udio. Furthermore, we highlight the importance of\nmodeling long-range temporal dependencies in songs for effective authenticity\ndetection, an aspect overlooked in existing methods. To capture these patterns,\nwe propose a novel model, SpecTTTra, that is up to 3 times faster and 6 times\nmore memory efficient compared to popular CNN and Transformer-based models\nwhile maintaining competitive performance. Finally, we offer both AI-based and\nHuman evaluation benchmarks, addressing another deficiency in current research.\n","authors":["Md Awsafur Rahman","Zaber Ibn Abdul Hakim","Najibul Haque Sarker","Bishmoy Paul","Shaikh Anowarul Fattah"],"pdf_url":"https://arxiv.org/pdf/2408.14080v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13820v2","updated":"2024-08-27T03:58:09Z","published":"2023-10-20T21:14:07Z","title":"FERI: A Multitask-based Fairness Achieving Algorithm with Applications\n  to Fair Organ Transplantation","summary":"  Liver transplantation often faces fairness challenges across subgroups\ndefined by sensitive attributes such as age group, gender, and race/ethnicity.\nMachine learning models for outcome prediction can introduce additional biases.\nTherefore, we introduce Fairness through the Equitable Rate of Improvement in\nMultitask Learning (FERI) algorithm for fair predictions of graft failure risk\nin liver transplant patients. FERI constrains subgroup loss by balancing\nlearning rates and preventing subgroup dominance in the training process. Our\nresults show that FERI maintained high predictive accuracy with AUROC and AUPRC\ncomparable to baseline models. More importantly, FERI demonstrated an ability\nto improve fairness without sacrificing accuracy. Specifically, for the gender,\nFERI reduced the demographic parity disparity by 71.74%, and for the age group,\nit decreased the equalized odds disparity by 40.46%. Therefore, the FERI\nalgorithm advanced fairness-aware predictive modeling in healthcare and\nprovides an invaluable tool for equitable healthcare systems.\n","authors":["Can Li","Dejian Lai","Xiaoqian Jiang","Kai Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.13820v2.pdf","comment":"First Prize Student Award Paper, American Medical Informatics\n  Association 2024 Informatics Summit"},{"id":"http://arxiv.org/abs/2405.19730v4","updated":"2024-08-27T03:45:18Z","published":"2024-05-30T06:21:34Z","title":"Research on the Spatial Data Intelligent Foundation Model","summary":"  This report focuses on spatial data intelligent large models, delving into\nthe principles, methods, and cutting-edge applications of these models. It\nprovides an in-depth discussion on the definition, development history, current\nstatus, and trends of spatial data intelligent large models, as well as the\nchallenges they face. The report systematically elucidates the key technologies\nof spatial data intelligent large models and their applications in urban\nenvironments, aerospace remote sensing, geography, transportation, and other\nscenarios. Additionally, it summarizes the latest application cases of spatial\ndata intelligent large models in themes such as urban development, multimodal\nsystems, remote sensing, smart transportation, and resource environments.\nFinally, the report concludes with an overview and outlook on the development\nprospects of spatial data intelligent large models.\n","authors":["Shaohua Wang","Xing Xie","Yong Li","Danhuai Guo","Zhi Cai","Yu Liu","Yang Yue","Xiao Pan","Feng Lu","Huayi Wu","Zhipeng Gui","Zhiming Ding","Bolong Zheng","Fuzheng Zhang","Jingyuan Wang","Zhengchao Chen","Hao Lu","Jiayi Li","Peng Yue","Wenhao Yu","Yao Yao","Leilei Sun","Yong Zhang","Longbiao Chen","Xiaoping Du","Xiang Li","Xueying Zhang","Kun Qin","Zhaoya Gong","Weihua Dong","Xiaofeng Meng"],"pdf_url":"https://arxiv.org/pdf/2405.19730v4.pdf","comment":"V1 and V2 are in Chinese language, other versions are in English"},{"id":"http://arxiv.org/abs/2402.10260v2","updated":"2024-08-27T03:32:47Z","published":"2024-02-15T18:58:09Z","title":"A StrongREJECT for Empty Jailbreaks","summary":"  Most jailbreak papers claim the jailbreaks they propose are highly effective,\noften boasting near-100% attack success rates. However, it is perhaps more\ncommon than not for jailbreak developers to substantially exaggerate the\neffectiveness of their jailbreaks. We suggest this problem arises because\njailbreak researchers lack a standard, high-quality benchmark for evaluating\njailbreak performance, leaving researchers to create their own. To create a\nbenchmark, researchers must choose a dataset of forbidden prompts to which a\nvictim model will respond, along with an evaluation method that scores the\nharmfulness of the victim model's responses. We show that existing benchmarks\nsuffer from significant shortcomings and introduce the StrongREJECT benchmark\nto address these issues. StrongREJECT's dataset contains prompts that victim\nmodels must answer with specific, harmful information, while its automated\nevaluator measures the extent to which a response gives useful information to\nforbidden prompts. In doing so, the StrongREJECT evaluator achieves\nstate-of-the-art agreement with human judgments of jailbreak effectiveness.\nNotably, we find that existing evaluation methods significantly overstate\njailbreak effectiveness compared to human judgments and the StrongREJECT\nevaluator. We describe a surprising and novel phenomenon that explains this\ndiscrepancy: jailbreaks bypassing a victim model's safety fine-tuning tend to\nreduce its capabilities. Together, our findings underscore the need for\nresearchers to use a high-quality benchmark, such as StrongREJECT, when\ndeveloping new jailbreak attacks. We release the StrongREJECT code and data at\nhttps://strong-reject.readthedocs.io/en/latest/.\n","authors":["Alexandra Souly","Qingyuan Lu","Dillon Bowen","Tu Trinh","Elvis Hsieh","Sana Pandey","Pieter Abbeel","Justin Svegliato","Scott Emmons","Olivia Watkins","Sam Toyer"],"pdf_url":"https://arxiv.org/pdf/2402.10260v2.pdf","comment":"Code and data at https://strong-reject.readthedocs.io/en/latest/"},{"id":"http://arxiv.org/abs/2408.14763v1","updated":"2024-08-27T03:30:18Z","published":"2024-08-27T03:30:18Z","title":"Channel-wise Influence: Estimating Data Influence for Multivariate Time\n  Series","summary":"  The influence function, a technique from robust statistics, measures the\nimpact on model parameters or related functions when training data is removed\nor modified. This effective and valuable post-hoc method allows for studying\nthe interpretability of machine learning models without requiring costly model\nretraining. It would provide extensions like increasing model performance,\nimproving model generalization, and offering interpretability. Recently,\nMultivariate Time Series (MTS) analysis has become an important yet challenging\ntask, attracting significant attention. However, there is no preceding research\non the influence functions of MTS to shed light on the effects of modifying the\nchannel of training MTS. Given that each channel in an MTS plays a crucial role\nin its analysis, it is essential to characterize the influence of different\nchannels. To fill this gap, we propose a channel-wise influence function, which\nis the first method that can estimate the influence of different channels in\nMTS, utilizing a first-order gradient approximation that leverages the more\ninformative average gradient of the data set. Additionally, we demonstrate how\nthis influence function can be used to estimate the impact of a channel in MTS.\nFinally, we validated the accuracy and effectiveness of our influence\nestimation function in critical MTS analysis tasks, such as MTS anomaly\ndetection and MTS forecasting. According to abundant experiments on real-world\ndataset, the original influence function performs worse than our method and\neven fail for the channel pruning problem, which demonstrate the superiority\nand necessity of channel-wise influence function in MTS analysis tasks.\n","authors":["Muyao Wang","Zeke Xie","Bo Chen"],"pdf_url":"https://arxiv.org/pdf/2408.14763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14762v1","updated":"2024-08-27T03:30:01Z","published":"2024-08-27T03:30:01Z","title":"Explainable Hierarchical Urban Representation Learning for Commuting\n  Flow Prediction","summary":"  Commuting flow prediction is an essential task for municipal operations in\nthe real world. Previous studies have revealed that it is feasible to estimate\nthe commuting origin-destination (OD) demand within a city using multiple\nauxiliary data. However, most existing methods are not suitable to deal with a\nsimilar task at a large scale, namely within a prefecture or the whole nation,\nowing to the increased number of geographical units that need to be maintained.\nIn addition, region representation learning is a universal approach for gaining\nurban knowledge for diverse metropolitan downstream tasks. Although many\nresearchers have developed comprehensive frameworks to describe urban units\nfrom multi-source data, they have not clarified the relationship between the\nselected geographical elements. Furthermore, metropolitan areas naturally\npreserve ranked structures, like cities and their inclusive districts, which\nmakes elucidating relations between cross-level urban units necessary.\nTherefore, we develop a heterogeneous graph-based model to generate meaningful\nregion embeddings at multiple spatial resolutions for predicting different\ntypes of inter-level OD flows. To demonstrate the effectiveness of the proposed\nmethod, extensive experiments were conducted using real-world aggregated mobile\nphone datasets collected from Shizuoka Prefecture, Japan. The results indicate\nthat our proposed model outperforms existing models in terms of a uniform urban\nstructure. We extend the understanding of predicted results using reasonable\nexplanations to enhance the credibility of the model.\n","authors":["Mingfei Cai","Yanbo Pang","Yoshihide Sekimoto"],"pdf_url":"https://arxiv.org/pdf/2408.14762v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.13448v2","updated":"2024-08-27T03:28:50Z","published":"2024-08-24T03:12:21Z","title":"ALIAS: DAG Learning with Efficient Unconstrained Policies","summary":"  Recently, reinforcement learning (RL) has proved a promising alternative for\nconventional local heuristics in score-based approaches to learning directed\nacyclic causal graphs (DAGs) from observational data. However, the intricate\nacyclicity constraint still challenges the efficient exploration of the vast\nspace of DAGs in existing methods. In this study, we introduce ALIAS\n(reinforced dAg Learning wIthout Acyclicity conStraints), a novel approach to\ncausal discovery powered by the RL machinery. Our method features an efficient\npolicy for generating DAGs in just a single step with an optimal quadratic\ncomplexity, fueled by a novel parametrization of DAGs that directly translates\na continuous space to the space of all DAGs, bypassing the need for explicitly\nenforcing acyclicity constraints. This approach enables us to navigate the\nsearch space more effectively by utilizing policy gradient methods and\nestablished scoring functions. In addition, we provide compelling empirical\nevidence for the strong performance of ALIAS in comparison with\nstate-of-the-arts in causal discovery over increasingly difficult experiment\nconditions on both synthetic and real datasets.\n","authors":["Bao Duong","Hung Le","Thin Nguyen"],"pdf_url":"https://arxiv.org/pdf/2408.13448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03865v3","updated":"2024-08-27T03:25:58Z","published":"2023-11-07T10:28:17Z","title":"When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary\n  Classifiers via Membership Inference Attacks","summary":"  Previous studies have developed fairness methods for biased models that\nexhibit discriminatory behaviors towards specific subgroups. While these models\nhave shown promise in achieving fair predictions, recent research has\nidentified their potential vulnerability to score-based membership inference\nattacks (MIAs). In these attacks, adversaries can infer whether a particular\ndata sample was used during training by analyzing the model's prediction\nscores. However, our investigations reveal that these score-based MIAs are\nineffective when targeting fairness-enhanced models in binary classifications.\nThe attack models trained to launch the MIAs degrade into simplistic threshold\nmodels, resulting in lower attack performance. Meanwhile, we observe that\nfairness methods often lead to prediction performance degradation for the\nmajority subgroups of the training data. This raises the barrier to successful\nattacks and widens the prediction gaps between member and non-member data.\nBuilding upon these insights, we propose an efficient MIA method against\nfairness-enhanced models based on fairness discrepancy results (FD-MIA). It\nleverages the difference in the predictions from both the original and\nfairness-enhanced models and exploits the observed prediction gaps as attack\nclues. We also explore potential strategies for mitigating privacy leakages.\nExtensive experiments validate our findings and demonstrate the efficacy of the\nproposed method.\n","authors":["Huan Tian","Guangsheng Zhang","Bo Liu","Tianqing Zhu","Ming Ding","Wanlei Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.03865v3.pdf","comment":"Accepted by IJCAI 2024"},{"id":"http://arxiv.org/abs/2408.14757v1","updated":"2024-08-27T03:17:52Z","published":"2024-08-27T03:17:52Z","title":"Learning effective pruning at initialization from iterative pruning","summary":"  Pruning at initialization (PaI) reduces training costs by removing weights\nbefore training, which becomes increasingly crucial with the growing network\nsize. However, current PaI methods still have a large accuracy gap with\niterative pruning, especially at high sparsity levels. This raises an\nintriguing question: can we get inspiration from iterative pruning to improve\nthe PaI performance? In the lottery ticket hypothesis, the iterative rewind\npruning (IRP) finds subnetworks retroactively by rewinding the parameter to the\noriginal initialization in every pruning iteration, which means all the\nsubnetworks are based on the initial state. Here, we hypothesise the surviving\nsubnetworks are more important and bridge the initial feature and their\nsurviving score as the PaI criterion. We employ an end-to-end neural network\n(\\textbf{AutoS}parse) to learn this correlation, input the model's initial\nfeatures, output their score and then prune the lowest score parameters before\ntraining. To validate the accuracy and generalization of our method, we\nperformed PaI across various models. Results show that our approach outperforms\nexisting methods in high-sparsity settings. Notably, as the underlying logic of\nmodel pruning is consistent in different models, only one-time IRP on one model\nis needed (e.g., once IRP on ResNet-18/CIFAR-10, AutoS can be generalized to\nVGG-16/CIFAR-10, ResNet-18/TinyImageNet, et al.). As the first neural\nnetwork-based PaI method, we conduct extensive experiments to validate the\nfactors influencing this approach. These results reveal the learning tendencies\nof neural networks and provide new insights into our understanding and research\nof PaI from a practical perspective. Our code is available at:\nhttps://github.com/ChengYaofeng/AutoSparse.git.\n","authors":["Shengkai Liu","Yaofeng Cheng","Fusheng Zha","Wei Guo","Lining Sun","Zhenshan Bing","Chenguang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.14757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14756v1","updated":"2024-08-27T03:12:08Z","published":"2024-08-27T03:12:08Z","title":"Training-Free Time-Series Anomaly Detection: Leveraging Image Foundation\n  Models","summary":"  Recent advancements in time-series anomaly detection have relied on deep\nlearning models to handle the diverse behaviors of time-series data. However,\nthese models often suffer from unstable training and require extensive\nhyperparameter tuning, leading to practical limitations. Although foundation\nmodels present a potential solution, their use in time series is limited. To\novercome these issues, we propose an innovative image-based, training-free\ntime-series anomaly detection (ITF-TAD) approach. ITF-TAD converts time-series\ndata into images using wavelet transform and compresses them into a single\nrepresentation, leveraging image foundation models for anomaly detection. This\napproach achieves high-performance anomaly detection without unstable neural\nnetwork training or hyperparameter tuning. Furthermore, ITF-TAD identifies\nanomalies across different frequencies, providing users with a detailed\nvisualization of anomalies and their corresponding frequencies. Comprehensive\nexperiments on five benchmark datasets, including univariate and multivariate\ntime series, demonstrate that ITF-TAD offers a practical and effective solution\nwith performance exceeding or comparable to that of deep models.\n","authors":["Nobuo Namura","Yuma Ichikawa"],"pdf_url":"https://arxiv.org/pdf/2408.14756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14747v1","updated":"2024-08-27T02:52:15Z","published":"2024-08-27T02:52:15Z","title":"Benchmarking Reinforcement Learning Methods for Dexterous Robotic\n  Manipulation with a Three-Fingered Gripper","summary":"  Reinforcement Learning (RL) training is predominantly conducted in\ncost-effective and controlled simulation environments. However, the transfer of\nthese trained models to real-world tasks often presents unavoidable challenges.\nThis research explores the direct training of RL algorithms in controlled yet\nrealistic real-world settings for the execution of dexterous manipulation. The\nbenchmarking results of three RL algorithms trained on intricate in-hand\nmanipulation tasks within practical real-world contexts are presented. Our\nstudy not only demonstrates the practicality of RL training in authentic\nreal-world scenarios, facilitating direct real-world applications, but also\nprovides insights into the associated challenges and considerations.\nAdditionally, our experiences with the employed experimental methods are\nshared, with the aim of empowering and engaging fellow researchers and\npractitioners in this dynamic field of robotics.\n","authors":["Elizabeth Cutler","Yuning Xing","Tony Cui","Brendan Zhou","Koen van Rijnsoever","Ben Hart","David Valencia","Lee Violet C. Ong","Trevor Gee","Minas Liarokapis","Henry Williams"],"pdf_url":"https://arxiv.org/pdf/2408.14747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13713v2","updated":"2024-08-27T02:39:56Z","published":"2024-08-25T03:26:00Z","title":"Verifiable cloud-based variational quantum algorithms","summary":"  Variational quantum algorithms (VQAs) have shown potential for quantum\nadvantage with noisy intermediate-scale quantum (NISQ) devices for quantum\nmachine learning (QML). However, given the high cost and limited availability\nof quantum resources, delegating VQAs via cloud networks is a more practical\nsolution for clients with limited quantum capabilities. Recently, Shingu et\nal.[Physical Review A, 105, 022603 (2022)] proposed a variational secure cloud\nquantum computing protocol, utilizing ancilla-driven quantum computation (ADQC)\nfor cloud-based VQAs with minimal quantum resource consumption. However, their\nprotocol lacks verifiability, which exposes it to potential malicious behaviors\nby the server. Additionally, channel loss requires frequent re-delegation as\nthe size of the delegated variational circuit grows, complicating verification\ndue to increased circuit complexity. This paper introduces a new protocol to\naddress these challenges and enhance both verifiability and tolerance to\nchannel loss in cloud-based VQAs.\n","authors":["Junhong Yang","Banghai Wang","Junyu Quan","Qin Li"],"pdf_url":"https://arxiv.org/pdf/2408.13713v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14156v2","updated":"2024-08-27T02:31:50Z","published":"2024-06-20T09:53:56Z","title":"Tractable Equilibrium Computation in Markov Games through Risk Aversion","summary":"  A significant roadblock to the development of principled multi-agent\nreinforcement learning is the fact that desired solution concepts like Nash\nequilibria may be intractable to compute. To overcome this obstacle, we take\ninspiration from behavioral economics and show that -- by imbuing agents with\nimportant features of human decision-making like risk aversion and bounded\nrationality -- a class of risk-averse quantal response equilibria (RQE) become\ntractable to compute in all $n$-player matrix and finite-horizon Markov games.\nIn particular, we show that they emerge as the endpoint of no-regret learning\nin suitably adjusted versions of the games. Crucially, the class of\ncomputationally tractable RQE is independent of the underlying game structure\nand only depends on agents' degree of risk-aversion and bounded rationality. To\nvalidate the richness of this class of solution concepts we show that it\ncaptures peoples' patterns of play in a number of 2-player matrix games\npreviously studied in experimental economics. Furthermore, we give a first\nanalysis of the sample complexity of computing these equilibria in\nfinite-horizon Markov games when one has access to a generative model and\nvalidate our findings on a simple multi-agent reinforcement learning benchmark.\n","authors":["Eric Mazumdar","Kishan Panaganti","Laixi Shi"],"pdf_url":"https://arxiv.org/pdf/2406.14156v2.pdf","comment":"preprint of multi-agent RL with risk-averse equilibria"},{"id":"http://arxiv.org/abs/2408.14738v1","updated":"2024-08-27T02:29:29Z","published":"2024-08-27T02:29:29Z","title":"Learning Differentially Private Diffusion Models via Stochastic\n  Adversarial Distillation","summary":"  While the success of deep learning relies on large amounts of training\ndatasets, data is often limited in privacy-sensitive domains. To address this\nchallenge, generative model learning with differential privacy has emerged as a\nsolution to train private generative models for desensitized data generation.\nHowever, the quality of the images generated by existing methods is limited due\nto the complexity of modeling data distribution. We build on the success of\ndiffusion models and introduce DP-SAD, which trains a private diffusion model\nby a stochastic adversarial distillation method. Specifically, we first train a\ndiffusion model as a teacher and then train a student by distillation, in which\nwe achieve differential privacy by adding noise to the gradients from other\nmodels to the student. For better generation quality, we introduce a\ndiscriminator to distinguish whether an image is from the teacher or the\nstudent, which forms the adversarial training. Extensive experiments and\nanalysis clearly demonstrate the effectiveness of our proposed method.\n","authors":["Bochao Liu","Pengju Wang","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2408.14738v1.pdf","comment":"accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.14736v1","updated":"2024-08-27T02:28:27Z","published":"2024-08-27T02:28:27Z","title":"Bandwidth-Aware and Overlap-Weighted Compression for\n  Communication-Efficient Federated Learning","summary":"  Current data compression methods, such as sparsification in Federated\nAveraging (FedAvg), effectively enhance the communication efficiency of\nFederated Learning (FL). However, these methods encounter challenges such as\nthe straggler problem and diminished model performance due to heterogeneous\nbandwidth and non-IID (Independently and Identically Distributed) data. To\naddress these issues, we introduce a bandwidth-aware compression framework for\nFL, aimed at improving communication efficiency while mitigating the problems\nassociated with non-IID data. First, our strategy dynamically adjusts\ncompression ratios according to bandwidth, enabling clients to upload their\nmodels at a close pace, thus exploiting the otherwise wasted time to transmit\nmore data. Second, we identify the non-overlapped pattern of retained\nparameters after compression, which results in diminished client update signals\ndue to uniformly averaged weights. Based on this finding, we propose a\nparameter mask to adjust the client-averaging coefficients at the parameter\nlevel, thereby more closely approximating the original updates, and improving\nthe training convergence under heterogeneous environments. Our evaluations\nreveal that our method significantly boosts model accuracy, with a maximum\nimprovement of 13% over the uncompressed FedAvg. Moreover, it achieves a\n$3.37\\times$ speedup in reaching the target accuracy compared to FedAvg with a\nTop-K compressor, demonstrating its effectiveness in accelerating convergence\nwith compression. The integration of common compression techniques into our\nframework further establishes its potential as a versatile foundation for\nfuture cross-device, communication-efficient FL research, addressing critical\nchallenges in FL and advancing the field of distributed machine learning.\n","authors":["Zichen Tang","Junlin Huang","Rudan Yan","Yuxin Wang","Zhenheng Tang","Shaohuai Shi","Amelie Chi Zhou","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2408.14736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05579v2","updated":"2024-08-27T02:23:23Z","published":"2023-12-09T13:53:35Z","title":"Conditional Stochastic Interpolation for Generative Learning","summary":"  We propose a conditional stochastic interpolation (CSI) method for learning\nconditional distributions. CSI is based on estimating probability flow\nequations or stochastic differential equations that transport a reference\ndistribution to the target conditional distribution. This is achieved by first\nlearning the conditional drift and score functions based on CSI, which are then\nused to construct a deterministic process governed by an ordinary differential\nequation or a diffusion process for conditional sampling. In our proposed\napproach, we incorporate an adaptive diffusion term to address the instability\nissues arising in the diffusion process. We derive explicit expressions of the\nconditional drift and score functions in terms of conditional expectations,\nwhich naturally lead to an nonparametric regression approach to estimating\nthese functions. Furthermore, we establish nonasymptotic error bounds for\nlearning the target conditional distribution. We illustrate the application of\nCSI on image generation using a benchmark image dataset.\n","authors":["Ding Huang","Jian Huang","Ting Li","Guohao Shen"],"pdf_url":"https://arxiv.org/pdf/2312.05579v2.pdf","comment":"57 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.13452v2","updated":"2024-08-27T02:19:31Z","published":"2024-08-24T03:43:35Z","title":"Data Augmentation for Continual RL via Adversarial Gradient Episodic\n  Memory","summary":"  Data efficiency of learning, which plays a key role in the Reinforcement\nLearning (RL) training process, becomes even more important in continual RL\nwith sequential environments. In continual RL, the learner interacts with\nnon-stationary, sequential tasks and is required to learn new tasks without\nforgetting previous knowledge. However, there is little work on implementing\ndata augmentation for continual RL. In this paper, we investigate the efficacy\nof data augmentation for continual RL. Specifically, we provide benchmarking\ndata augmentations for continual RL, by (1) summarising existing data\naugmentation methods and (2) including a new augmentation method for continual\nRL: Adversarial Augmentation with Gradient Episodic Memory (Adv-GEM). Extensive\nexperiments show that data augmentations, such as random amplitude scaling,\nstate-switch, mixup, adversarial augmentation, and Adv-GEM, can improve\nexisting continual RL algorithms in terms of their average performance,\ncatastrophic forgetting, and forward transfer, on robot control tasks. All data\naugmentation methods are implemented as plug-in modules for trivial integration\ninto continual RL methods.\n","authors":["Sihao Wu","Xingyu Zhao","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2408.13452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14734v1","updated":"2024-08-27T02:03:22Z","published":"2024-08-27T02:03:22Z","title":"General-Kindred Physics-Informed Neural Network to the Solutions of\n  Singularly Perturbed Differential Equations","summary":"  Physics-Informed Neural Networks (PINNs) have become a promising research\ndirection in the field of solving Partial Differential Equations (PDEs).\nDealing with singular perturbation problems continues to be a difficult\nchallenge in the field of PINN. The solution of singular perturbation problems\noften exhibits sharp boundary layers and steep gradients, and traditional PINN\ncannot achieve approximation of boundary layers. In this manuscript, we propose\nthe General-Kindred Physics-Informed Neural Network (GKPINN) for solving\nSingular Perturbation Differential Equations (SPDEs). This approach utilizes\nasymptotic analysis to acquire prior knowledge of the boundary layer from the\nequation and establishes a novel network to assist PINN in approximating the\nboundary layer. It is compared with traditional PINN by solving examples of\none-dimensional, two-dimensional, and time-varying SPDE equations. The research\nfindings underscore the exceptional performance of our novel approach, GKPINN,\nwhich delivers a remarkable enhancement in reducing the $L_2$ error by two to\nfour orders of magnitude compared to the established PINN methodology. This\nsignificant improvement is accompanied by a substantial acceleration in\nconvergence rates, without compromising the high precision that is critical for\nour applications. Furthermore, GKPINN still performs well in extreme cases with\nperturbation parameters of ${1\\times10}^{-38}$, demonstrating its excellent\ngeneralization ability.\n","authors":["Sen Wang","Peizhi Zhao","Qinglong Ma","Tao Song"],"pdf_url":"https://arxiv.org/pdf/2408.14734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14728v1","updated":"2024-08-27T01:41:21Z","published":"2024-08-27T01:41:21Z","title":"TART: Boosting Clean Accuracy Through Tangent Direction Guided\n  Adversarial Training","summary":"  Adversarial training has been shown to be successful in enhancing the\nrobustness of deep neural networks against adversarial attacks. However, this\nrobustness is accompanied by a significant decline in accuracy on clean data.\nIn this paper, we propose a novel method, called Tangent Direction Guided\nAdversarial Training (TART), that leverages the tangent space of the data\nmanifold to ameliorate the existing adversarial defense algorithms. We argue\nthat training with adversarial examples having large normal components\nsignificantly alters the decision boundary and hurts accuracy. TART mitigates\nthis issue by estimating the tangent direction of adversarial examples and\nallocating an adaptive perturbation limit according to the norm of their\ntangential component. To the best of our knowledge, our paper is the first work\nto consider the concept of tangent space and direction in the context of\nadversarial defense. We validate the effectiveness of TART through extensive\nexperiments on both simulated and benchmark datasets. The results demonstrate\nthat TART consistently boosts clean accuracy while retaining a high level of\nrobustness against adversarial attacks. Our findings suggest that incorporating\nthe geometric properties of data can lead to more effective and efficient\nadversarial training methods.\n","authors":["Bongsoo Yi","Rongjie Lai","Yao Li"],"pdf_url":"https://arxiv.org/pdf/2408.14728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06635v6","updated":"2024-08-27T01:27:29Z","published":"2023-12-11T18:51:59Z","title":"Gated Linear Attention Transformers with Hardware-Efficient Training","summary":"  Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n","authors":["Songlin Yang","Bailin Wang","Yikang Shen","Rameswar Panda","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2312.06635v6.pdf","comment":"minor update"},{"id":"http://arxiv.org/abs/2404.13621v5","updated":"2024-08-27T01:23:50Z","published":"2024-04-21T11:21:27Z","title":"Attack on Scene Flow using Point Clouds","summary":"  Deep neural networks have made significant advancements in accurately\nestimating scene flow using point clouds, which is vital for many applications\nlike video analysis, action recognition, and navigation. The robustness of\nthese techniques, however, remains a concern, particularly in the face of\nadversarial attacks that have been proven to deceive state-of-the-art deep\nneural networks in many domains. Surprisingly, the robustness of scene flow\nnetworks against such attacks has not been thoroughly investigated. To address\nthis problem, the proposed approach aims to bridge this gap by introducing\nadversarial white-box attacks specifically tailored for scene flow networks.\nExperimental results show that the generated adversarial examples obtain up to\n33.7 relative degradation in average end-point error on the KITTI and\nFlyingThings3D datasets. The study also reveals the significant impact that\nattacks targeting point clouds in only one dimension or color channel have on\naverage end-point error. Analyzing the success and failure of these attacks on\nthe scene flow networks and their 2D optical flow network variants shows a\nhigher vulnerability for the optical flow networks. Code is available at\nhttps://github.com/aheldis/Attack-on-Scene-Flow-using-Point-Clouds.git.\n","authors":["Haniyeh Ehsani Oskouie","Mohammad-Shahram Moin","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2404.13621v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16338v2","updated":"2024-08-27T01:17:34Z","published":"2023-09-28T10:51:12Z","title":"Anti-Matthew FL: Bridging the Performance Gap in Federated Learning to\n  Counteract the Matthew Effect","summary":"  Federated learning (FL) stands as a paradigmatic approach that facilitates\nmodel training across heterogeneous and diverse datasets originating from\nvarious data providers. However, conventional FLs fall short of achieving\nconsistent performance, potentially leading to performance degradation for\nclients who are disadvantaged in data resources. Influenced by the Matthew\neffect, deploying a performance-imbalanced global model in applications further\nimpedes the generation of high-quality data from disadvantaged clients,\nexacerbating the disparities in data resources among clients. In this work, we\npropose anti-Matthew fairness for the global model at the client level,\nrequiring equal accuracy and equal decision bias across clients. To balance the\ntrade-off between achieving anti-Matthew fairness and performance optimality,\nwe formalize the anti-Matthew effect federated learning (anti-Matthew FL) as a\nmulti-constrained multi-objectives optimization (MCMOO) problem and propose a\nthree-stage multi-gradient descent algorithm to obtain the Pareto optimality.\nWe theoretically analyze the convergence and time complexity of our proposed\nalgorithms. Additionally, through extensive experimentation, we demonstrate\nthat our proposed anti-Matthew FL outperforms other state-of-the-art FL\nalgorithms in achieving a high-performance global model while effectively\nbridging performance gaps among clients. We hope this work provides valuable\ninsights into the manifestation of the Matthew effect in FL and other\ndecentralized learning scenarios and can contribute to designing fairer\nlearning mechanisms, ultimately fostering societal welfare.\n","authors":["Jiashi Gao","Xin Yao","Xuetao Wei"],"pdf_url":"https://arxiv.org/pdf/2309.16338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14721v1","updated":"2024-08-27T01:04:14Z","published":"2024-08-27T01:04:14Z","title":"PAT: Pruning-Aware Tuning for Large Language Models","summary":"  Large language models (LLMs) excel in language tasks, especially with\nsupervised fine-tuning after pre-training. However, their substantial memory\nand computational requirements hinder practical applications. Structural\npruning, which reduces less significant weight dimensions, is one solution.\nYet, traditional post-hoc pruning often leads to significant performance loss,\nwith limited recovery from further fine-tuning due to reduced capacity. Since\nthe model fine-tuning refines the general and chaotic knowledge in pre-trained\nmodels, we aim to incorporate structural pruning with the fine-tuning, and\npropose the Pruning-Aware Tuning (PAT) paradigm to eliminate model redundancy\nwhile preserving the model performance to the maximum extend. Specifically, we\ninsert the innovative Hybrid Sparsification Modules (HSMs) between the\nAttention and FFN components to accordingly sparsify the upstream and\ndownstream linear modules. The HSM comprises a lightweight operator and a\nglobally shared trainable mask. The lightweight operator maintains a training\noverhead comparable to that of LoRA, while the trainable mask unifies the\nchannels to be sparsified, ensuring structural pruning. Additionally, we\npropose the Identity Loss which decouples the transformation and scaling\nproperties of the HSMs to enhance training robustness. Extensive experiments\ndemonstrate that PAT excels in both performance and efficiency. For example,\nour Llama2-7b model with a 25\\% pruning ratio achieves 1.33$\\times$ speedup\nwhile outperforming the LoRA-finetuned model by up to 1.26\\% in accuracy with a\nsimilar training cost. Code:\nhttps://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning\n","authors":["Yijiang Liu","Huanrui Yang","Youxin Chen","Rongyu Zhang","Miao Wang","Yuan Du","Li Du"],"pdf_url":"https://arxiv.org/pdf/2408.14721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12023v4","updated":"2024-08-27T00:48:35Z","published":"2023-11-20T18:57:41Z","title":"LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient\n  Language Model Finetuning","summary":"  We propose a simple approach for memory-efficient adaptation of pretrained\nlanguage models. Our approach uses an iterative algorithm to decompose each\npretrained matrix into a high-precision low-rank component and a\nmemory-efficient quantized component. During finetuning, the quantized\ncomponent remains fixed and only the low-rank component is updated. We present\nan integer linear programming formulation of the quantization component which\nenables dynamic configuration of quantization parameters (e.g., bit-width,\nblock size) for each matrix given an overall target memory budget. We further\nexplore a data-aware version of the algorithm which uses an approximation of\nthe Fisher information matrix to weight the reconstruction objective during\nmatrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and\n70B) demonstrate that our low-rank plus quantized matrix decomposition approach\n(LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables\naggressive quantization to sub-3 bits with only minor performance degradations.\nWhen finetuned on a language modeling calibration dataset, LQ-LoRA can also be\nused for model compression; in this setting our 2.75-bit LLaMA-2-70B model\n(which has 2.85 bits on average when including the low-rank components and\nrequires 27GB of GPU memory) performs respectably compared to the 16-bit\nbaseline.\n","authors":["Han Guo","Philip Greengard","Eric P. Xing","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2311.12023v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10960v2","updated":"2024-08-27T00:27:12Z","published":"2024-07-15T17:55:42Z","title":"Fast Matrix Multiplications for Lookup Table-Quantized LLMs","summary":"  The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.\n","authors":["Han Guo","William Brandon","Radostin Cholakov","Jonathan Ragan-Kelley","Eric P. Xing","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.10960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15449v1","updated":"2024-08-27T23:58:51Z","published":"2024-08-27T23:58:51Z","title":"Graph Attention Inference of Network Topology in Multi-Agent Systems","summary":"  Accurately identifying the underlying graph structures of multi-agent systems\nremains a difficult challenge. Our work introduces a novel machine\nlearning-based solution that leverages the attention mechanism to predict\nfuture states of multi-agent systems by learning node representations. The\ngraph structure is then inferred from the strength of the attention values.\nThis approach is applied to both linear consensus dynamics and the non-linear\ndynamics of Kuramoto oscillators, resulting in implicit learning the graph by\nlearning good agent representations. Our results demonstrate that the presented\ndata-driven graph attention machine learning model can identify the network\ntopology in multi-agent systems, even when the underlying dynamic model is not\nknown, as evidenced by the F1 scores achieved in the link prediction.\n","authors":["Akshay Kolli","Reza Azadeh","Kshitj Jerath"],"pdf_url":"https://arxiv.org/pdf/2408.15449v1.pdf","comment":"Accepted for publication at Modeling and Estimation Control\n  Conference 2024; 6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.07877v2","updated":"2024-08-27T22:55:03Z","published":"2024-08-15T01:33:06Z","title":"IReCa: Intrinsic Reward-enhanced Context-aware Reinforcement Learning\n  for Human-AI Coordination","summary":"  In human-AI coordination scenarios, human agents usually exhibit asymmetric\nbehaviors that are significantly sparse and unpredictable compared to those of\nAI agents. These characteristics introduce two primary challenges to human-AI\ncoordination: the effectiveness of obtaining sparse rewards and the efficiency\nof training the AI agents. To tackle these challenges, we propose an Intrinsic\nReward-enhanced Context-aware (IReCa) reinforcement learning (RL) algorithm,\nwhich leverages intrinsic rewards to facilitate the acquisition of sparse\nrewards and utilizes environmental context to enhance training efficiency. Our\nIReCa RL algorithm introduces three unique features: (i) it encourages the\nexploration of sparse rewards by incorporating intrinsic rewards that\nsupplement traditional extrinsic rewards from the environment; (ii) it improves\nthe acquisition of sparse rewards by prioritizing the corresponding sparse\nstate-action pairs; and (iii) it enhances the training efficiency by optimizing\nthe exploration and exploitation through innovative context-aware weights of\nextrinsic and intrinsic rewards. Extensive simulations executed in the\nOvercooked layouts demonstrate that our IReCa RL algorithm can increase the\naccumulated rewards by approximately 20% and reduce the epochs required for\nconvergence by approximately 67% compared to state-of-the-art baselines.\n","authors":["Xin Hao","Bahareh Nakisa","Mohmmad Naim Rastgoo","Richard Dazeley"],"pdf_url":"https://arxiv.org/pdf/2408.07877v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15826v2","updated":"2024-08-27T22:18:07Z","published":"2024-03-23T12:53:51Z","title":"Scaling Learning based Policy Optimization for Temporal Logic Tasks by\n  Controller Network Dropout","summary":"  This paper introduces a model-based approach for training feedback\ncontrollers for an autonomous agent operating in a highly nonlinear (albeit\ndeterministic) environment. We desire the trained policy to ensure that the\nagent satisfies specific task objectives and safety constraints, both expressed\nin Discrete-Time Signal Temporal Logic (DT-STL). One advantage for\nreformulation of a task via formal frameworks, like DT-STL, is that it permits\nquantitative satisfaction semantics. In other words, given a trajectory and a\nDT-STL formula, we can compute the {\\em robustness}, which can be interpreted\nas an approximate signed distance between the trajectory and the set of\ntrajectories satisfying the formula. We utilize feedback control, and we assume\na feed forward neural network for learning the feedback controller. We show how\nthis learning problem is similar to training recurrent neural networks (RNNs),\nwhere the number of recurrent units is proportional to the temporal horizon of\nthe agent's task objectives. This poses a challenge: RNNs are susceptible to\nvanishing and exploding gradients, and na\\\"{i}ve gradient descent-based\nstrategies to solve long-horizon task objectives thus suffer from the same\nproblems. To tackle this challenge, we introduce a novel gradient approximation\nalgorithm based on the idea of dropout or gradient sampling. One of the main\ncontributions is the notion of {\\em controller network dropout}, where we\napproximate the NN controller in several time-steps in the task horizon by the\ncontrol input obtained using the controller in a previous training step. We\nshow that our control synthesis methodology, can be quite helpful for\nstochastic gradient descent to converge with less numerical issues, enabling\nscalable backpropagation over long time horizons and trajectories over high\ndimensional state spaces.\n","authors":["Navid Hashemi","Bardh Hoxha","Danil Prokhorov","Georgios Fainekos","Jyotirmoy Deshmukh"],"pdf_url":"https://arxiv.org/pdf/2403.15826v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15421v1","updated":"2024-08-27T21:54:26Z","published":"2024-08-27T21:54:26Z","title":"Simultaneous Training of First- and Second-Order Optimizers in\n  Population-Based Reinforcement Learning","summary":"  The tuning of hyperparameters in reinforcement learning (RL) is critical, as\nthese parameters significantly impact an agent's performance and learning\nefficiency. Dynamic adjustment of hyperparameters during the training process\ncan significantly enhance both the performance and stability of learning.\nPopulation-based training (PBT) provides a method to achieve this by\ncontinuously tuning hyperparameters throughout the training. This ongoing\nadjustment enables models to adapt to different learning stages, resulting in\nfaster convergence and overall improved performance. In this paper, we propose\nan enhancement to PBT by simultaneously utilizing both first- and second-order\noptimizers within a single population. We conducted a series of experiments\nusing the TD3 algorithm across various MuJoCo environments. Our results, for\nthe first time, empirically demonstrate the potential of incorporating\nsecond-order optimizers within PBT-based RL. Specifically, the combination of\nthe K-FAC optimizer with Adam led to up to a 10% improvement in overall\nperformance compared to PBT using only Adam. Additionally, in environments\nwhere Adam occasionally fails, such as the Swimmer environment, the mixed\npopulation with K-FAC exhibited more reliable learning outcomes, offering a\nsignificant advantage in training stability without a substantial increase in\ncomputational time.\n","authors":["Felix Pfeiffer","Shahram Eivazi"],"pdf_url":"https://arxiv.org/pdf/2408.15421v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.15418v1","updated":"2024-08-27T21:47:09Z","published":"2024-08-27T21:47:09Z","title":"Understanding GNNs for Boolean Satisfiability through Approximation\n  Algorithms","summary":"  The paper deals with the interpretability of Graph Neural Networks in the\ncontext of Boolean Satisfiability. The goal is to demystify the internal\nworkings of these models and provide insightful perspectives into their\ndecision-making processes. This is done by uncovering connections to two\napproximation algorithms studied in the domain of Boolean Satisfiability:\nBelief Propagation and Semidefinite Programming Relaxations. Revealing these\nconnections has empowered us to introduce a suite of impactful enhancements.\nThe first significant enhancement is a curriculum training procedure, which\nincrementally increases the problem complexity in the training set, together\nwith increasing the number of message passing iterations of the Graph Neural\nNetwork. We show that the curriculum, together with several other\noptimizations, reduces the training time by more than an order of magnitude\ncompared to the baseline without the curriculum. Furthermore, we apply\ndecimation and sampling of initial embeddings, which significantly increase the\npercentage of solved problems.\n","authors":["Jan HÅ¯la","David MojÅ¾Ã­Å¡ek","MikolÃ¡Å¡ Janota"],"pdf_url":"https://arxiv.org/pdf/2408.15418v1.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2408.15417v1","updated":"2024-08-27T21:46:47Z","published":"2024-08-27T21:46:47Z","title":"Implicit Geometry of Next-token Prediction: From Language Sparsity\n  Patterns to Model Representations","summary":"  Next-token prediction (NTP) over large text corpora has become the go-to\nparadigm to train large language models. Yet, it remains unclear how NTP\ninfluences the mapping of linguistic patterns to geometric properties of the\nresulting model representations. We frame training of large language models as\nsoft-label classification over sparse probabilistic label vectors, coupled with\nan analytical approximation that allows unrestricted generation of context\nembeddings. This approach links NTP training to rank-constrained, nuclear-norm\nregularized optimization in the logit domain, offering a framework for\nanalyzing the geometry of word and context embeddings. In large embedding\nspaces, we find that NTP implicitly favors learning logits with a sparse plus\nlow-rank structure. While the sparse component captures the co-occurrence\nfrequency of context-word pairs, the orthogonal low-rank component, which\nbecomes dominant as training progresses, depends solely on the sparsity pattern\nof the co-occurrence matrix. Consequently, when projected onto an appropriate\nsubspace, representations of contexts that are followed by the same set of\nnext-tokens collapse, a phenomenon we term subspace-collapse. We validate our\nfindings on synthetic and small-scale real language datasets. Finally, we\noutline potential research directions aimed at deepening the understanding of\nNTP's influence on the learning of linguistic patterns and regularities.\n","authors":["Yize Zhao","Tina Behnia","Vala Vakilian","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2408.15417v1.pdf","comment":"Accepted at COLM 2024"},{"id":"http://arxiv.org/abs/2310.07819v3","updated":"2024-08-27T21:37:57Z","published":"2023-10-11T19:00:40Z","title":"Faithfulness Measurable Masked Language Models","summary":"  A common approach to explaining NLP models is to use importance measures that\nexpress which tokens are important for a prediction. Unfortunately, such\nexplanations are often wrong despite being persuasive. Therefore, it is\nessential to measure their faithfulness. One such metric is if tokens are truly\nimportant, then masking them should result in worse model performance. However,\ntoken masking introduces out-of-distribution issues, and existing solutions\nthat address this are computationally expensive and employ proxy models.\nFurthermore, other metrics are very limited in scope. This work proposes an\ninherently faithfulness measurable model that addresses these challenges. This\nis achieved using a novel fine-tuning method that incorporates masking, such\nthat masking tokens become in-distribution by design. This differs from\nexisting approaches, which are completely model-agnostic but are inapplicable\nin practice. We demonstrate the generality of our approach by applying it to 16\ndifferent datasets and validate it using statistical in-distribution tests. The\nfaithfulness is then measured with 9 different importance measures. Because\nmasking is in-distribution, importance measures that themselves use masking\nbecome consistently more faithful. Additionally, because the model makes\nfaithfulness cheap to measure, we can optimize explanations towards maximal\nfaithfulness; thus, our model becomes indirectly inherently explainable.\n","authors":["Andreas Madsen","Siva Reddy","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2310.07819v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11986v2","updated":"2024-08-27T21:25:39Z","published":"2023-07-22T05:34:18Z","title":"Expert Knowledge-Aware Image Difference Graph Representation Learning\n  for Difference-Aware Medical Visual Question Answering","summary":"  To contribute to automating the medical vision-language model, we propose a\nnovel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair\nof main and reference images, this task attempts to answer several questions on\nboth diseases and, more importantly, the differences between them. This is\nconsistent with the radiologist's diagnosis practice that compares the current\nimage with the reference before concluding the report. We collect a new\ndataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs\nof main and reference images. Compared to existing medical VQA datasets, our\nquestions are tailored to the Assessment-Diagnosis-Intervention-Evaluation\ntreatment procedure used by clinical professionals. Meanwhile, we also propose\na novel expert knowledge-aware graph representation learning model to address\nthis task. The proposed baseline model leverages expert knowledge such as\nanatomical structure prior, semantic, and spatial knowledge to construct a\nmulti-relationship graph, representing the image differences between two images\nfor the image difference VQA task. The dataset and code can be found at\nhttps://github.com/Holipori/MIMIC-Diff-VQA. We believe this work would further\npush forward the medical vision language model.\n","authors":["Xinyue Hu","Lin Gu","Qiyuan An","Mengliang Zhang","Liangchen Liu","Kazuma Kobayashi","Tatsuya Harada","Ronald M. Summers","Yingying Zhu"],"pdf_url":"https://arxiv.org/pdf/2307.11986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15408v1","updated":"2024-08-27T21:18:41Z","published":"2024-08-27T21:18:41Z","title":"Divergence-free neural operators for stress field modeling in\n  polycrystalline materials","summary":"  The purpose of the current work is the development and comparison of Fourier\nneural operators (FNOs) for surrogate modeling of the quasi-static mechanical\nresponse of polycrystalline materials. Three types of such FNOs are considered\nhere: a physics-guided FNO (PgFNO), a physics-informed FNO (PiFNO), and a\nphysics-encoded FNO (PeFNO). These are trained and compared with the help of\nstress field data from a reference model for heterogeneous elastic materials\nwith a periodic grain microstructure. Whereas PgFNO training is based solely on\nthese data, that of the PiFNO and PeFNO is in addition constrained by the\nrequirement that stress fields satisfy mechanical equilibrium, i.e., be\ndivergence-free. The difference between the PiFNO and PeFNO lies in how this\nconstraint is taken into account; in the PiFNO, it is included in the loss\nfunction, whereas in the PeFNO, it is \"encoded\" in the operator architecture.\nIn the current work, this encoding is based on a stress potential and Fourier\ntransforms. As a result, only the training of the PiFNO is constrained by\nmechanical equilibrium; in contrast, mechanical equilibrium constrains both the\ntraining and output of the PeFNO. Due in particular to this, stress fields\ncalculated by the trained PeFNO are significantly more accurate than those\ncalculated by the trained PiFNO in the example cases considered.\n","authors":["Mohammad S. Khorrami","Pawan Goyal","Jaber R. Mianroodi","Bob Svendsen","Peter Benner","Dierk Raabe"],"pdf_url":"https://arxiv.org/pdf/2408.15408v1.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.15404v1","updated":"2024-08-27T20:57:26Z","published":"2024-08-27T20:57:26Z","title":"Evaluating Credit VIX (CDS IV) Prediction Methods with Incremental Batch\n  Learning","summary":"  This paper presents the experimental process and results of SVM, Gradient\nBoosting, and an Attention-GRU Hybrid model in predicting the Implied\nVolatility of rolled-over five-year spread contracts of credit default swaps\n(CDS) on European corporate debt during the quarter following mid-May '24, as\nrepresented by the iTraxx/Cboe Europe Main 1-Month Volatility Index (BP\nVolatility). The analysis employs a feature matrix inspired by Merton's\ndeterminants of default probability. Our comparative assessment aims to\nidentify strengths in SOTA and classical machine learning methods for financial\nrisk prediction\n","authors":["Robert Taylor"],"pdf_url":"https://arxiv.org/pdf/2408.15404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11552v5","updated":"2024-08-27T20:56:53Z","published":"2023-02-22T18:48:46Z","title":"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based\n  Diffusion Models and MCMC","summary":"  Since their introduction, diffusion models have quickly become the prevailing\napproach to generative modeling in many domains. They can be interpreted as\nlearning the gradients of a time-varying sequence of log-probability density\nfunctions. This interpretation has motivated classifier-based and\nclassifier-free guidance as methods for post-hoc control of diffusion models.\nIn this work, we build upon these ideas using the score-based interpretation of\ndiffusion models, and explore alternative ways to condition, modify, and reuse\ndiffusion models for tasks involving compositional generation and guidance. In\nparticular, we investigate why certain types of composition fail using current\ntechniques and present a number of solutions. We conclude that the sampler (not\nthe model) is responsible for this failure and propose new samplers, inspired\nby MCMC, which enable successful compositional generation. Further, we propose\nan energy-based parameterization of diffusion models which enables the use of\nnew compositional operators and more sophisticated, Metropolis-corrected\nsamplers. Intriguingly we find these samplers lead to notable improvements in\ncompositional generation across a wide set of problems such as\nclassifier-guided ImageNet modeling and compositional text-to-image generation.\n","authors":["Yilun Du","Conor Durkan","Robin Strudel","Joshua B. Tenenbaum","Sander Dieleman","Rob Fergus","Jascha Sohl-Dickstein","Arnaud Doucet","Will Grathwohl"],"pdf_url":"https://arxiv.org/pdf/2302.11552v5.pdf","comment":"ICML 2023, Project Webpage:\n  https://energy-based-model.github.io/reduce-reuse-recycle/"},{"id":"http://arxiv.org/abs/2408.15400v1","updated":"2024-08-27T20:51:48Z","published":"2024-08-27T20:51:48Z","title":"Exploring the origins of switching dynamics in a multifunctional\n  reservoir computer","summary":"  The concept of multifunctionality has enabled reservoir computers (RCs), a\ntype of dynamical system that is typically realised as an artificial neural\nnetwork, to reconstruct multiple attractors simultaneously using the same set\nof trained weights. However there are many additional phenomena that arise when\ntraining a RC to reconstruct more than one attractor. Previous studies have\nfound that, in certain cases, if the RC fails to reconstruct a coexistence of\nattractors then it exhibits a form of metastability whereby, without any\nexternal input, the state of the RC switches between different modes of\nbehaviour that resemble properties of the attractors it failed to reconstruct.\nIn this paper we explore the origins of these switching dynamics in a\nparadigmatic setting via the `seeing double' problem.\n","authors":["Andrew Flynn","Andreas Amann"],"pdf_url":"https://arxiv.org/pdf/2408.15400v1.pdf","comment":"Preprint submitted to Frontiers in Network Physiology"},{"id":"http://arxiv.org/abs/2408.15399v1","updated":"2024-08-27T20:51:06Z","published":"2024-08-27T20:51:06Z","title":"A Statistical Framework for Data-dependent Retrieval-Augmented Models","summary":"  Modern ML systems increasingly augment input instances with additional\nrelevant information to enhance final prediction. Despite growing interest in\nsuch retrieval-augmented models, their fundamental properties and training are\nnot well understood. We propose a statistical framework to study such models\nwith two components: 1) a {\\em retriever} to identify the relevant information\nout of a large corpus via a data-dependent metric; and 2) a {\\em predictor}\nthat consumes the input instances along with the retrieved information to make\nthe final predictions. We present a principled method for end-to-end training\nof both components and draw connections with various training approaches in the\nliterature. Furthermore, we establish excess risk bounds for\nretrieval-augmented models while delineating the contributions of both\nretriever and predictor towards the model performance. We validate the utility\nof our proposed training methods along with the key takeaways from our\nstatistical analysis on open domain question answering task where retrieval\naugmentation is important.\n","authors":["Soumya Basu","Ankit Singh Rawat","Manzil Zaheer"],"pdf_url":"https://arxiv.org/pdf/2408.15399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15398v1","updated":"2024-08-27T20:49:11Z","published":"2024-08-27T20:49:11Z","title":"Evaluating Pre-Training Bias on Severe Acute Respiratory Syndrome\n  Dataset","summary":"  Machine learning (ML) is a growing field of computer science that has found\nmany practical applications in several domains, including Health. However, as\ndata grows in size and availability, and the number of models that aim to aid\nor replace human decisions, it raises the concern that these models can be\nsusceptible to bias, which can lead to harm to specific individuals by basing\nits decisions on protected attributes such as gender, religion, sexual\norientation, ethnicity, and others. Visualization techniques might generate\ninsights and help summarize large datasets, enabling data scientists to\nunderstand the data better before training a model by evaluating pre-training\nmetrics applied to the datasets before training, which might contribute to\nidentifying potential harm before any effort is put into training and deploying\nthe models. This work uses the severe acute respiratory syndrome dataset from\nOpenDataSUS to visualize three pre-training bias metrics and their distribution\nacross different regions in Brazil. A random forest model is trained in each\nregion and applied to the others. The aim is to compare the bias for the\ndifferent regions, focusing on their protected attributes and comparing the\nmodel's performance with the metric values.\n","authors":["Diego Dimer Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2408.15398v1.pdf","comment":"short paper for eurovis, 5 pages"},{"id":"http://arxiv.org/abs/2408.15395v1","updated":"2024-08-27T20:39:09Z","published":"2024-08-27T20:39:09Z","title":"SCAN-Edge: Finding MobileNet-speed Hybrid Networks for Diverse Edge\n  Devices via Hardware-Aware Evolutionary Search","summary":"  Designing low-latency and high-efficiency hybrid networks for a variety of\nlow-cost commodity edge devices is both costly and tedious, leading to the\nadoption of hardware-aware neural architecture search (NAS) for finding optimal\narchitectures. However, unifying NAS for a wide range of edge devices presents\nchallenges due to the variety of hardware designs, supported operations, and\ncompilation optimizations. Existing methods often fix the search space of\narchitecture choices (e.g., activation, convolution, or self-attention) and\nestimate latency using hardware-agnostic proxies (e.g., FLOPs), which fail to\nachieve proclaimed latency across various edge devices. To address this issue,\nwe propose SCAN-Edge, a unified NAS framework that jointly searches for\nself-attention, convolution, and activation to accommodate the wide variety of\nedge devices, including CPU-, GPU-, and hardware accelerator-based systems. To\nhandle the large search space, SCAN-Edge relies on with a hardware-aware\nevolutionary algorithm that improves the quality of the search space to\naccelerate the sampling process. Experiments on large-scale datasets\ndemonstrate that our hybrid networks match the actual MobileNetV2 latency for\n224x224 input resolution on various commodity edge devices.\n","authors":["Hung-Yueh Chiang","Diana Marculescu"],"pdf_url":"https://arxiv.org/pdf/2408.15395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15393v1","updated":"2024-08-27T20:33:16Z","published":"2024-08-27T20:33:16Z","title":"Stability Analysis of Physics-Informed Neural Networks for Stiff Linear\n  Differential Equations","summary":"  We present a stability analysis of Physics-Informed Neural Networks (PINNs)\ncoupled with random projections, for the numerical solution of (stiff) linear\ndifferential equations. For our analysis, we consider systems of linear ODEs,\nand linear parabolic PDEs. We prove that properly designed PINNs offer\nconsistent and asymptotically stable numerical schemes, thus convergent\nschemes. In particular, we prove that multi-collocation random projection PINNs\nguarantee asymptotic stability for very high stiffness and that\nsingle-collocation PINNs are $A$-stable. To assess the performance of the PINNs\nin terms of both numerical approximation accuracy and computational cost, we\ncompare it with other implicit schemes and in particular backward Euler, the\nmidpoint, trapezoidal (Crank-Nikolson), the 2-stage Gauss scheme and the 2 and\n3 stages Radau schemes. We show that the proposed PINNs outperform the above\ntraditional schemes, in both numerical approximation accuracy and importantly\ncomputational cost, for a wide range of step sizes.\n","authors":["Gianluca Fabiani","Erik Bollt","Constantinos Siettos","Athanasios N. Yannacopoulos"],"pdf_url":"https://arxiv.org/pdf/2408.15393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15388v1","updated":"2024-08-27T20:14:42Z","published":"2024-08-27T20:14:42Z","title":"Panoptic Perception for Autonomous Driving: A Survey","summary":"  Panoptic perception represents a forefront advancement in autonomous driving\ntechnology, unifying multiple perception tasks into a singular, cohesive\nframework to facilitate a thorough understanding of the vehicle's surroundings.\nThis survey reviews typical panoptic perception models for their unique inputs\nand architectures and compares them to performance, responsiveness, and\nresource utilization. It also delves into the prevailing challenges faced in\npanoptic perception and explores potential trajectories for future research.\nOur goal is to furnish researchers in autonomous driving with a detailed\nsynopsis of panoptic perception, positioning this survey as a pivotal reference\nin the ever-evolving landscape of autonomous driving technologies.\n","authors":["Yunge Li","Lanyu Xu"],"pdf_url":"https://arxiv.org/pdf/2408.15388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01959v2","updated":"2024-08-27T19:57:45Z","published":"2024-08-04T08:26:58Z","title":"Dataset Scale and Societal Consistency Mediate Facial Impression Bias in\n  Vision-Language AI","summary":"  Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting.\n","authors":["Robert Wolfe","Aayushi Dangol","Alexis Hiniker","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.01959v2.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2401.03717v3","updated":"2024-08-27T19:45:07Z","published":"2024-01-08T08:00:04Z","title":"Universal Time-Series Representation Learning: A Survey","summary":"  Time-series data exists in every corner of real-world systems and services,\nranging from satellites in the sky to wearable devices on human bodies.\nLearning representations by extracting and inferring valuable information from\nthese time series is crucial for understanding the complex dynamics of\nparticular phenomena and enabling informed decisions. With the learned\nrepresentations, we can perform numerous downstream analyses more effectively.\nAmong several approaches, deep learning has demonstrated remarkable performance\nin extracting hidden patterns and features from time-series data without manual\nfeature engineering. This survey first presents a novel taxonomy based on three\nfundamental elements in designing state-of-the-art universal representation\nlearning methods for time series. According to the proposed taxonomy, we\ncomprehensively review existing studies and discuss their intuitions and\ninsights into how these methods enhance the quality of learned representations.\nFinally, as a guideline for future studies, we summarize commonly used\nexperimental setups and datasets and discuss several promising research\ndirections. An up-to-date corresponding resource is available at\nhttps://github.com/itouchz/awesome-deep-time-series-representations.\n","authors":["Patara Trirat","Yooju Shin","Junhyeok Kang","Youngeun Nam","Jihye Na","Minyoung Bae","Joeun Kim","Byunghyun Kim","Jae-Gil Lee"],"pdf_url":"https://arxiv.org/pdf/2401.03717v3.pdf","comment":"41 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.13683v2","updated":"2024-08-27T19:27:07Z","published":"2024-08-24T22:40:31Z","title":"Submodular Maximization Approaches for Equitable Client Selection in\n  Federated Learning","summary":"  In a conventional Federated Learning framework, client selection for training\ntypically involves the random sampling of a subset of clients in each\niteration. However, this random selection often leads to disparate performance\namong clients, raising concerns regarding fairness, particularly in\napplications where equitable outcomes are crucial, such as in medical or\nfinancial machine learning tasks. This disparity typically becomes more\npronounced with the advent of performance-centric client sampling techniques.\nThis paper introduces two novel methods, namely SUBTRUNC and UNIONFL, designed\nto address the limitations of random client selection. Both approaches utilize\nsubmodular function maximization to achieve more balanced models. By modifying\nthe facility location problem, they aim to mitigate the fairness concerns\nassociated with random selection. SUBTRUNC leverages client loss information to\ndiversify solutions, while UNIONFL relies on historical client selection data\nto ensure a more equitable performance of the final model. Moreover, these\nalgorithms are accompanied by robust theoretical guarantees regarding\nconvergence under reasonable assumptions. The efficacy of these methods is\ndemonstrated through extensive evaluations across heterogeneous scenarios,\nrevealing significant improvements in fairness as measured by a client\ndissimilarity metric.\n","authors":["AndrÃ©s Catalino Castillo JimÃ©nez","Ege C. Kaya","Lintao Ye","Abolfazl Hashemi"],"pdf_url":"https://arxiv.org/pdf/2408.13683v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2408.15374v1","updated":"2024-08-27T19:22:06Z","published":"2024-08-27T19:22:06Z","title":"CycleGAN with Better Cycles","summary":"  CycleGAN provides a framework to train image-to-image translation with\nunpaired datasets using cycle consistency loss [4]. While results are great in\nmany applications, the pixel level cycle consistency can potentially be\nproblematic and causes unrealistic images in certain cases. In this project, we\npropose three simple modifications to cycle consistency, and show that such an\napproach achieves better results with fewer artifacts.\n","authors":["Tongzhou Wang","Yihan Lin"],"pdf_url":"https://arxiv.org/pdf/2408.15374v1.pdf","comment":"Technical Report 2018"},{"id":"http://arxiv.org/abs/2408.15373v1","updated":"2024-08-27T19:13:15Z","published":"2024-08-27T19:13:15Z","title":"Handling Geometric Domain Shifts in Semantic Segmentation of Surgical\n  RGB and Hyperspectral Images","summary":"  Robust semantic segmentation of intraoperative image data holds promise for\nenabling automatic surgical scene understanding and autonomous robotic surgery.\nWhile model development and validation are primarily conducted on idealistic\nscenes, geometric domain shifts, such as occlusions of the situs, are common in\nreal-world open surgeries. To close this gap, we (1) present the first analysis\nof state-of-the-art (SOA) semantic segmentation models when faced with\ngeometric out-of-distribution (OOD) data, and (2) propose an augmentation\ntechnique called \"Organ Transplantation\", to enhance generalizability. Our\ncomprehensive validation on six different OOD datasets, comprising 600 RGB and\nhyperspectral imaging (HSI) cubes from 33 pigs, each annotated with 19 classes,\nreveals a large performance drop in SOA organ segmentation models on geometric\nOOD data. This performance decline is observed not only in conventional RGB\ndata (with a dice similarity coefficient (DSC) drop of 46 %) but also in HSI\ndata (with a DSC drop of 45 %), despite the richer spectral information\ncontent. The performance decline increases with the spatial granularity of the\ninput data. Our augmentation technique improves SOA model performance by up to\n67 % for RGB data and 90 % for HSI data, achieving performance at the level of\nin-distribution performance on real OOD test data. Given the simplicity and\neffectiveness of our augmentation method, it is a valuable tool for addressing\ngeometric domain shifts in surgical scene segmentation, regardless of the\nunderlying model. Our code and pre-trained models are publicly available at\nhttps://github.com/IMSY-DKFZ/htc.\n","authors":["Silvia Seidlitz","Jan Sellner","Alexander Studier-Fischer","Alessandro Motta","Berkin Ãzdemir","Beat P. MÃ¼ller-Stich","Felix Nickel","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2408.15373v1.pdf","comment":"Silvia Seidlitz and Jan Sellner contributed equally"},{"id":"http://arxiv.org/abs/2408.15371v1","updated":"2024-08-27T19:10:21Z","published":"2024-08-27T19:10:21Z","title":"Temporal Graph Neural Network-Powered Paper Recommendation on Dynamic\n  Citation Networks","summary":"  Due to the rapid growth of scientific publications, identifying all related\nreference articles in the literature has become increasingly challenging yet\nhighly demanding. Existing methods primarily assess candidate publications from\na static perspective, focusing on the content of articles and their structural\ninformation, such as citation relationships. There is a lack of research\nregarding how to account for the evolving impact among papers on their\nembeddings. Toward this goal, this paper introduces a temporal dimension to\npaper recommendation strategies. The core idea is to continuously update a\npaper's embedding when new citation relationships appear, enhancing its\nrelevance for future recommendations. Whenever a citation relationship is added\nto the literature upon the publication of a paper, the embeddings of the two\nrelated papers are updated through a Temporal Graph Neural Network (TGN). A\nlearnable memory update module based on a Recurrent Neural Network (RNN) is\nutilized to study the evolution of the embedding of a paper in order to predict\nits reference impact in a future timestamp. Such a TGN-based model learns a\npattern of how people's views of the paper may evolve, aiming to guide paper\nrecommendations more precisely. Extensive experiments on an open citation\nnetwork dataset, including 313,278 articles from\nhttps://paperswithcode.com/about PaperWithCode, have demonstrated the\neffectiveness of the proposed approach.\n","authors":["Junhao Shen","Mohammad Ausaf Ali Haqqani","Beichen Hu","Cheng Huang","Xihao Xie","Tsengdar Lee","Jia Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15371v1.pdf","comment":"10 pages, 4 figures, accepted by SDU@AAAI-2024. The AAAI Workshop on\n  Scientific Document Understanding (2024)"},{"id":"http://arxiv.org/abs/2408.13912v2","updated":"2024-08-27T19:06:57Z","published":"2024-08-25T18:27:20Z","title":"Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs","summary":"  In this paper, we introduce Splatt3R, a pose-free, feed-forward method for\nin-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given\nuncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without\nrequiring any camera parameters or depth information. For generalizability, we\nbuild Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R,\nby extending it to deal with both 3D structure and appearance. Specifically,\nunlike the original MASt3R which reconstructs only 3D point clouds, we predict\nthe additional Gaussian attributes required to construct a Gaussian primitive\nfor each point. Hence, unlike other novel view synthesis methods, Splatt3R is\nfirst trained by optimizing the 3D point cloud's geometry loss, and then a\nnovel view synthesis objective. By doing this, we avoid the local minima\npresent in training 3D Gaussian Splats from stereo views. We also propose a\nnovel loss masking strategy that we empirically find is critical for strong\nperformance on extrapolated viewpoints. We train Splatt3R on the ScanNet++\ndataset and demonstrate excellent generalisation to uncalibrated, in-the-wild\nimages. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and\nthe resultant splats can be rendered in real-time.\n","authors":["Brandon Smart","Chuanxia Zheng","Iro Laina","Victor Adrian Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2408.13912v2.pdf","comment":"Our project page can be found at: https://splatt3r.active.vision/"},{"id":"http://arxiv.org/abs/2408.15368v1","updated":"2024-08-27T19:04:32Z","published":"2024-08-27T19:04:32Z","title":"Optimization Solution Functions as Deterministic Policies for Offline\n  Reinforcement Learning","summary":"  Offline reinforcement learning (RL) is a promising approach for many control\napplications but faces challenges such as limited data coverage and value\nfunction overestimation. In this paper, we propose an implicit actor-critic\n(iAC) framework that employs optimization solution functions as a deterministic\npolicy (actor) and a monotone function over the optimal value of optimization\nas a critic. By encoding optimality in the actor policy, we show that the\nlearned policies are robust to the suboptimality of the learned actor\nparameters via the exponentially decaying sensitivity (EDS) property. We obtain\nperformance guarantees for the proposed iAC framework and show its benefits\nover general function approximation schemes. Finally, we validate the proposed\nframework on two real-world applications and show a significant improvement\nover state-of-the-art (SOTA) offline RL methods.\n","authors":["Vanshaj Khattar","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2408.15368v1.pdf","comment":"American Control Conference 2024"},{"id":"http://arxiv.org/abs/2301.06267v5","updated":"2024-08-27T19:00:47Z","published":"2023-01-16T05:40:42Z","title":"Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with\n  Multimodal Models","summary":"  The ability to quickly learn a new task with minimal instruction - known as\nfew-shot learning - is a central aspect of intelligent agents. Classical\nfew-shot benchmarks make use of few-shot samples from a single modality, but\nsuch samples may not be sufficient to characterize an entire concept class. In\ncontrast, humans use cross-modal information to learn new concepts efficiently.\nIn this work, we demonstrate that one can indeed build a better ${\\bf visual}$\ndog classifier by ${\\bf read}$ing about dogs and ${\\bf listen}$ing to them\nbark. To do so, we exploit the fact that recent multimodal foundation models\nsuch as CLIP learn cross-modal encoders that map different modalities to the\nsame representation space. Specifically, we propose a simple strategy for ${\\bf\ncross-modal}$ ${\\bf adaptation}$: we treat examples from different modalities\nas additional few-shot examples. For example, by simply repurposing class names\nas an additional training sample, we trivially turn any n-shot learning problem\ninto a (n+1)-shot problem. This allows us to produce SOTA results with\nembarrassingly simple linear classifiers. We show that our approach can be\ncombined with existing methods such as prefix tuning, adapters, and classifier\nensembling. Finally, to explore other modalities beyond vision and language, we\nconstruct the first (to our knowledge) audiovisual few-shot benchmark and use\ncross-modal training to improve the performance of both image and audio\nclassification.\n","authors":["Zhiqiu Lin","Samuel Yu","Zhiyi Kuang","Deepak Pathak","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2301.06267v5.pdf","comment":"Published at CVPR 2023. Project site:\n  https://linzhiqiu.github.io/papers/cross_modal/"},{"id":"http://arxiv.org/abs/2403.13724v2","updated":"2024-08-27T18:42:55Z","published":"2024-03-20T16:33:06Z","title":"Probabilistic Forecasting with Stochastic Interpolants and FÃ¶llmer\n  Processes","summary":"  We propose a framework for probabilistic forecasting of dynamical systems\nbased on generative modeling. Given observations of the system state over time,\nwe formulate the forecasting problem as sampling from the conditional\ndistribution of the future system state given its current state. To this end,\nwe leverage the framework of stochastic interpolants, which facilitates the\nconstruction of a generative model between an arbitrary base distribution and\nthe target. We design a fictitious, non-physical stochastic dynamics that takes\nas initial condition the current system state and produces as output a sample\nfrom the target conditional distribution in finite time and without bias. This\nprocess therefore maps a point mass centered at the current state onto a\nprobabilistic ensemble of forecasts. We prove that the drift coefficient\nentering the stochastic differential equation (SDE) achieving this task is\nnon-singular, and that it can be learned efficiently by square loss regression\nover the time-series data. We show that the drift and the diffusion\ncoefficients of this SDE can be adjusted after training, and that a specific\nchoice that minimizes the impact of the estimation error gives a F\\\"ollmer\nprocess. We highlight the utility of our approach on several complex,\nhigh-dimensional forecasting problems, including stochastically forced\nNavier-Stokes and video prediction on the KTH and CLEVRER datasets.\n","authors":["Yifan Chen","Mark Goldstein","Mengjian Hua","Michael S. Albergo","Nicholas M. Boffi","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2403.13724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12091v2","updated":"2024-08-27T18:34:24Z","published":"2024-08-22T03:00:21Z","title":"Unsupervised discovery of the shared and private geometry in multi-view\n  data","summary":"  Modern applications often leverage multiple views of a subject of study.\nWithin neuroscience, there is growing interest in large-scale simultaneous\nrecordings across multiple brain regions. Understanding the relationship\nbetween views (e.g., the neural activity in each region recorded) can reveal\nfundamental principles about the characteristics of each representation and\nabout the system. However, existing methods to characterize such relationships\neither lack the expressivity required to capture complex nonlinearities,\ndescribe only sources of variance that are shared between views, or discard\ngeometric information that is crucial to interpreting the data. Here, we\ndevelop a nonlinear neural network-based method that, given paired samples of\nhigh-dimensional views, disentangles low-dimensional shared and private latent\nvariables underlying these views while preserving intrinsic data geometry.\nAcross multiple simulated and real datasets, we demonstrate that our method\noutperforms competing methods. Using simulated populations of lateral\ngeniculate nucleus (LGN) and V1 neurons we demonstrate our model's ability to\ndiscover interpretable shared and private structure across different noise\nconditions. On a dataset of unrotated and corresponding but randomly rotated\nMNIST digits, we recover private latents for the rotated view that encode\nrotation angle regardless of digit class, and places the angle representation\non a 1-d manifold, while shared latents encode digit class but not rotation\nangle. Applying our method to simultaneous Neuropixels recordings of\nhippocampus and prefrontal cortex while mice run on a linear track, we discover\na low-dimensional shared latent space that encodes the animal's position. We\npropose our approach as a general-purpose method for finding succinct and\ninterpretable descriptions of paired data sets in terms of disentangled shared\nand private latent variables.\n","authors":["Sai Koukuntla","Joshua B. Julian","Jesse C. Kaminsky","Manuel Schottdorf","David W. Tank","Carlos D. Brody","Adam S. Charles"],"pdf_url":"https://arxiv.org/pdf/2408.12091v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15357v1","updated":"2024-08-27T18:29:47Z","published":"2024-08-27T18:29:47Z","title":"On the effectiveness of smartphone IMU sensors and Deep Learning in the\n  detection of cardiorespiratory conditions","summary":"  This research introduces an innovative method for the early screening of\ncardiorespiratory diseases based on an acquisition protocol, which leverages\ncommodity smartphone's Inertial Measurement Units (IMUs) and deep learning\ntechniques. We collected, in a clinical setting, a dataset featuring recordings\nof breathing kinematics obtained by accelerometer and gyroscope readings from\nfive distinct body regions. We propose an end-to-end deep learning pipeline for\nearly cardiorespiratory disease screening, incorporating a preprocessing step\nsegmenting the data into individual breathing cycles, and a recurrent\nbidirectional module capturing features from diverse body regions. We employed\nLeave-one-out-cross-validation with Bayesian optimization for hyperparameter\ntuning and model selection. The experimental results consistently demonstrated\nthe superior performance of a bidirectional Long-Short Term Memory (Bi-LSTM) as\na feature encoder architecture, yielding an average sensitivity of $0.81 \\pm\n0.02$, specificity of $0.82 \\pm 0.05$, F1 score of $0.81 \\pm 0.02$, and\naccuracy of $80.2\\% \\pm 3.9$ across diverse seed variations. We also assessed\ngeneralization capabilities on a skewed distribution, comprising exclusively\nhealthy patients not used in training, revealing a true negative rate of $74.8\n\\% \\pm 4.5$. The sustained accuracy of predictions over time during breathing\ncycles within a single patient underscores the efficacy of the preprocessing\nstrategy, highlighting the model's ability to discern significant patterns\nthroughout distinct phases of the respiratory cycle. This investigation\nunderscores the potential usefulness of widely available smartphones as devices\nfor timely cardiorespiratory disease screening in the general population, in\nat-home settings, offering crucial assistance to public health efforts\n(especially during a pandemic outbreaks, such as the recent COVID-19).\n","authors":["Lorenzo Simone","Luca Miglior","Vincenzo Gervasi","Luca Moroni","Emanuele Vignali","Emanuele Gasparotti","Simona Celi"],"pdf_url":"https://arxiv.org/pdf/2408.15357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15356v1","updated":"2024-08-27T18:28:31Z","published":"2024-08-27T18:28:31Z","title":"Optimal level set estimation for non-parametric tournament and\n  crowdsourcing problems","summary":"  Motivated by crowdsourcing, we consider a problem where we partially observe\nthe correctness of the answers of $n$ experts on $d$ questions. In this paper,\nwe assume that both the experts and the questions can be ordered, namely that\nthe matrix $M$ containing the probability that expert $i$ answers correctly to\nquestion $j$ is bi-isotonic up to a permutation of it rows and columns. When\n$n=d$, this also encompasses the strongly stochastic transitive (SST) model\nfrom the tournament literature. Here, we focus on the relevant problem of\ndeciphering small entries of $M$ from large entries of $M$, which is key in\ncrowdsourcing for efficient allocation of workers to questions. More precisely,\nwe aim at recovering a (or several) level set $p$ of the matrix up to a\nprecision $h$, namely recovering resp. the sets of positions $(i,j)$ in $M$\nsuch that $M_{ij}>p+h$ and $M_{i,j}<p-h$. We consider, as a loss measure, the\nnumber of misclassified entries. As our main result, we construct an efficient\npolynomial-time algorithm that turns out to be minimax optimal for this\nclassification problem. This heavily contrasts with existing literature in the\nSST model where, for the stronger reconstruction loss,\nstatistical-computational gaps have been conjectured. More generally, this\nshades light on the nature of statistical-computational gaps for permutations\nmodels.\n","authors":["Maximilian Graf","Alexandra Carpentier","Nicolas Verzelen"],"pdf_url":"https://arxiv.org/pdf/2408.15356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15355v1","updated":"2024-08-27T18:27:47Z","published":"2024-08-27T18:27:47Z","title":"Optimizing Lung Cancer Detection in CT Imaging: A Wavelet Multi-Layer\n  Perceptron (WMLP) Approach Enhanced by Dragonfly Algorithm (DA)","summary":"  Lung cancer stands as the preeminent cause of cancer-related mortality\nglobally. Prompt and precise diagnosis, coupled with effective treatment, is\nimperative to reduce the fatality rates associated with this formidable\ndisease. This study introduces a cutting-edge deep learning framework for the\nclassification of lung cancer from CT scan imagery. The research encompasses a\nsuite of image pre-processing strategies, notably Canny edge detection, and\nwavelet transformations, which precede the extraction of salient features and\nsubsequent classification via a Multi-Layer Perceptron (MLP). The optimization\nprocess is further refined using the Dragonfly Algorithm (DA). The methodology\nput forth has attained an impressive training and testing accuracy of 99.82\\%,\nunderscoring its efficacy and reliability in the accurate diagnosis of lung\ncancer.\n","authors":["Bitasadat Jamshidi","Nastaran Ghorbani","Mohsen Rostamy-Malkhalifeh"],"pdf_url":"https://arxiv.org/pdf/2408.15355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15897v3","updated":"2024-08-27T18:23:44Z","published":"2024-01-29T05:46:14Z","title":"Red-Teaming for Generative AI: Silver Bullet or Security Theater?","summary":"  In response to rising concerns surrounding the safety, security, and\ntrustworthiness of Generative AI (GenAI) models, practitioners and regulators\nalike have pointed to AI red-teaming as a key component of their strategies for\nidentifying and mitigating these risks. However, despite AI red-teaming's\ncentral role in policy discussions and corporate messaging, significant\nquestions remain about what precisely it means, what role it can play in\nregulation, and how it relates to conventional red-teaming practices as\noriginally conceived in the field of cybersecurity. In this work, we identify\nrecent cases of red-teaming activities in the AI industry and conduct an\nextensive survey of relevant research literature to characterize the scope,\nstructure, and criteria for AI red-teaming practices. Our analysis reveals that\nprior methods and practices of AI red-teaming diverge along several axes,\nincluding the purpose of the activity (which is often vague), the artifact\nunder evaluation, the setting in which the activity is conducted (e.g., actors,\nresources, and methods), and the resulting decisions it informs (e.g.,\nreporting, disclosure, and mitigation). In light of our findings, we argue that\nwhile red-teaming may be a valuable big-tent idea for characterizing GenAI harm\nmitigations, and that industry may effectively apply red-teaming and other\nstrategies behind closed doors to safeguard AI, gestures towards red-teaming\n(based on public definitions) as a panacea for every possible risk verge on\nsecurity theater. To move toward a more robust toolbox of evaluations for\ngenerative AI, we synthesize our recommendations into a question bank meant to\nguide and scaffold future AI red-teaming practices.\n","authors":["Michael Feffer","Anusha Sinha","Wesley Hanwen Deng","Zachary C. Lipton","Hoda Heidari"],"pdf_url":"https://arxiv.org/pdf/2401.15897v3.pdf","comment":"AIES 2024"},{"id":"http://arxiv.org/abs/2408.15344v1","updated":"2024-08-27T18:06:45Z","published":"2024-08-27T18:06:45Z","title":"Conformal Disentanglement: A Neural Framework for Perspective Synthesis\n  and Differentiation","summary":"  For multiple scientific endeavors it is common to measure a phenomenon of\ninterest in more than one ways. We make observations of objects from several\ndifferent perspectives in space, at different points in time; we may also\nmeasure different properties of a mixture using different types of instruments.\nAfter collecting this heterogeneous information, it is necessary to be able to\nsynthesize a complete picture of what is `common' across its sources: the\nsubject we ultimately want to study. However, isolated (`clean') observations\nof a system are not always possible: observations often contain information\nabout other systems in its environment, or about the measuring instruments\nthemselves. In that sense, each observation may contain information that `does\nnot matter' to the original object of study; this `uncommon' information\nbetween sensors observing the same object may still be important, and\ndecoupling it from the main signal(s) useful. We introduce a neural network\nautoencoder framework capable of both tasks: it is structured to identify\n`common' variables, and, making use of orthogonality constraints to define\ngeometric independence, to also identify disentangled `uncommon' information\noriginating from the heterogeneous sensors. We demonstrate applications in\nseveral computational examples.\n","authors":["George A. Kevrekidis","Eleni D. Koronaki","Yannis G. Kevrekidis"],"pdf_url":"https://arxiv.org/pdf/2408.15344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15339v1","updated":"2024-08-27T18:04:07Z","published":"2024-08-27T18:04:07Z","title":"UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized\n  Implicit Reward Function","summary":"  An LLM is pretrained on trillions of tokens, but the pretrained LLM may still\ngenerate undesired responses. To solve this problem, alignment techniques such\nas RLHF, DPO and KTO are proposed. However, these alignment techniques have\nlimitations. For example, RLHF requires training the reward model and policy\nseparately, which is complex, time-consuming, memory intensive and unstable\nduring training processes. DPO proposes a mapping between an optimal policy and\na reward, greatly simplifying the training process of RLHF. However, it can not\ntake full advantages of a reward model and it is limited to pairwise preference\ndata.\n  In this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which\nunifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the\nclassical RLHF objective, the optimal policy is induced by a generalize\nimplicit reward function. With this novel mapping between a reward model and an\noptimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised\nlearning of minimizing the difference between an implicit reward and an\nexplicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and\nreduce memory burden of RL fine-tuning process; 3. accommodate different\nfeedback types including pairwise, binary and scalar feedback. Downstream\nexperiments show UNA outperforms DPO, KTO and RLHF.\n","authors":["Zhichao Wang","Bin Bi","Can Huang","Shiva Kumar Pentyala","Zixu James Zhu","Sitaram Asur","Na Claire Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.15339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15332v1","updated":"2024-08-27T18:00:06Z","published":"2024-08-27T18:00:06Z","title":"What makes math problems hard for reinforcement learning: a case study","summary":"  Using a long-standing conjecture from combinatorial group theory, we explore,\nfrom multiple angles, the challenges of finding rare instances carrying\ndisproportionately high rewards. Based on lessons learned in the mathematical\ncontext defined by the Andrews-Curtis conjecture, we propose algorithmic\nimprovements that can be relevant in other domains with ultra-sparse reward\nproblems. Although our case study can be formulated as a game, its shortest\nwinning sequences are potentially $10^6$ or $10^9$ times longer than those\nencountered in chess. In the process of our study, we demonstrate that one of\nthe potential counterexamples due to Akbulut and Kirby, whose status escaped\ndirect mathematical methods for 39 years, is stably AC-trivial.\n","authors":["Ali Shehper","Anibal M. Medina-Mardones","BartÅomiej Lewandowski","Angus Gruen","Piotr Kucharski","Sergei Gukov"],"pdf_url":"https://arxiv.org/pdf/2408.15332v1.pdf","comment":"39 pages, 18 figures, 1 table"},{"id":"http://arxiv.org/abs/2408.15328v1","updated":"2024-08-27T18:00:02Z","published":"2024-08-27T18:00:02Z","title":"Artificially intelligent Maxwell's demon for optimal control of open\n  quantum systems","summary":"  Feedback control of open quantum systems is of fundamental importance for\npractical applications in various contexts, ranging from quantum computation to\nquantum error correction and quantum metrology. Its use in the context of\nthermodynamics further enables the study of the interplay between information\nand energy. However, deriving optimal feedback control strategies is highly\nchallenging, as it involves the optimal control of open quantum systems, the\nstochastic nature of quantum measurement, and the inclusion of policies that\nmaximize a long-term time- and trajectory-averaged goal. In this work, we\nemploy a reinforcement learning approach to automate and capture the role of a\nquantum Maxwell's demon: the agent takes the literal role of discovering\noptimal feedback control strategies in qubit-based systems that maximize a\ntrade-off between measurement-powered cooling and measurement efficiency.\nConsidering weak or projective quantum measurements, we explore different\nregimes based on the ordering between the thermalization, the measurement, and\nthe unitary feedback timescales, finding different and highly non-intuitive,\nyet interpretable, strategies. In the thermalization-dominated regime, we find\nstrategies with elaborate finite-time thermalization protocols conditioned on\nmeasurement outcomes. In the measurement-dominated regime, we find that optimal\nstrategies involve adaptively measuring different qubit observables reflecting\nthe acquired information, and repeating multiple weak measurements until the\nquantum state is \"sufficiently pure\", leading to random walks in state space.\nFinally, we study the case when all timescales are comparable, finding new\nfeedback control strategies that considerably outperform more intuitive ones.\nWe discuss a two-qubit example where we explore the role of entanglement and\nconclude discussing the scaling of our results to quantum many-body systems.\n","authors":["Paolo Andrea Erdman","Robert Czupryniak","Bibek Bhandari","Andrew N. Jordan","Frank NoÃ©","Jens Eisert","Giacomo Guarnieri"],"pdf_url":"https://arxiv.org/pdf/2408.15328v1.pdf","comment":"16+10 pages, 21 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.15209v1","updated":"2024-08-27T17:18:02Z","published":"2024-08-27T17:18:02Z","title":"Sec2Sec Co-attention for Video-Based Apparent Affective Prediction","summary":"  Video-based apparent affect detection plays a crucial role in video\nunderstanding, as it encompasses various elements such as vision, audio,\naudio-visual interactions, and spatiotemporal information, which are essential\nfor accurate video predictions. However, existing approaches often focus on\nextracting only a subset of these elements, resulting in the limited predictive\ncapacity of their models. To address this limitation, we propose a novel\nLSTM-based network augmented with a Transformer co-attention mechanism for\npredicting apparent affect in videos. We demonstrate that our proposed Sec2Sec\nCo-attention Transformer surpasses multiple state-of-the-art methods in\npredicting apparent affect on two widely used datasets: LIRIS-ACCEDE and First\nImpressions. Notably, our model offers interpretability, allowing us to examine\nthe contributions of different time points to the overall prediction. The\nimplementation is available at: https://github.com/nestor-sun/sec2sec.\n","authors":["Mingwei Sun","Kunpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15209v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.13993v3","updated":"2024-08-27T15:56:33Z","published":"2024-04-22T08:59:35Z","title":"Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion","summary":"  Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.\n","authors":["Yingxuan Li","Ryota Hinami","Kiyoharu Aizawa","Yusuke Matsui"],"pdf_url":"https://arxiv.org/pdf/2404.13993v3.pdf","comment":"Accepted to ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.14826v1","updated":"2024-08-27T07:13:44Z","published":"2024-08-27T07:13:44Z","title":"Alfie: Democratising RGBA Image Generation With No $$$","summary":"  Designs and artworks are ubiquitous across various creative fields, requiring\ngraphic design skills and dedicated software to create compositions that\ninclude many graphical elements, such as logos, icons, symbols, and art scenes,\nwhich are integral to visual storytelling. Automating the generation of such\nvisual elements improves graphic designers' productivity, democratizes and\ninnovates the creative industry, and helps generate more realistic synthetic\ndata for related tasks. These illustration elements are mostly RGBA images with\nirregular shapes and cutouts, facilitating blending and scene composition.\nHowever, most image generation models are incapable of generating such images\nand achieving this capability requires expensive computational resources,\nspecific training recipes, or post-processing solutions. In this work, we\npropose a fully-automated approach for obtaining RGBA illustrations by\nmodifying the inference-time behavior of a pre-trained Diffusion Transformer\nmodel, exploiting the prompt-guided controllability and visual quality offered\nby such models with no additional computational cost. We force the generation\nof entire subjects without sharp croppings, whose background is easily removed\nfor seamless integration into design projects or artistic scenes. We show with\na user study that, in most cases, users prefer our solution over generating and\nthen matting an image, and we show that our generated illustrations yield good\nresults when used as inputs for composite scene generation pipelines. We\nrelease the code at https://github.com/aimagelab/Alfie.\n","authors":["Fabio Quattrini","Vittorio Pippi","Silvia Cascianelli","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2408.14826v1.pdf","comment":"Accepted at ECCV AI for Visual Arts Workshop and Challenges"},{"id":"http://arxiv.org/abs/2408.14823v1","updated":"2024-08-27T07:06:49Z","published":"2024-08-27T07:06:49Z","title":"LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive\n  Streaming","summary":"  The rise of Extended Reality (XR) requires efficient streaming of 3D online\nworlds, challenging current 3DGS representations to adapt to\nbandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS\nthat supports adaptive streaming and progressive rendering. Our method\nconstructs a layered structure for cumulative representation, incorporates\ndynamic opacity optimization to maintain visual fidelity, and utilizes\noccupancy maps to efficiently manage Gaussian splats. This proposed model\noffers a progressive representation supporting a continuous rendering quality\nadapted for bandwidth-aware streaming. Extensive experiments validate the\neffectiveness of our approach in balancing visual fidelity with the compactness\nof the model, with up to 50.71% improvement in SSIM, 286.53% improvement in\nLPIPS, and 318.41% reduction in model size, and shows its potential for\nbandwidth-adapted 3D streaming and rendering applications.\n","authors":["Yuang Shi","Simone Gasparini","GÃ©raldine Morin","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2408.14823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14764v1","updated":"2024-08-27T03:31:24Z","published":"2024-08-27T03:31:24Z","title":"SynthDoc: Bilingual Documents Synthesis for Visual Document\n  Understanding","summary":"  This paper introduces SynthDoc, a novel synthetic document generation\npipeline designed to enhance Visual Document Understanding (VDU) by generating\nhigh-quality, diverse datasets that include text, images, tables, and charts.\nAddressing the challenges of data acquisition and the limitations of existing\ndatasets, SynthDoc leverages publicly available corpora and advanced rendering\ntools to create a comprehensive and versatile dataset. Our experiments,\nconducted using the Donut model, demonstrate that models trained with\nSynthDoc's data achieve superior performance in pre-training read tasks and\nmaintain robustness in downstream tasks, despite language inconsistencies. The\nrelease of a benchmark dataset comprising 5,000 image-text pairs not only\nshowcases the pipeline's capabilities but also provides a valuable resource for\nthe VDU community to advance research and development in document image\nrecognition. This work significantly contributes to the field by offering a\nscalable solution to data scarcity and by validating the efficacy of end-to-end\nmodels in parsing complex, real-world documents.\n","authors":["Chuanghao Ding","Xuejing Liu","Wei Tang","Juan Li","Xiaoliang Wang","Rui Zhao","Cam-Tu Nguyen","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2408.14764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14735v1","updated":"2024-08-27T02:03:36Z","published":"2024-08-27T02:03:36Z","title":"PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy","summary":"  Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.\n","authors":["Xianzhi Zhang","Yipeng Zhou","Di Wu","Quan Z. Sheng","Miao Hu","Linchang Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.14735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13621v5","updated":"2024-08-27T01:23:50Z","published":"2024-04-21T11:21:27Z","title":"Attack on Scene Flow using Point Clouds","summary":"  Deep neural networks have made significant advancements in accurately\nestimating scene flow using point clouds, which is vital for many applications\nlike video analysis, action recognition, and navigation. The robustness of\nthese techniques, however, remains a concern, particularly in the face of\nadversarial attacks that have been proven to deceive state-of-the-art deep\nneural networks in many domains. Surprisingly, the robustness of scene flow\nnetworks against such attacks has not been thoroughly investigated. To address\nthis problem, the proposed approach aims to bridge this gap by introducing\nadversarial white-box attacks specifically tailored for scene flow networks.\nExperimental results show that the generated adversarial examples obtain up to\n33.7 relative degradation in average end-point error on the KITTI and\nFlyingThings3D datasets. The study also reveals the significant impact that\nattacks targeting point clouds in only one dimension or color channel have on\naverage end-point error. Analyzing the success and failure of these attacks on\nthe scene flow networks and their 2D optical flow network variants shows a\nhigher vulnerability for the optical flow networks. Code is available at\nhttps://github.com/aheldis/Attack-on-Scene-Flow-using-Point-Clouds.git.\n","authors":["Haniyeh Ehsani Oskouie","Mohammad-Shahram Moin","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2404.13621v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14713v1","updated":"2024-08-27T00:37:07Z","published":"2024-08-27T00:37:07Z","title":"StyleSpeech: Parameter-efficient Fine Tuning for Pre-trained\n  Controllable Text-to-Speech","summary":"  This paper introduces StyleSpeech, a novel Text-to-Speech~(TTS) system that\nenhances the naturalness and accuracy of synthesized speech. Building upon\nexisting TTS technologies, StyleSpeech incorporates a unique Style Decorator\nstructure that enables deep learning models to simultaneously learn style and\nphoneme features, improving adaptability and efficiency through the principles\nof Lower Rank Adaptation~(LoRA). LoRA allows efficient adaptation of style\nfeatures in pre-trained models. Additionally, we introduce a novel automatic\nevaluation metric, the LLM-Guided Mean Opinion Score (LLM-MOS), which employs\nlarge language models to offer an objective and robust protocol for\nautomatically assessing TTS system performance. Extensive testing on benchmark\ndatasets shows that our approach markedly outperforms existing state-of-the-art\nbaseline methods in producing natural, accurate, and high-quality speech. These\nadvancements not only pushes the boundaries of current TTS system capabilities,\nbut also facilitate the application of TTS system in more dynamic and\nspecialized, such as interactive virtual assistants, adaptive audiobooks, and\ncustomized voice for gaming. Speech samples can be found in\nhttps://style-speech.vercel.app\n","authors":["Haowei Lou","Helen Paik","Wen Hu","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2408.14713v1.pdf","comment":null}]},"2024-08-28T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2408.15992v1","updated":"2024-08-28T17:58:39Z","published":"2024-08-28T17:58:39Z","title":"CoGen: Learning from Feedback with Coupled Comprehension and Generation","summary":"  Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.\n","authors":["Mustafa Omer Gul","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.15992v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.15971v1","updated":"2024-08-28T17:43:55Z","published":"2024-08-28T17:43:55Z","title":"BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition\n  Capabilities of Language Models in Multi-Agent Systems","summary":"  Large Language Models (LLMs) are becoming increasingly powerful and capable\nof handling complex tasks, e.g., building single agents and multi-agent\nsystems. Compared to single agents, multi-agent systems have higher\nrequirements for the collaboration capabilities of language models. Many\nbenchmarks are proposed to evaluate their collaborative abilities. However,\nthese benchmarks lack fine-grained evaluations of LLM collaborative\ncapabilities. Additionally, multi-agent collaborative and competitive scenarios\nare ignored in existing works. To address these two problems, we propose a\nbenchmark, called BattleAgentBench, which defines seven sub-stages of three\nvarying difficulty levels and conducts a fine-grained evaluation of language\nmodels in terms of single-agent scenario navigation capabilities, paired-agent\ntask execution abilities, and multi-agent collaboration and competition\ncapabilities. We conducted extensive evaluations on leading four closed-source\nand seven open-source models. Experimental results indicate that API-based\nmodels perform excellently on simple tasks but open-source small models\nstruggle with simple tasks. Regarding difficult tasks that require\ncollaborative and competitive abilities, although API-based models have\ndemonstrated some collaborative capabilities, there is still enormous room for\nimprovement.\n","authors":["Wei Wang","Dan Zhang","Tao Feng","Boyan Wang","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2408.15971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15966v1","updated":"2024-08-28T17:38:44Z","published":"2024-08-28T17:38:44Z","title":"More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding","summary":"  Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.\n","authors":["Yuan Tang","Xu Han","Xianzhi Li","Qiao Yu","Jinfeng Xu","Yixue Hao","Long Hu","Min Chen"],"pdf_url":"https://arxiv.org/pdf/2408.15966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10260v2","updated":"2024-08-28T17:26:03Z","published":"2024-06-11T01:16:10Z","title":"Flextron: Many-in-One Flexible Large Language Model","summary":"  Training modern LLMs is extremely resource intensive, and customizing them\nfor various deployment scenarios characterized by limited compute and memory\nresources through repeated training is impractical. In this paper, we introduce\nFlextron, a network architecture and post-training model optimization framework\nsupporting flexible model deployment. The Flextron architecture utilizes a\nnested elastic structure to rapidly adapt to specific user-defined latency and\naccuracy targets during inference with no additional fine-tuning required. It\nis also input-adaptive, and can automatically route tokens through its\nsub-networks for improved performance and efficiency. We present a\nsample-efficient training method and associated routing algorithms for\nsystematically transforming an existing trained LLM into a Flextron model. We\nevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate\nsuperior performance over multiple end-to-end trained variants and other\nstate-of-the-art elastic networks, all with a single pretraining run that\nconsumes a mere 7.63% tokens compared to original pretraining.\n","authors":["Ruisi Cai","Saurav Muralidharan","Greg Heinrich","Hongxu Yin","Zhangyang Wang","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2406.10260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15915v1","updated":"2024-08-28T16:28:07Z","published":"2024-08-28T16:28:07Z","title":"Leveraging Open Knowledge for Advancing Task Expertise in Large Language\n  Models","summary":"  The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Codes and models\nwill be released later.\n","authors":["Yuncheng Yang","Yulei Qin","Tong Wu","Zihan Xu","Gang Li","Pengcheng Guo","Hang Shao","Yucheng Shi","Ke Li","Xing Sun","Jie Yang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2408.15915v1.pdf","comment":"28 pages, 12 tables, 10 figures"},{"id":"http://arxiv.org/abs/2311.11844v3","updated":"2024-08-28T16:26:16Z","published":"2023-11-20T15:34:45Z","title":"Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles\n  in Public Policy Documents","summary":"  Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4\npromise automation with better results and less programming, opening up new\nopportunities for text analysis in political science. In this study, we\nevaluate LLMs on three original coding tasks involving typical complexities\nencountered in political science settings: a non-English language, legal and\npolitical jargon, and complex labels based on abstract constructs. Along the\npaper, we propose a practical workflow to optimize the choice of the model and\nthe prompt. We find that the best prompting strategy consists of providing the\nLLMs with a detailed codebook, as the one provided to human coders. In this\nsetting, an LLM can be as good as or possibly better than a human annotator\nwhile being much faster, considerably cheaper, and much easier to scale to\nlarge amounts of text. We also provide a comparison of GPT and popular\nopen-source LLMs, discussing the trade-offs in the model's choice. Our software\nallows LLMs to be easily used as annotators and is publicly available:\nhttps://github.com/lorelupo/pappa.\n","authors":["Lorenzo Lupo","Oscar Magnusson","Dirk Hovy","Elin Naurin","Lena WÃ¤ngnerud"],"pdf_url":"https://arxiv.org/pdf/2311.11844v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15903v1","updated":"2024-08-28T16:15:45Z","published":"2024-08-28T16:15:45Z","title":"LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments","summary":"  The rapid obsolescence of information in Large Language Models (LLMs) has\ndriven the development of various techniques to incorporate new facts. However,\nexisting methods for knowledge editing still face difficulties with multi-hop\nquestions that require accurate fact identification and sequential logical\nreasoning, particularly among numerous fact updates. To tackle these\nchallenges, this paper introduces Graph Memory-based Editing for Large Language\nModels (GMeLLo), a straitforward and effective method that merges the explicit\nknowledge representation of Knowledge Graphs (KGs) with the linguistic\nflexibility of LLMs. Beyond merely leveraging LLMs for question answering,\nGMeLLo employs these models to convert free-form language into structured\nqueries and fact triples, facilitating seamless interaction with KGs for rapid\nupdates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art knowledge editing methods in\nthe multi-hop question answering benchmark, MQuAKE, especially in scenarios\nwith extensive knowledge edits.\n","authors":["Ruirui Chen","Weifeng Jiang","Chengwei Qin","Ishaan Singh Rawal","Cheston Tan","Dongkyu Choi","Bo Xiong","Bo Ai"],"pdf_url":"https://arxiv.org/pdf/2408.15903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15901v1","updated":"2024-08-28T16:12:55Z","published":"2024-08-28T16:12:55Z","title":"Nexus: Specialization meets Adaptability for Efficiently Training\n  Mixture of Experts","summary":"  Efficiency, specialization, and adaptability to new data distributions are\nqualities that are hard to combine in current Large Language Models. The\nMixture of Experts (MoE) architecture has been the focus of significant\nresearch because its inherent conditional computation enables such desirable\nproperties. In this work, we focus on \"upcycling\" dense expert models into an\nMoE, aiming to improve specialization while also adding the ability to adapt to\nnew tasks easily. We introduce Nexus, an enhanced MoE architecture with\nadaptive routing where the model learns to project expert embeddings from\ndomain representations. This approach allows Nexus to flexibly add new experts\nafter the initial upcycling through separately trained dense models, without\nrequiring large-scale MoE training for unseen data domains. Our experiments\nshow that Nexus achieves a relative gain of up to 2.1% over the baseline for\ninitial upcycling, and a 18.8% relative gain for extending the MoE with a new\nexpert by using limited finetuning data. This flexibility of Nexus is crucial\nto enable an open-source ecosystem where every user continuously assembles\ntheir own MoE-mix according to their needs.\n","authors":["Nikolas Gritsch","Qizhen Zhang","Acyr Locatelli","Sara Hooker","Ahmet ÃstÃ¼n"],"pdf_url":"https://arxiv.org/pdf/2408.15901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15896v1","updated":"2024-08-28T16:06:12Z","published":"2024-08-28T16:06:12Z","title":"A New Method for Cross-Lingual-based Semantic Role Labeling","summary":"  Semantic role labeling is a crucial task in natural language processing,\nenabling better comprehension of natural language. However, the lack of\nannotated data in multiple languages has posed a challenge for researchers. To\naddress this, a deep learning algorithm based on model transfer has been\nproposed. The algorithm utilizes a dataset consisting of the English portion of\nCoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiency\nof training, only ten percent of the educational data from each language is\nused. The results of the proposed model demonstrate significant improvements\ncompared to Niksirt et al.'s model. In monolingual mode, the proposed model\nachieved a 2.05 percent improvement on F1-score, while in cross-lingual mode,\nthe improvement was even more substantial, reaching 6.23 percent. Worth noting\nis that the compared model only trained two of the four stages of semantic role\nlabeling and employed golden data for the remaining two stages. This suggests\nthat the actual superiority of the proposed model surpasses the reported\nnumbers by a significant margin. The development of cross-lingual methods for\nsemantic role labeling holds promise, particularly in addressing the scarcity\nof annotated data for various languages. These advancements pave the way for\nfurther research in understanding and processing natural language across\ndifferent linguistic contexts.\n","authors":["Mohammad Ebrahimi","Behrouz Minaei Bidgoli","Nasim Khozouei"],"pdf_url":"https://arxiv.org/pdf/2408.15896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15895v1","updated":"2024-08-28T16:05:20Z","published":"2024-08-28T16:05:20Z","title":"Bias in LLMs as Annotators: The Effect of Party Cues on Labelling\n  Decision by Large Language Models","summary":"  Human coders are biased. We test similar biases in Large Language Models\n(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and\nMeyer (2018), we find evidence that LLMs use political information, and\nspecifically party cues, to judge political statements. Not only do LLMs use\nrelevant information to contextualize whether a statement is positive,\nnegative, or neutral based on the party cue, they also reflect the biases of\nthe human-generated data upon which they have been trained. We also find that\nunlike humans, who are only biased when faced with statements from extreme\nparties, LLMs exhibit significant bias even when prompted with statements from\ncenter-left and center-right parties. The implications of our findings are\ndiscussed in the conclusion.\n","authors":["Sebastian Vallejo Vera","Hunter Driggers"],"pdf_url":"https://arxiv.org/pdf/2408.15895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15879v1","updated":"2024-08-28T15:50:41Z","published":"2024-08-28T15:50:41Z","title":"Persuasion Games using Large Language Models","summary":"  Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape human perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nusers through persuasive dialogue, while the auxiliary agents perform tasks\nsuch as information retrieval, response analysis, development of persuasion\nstrategies, and validation of facts. Empirical evidence from our experiments\ndemonstrates that this collaborative methodology significantly enhances the\npersuasive efficacy of the LLM. We analyze user resistance to persuasive\nefforts continuously and counteract it by employing a combination of rule-based\nand LLM-based resistance-persuasion mapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase).\n","authors":["Ganesh Prasath Ramani","Shirish Karande","Santhosh V","Yash Bhatia"],"pdf_url":"https://arxiv.org/pdf/2408.15879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02731v3","updated":"2024-08-28T15:40:22Z","published":"2023-09-06T05:33:57Z","title":"HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus","summary":"  ChatGPT has garnered significant interest due to its impressive performance;\nhowever, there is growing concern about its potential risks, particularly in\nthe detection of AI-generated content (AIGC), which is often challenging for\nuntrained individuals to identify. Current datasets used for detecting\nChatGPT-generated text primarily focus on question-answering tasks, often\noverlooking tasks with semantic-invariant properties, such as summarization,\ntranslation, and paraphrasing. In this paper, we demonstrate that detecting\nmodel-generated text in semantic-invariant tasks is more challenging. To\naddress this gap, we introduce a more extensive and comprehensive dataset that\nincorporates a wider range of tasks than previous work, including those with\nsemantic-invariant properties.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2309.02731v3.pdf","comment":"This paper has been accepted by CIKM2023 workshop"},{"id":"http://arxiv.org/abs/2405.00706v3","updated":"2024-08-28T15:29:10Z","published":"2024-04-23T14:43:35Z","title":"From Complexity to Clarity: How AI Enhances Perceptions of Scientists\n  and the Public's Understanding of Science","summary":"  This paper evaluated the effectiveness of using generative AI to simplify\nscience communication and enhance the public's understanding of science. By\ncomparing lay summaries of journal articles from PNAS, yoked to those generated\nby AI, this work first assessed linguistic simplicity differences across such\nsummaries and public perceptions in follow-up experiments. Specifically, Study\n1a analyzed simplicity features of PNAS abstracts (scientific summaries) and\nsignificance statements (lay summaries), observing that lay summaries were\nindeed linguistically simpler, but effect size differences were small. Study 1b\nused a large language model, GPT-4, to create significance statements based on\npaper abstracts and this more than doubled the average effect size without\nfine-tuning. Study 2 experimentally demonstrated that simply-written GPT\nsummaries facilitated more favorable perceptions of scientists (they were\nperceived as more credible and trustworthy, but less intelligent) than more\ncomplexly-written human PNAS summaries. Crucially, Study 3 experimentally\ndemonstrated that participants comprehended scientific writing better after\nreading simple GPT summaries compared to complex PNAS summaries. In their own\nwords, participants also summarized scientific papers in a more detailed and\nconcrete manner after reading GPT summaries compared to PNAS summaries of the\nsame article. AI has the potential to engage scientific communities and the\npublic via a simple language heuristic, advocating for its integration into\nscientific dissemination for a more informed society.\n","authors":["David M. Markowitz"],"pdf_url":"https://arxiv.org/pdf/2405.00706v3.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2404.07839v2","updated":"2024-08-28T15:05:42Z","published":"2024-04-11T15:27:22Z","title":"RecurrentGemma: Moving Past Transformers for Efficient Open Language\n  Models","summary":"  We introduce RecurrentGemma, a family of open language models which uses\nGoogle's novel Griffin architecture. Griffin combines linear recurrences with\nlocal attention to achieve excellent performance on language. It has a\nfixed-sized state, which reduces memory use and enables efficient inference on\nlong sequences. We provide two sizes of models, containing 2B and 9B\nparameters, and provide pre-trained and instruction tuned variants for both.\nOur models achieve comparable performance to similarly-sized Gemma baselines\ndespite being trained on fewer tokens.\n","authors":["Aleksandar Botev","Soham De","Samuel L Smith","Anushan Fernando","George-Cristian Muraru","Ruba Haroun","Leonard Berrada","Razvan Pascanu","Pier Giuseppe Sessa","Robert Dadashi","LÃ©onard Hussenot","Johan Ferret","Sertan Girgin","Olivier Bachem","Alek Andreev","Kathleen Kenealy","Thomas Mesnard","Cassidy Hardin","Surya Bhupatiraju","Shreya Pathak","Laurent Sifre","Morgane RiviÃ¨re","Mihir Sanjay Kale","Juliette Love","Pouya Tafti","Armand Joulin","Noah Fiedel","Evan Senter","Yutian Chen","Srivatsan Srinivasan","Guillaume Desjardins","David Budden","Arnaud Doucet","Sharad Vikram","Adam Paszke","Trevor Gale","Sebastian Borgeaud","Charlie Chen","Andy Brock","Antonia Paterson","Jenny Brennan","Meg Risdal","Raj Gundluru","Nesh Devanathan","Paul Mooney","Nilay Chauhan","Phil Culliton","Luiz Gustavo Martins","Elisa Bandy","David Huntsperger","Glenn Cameron","Arthur Zucker","Tris Warkentin","Ludovic Peran","Minh Giang","Zoubin Ghahramani","ClÃ©ment Farabet","Koray Kavukcuoglu","Demis Hassabis","Raia Hadsell","Yee Whye Teh","Nando de Frietas"],"pdf_url":"https://arxiv.org/pdf/2404.07839v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01245v2","updated":"2024-08-28T15:01:04Z","published":"2024-04-01T17:03:41Z","title":"A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules","summary":"  Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.\n","authors":["Xiang Li","Feng Ruan","Huiyuan Wang","Qi Long","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2404.01245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00612v2","updated":"2024-08-28T14:59:31Z","published":"2024-08-01T14:52:04Z","title":"Downstream bias mitigation is all you need","summary":"  The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model.\n","authors":["Arkadeep Baksi","Rahul Singh","Tarun Joshi"],"pdf_url":"https://arxiv.org/pdf/2408.00612v2.pdf","comment":"arXiv admin note: This work has been withdrawn by arXiv\n  administrators due to inappropriate text reuse from external sources"},{"id":"http://arxiv.org/abs/2402.16696v3","updated":"2024-08-28T14:54:11Z","published":"2024-02-26T16:11:03Z","title":"Look Before You Leap: Towards Decision-Aware and Generalizable\n  Tool-Usage for Large Language Models","summary":"  Tool-augmented large language models (LLMs) are attracting widespread\nattention when accessing up-to-date knowledge and alleviating hallucination\nissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated\nsurprising tool-usage capabilities through prompting and in-context learning\ntechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in\nmanipulating tools, current efforts focus on either template-driven or\ntoken-triggered tool-usage. However, the former hampers LLMs' flexibility to\naddress diverse user's queries due to constrained tool interactions, while the\nlatter limits the generalizability when engaging with new tools, since\ntool-usage learning is based on task- and tool-specific datasets. To alleviate\nthese concerns, in this paper, we propose a decision-aware and generalizable\ntool-usage framework (DEER). Specifically, we first construct the tool-usage\nsamples with multiple decision branches via an automatic generation pipeline,\nthereby inspiring the decision-making awareness of LLMs under diverse\nscenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the\ngeneralizability of LLMs over unseen tools. Extensive experiments demonstrate\nthat our proposed DEER is effective and significantly outperforms baselines\nacross various datasets.\n","authors":["Anchun Gui","Jian Li","Yong Dai","Nan Du","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2402.16696v3.pdf","comment":"20 pages, 18 figures"},{"id":"http://arxiv.org/abs/2408.15836v1","updated":"2024-08-28T14:48:37Z","published":"2024-08-28T14:48:37Z","title":"Knowledge Navigator: LLM-guided Browsing Framework for Exploratory\n  Search in Scientific Literature","summary":"  The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available.\n","authors":["Uri Katz","Mosh Levy","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2408.15836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13560v2","updated":"2024-08-28T14:45:57Z","published":"2024-03-20T12:52:38Z","title":"eRST: A Signaled Graph Theory of Discourse Relations and Organization","summary":"  In this article we present Enhanced Rhetorical Structure Theory (eRST), a new\ntheoretical framework for computational discourse analysis, based on an\nexpansion of Rhetorical Structure Theory (RST). The framework encompasses\ndiscourse relation graphs with tree-breaking, non-projective and concurrent\nrelations, as well as implicit and explicit signals which give explainable\nrationales to our analyses. We survey shortcomings of RST and other existing\nframeworks, such as Segmented Discourse Representation Theory (SDRT), the Penn\nDiscourse Treebank (PDTB) and Discourse Dependencies, and address these using\nconstructs in the proposed theory. We provide annotation, search and\nvisualization tools for data, and present and evaluate a freely available\ncorpus of English annotated according to our framework, encompassing 12 spoken\nand written genres with over 200K tokens. Finally, we discuss automatic\nparsing, evaluation metrics and applications for data in our framework.\n","authors":["Amir Zeldes","Tatsuya Aoyama","Yang Janet Liu","Siyao Peng","Debopam Das","Luke Gessler"],"pdf_url":"https://arxiv.org/pdf/2403.13560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15827v1","updated":"2024-08-28T14:40:15Z","published":"2024-08-28T14:40:15Z","title":"Automatic Differential Diagnosis using Transformer-Based Multi-Label\n  Sequence Classification","summary":"  As the field of artificial intelligence progresses, assistive technologies\nare becoming more widely used across all industries. The healthcare industry is\nno different, with numerous studies being done to develop assistive tools for\nhealthcare professionals. Automatic diagnostic systems are one such beneficial\ntool that can assist with a variety of tasks, including collecting patient\ninformation, analyzing test results, and diagnosing patients. However, the idea\nof developing systems that can provide a differential diagnosis has been\nlargely overlooked in most of these research studies. In this study, we propose\na transformer-based approach for providing differential diagnoses based on a\npatient's age, sex, medical history, and symptoms. We use the DDXPlus dataset,\nwhich provides differential diagnosis information for patients based on 49\ndisease types. Firstly, we propose a method to process the tabular patient data\nfrom the dataset and engineer them into patient reports to make them suitable\nfor our research. In addition, we introduce two data modification modules to\ndiversify the training data and consequently improve the robustness of the\nmodels. We approach the task as a multi-label classification problem and\nconduct extensive experiments using four transformer models. All the models\ndisplayed promising results by achieving over 97% F1 score on the held-out test\nset. Moreover, we design additional behavioral tests to get a broader\nunderstanding of the models. In particular, for one of our test cases, we\nprepared a custom test set of 100 samples with the assistance of a doctor. The\nresults on the custom set showed that our proposed data modification modules\nimproved the model's generalization capabilities. We hope our findings will\nprovide future researchers with valuable insights and inspire them to develop\nreliable systems for automatic differential diagnosis.\n","authors":["Abu Adnan Sadi","Mohammad Ashrafuzzaman Khan","Lubaba Binte Saber"],"pdf_url":"https://arxiv.org/pdf/2408.15827v1.pdf","comment":"25 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.14511v2","updated":"2024-08-28T14:13:41Z","published":"2024-08-25T04:07:18Z","title":"Unveiling the Statistical Foundations of Chain-of-Thought Prompting\n  Methods","summary":"  Chain-of-Thought (CoT) prompting and its variants have gained popularity as\neffective methods for solving multi-step reasoning problems using pretrained\nlarge language models (LLMs). In this work, we analyze CoT prompting from a\nstatistical estimation perspective, providing a comprehensive characterization\nof its sample complexity. To this end, we introduce a multi-step latent\nvariable model that encapsulates the reasoning process, where the latent\nvariable encodes the task information. Under this framework, we demonstrate\nthat when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator\neffectively solves the multi-step reasoning problem by aggregating a posterior\ndistribution inferred from the demonstration examples in the prompt. Moreover,\nwe prove that the statistical error of the CoT estimator can be decomposed into\ntwo main components: (i) a prompting error, which arises from inferring the\ntrue task using CoT prompts, and (ii) the statistical error of the pretrained\nLLM. We establish that, under appropriate assumptions, the prompting error\ndecays exponentially to zero as the number of demonstrations increases.\nAdditionally, we explicitly characterize the approximation and generalization\nerrors of the pretrained LLM. Notably, we construct a transformer model that\napproximates the target distribution of the multi-step reasoning problem with\nan error that decreases exponentially in the number of transformer blocks. Our\nanalysis extends to other variants of CoT, including Self-Consistent CoT,\nTree-of-Thought, and Selection-Inference, offering a broad perspective on the\nefficacy of these methods. We also provide numerical experiments to validate\nthe theoretical findings.\n","authors":["Xinyang Hu","Fengzhuo Zhang","Siyu Chen","Zhuoran Yang"],"pdf_url":"https://arxiv.org/pdf/2408.14511v2.pdf","comment":"150 pages, 18 figures, 3 tables"},{"id":"http://arxiv.org/abs/2402.14846v4","updated":"2024-08-28T14:04:05Z","published":"2024-02-19T14:53:01Z","title":"Stick to your Role! Stability of Personal Values Expressed in Large\n  Language Models","summary":"  The standard way to study Large Language Models (LLMs) with benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLMs' highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence\n(specifically, value stability) should be studied as a specific property of\nLLMs and used as another dimension of LLM comparison (alongside others such as\ncognitive abilities, knowledge, or model size). We present a case-study on the\nstability of value expression over different contexts (simulated conversations\non different topics) as measured using a standard psychology questionnaire\n(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we\nstudy Rank-order stability on the population (interpersonal) level, and\nIpsative stability on the individual (intrapersonal) level. We consider two\nsettings (with and without instructing LLMs to simulate particular personas),\ntwo simulated populations, and three downstream tasks. We observe consistent\ntrends in the stability of models and model families - Mixtral, Mistral,\nGPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency\nof these trends implies that some models exhibit higher value stability than\nothers, and that stability can be estimated with the set of introduced\nmethodological tools. When instructed to simulate particular personas, LLMs\nexhibit low Rank-order stability, which further diminishes with conversation\nlength. This highlights the need for future research on LLMs that coherently\nsimulate different personas. This paper provides a foundational step in that\ndirection, and, to our knowledge, it is the first study of value stability in\nLLMs.\n","authors":["Grgur KovaÄ","RÃ©my Portelas","Masataka Sawayama","Peter Ford Dominey","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2402.14846v4.pdf","comment":"The project website and code are available at\n  https://sites.google.com/view/llmvaluestability Published in PLOS ONE (\n  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0309114 ),\n  and a shorter version at CogSci 24 (\n  https://escholarship.org/uc/item/7w4823c6 )"},{"id":"http://arxiv.org/abs/2408.15801v1","updated":"2024-08-28T13:52:19Z","published":"2024-08-28T13:52:19Z","title":"Scaling Up Summarization: Leveraging Large Language Models for Long Text\n  Extractive Summarization","summary":"  In an era where digital text is proliferating at an unprecedented rate,\nefficient summarization tools are becoming indispensable. While Large Language\nModels (LLMs) have been successfully applied in various NLP tasks, their role\nin extractive text summarization remains underexplored. This paper introduces\nEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive\nSummarization), a framework that leverages LLMs, specifically LLAMA2-7B and\nChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of\nabstractive methods, which often suffer from issues like factual inaccuracies\nand hallucinations, EYEGLAXS focuses on extractive summarization to ensure\nfactual and grammatical integrity. Utilizing state-of-the-art techniques such\nas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS\naddresses the computational and resource challenges typically associated with\nLLMs. The system sets new performance benchmarks on well-known datasets like\nPubMed and ArXiv. Furthermore, we extend our research through additional\nanalyses that explore the adaptability of LLMs in handling different sequence\nlengths and their efficiency in training on smaller datasets. These\ncontributions not only set a new standard in the field but also open up\npromising avenues for future research in extractive text summarization.\n","authors":["LÃ©o Hemamou","Mehdi Debiane"],"pdf_url":"https://arxiv.org/pdf/2408.15801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15793v1","updated":"2024-08-28T13:37:07Z","published":"2024-08-28T13:37:07Z","title":"Language Adaptation on a Tight Academic Compute Budget: Tokenizer\n  Swapping Works and Pure bfloat16 Is Enough","summary":"  We investigate continued pretraining of LLMs for language adaptation on a\ntight academic budget: a setting in which only a few GPUs can be used in\nparallel, for a heavily constrained duration. We focus on adapting Mistral-7B\nto German or Arabic and evaluate several techniques to improve efficiency and\neffectiveness in this setting. Our German models adapted on this tight compute\nbudget underperform compared to the base Mistral-7B, while our Arabic models\noutperform several baselines, showing that for sufficiently well-represented\nlanguages, continued pretraining for specialization is not always helpful. Our\nmain findings focus on training precision and tokenizer swapping. Our results\nshow that pure bfloat16 training is a viable alternative to mixed-precision\ntraining, while being much faster when only using a few GPUs. Swapping the\ntokenizer for a specialized one yields more efficient tokenization and is\ncompetitive with the original tokenizer, which already contains some German\ntokens, but did not significantly increase performance for German. Code and\nmodel weights are available at on GitHub.\n","authors":["Konstantin Dobler","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2408.15793v1.pdf","comment":"WANT@ICML 2024"},{"id":"http://arxiv.org/abs/2408.15787v1","updated":"2024-08-28T13:29:59Z","published":"2024-08-28T13:29:59Z","title":"Interactive Agents: Simulating Counselor-Client Psychological Counseling\n  via Role-Playing LLM-to-LLM Interactions","summary":"  Virtual counselors powered by large language models (LLMs) aim to create\ninteractive support systems that effectively assist clients struggling with\nmental health challenges. To replicate counselor-client conversations,\nresearchers have built an online mental health platform that allows\nprofessional counselors to provide clients with text-based counseling services\nfor about an hour per session. Notwithstanding its effectiveness, challenges\nexist as human annotation is time-consuming, cost-intensive, privacy-protected,\nand not scalable. To address this issue and investigate the applicability of\nLLMs in psychological counseling conversation simulation, we propose a\nframework that employs two LLMs via role-playing for simulating\ncounselor-client interactions. Our framework involves two LLMs, one acting as a\nclient equipped with a specific and real-life user profile and the other\nplaying the role of an experienced counselor, generating professional responses\nusing integrative therapy techniques. We implement both the counselor and the\nclient by zero-shot prompting the GPT-4 model. In order to assess the\neffectiveness of LLMs in simulating counselor-client interactions and\nunderstand the disparities between LLM- and human-generated conversations, we\nevaluate the synthetic data from various perspectives. We begin by assessing\nthe client's performance through automatic evaluations. Next, we analyze and\ncompare the disparities between dialogues generated by the LLM and those\ngenerated by professional counselors. Furthermore, we conduct extensive\nexperiments to thoroughly examine the performance of our LLM-based counselor\ntrained with synthetic interactive dialogues by benchmarking against\nstate-of-the-art models for mental health.\n","authors":["Huachuan Qiu","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2408.15787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14438v2","updated":"2024-08-28T13:19:36Z","published":"2024-08-26T17:25:16Z","title":"Evaluating Large Language Models on Spatial Tasks: A Multi-Task\n  Benchmarking Study","summary":"  The advent of large language models such as ChatGPT, Gemini, and others has\nunderscored the importance of evaluating their diverse capabilities, ranging\nfrom natural language understanding to code generation. However, their\nperformance on spatial tasks has not been comprehensively assessed. This study\naddresses this gap by introducing a novel multi-task spatial evaluation\ndataset, designed to systematically explore and compare the performance of\nseveral advanced models on spatial tasks. The dataset encompasses twelve\ndistinct task types, including spatial understanding and path planning, each\nwith verified, accurate answers. We evaluated multiple models, including\nOpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase\ntesting approach. Initially, we conducted zero-shot testing, followed by\ncategorizing the dataset by difficulty and performing prompt tuning tests.\nResults indicate that gpt-4o achieved the highest overall accuracy in the first\nphase, with an average of 71.3%. Although moonshot-v1-8k slightly\nunderperformed overall, it surpassed gpt-4o in place name recognition tasks.\nThe study also highlights the impact of prompt strategies on model performance\nin specific tasks. For example, the Chain-of-Thought (COT) strategy increased\ngpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot\nstrategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to\n76.3%.\n","authors":["Liuchang Xu","Shuo Zhao","Qingming Lin","Luyao Chen","Qianqian Luo","Sensen Wu","Xinyue Ye","Hailin Feng","Zhenhong Du"],"pdf_url":"https://arxiv.org/pdf/2408.14438v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15778v1","updated":"2024-08-28T13:16:41Z","published":"2024-08-28T13:16:41Z","title":"LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models","summary":"  Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.\n","authors":["Jiayi Gui","Yiming Liu","Jiale Cheng","Xiaotao Gu","Xiao Liu","Hongning Wang","Yuxiao Dong","Jie Tang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2408.15778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15769v1","updated":"2024-08-28T13:05:55Z","published":"2024-08-28T13:05:55Z","title":"A Survey on Evaluation of Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) mimic human perception and reasoning\nsystem by integrating powerful Large Language Models (LLMs) with various\nmodality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and\nvarious modality encoders as sensory organs. This framework endows MLLMs with\nhuman-like capabilities, and suggests a potential pathway towards achieving\nartificial general intelligence (AGI). With the emergence of all-round MLLMs\nlike GPT-4V and Gemini, a multitude of evaluation methods have been developed\nto assess their capabilities across different dimensions. This paper presents a\nsystematic and comprehensive review of MLLM evaluation methods, covering the\nfollowing key aspects: (1) the background of MLLMs and their evaluation; (2)\n\"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks\nbased on the capabilities assessed, including general multimodal recognition,\nperception, reasoning and trustworthiness, and domain-specific applications\nsuch as socioeconomic, natural sciences and engineering, medical usage, AI\nagent, remote sensing, video and audio processing, 3D point cloud analysis, and\nothers; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into\ngeneral and specific benchmarks; (4) \"how to evaluate\" that reviews and\nillustrates MLLM evaluation steps and metrics; Our overarching goal is to\nprovide valuable insights for researchers in the field of MLLM evaluation,\nthereby facilitating the development of more capable and reliable MLLMs. We\nemphasize that evaluation should be regarded as a critical discipline,\nessential for advancing the field of MLLMs.\n","authors":["Jiaxing Huang","Jingyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15766v1","updated":"2024-08-28T12:59:12Z","published":"2024-08-28T12:59:12Z","title":"Harmonized Speculative Sampling","summary":"  Speculative sampling has proven to be an effective solution to accelerate\ndecoding from large language models, where the acceptance rate significantly\ndetermines the performance. Most previous works on improving the acceptance\nrate focus on aligned training and efficient decoding, implicitly paying less\nattention to the linkage of training and decoding. In this work, we first\ninvestigate the linkage of training and decoding for speculative sampling and\nthen propose a solution named HArmonized Speculative Sampling (HASS). HASS\nimproves the acceptance rate without extra inference overhead by harmonizing\ntraining and decoding on their objectives and contexts. Experiments on three\nLLaMA models demonstrate that HASS achieves 2.81x-3.65x wall-clock time speedup\nratio averaging across three datasets, which is 8%-15% faster than EAGLE-2.\n","authors":["Lefan Zhang","Xiaodan Wang","Yanhua Huang","Ruiwen Xu"],"pdf_url":"https://arxiv.org/pdf/2408.15766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15747v1","updated":"2024-08-28T12:25:45Z","published":"2024-08-28T12:25:45Z","title":"Form and meaning co-determine the realization of tone in Taiwan Mandarin\n  spontaneous speech: the case of Tone 3 sandhi","summary":"  In Standard Chinese, Tone 3 (the dipping tone) becomes Tone 2 (rising tone)\nwhen followed by another Tone 3. Previous studies have noted that this sandhi\nprocess may be incomplete, in the sense that the assimilated Tone 3 is still\ndistinct from a true Tone 2. While Mandarin Tone 3 sandhi is widely studied\nusing carefully controlled laboratory speech (Xu, 1997) and more formal\nregisters of Beijing Mandarin (Yuan and Chen, 2014), less is known about its\nrealization in spontaneous speech, and about the effect of contextual factors\non tonal realization. The present study investigates the pitch contours of\ntwo-character words with T2-T3 and T3-T3 tone patterns in spontaneous Taiwan\nMandarin conversations. Our analysis makes use of the Generative Additive Mixed\nModel (GAMM, Wood, 2017) to examine fundamental frequency (f0) contours as a\nfunction of normalized time. We consider various factors known to influence\npitch contours, including gender, speaking rate, speaker, neighboring tones,\nword position, bigram probability, and also novel predictors, word and word\nsense (Chuang et al., 2024). Our analyses revealed that in spontaneous Taiwan\nMandarin, T3-T3 words become indistinguishable from T2-T3 words, indicating\ncomplete sandhi, once the strong effect of word (or word sense) is taken into\naccount. For our data, the shape of f0 contours is not co-determined by word\nfrequency. In contrast, the effect of word meaning on f0 contours is robust, as\nstrong as the effect of adjacent tones, and is present for both T2-T3 and T3-T3\nwords.\n","authors":["Yuxin Lu","Yu-Ying Chuang","R. Harald Baayen"],"pdf_url":"https://arxiv.org/pdf/2408.15747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14398v2","updated":"2024-08-28T12:03:54Z","published":"2024-08-26T16:29:13Z","title":"Language-specific Calibration for Pruning Multilingual Language Models","summary":"  Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.\n","authors":["Simon Kurz","Jian-Jia Chen","Lucie Flek","Zhixue Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.14398v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15729v1","updated":"2024-08-28T11:44:52Z","published":"2024-08-28T11:44:52Z","title":"LM-PUB-QUIZ: A Comprehensive Framework for Zero-Shot Evaluation of\n  Relational Knowledge in Language Models","summary":"  Knowledge probing evaluates the extent to which a language model (LM) has\nacquired relational knowledge during its pre-training phase. It provides a\ncost-effective means of comparing LMs of different sizes and training setups\nand is useful for monitoring knowledge gained or lost during continual learning\n(CL). In prior work, we presented an improved knowledge probe called BEAR\n(Wiland et al., 2024), which enables the comparison of LMs trained with\ndifferent pre-training objectives (causal and masked LMs) and addresses issues\nof skewed distributions in previous probes to deliver a more unbiased reading\nof LM knowledge. With this paper, we present LM-PUB- QUIZ, a Python framework\nand leaderboard built around the BEAR probing mechanism that enables\nresearchers and practitioners to apply it in their work. It provides options\nfor standalone evaluation and direct integration into the widely-used training\npipeline of the Hugging Face TRANSFORMERS library. Further, it provides a\nfine-grained analysis of different knowledge types to assist users in better\nunderstanding the knowledge in each evaluated LM. We publicly release\nLM-PUB-QUIZ as an open-source project.\n","authors":["Max Ploner","Jacek Wiland","Sebastian Pohl","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2408.15729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15720v1","updated":"2024-08-28T11:36:29Z","published":"2024-08-28T11:36:29Z","title":"An Evaluation of Sindhi Word Embedding in Semantic Analogies and\n  Downstream Tasks","summary":"  In this paper, we propose a new word embedding based corpus consisting of\nmore than 61 million words crawled from multiple web resources. We design a\npreprocessing pipeline for the filtration of unwanted text from crawled data.\nAfterwards, the cleaned vocabulary is fed to state-of-the-art\ncontinuous-bag-of-words, skip-gram, and GloVe word embedding algorithms. For\nthe evaluation of pretrained embeddings, we use popular intrinsic and extrinsic\nevaluation approaches. The evaluation results reveal that\ncontinuous-bag-of-words and skip-gram perform better than GloVe and existing\nSindhi fastText word embedding on both intrinsic and extrinsic evaluation\napproaches\n","authors":["Wazir Ali","Saifullah Tumrani","Jay Kumar","Tariq Rahim Soomro"],"pdf_url":"https://arxiv.org/pdf/2408.15720v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:1911.12579"},{"id":"http://arxiv.org/abs/2408.15710v1","updated":"2024-08-28T11:18:06Z","published":"2024-08-28T11:18:06Z","title":"Conan-embedding: General Text Embedding with More and Better Negative\n  Samples","summary":"  With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark\n","authors":["Shiyu Li","Yang Tang","Shizhe Chen","Xi Chen"],"pdf_url":"https://arxiv.org/pdf/2408.15710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11239v2","updated":"2024-08-28T11:10:59Z","published":"2024-06-17T06:07:32Z","title":"Evading AI-Generated Content Detectors using Homoglyphs","summary":"  The advent of large language models (LLMs) has enabled the generation of text\nthat increasingly exhibits human-like characteristics. As the detection of such\ncontent is of significant importance, numerous studies have been conducted with\nthe aim of developing reliable AI-generated text detectors. These detectors\nhave demonstrated promising results on test data, but recent research has\nrevealed that they can be circumvented by employing different techniques. In\nthis paper, we present homoglyph-based attacks ($a \\rightarrow {\\alpha}$) as a\nmeans of circumventing existing detectors. A comprehensive evaluation was\nconducted to assess the effectiveness of these attacks on seven detectors,\nincluding ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's\ndetector, and watermarking techniques, on five different datasets. Our findings\ndemonstrate that homoglyph-based attacks can effectively circumvent\nstate-of-the-art detectors, leading them to classify all texts as either\nAI-generated or human-written (decreasing the average Matthews Correlation\nCoefficient from 0.64 to -0.01). We then examine the effectiveness of these\nattacks by analyzing how homoglyphs impact different families of detectors.\nFinally, we discuss the implications of these findings and potential defenses\nagainst such attacks.\n","authors":["Aldan Creo","Shushanta Pudasaini"],"pdf_url":"https://arxiv.org/pdf/2406.11239v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11537v3","updated":"2024-08-28T10:39:11Z","published":"2024-02-18T10:36:05Z","title":"Deciphering the Impact of Pretraining Data on Large Language Models\n  through Machine Unlearning","summary":"  Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs.\n","authors":["Yang Zhao","Li Du","Xiao Ding","Kai Xiong","Zhouhao Sun","Jun Shi","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2402.11537v3.pdf","comment":"Accepted by ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2408.15689v1","updated":"2024-08-28T10:25:53Z","published":"2024-08-28T10:25:53Z","title":"TempoFormer: A Transformer for Temporally-aware Representations in\n  Change Detection","summary":"  Dynamic representation learning plays a pivotal role in understanding the\nevolution of linguistic content over time. On this front both context and time\ndynamics as well as their interplay are of prime importance. Current approaches\nmodel context via pre-trained representations, which are typically temporally\nagnostic. Previous work on modeling context and temporal dynamics has used\nrecurrent methods, which are slow and prone to overfitting. Here we introduce\nTempoFormer, the fist task-agnostic transformer-based and temporally-aware\nmodel for dynamic representation learning. Our approach is jointly trained on\ninter and intra context dynamics and introduces a novel temporal variation of\nrotary positional embeddings. The architecture is flexible and can be used as\nthe temporal representation foundation of other models or applied to different\ntransformer-based architectures. We show new SOTA performance on three\ndifferent real-time change detection tasks.\n","authors":["Talia Tseriotou","Adam Tsakalidis","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2408.15689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15666v1","updated":"2024-08-28T09:35:15Z","published":"2024-08-28T09:35:15Z","title":"StyleRemix: Interpretable Authorship Obfuscation via Distillation and\n  Perturbation of Style Elements","summary":"  Authorship obfuscation, rewriting a text to intentionally obscure the\nidentity of the author, is an important but challenging task. Current methods\nusing large language models (LLMs) lack interpretability and controllability,\noften ignoring author-specific stylistic features, resulting in less robust\nperformance overall.\n  To address this, we develop StyleRemix, an adaptive and interpretable\nobfuscation method that perturbs specific, fine-grained style elements of the\noriginal input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA)\nmodules to rewrite an input specifically along various stylistic axes (e.g.,\nformality and length) while maintaining low computational cost. StyleRemix\noutperforms state-of-the-art baselines and much larger LLMs in a variety of\ndomains as assessed by both automatic and human evaluation.\n  Additionally, we release AuthorMix, a large set of 30K high-quality,\nlong-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a\nparallel corpus of 1,500 texts spanning seven style axes in 16 unique\ndirections\n","authors":["Jillian Fisher","Skyler Hallinan","Ximing Lu","Mitchell Gordon","Zaid Harchaoui","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2408.15666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15664v1","updated":"2024-08-28T09:31:09Z","published":"2024-08-28T09:31:09Z","title":"Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts","summary":"  For Mixture-of-Experts (MoE) models, an unbalanced expert load will lead to\nrouting collapse or increased computational overhead. Existing methods commonly\nemploy an auxiliary loss to encourage load balance, but a large auxiliary loss\nwill introduce non-negligible interference gradients into training and thus\nimpair the model performance. In order to control load balance while not\nproducing undesired gradients during training, we propose Loss-Free Balancing,\nfeatured by an auxiliary-loss-free load balancing strategy. To be specific,\nbefore the top-K routing decision, Loss-Free Balancing will first apply an\nexpert-wise bias to the routing scores of each expert. By dynamically updating\nthe bias of each expert according to its recent load, Loss-Free Balancing can\nconsistently maintain a balanced distribution of expert load. In addition,\nsince Loss-Free Balancing does not produce any interference gradients, it also\nelevates the upper bound of model performance gained from MoE training. We\nvalidate the performance of Loss-Free Balancing on MoE models with up to 3B\nparameters trained on up to 200B tokens. Experimental results show that\nLoss-Free Balancing achieves both better performance and better load balance\ncompared with traditional auxiliary-loss-controlled load balancing strategies.\n","authors":["Lean Wang","Huazuo Gao","Chenggang Zhao","Xu Sun","Damai Dai"],"pdf_url":"https://arxiv.org/pdf/2408.15664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15650v1","updated":"2024-08-28T09:07:30Z","published":"2024-08-28T09:07:30Z","title":"Harnessing the Intrinsic Knowledge of Pretrained Language Models for\n  Challenging Text Classification Settings","summary":"  Text classification is crucial for applications such as sentiment analysis\nand toxic text filtering, but it still faces challenges due to the complexity\nand ambiguity of natural language. Recent advancements in deep learning,\nparticularly transformer architectures and large-scale pretraining, have\nachieved inspiring success in NLP fields. Building on these advancements, this\nthesis explores three challenging settings in text classification by leveraging\nthe intrinsic knowledge of pretrained language models (PLMs). Firstly, to\naddress the challenge of selecting misleading yet incorrect distractors for\ncloze questions, we develop models that utilize features based on\ncontextualized word representations from PLMs, achieving performance that\nrivals or surpasses human accuracy. Secondly, to enhance model generalization\nto unseen labels, we create small finetuning datasets with domain-independent\ntask label descriptions, improving model performance and robustness. Lastly, we\ntackle the sensitivity of large language models to in-context learning prompts\nby selecting effective demonstrations, focusing on misclassified examples and\nresolving model ambiguity regarding test example labels.\n","authors":["Lingyu Gao"],"pdf_url":"https://arxiv.org/pdf/2408.15650v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2205.11245v4","updated":"2024-08-28T08:51:57Z","published":"2022-05-18T04:38:15Z","title":"PASH at TREC 2021 Deep Learning Track: Generative Enhanced Model for\n  Multi-stage Ranking","summary":"  This paper describes the PASH participation in TREC 2021 Deep Learning Track.\nIn the recall stage, we adopt a scheme combining sparse and dense retrieval\nmethod. In the multi-stage ranking phase, point-wise and pair-wise ranking\nstrategies are used one after another based on model continual pre-trained on\ngeneral knowledge and document-level data. Compared to TREC 2020 Deep Learning\nTrack, we have additionally introduced the generative model T5 to further\nenhance the performance.\n","authors":["Yixuan Qiao","Hao Chen","Jun Wang","Tuozhen Liu","Xianbin Ye","Xin Tang","Rui Fang","Peng Gao","Wenfeng Xie","Guotong Xie"],"pdf_url":"https://arxiv.org/pdf/2205.11245v4.pdf","comment":"TREC 2021"},{"id":"http://arxiv.org/abs/2405.20770v3","updated":"2024-08-28T08:46:17Z","published":"2024-05-24T07:23:56Z","title":"Large Language Model Sentinel: LLM Agent for Adversarial Purification","summary":"  Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.\n","authors":["Guang Lin","Qibin Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.20770v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15625v1","updated":"2024-08-28T08:25:22Z","published":"2024-08-28T08:25:22Z","title":"CBF-LLM: Safe Control for LLM Alignment","summary":"  This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks.\n","authors":["Yuya Miyaoka","Masaki Inoue"],"pdf_url":"https://arxiv.org/pdf/2408.15625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15616v1","updated":"2024-08-28T08:14:51Z","published":"2024-08-28T08:14:51Z","title":"Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error\n  Rate Computations And Granular Error Classifications","summary":"  The Word Error Rate (WER) is the common measure of accuracy for Automatic\nSpeech Recognition (ASR). Transcripts are usually pre-processed by substituting\nspecific characters to account for non-semantic differences. As a result of\nthis normalisation, information on the accuracy of punctuation or\ncapitalisation is lost. We present a non-destructive, token-based approach\nusing an extended Levenshtein distance algorithm to compute a robust WER and\nadditional orthographic metrics. Transcription errors are also classified more\ngranularly by existing string similarity and phonetic algorithms. An evaluation\non several datasets demonstrates the practical equivalence of our approach\ncompared to common WER computations. We also provide an exemplary analysis of\nderived use cases, such as a punctuation error rate, and a web application for\ninteractive use and visualisation of our implementation. The code is available\nopen-source.\n","authors":["Korbinian Kuhn","Verena Kersken","Gottfried Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2408.15616v1.pdf","comment":"Accepted in INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.18312v4","updated":"2024-08-28T08:07:49Z","published":"2024-06-26T12:51:37Z","title":"AI-native Memory: A Pathway from LLMs Towards AGI","summary":"  Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions.\n","authors":["Jingbo Shang","Zai Zheng","Jiale Wei","Xiang Ying","Felix Tao","Mindverse Team"],"pdf_url":"https://arxiv.org/pdf/2406.18312v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09333v2","updated":"2024-08-28T07:34:37Z","published":"2024-08-18T02:27:25Z","title":"SkyScript-100M: 1,000,000,000 Pairs of Scripts and Shooting Scripts for\n  Short Drama","summary":"  Generating high-quality shooting scripts containing information such as scene\nand shot language is essential for short drama script generation. We collect\n6,660 popular short drama episodes from the Internet, each with an average of\n100 short episodes, and the total number of short episodes is about 80,000,\nwith a total duration of about 2,000 hours and totaling 10 terabytes (TB). We\nperform keyframe extraction and annotation on each episode to obtain about\n10,000,000 shooting scripts. We perform 100 script restorations on the\nextracted shooting scripts based on our self-developed large short drama\ngeneration model SkyReels. This leads to a dataset containing 1,000,000,000\npairs of scripts and shooting scripts for short dramas, called SkyScript-100M.\nWe compare SkyScript-100M with the existing dataset in detail and demonstrate\nsome deeper insights that can be achieved based on SkyScript-100M. Based on\nSkyScript-100M, researchers can achieve several deeper and more far-reaching\nscript optimization goals, which may drive a paradigm shift in the entire field\nof text-to-video and significantly advance the field of short drama video\ngeneration. The data and code are available at\nhttps://github.com/vaew/SkyScript-100M.\n","authors":["Jing Tang","Quanlu Jia","Yuqiang Xie","Zeyu Gong","Xiang Wen","Jiayi Zhang","Yalong Guo","Guibin Chen","Jiangping Yang"],"pdf_url":"https://arxiv.org/pdf/2408.09333v2.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2408.13893v2","updated":"2024-08-28T07:16:37Z","published":"2024-08-25T17:07:39Z","title":"SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with\n  Flow-based Scalar Latent Transformer Diffusion Models","summary":"  Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as\nan effective method for improving the diversity and naturalness of synthesized\nspeech. At the high level, previous large-scale TTS models can be categorized\ninto either Auto-regressive (AR) based (\\textit{e.g.}, VALL-E) or\nNon-auto-regressive (NAR) based models (\\textit{e.g.}, NaturalSpeech 2/3).\nAlthough these works demonstrate good performance, they still have potential\nweaknesses. For instance, AR-based models are plagued by unstable generation\nquality and slow generation speed; meanwhile, some NAR-based models need\nphoneme-level duration alignment information, thereby increasing the complexity\nof data pre-processing, model design, and loss design. In this work, we build\nupon our previous publication by implementing a simple and efficient\nnon-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2\neffectively combines the strengths of both autoregressive (AR) and\nnon-autoregressive (NAR) methods, offering the following key advantages: (1)\nsimplified data preparation; (2) straightforward model and loss design; and (3)\nstable, high-quality generation performance with fast inference speed. Compared\nto our previous publication, we present ({\\romannumeral1}) a detailed analysis\nof the influence of speech tokenizer and noisy label for TTS performance;\n({\\romannumeral2}) four distinct types of sentence duration predictors;\n({\\romannumeral3}) a novel flow-based scalar latent transformer diffusion\nmodel. With these improvement, we show a significant improvement in generation\nperformance and generation speed compared to our previous work and other\nstate-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that\nSimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on\nmultilingual speech datasets. Demos are available on:\n{https://dongchaoyang.top/SimpleSpeech2\\_demo/}.\n","authors":["Dongchao Yang","Rongjie Huang","Yuanyuan Wang","Haohan Guo","Dading Chong","Songxiang Liu","Xixin Wu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2408.13893v2.pdf","comment":"Submit to TASLP"},{"id":"http://arxiv.org/abs/2408.15565v1","updated":"2024-08-28T06:33:03Z","published":"2024-08-28T06:33:03Z","title":"SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large\n  Language Models","summary":"  There is a growing trend of teaching large language models (LLMs) to solve\nmathematical problems through coding. Existing studies primarily focus on\nprompting powerful, closed-source models to generate seed training data\nfollowed by in-domain data augmentation, equipping LLMs with considerable\ncapabilities for code-aided mathematical reasoning. However, continually\ntraining these models on augmented data derived from a few datasets such as\nGSM8K may impair their generalization abilities and restrict their\neffectiveness to a narrow range of question types. Conversely, the potential of\nimproving such LLMs by leveraging large-scale, expert-written, diverse math\nquestion-answer pairs remains unexplored. To utilize these resources and tackle\nunique challenges such as code response assessment, we propose a novel paradigm\nthat uses a code-based critic model to guide steps including question-code data\nconstruction, quality control, and complementary evaluation. We also explore\ndifferent alignment algorithms with self-generated instruction/preference data\nto foster continuous improvement. Experiments across both in-domain (up to\n+5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate\nthe effectiveness of the proposed paradigm.\n","authors":["Dian Yu","Baolin Peng","Ye Tian","Linfeng Song","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2408.15565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15562v1","updated":"2024-08-28T06:28:01Z","published":"2024-08-28T06:28:01Z","title":"Boosting Lossless Speculative Decoding via Feature Sampling and Partial\n  Alignment Distillation","summary":"  Lossless speculative decoding accelerates target large language model (LLM)\ninference by employing a lightweight draft model for generating tree-structured\ncandidates, which are subsequently verified in parallel by the target LLM.\nCurrently, effective approaches leverage feature-level rather than token-level\nautoregression within the draft model to facilitate more straightforward\npredictions and enhanced knowledge distillation. In this paper, we reassess\nthese approaches and propose FSPAD (Feature Sampling and Partial Alignment\nDistillation for Lossless Speculative Decoding), which introduces two\nstraightforward and effective components within the existing framework to boost\nlossless speculative decoding. Firstly, FSPAD utilizes token embeddings to\nsample features of the target LLM in high-dimensional space before feeding them\ninto the draft model, due to the inherent uncertainty of the features\npreventing the draft model from obtaining the specific token output by the\ntarget LLM. Secondly, FSPAD introduces partial alignment distillation to weaken\nthe draft model's connection between features and logits, aiming to reduce the\nconflict between feature alignment and logit confidence during training. Our\nexperiments include both greedy and non-greedy decoding on the largest and\nsmallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in\nmulti-turn conversation, translation, summarization, question answering,\nmathematical reasoning, and retrieval-augmented generation. The results show\nthat FSPAD outperforms the state-of-the-art method across all the\naforementioned tasks and target LLMs.\n","authors":["Lujun Gui","Bin Xiao","Lei Su","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.15562v1.pdf","comment":"The work was not submitted to AAAI 2025"},{"id":"http://arxiv.org/abs/2408.15549v1","updated":"2024-08-28T05:53:46Z","published":"2024-08-28T05:53:46Z","title":"WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback","summary":"  As large language models (LLMs) continue to advance, aligning these models\nwith human preferences has emerged as a critical challenge. Traditional\nalignment methods, relying on human or LLM annotated datasets, are limited by\ntheir resource-intensive nature, inherent subjectivity, and the risk of\nfeedback loops that amplify model biases. To overcome these limitations, we\nintroduce WildFeedback, a novel framework that leverages real-time, in-situ\nuser interactions to create preference datasets that more accurately reflect\nauthentic human values. WildFeedback operates through a three-step process:\nfeedback signal identification, preference data construction, and user-guided\nevaluation. We applied this framework to a large corpus of user-LLM\nconversations, resulting in a rich preference dataset that reflects genuine\nuser preferences. This dataset captures the nuances of user preferences by\nidentifying and classifying feedback signals within natural conversations,\nthereby enabling the construction of more representative and context-sensitive\nalignment data. Our extensive experiments demonstrate that LLMs fine-tuned on\nWildFeedback exhibit significantly improved alignment with user preferences, as\nevidenced by both traditional benchmarks and our proposed user-guided\nevaluation. By incorporating real-time feedback from actual users, WildFeedback\naddresses the scalability, subjectivity, and bias challenges that plague\nexisting approaches, marking a significant step toward developing LLMs that are\nmore responsive to the diverse and evolving needs of their users. In summary,\nWildFeedback offers a robust, scalable solution for aligning LLMs with true\nhuman values, setting a new standard for the development and evaluation of\nuser-centric language models.\n","authors":["Taiwei Shi","Zhuoer Wang","Longqi Yang","Ying-Chun Lin","Zexue He","Mengting Wan","Pei Zhou","Sujay Jauhar","Xiaofeng Xu","Xia Song","Jennifer Neville"],"pdf_url":"https://arxiv.org/pdf/2408.15549v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2408.15545v1","updated":"2024-08-28T05:41:52Z","published":"2024-08-28T05:41:52Z","title":"SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding","summary":"  Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.\n","authors":["Sihang Li","Jian Huang","Jiaxi Zhuang","Yaorui Shi","Xiaochen Cai","Mingjun Xu","Xiang Wang","Linfeng Zhang","Guolin Ke","Hengxing Cai"],"pdf_url":"https://arxiv.org/pdf/2408.15545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15543v1","updated":"2024-08-28T05:36:25Z","published":"2024-08-28T05:36:25Z","title":"An Investigation of Warning Erroneous Chat Translations in Cross-lingual\n  Communication","summary":"  The complexities of chats pose significant challenges for machine translation\nmodels. Recognizing the need for a precise evaluation metric to address the\nissues of chat translation, this study introduces Multidimensional Quality\nMetrics for Chat Translation (MQM-Chat). Through the experiments of five models\nusing MQM-Chat, we observed that all models generated certain fundamental\nerrors, while each of them has different shortcomings, such as omission, overly\ncorrecting ambiguous source content, and buzzword issues, resulting in the loss\nof stylized information. Our findings underscore the effectiveness of MQM-Chat\nin evaluating chat translation, emphasizing the importance of stylized content\nand dialogue consistency for future studies.\n","authors":["Yunmeng Li","Jun Suzuki","Makoto Morishita","Kaori Abe","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2408.15543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08872v2","updated":"2024-08-28T05:03:34Z","published":"2024-08-16T17:57:01Z","title":"xGen-MM (BLIP-3): A Family of Open Large Multimodal Models","summary":"  This report introduces xGen-MM (also known as BLIP-3), a framework for\ndeveloping Large Multimodal Models (LMMs). The framework comprises meticulously\ncurated datasets, a training recipe, model architectures, and a resulting suite\nof LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen\ninitiative on foundation AI models. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\npre-trained base model exhibits strong in-context learning capabilities and the\ninstruction-tuned model demonstrates competitive performance among open-source\nLMMs with similar model sizes. In addition, we introduce a safety-tuned model\nwith DPO, aiming to mitigate harmful behaviors such as hallucinations and\nimprove safety. We open-source our models, curated large-scale datasets, and\nour fine-tuning codebase to facilitate further advancements in LMM research.\nAssociated resources will be available on our project page above.\n","authors":["Le Xue","Manli Shu","Anas Awadalla","Jun Wang","An Yan","Senthil Purushwalkam","Honglu Zhou","Viraj Prabhu","Yutong Dai","Michael S Ryoo","Shrikant Kendre","Jieyu Zhang","Can Qin","Shu Zhang","Chia-Chih Chen","Ning Yu","Juntao Tan","Tulika Manoj Awalgaonkar","Shelby Heinecke","Huan Wang","Yejin Choi","Ludwig Schmidt","Zeyuan Chen","Silvio Savarese","Juan Carlos Niebles","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2408.08872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15533v1","updated":"2024-08-28T04:44:43Z","published":"2024-08-28T04:44:43Z","title":"LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation","summary":"  Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.\n","authors":["Haichuan Hu","Yuhan Sun","Qunjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15518v1","updated":"2024-08-28T04:06:14Z","published":"2024-08-28T04:06:14Z","title":"Dolphin: Long Context as a New Modality for Energy-Efficient On-Device\n  Language Models","summary":"  This paper presents Dolphin, a novel decoder-decoder architecture for\nenergy-efficient processing of long contexts in language models. Our approach\naddresses the significant energy consumption and latency challenges inherent in\non-device models. Dolphin employs a compact 0.5B parameter decoder to distill\nextensive contextual information into a memory embedding, substantially\nreducing the input length for the primary 7B parameter decoder model. Inspired\nby vision-language models, we repurpose the image embedding projector to encode\nlong textual contexts, effectively treating extended context as a distinct\nmodality. This innovative method enables processing of substantially longer\ncontexts without the typical computational overhead associated with extended\ninput sequences. Empirical evaluations demonstrate a 10-fold improvement in\nenergy efficiency and a 5-fold reduction in latency compared to conventional\nfull-length context processing methods without losing quality of the response.\nOur work contributes to the development of more sustainable and scalable\nlanguage models for on-device applications, addressing the critical need for\nenergy-efficient and responsive AI technologies in resource-constrained\nenvironments while maintaining the accuracy to understand long contexts. This\nresearch has implications for the broader field of natural language processing,\nparticularly in the domain of efficient model design for resource-limited\nsettings. By enabling more sophisticated AI capabilities on edge devices,\nDolphin paves the way for advanced language processing in a wide range of\napplications where computational resources are at a premium. The Dolphin model\nis publicly available at https://huggingface.co/NexaAIDev/Dolphin.\n","authors":["Wei Chen","Zhiyuan Li","Shuo Xin","Yihao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.15518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15040v2","updated":"2024-08-28T03:56:37Z","published":"2024-08-27T13:10:05Z","title":"A Survey of Large Language Models for European Languages","summary":"  Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining large language\nmodels.\n","authors":["Wazir Ali","Sampo Pyysalo"],"pdf_url":"https://arxiv.org/pdf/2408.15040v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15512v1","updated":"2024-08-28T03:48:05Z","published":"2024-08-28T03:48:05Z","title":"Towards Fully Autonomous Research Powered by LLMs: Case Study on\n  Simulations","summary":"  The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research, spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLM, through sophisticated API\nintegration, to automate the entire research process, from experimental design,\nremote upload and simulation execution, data analysis, to report compilation.\nUsing a simulation problem of polymer chain conformations as a case study, we\nassessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless\nexecution on designated research missions, underscoring the potential of LLMs\nto manage complete scientific investigations autonomously. The outlined\nautomation can be iteratively performed up to twenty cycles without human\nintervention, illustrating the potential of LLMs for large-scale autonomous\nresearch endeavors. Additionally, we discussed the intrinsic traits of ASAs in\nmanaging extensive tasks, focusing on self-validation mechanisms and the\nbalance between local attention and global oversight.\n","authors":["Zhihan Liu","Yubo Chai","Jianfeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.15512v1.pdf","comment":"For additional code and data, please visit our GitHub repository:\n  https://github.com/zokaraa/autonomous_simulation_agent"},{"id":"http://arxiv.org/abs/2408.07611v2","updated":"2024-08-28T03:47:28Z","published":"2024-08-14T15:19:16Z","title":"WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs","summary":"  Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions.\n","authors":["Weijian Xie","Xuefeng Liang","Yuhui Liu","Kaihua Ni","Hong Cheng","Zetian Hu"],"pdf_url":"https://arxiv.org/pdf/2408.07611v2.pdf","comment":"8 pages, 2 figures, technical report for 3rd place in Task 3 of Meta\n  KDD Cup 2024 CRAG Challenge"},{"id":"http://arxiv.org/abs/2408.15510v1","updated":"2024-08-28T03:45:49Z","published":"2024-08-28T03:45:49Z","title":"Measuring the Reliability of Causal Probing Methods: Tradeoffs,\n  Limitations, and the Plight of Nullifying Interventions","summary":"  Causal probing is an approach to interpreting foundation models, such as\nlarge language models, by training probes to recognize latent properties of\ninterest from embeddings, intervening on probes to modify this representation,\nand analyzing the resulting changes in the model's behavior. While some recent\nworks have cast doubt on the theoretical basis of several leading causal\nprobing intervention methods, it has been unclear how to systematically and\nempirically evaluate their effectiveness in practice. To address this problem,\nwe propose a general empirical analysis framework to evaluate the reliability\nof causal probing interventions, formally defining and quantifying two key\ncausal probing desiderata: completeness (fully transforming the representation\nof the target property) and selectivity (minimally impacting other properties).\nOur formalism allows us to make the first direct comparisons between different\nfamilies of causal probing methods (e.g., linear vs. nonlinear or\ncounterfactual vs. nullifying interventions). We conduct extensive experiments\nacross several leading methods, finding that (1) there is an inherent tradeoff\nbetween these criteria, and no method is able to consistently satisfy both at\nonce; and (2) across the board, nullifying interventions are always far less\ncomplete than counterfactual interventions, indicating that nullifying methods\nmay not be an effective approach to causal probing.\n","authors":["Marc Canby","Adam Davies","Chirag Rastogi","Julia Hockenmaier"],"pdf_url":"https://arxiv.org/pdf/2408.15510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15496v1","updated":"2024-08-28T02:47:27Z","published":"2024-08-28T02:47:27Z","title":"ReMamba: Equip Mamba with Effective Long-Sequence Modeling","summary":"  While the Mamba architecture demonstrates superior inference efficiency and\ncompetitive performance on short-context natural language processing (NLP)\ntasks, empirical evidence suggests its capacity to comprehend long contexts is\nlimited compared to transformer-based models. In this study, we investigate the\nlong-context efficiency issues of the Mamba models and propose ReMamba, which\nenhances Mamba's ability to comprehend long contexts. ReMamba incorporates\nselective compression and adaptation techniques within a two-stage re-forward\nprocess, incurring minimal additional inference costs overhead. Experimental\nresults on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy,\nimproving over the baselines by 3.2 and 1.6 points, respectively, and attaining\nperformance almost on par with same-size transformer models.\n","authors":["Danlong Yuan","Jiahao Liu","Bei Li","Huishuai Zhang","Jingang Wang","Xunliang Cai","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.15496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15491v1","updated":"2024-08-28T02:31:15Z","published":"2024-08-28T02:31:15Z","title":"Enhancing and Accelerating Large Language Models via Instruction-Aware\n  Contextual Compression","summary":"  Large Language Models (LLMs) have garnered widespread attention due to their\nremarkable performance across various tasks. However, to mitigate the issue of\nhallucinations, LLMs often incorporate retrieval-augmented pipeline to provide\nthem with rich external knowledge and context. Nevertheless, challenges stem\nfrom inaccurate and coarse-grained context retrieved from the retriever.\nSupplying irrelevant context to the LLMs can result in poorer responses,\nincreased inference latency, and higher costs. This paper introduces a method\ncalled Instruction-Aware Contextual Compression, which filters out less\ninformative content, thereby accelerating and enhancing the use of LLMs. The\nexperimental results demonstrate that Instruction-Aware Contextual Compression\nnotably reduces memory consumption and minimizes generation latency while\nmaintaining performance levels comparable to those achieved with the use of the\nfull context. Specifically, we achieved a 50% reduction in context-related\ncosts, resulting in a 5% reduction in inference memory usage and a 2.2-fold\nincrease in inference speed, with only a minor drop of 0.047 in Rouge-1. These\nfindings suggest that our method strikes an effective balance between\nefficiency and performance.\n","authors":["Haowen Hou","Fei Ma","Binwen Bai","Xinxin Zhu","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2408.15491v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.15488v1","updated":"2024-08-28T02:27:07Z","published":"2024-08-28T02:27:07Z","title":"Legilimens: Practical and Unified Content Moderation for Large Language\n  Model Services","summary":"  Given the societal impact of unsafe content generated by large language\nmodels (LLMs), ensuring that LLM services comply with safety standards is a\ncrucial concern for LLM service providers. Common content moderation methods\nare limited by an effectiveness-and-efficiency dilemma, where simple models are\nfragile while sophisticated models consume excessive computational resources.\nIn this paper, we reveal for the first time that effective and efficient\ncontent moderation can be achieved by extracting conceptual features from\nchat-oriented LLMs, despite their initial fine-tuning for conversation rather\nthan content moderation. We propose a practical and unified content moderation\nframework for LLM services, named Legilimens, which features both effectiveness\nand efficiency. Our red-team model-based data augmentation enhances the\nrobustness of Legilimens against state-of-the-art jailbreaking. Additionally,\nwe develop a framework to theoretically analyze the cost-effectiveness of\nLegilimens compared to other methods. We have conducted extensive experiments\non five host LLMs, seventeen datasets, and nine jailbreaking methods to verify\nthe effectiveness, efficiency, and robustness of Legilimens against normal and\nadaptive adversaries. A comparison of Legilimens with both commercial and\nacademic baselines demonstrates the superior performance of Legilimens.\nFurthermore, we confirm that Legilimens can be applied to few-shot scenarios\nand extended to multi-label classification tasks.\n","authors":["Jialin Wu","Jiangyi Deng","Shengyuan Pang","Yanjiao Chen","Jiayang Xu","Xinfeng Li","Wenyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2408.15488v1.pdf","comment":"Accepted by ACM Conference on Computer and Communications Security\n  (CCS) 2024"},{"id":"http://arxiv.org/abs/2407.05750v3","updated":"2024-08-28T02:04:24Z","published":"2024-07-08T09:03:12Z","title":"Large Language Models Understand Layout","summary":"  Large language models (LLMs) demonstrate extraordinary abilities in a wide\nrange of natural language processing (NLP) tasks. In this paper, we show that,\nbeyond text understanding capability, LLMs are capable of processing text\nlayouts that are denoted by spatial markers. They are able to answer questions\nthat require explicit spatial perceiving and reasoning, while a drastic\nperformance drop is observed when the spatial markers from the original data\nare excluded. We perform a series of experiments with the GPT-3.5, Baichuan2,\nLlama2 and ChatGLM3 models on various types of layout-sensitive datasets for\nfurther analysis. The experimental results reveal that the layout understanding\nability of LLMs is mainly introduced by the coding data for pretraining, which\nis further enhanced at the instruction-tuning stage. In addition, layout\nunderstanding can be enhanced by integrating low-cost, auto-generated data\napproached by a novel text game. Finally, we show that layout understanding\nability is beneficial for building efficient visual question-answering (VQA)\nsystems.\n","authors":["Weiming Li","Manni Duan","Dong An","Yan Shao"],"pdf_url":"https://arxiv.org/pdf/2407.05750v3.pdf","comment":"This paper has been accepted by ECAI-2024"},{"id":"http://arxiv.org/abs/2408.14895v2","updated":"2024-08-28T01:56:33Z","published":"2024-08-27T09:18:57Z","title":"VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view\n  Videos of Daily Activities","summary":"  Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data\n(e.g., images and videos) into symbols, have attracted attention as resources\nenabling knowledge processing and machine learning across modalities. However,\nthe construction of MMKGs for videos consisting of multiple events, such as\ndaily activities, is still in the early stages. In this paper, we construct an\nMMKG based on synchronized multi-view simulated videos of daily activities.\nBesides representing the content of daily life videos as event-centric\nknowledge, our MMKG also includes frame-by-frame fine-grained changes, such as\nbounding boxes within video frames. In addition, we provide support tools for\nquerying our MMKG. As an application example, we demonstrate that our MMKG\nfacilitates benchmarking vision-language models by providing the necessary\nvision-language datasets for a tailored task.\n","authors":["Shusaku Egami","Takahiro Ugai","Swe Nwe Nwe Htun","Ken Fukuda"],"pdf_url":"https://arxiv.org/pdf/2408.14895v2.pdf","comment":"5 pages, 4 figures, accepted by CIKM2024 Resource Track"},{"id":"http://arxiv.org/abs/2408.16163v1","updated":"2024-08-28T22:51:29Z","published":"2024-08-28T22:51:29Z","title":"FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational\n  Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench","summary":"  This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the\nsafety of Large Language Models (LLMs) against multi-turn conversational\nattacks. Building upon the SORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts by breaking down harmful\nqueries into seemingly innocuous sub-questions. Our approach achieves a maximum\nincrease of +46.22\\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,\nGPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We\ndemonstrate that this technique poses a challenge to current LLM safety\nmeasures and highlights the need for more robust defenses against subtle,\nmulti-turn attacks.\n","authors":["Aman Priyanshu","Supriti Vijay"],"pdf_url":"https://arxiv.org/pdf/2408.16163v1.pdf","comment":"4 pages, 2 tables"},{"id":"http://arxiv.org/abs/2408.16131v1","updated":"2024-08-28T20:36:35Z","published":"2024-08-28T20:36:35Z","title":"Evaluating Computational Representations of Character: An Austen\n  Character Similarity Benchmark","summary":"  Several systems have been developed to extract information about characters\nto aid computational analysis of English literature. We propose character\nsimilarity grouping as a holistic evaluation task for these pipelines. We\npresent AustenAlike, a benchmark suite of character similarities in Jane\nAusten's novels. Our benchmark draws on three notions of character similarity:\na structurally defined notion of similarity; a socially defined notion of\nsimilarity; and an expert defined set extracted from literary criticism.\n  We use AustenAlike to evaluate character features extracted using two\npipelines, BookNLP and FanfictionNLP. We build character representations from\nfour kinds of features and compare them to the three AustenAlike benchmarks and\nto GPT-4 similarity rankings. We find that though computational representations\ncapture some broad similarities based on shared social and narrative roles, the\nexpert pairings in our third benchmark are challenging for all systems,\nhighlighting the subtler aspects of similarity noted by human readers.\n","authors":["Funing Yang","Carolyn Jane Anderson"],"pdf_url":"https://arxiv.org/pdf/2408.16131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16098v1","updated":"2024-08-28T19:03:41Z","published":"2024-08-28T19:03:41Z","title":"Structured Event Reasoning with Large Language Models","summary":"  Reasoning about real-life events is a unifying challenge in AI and NLP that\nhas profound utility in a variety of domains, while fallacy in high-stake\napplications could be catastrophic. Able to work with diverse text in these\ndomains, large language models (LLMs) have proven capable of answering\nquestions and solving problems. However, I show that end-to-end LLMs still\nsystematically fail to reason about complex events, and they lack\ninterpretability due to their black-box nature. To address these issues, I\npropose three general approaches to use LLMs in conjunction with a structured\nrepresentation of events. The first is a language-based representation\ninvolving relations of sub-events that can be learned by LLMs via fine-tuning.\nThe second is a semi-symbolic representation involving states of entities that\ncan be predicted and leveraged by LLMs via few-shot prompting. The third is a\nfully symbolic representation that can be predicted by LLMs trained with\nstructured data and be executed by symbolic solvers. On a suite of event\nreasoning tasks spanning common-sense inference and planning, I show that each\napproach greatly outperforms end-to-end LLMs with more interpretability. These\nresults suggest manners of synergy between LLMs and structured representations\nfor event reasoning and beyond.\n","authors":["Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.16098v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2408.16089v1","updated":"2024-08-28T18:43:07Z","published":"2024-08-28T18:43:07Z","title":"Is Personality Prediction Possible Based on Reddit Comments?","summary":"  In this assignment, we examine whether there is a correlation between the\npersonality type of a person and the texts they wrote. In order to do this, we\naggregated datasets of Reddit comments labeled with the Myers-Briggs Type\nIndicator (MBTI) of the author and built different supervised classifiers based\non BERT to try to predict the personality of an author given a text. Despite\nexperiencing issues with the unfiltered character of the dataset, we can\nobserve potential in the classification.\n","authors":["Robert Deimann","Till Preidt","Shaptarshi Roy","Jan Stanicki"],"pdf_url":"https://arxiv.org/pdf/2408.16089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16081v1","updated":"2024-08-28T18:25:35Z","published":"2024-08-28T18:25:35Z","title":"Logic-Enhanced Language Model Agents for Trustworthy Social Simulations","summary":"  We introduce the Logic-Enhanced Language Model Agents (LELMA) framework, a\nnovel approach to enhance the trustworthiness of social simulations that\nutilize large language models (LLMs). While LLMs have gained attention as\nagents for simulating human behaviour, their applicability in this role is\nlimited by issues such as inherent hallucinations and logical inconsistencies.\nLELMA addresses these challenges by integrating LLMs with symbolic AI, enabling\nlogical verification of the reasoning generated by LLMs. This verification\nprocess provides corrective feedback, refining the reasoning output. The\nframework consists of three main components: an LLM-Reasoner for producing\nstrategic reasoning, an LLM-Translator for mapping natural language reasoning\nto logic queries, and a Solver for evaluating these queries. This study focuses\non decision-making in game-theoretic scenarios as a model of human interaction.\nExperiments involving the Hawk-Dove game, Prisoner's Dilemma, and Stag Hunt\nhighlight the limitations of state-of-the-art LLMs, GPT-4 Omni and Gemini 1.0\nPro, in producing correct reasoning in these contexts. LELMA demonstrates high\naccuracy in error detection and improves the reasoning correctness of LLMs via\nself-refinement, particularly in GPT-4 Omni.\n","authors":["Agnieszka Mensfelt","Kostas Stathis","Vince Trencsenyi"],"pdf_url":"https://arxiv.org/pdf/2408.16081v1.pdf","comment":"Source code: https://github.com/dicelab-rhul/LELMA"},{"id":"http://arxiv.org/abs/2408.16073v1","updated":"2024-08-28T18:14:39Z","published":"2024-08-28T18:14:39Z","title":"Using Large Language Models to Create AI Personas for Replication and\n  Prediction of Media Effects: An Empirical Test of 133 Published Experimental\n  Research Findings","summary":"  This report analyzes the potential for large language models (LLMs) to\nexpedite accurate replication of published message effects studies. We tested\nLLM-powered participants (personas) by replicating 133 experimental findings\nfrom 14 papers containing 45 recent studies in the Journal of Marketing\n(January 2023-May 2024). We used a new software tool, Viewpoints AI\n(https://viewpoints.ai/), that takes study designs, stimuli, and measures as\ninput, automatically generates prompts for LLMs to act as a specified sample of\nunique personas, and collects their responses to produce a final output in the\nform of a complete dataset and statistical analysis. The underlying LLM used\nwas Anthropic's Claude Sonnet 3.5. We generated 19,447 AI personas to replicate\nthese studies with the exact same sample attributes, study designs, stimuli,\nand measures reported in the original human research. Our LLM replications\nsuccessfully reproduced 76% of the original main effects (84 out of 111),\ndemonstrating strong potential for AI-assisted replication of studies in which\npeople respond to media stimuli. When including interaction effects, the\noverall replication rate was 68% (90 out of 133). The use of LLMs to replicate\nand accelerate marketing research on media effects is discussed with respect to\nthe replication crisis in social science, potential solutions to\ngeneralizability problems in sampling subjects and experimental conditions, and\nthe ability to rapidly test consumer responses to various media stimuli. We\nalso address the limitations of this approach, particularly in replicating\ncomplex interaction effects in media response studies, and suggest areas for\nfuture research and improvement in AI-assisted experimental replication of\nmedia effects.\n","authors":["Leo Yeykelis","Kaavya Pichai","James J. Cummings","Byron Reeves"],"pdf_url":"https://arxiv.org/pdf/2408.16073v1.pdf","comment":"24 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.14028v2","updated":"2024-08-28T18:06:50Z","published":"2024-08-26T05:38:27Z","title":"SurGen: Text-Guided Diffusion Model for Surgical Video Generation","summary":"  Diffusion-based video generation models have made significant strides,\nproducing outputs with improved visual fidelity, temporal coherence, and user\ncontrol. These advancements hold great promise for improving surgical education\nby enabling more realistic, diverse, and interactive simulation environments.\nIn this study, we introduce SurGen, a text-guided diffusion model tailored for\nsurgical video synthesis, producing the highest resolution and longest duration\nvideos among existing surgical video generation models. We validate the visual\nand temporal quality of the outputs using standard image and video generation\nmetrics. Additionally, we assess their alignment to the corresponding text\nprompts through a deep learning classifier trained on surgical data. Our\nresults demonstrate the potential of diffusion models to serve as valuable\neducational tools for surgical trainees.\n","authors":["Joseph Cho","Samuel Schmidgall","Cyril Zakka","Mrudang Mathur","Rohan Shad","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2408.14028v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.15998v1","updated":"2024-08-28T17:59:31Z","published":"2024-08-28T17:59:31Z","title":"Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders","summary":"  The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle\n","authors":["Min Shi","Fuxiao Liu","Shihao Wang","Shijia Liao","Subhashree Radhakrishnan","De-An Huang","Hongxu Yin","Karan Sapra","Yaser Yacoob","Humphrey Shi","Bryan Catanzaro","Andrew Tao","Jan Kautz","Zhiding Yu","Guilin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15998v1.pdf","comment":"Github: https://github.com/NVlabs/Eagle, HuggingFace:\n  https://huggingface.co/NVEagle"},{"id":"http://arxiv.org/abs/2408.15996v1","updated":"2024-08-28T17:59:05Z","published":"2024-08-28T17:59:05Z","title":"Spatio-Temporal Context Prompting for Zero-Shot Action Detection","summary":"  Spatio-temporal action detection encompasses the tasks of localizing and\nclassifying individual actions within a video. Recent works aim to enhance this\nprocess by incorporating interaction modeling, which captures the relationship\nbetween people and their surrounding context. However, these approaches have\nprimarily focused on fully-supervised learning, and the current limitation lies\nin the lack of generalization capability to recognize unseen action categories.\nIn this paper, we aim to adapt the pretrained image-language models to detect\nunseen actions. To this end, we propose a method which can effectively leverage\nthe rich knowledge of visual-language models to perform Person-Context\nInteraction. Meanwhile, our Context Prompting module will utilize contextual\ninformation to prompt labels, thereby enhancing the generation of more\nrepresentative text features. Moreover, to address the challenge of recognizing\ndistinct actions by multiple people at the same timestamp, we design the\nInterest Token Spotting mechanism which employs pretrained visual knowledge to\nfind each person's interest context tokens, and then these tokens will be used\nfor prompting to generate text features tailored to each individual. To\nevaluate the ability to detect unseen actions, we propose a comprehensive\nbenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our\nmethod achieves superior results compared to previous approaches and can be\nfurther extended to multi-action videos, bringing it closer to real-world\napplications. The code and data can be found in\nhttps://webber2933.github.io/ST-CLIP-project-page.\n","authors":["Wei-Jhe Huang","Min-Hung Chen","Shang-Hong Lai"],"pdf_url":"https://arxiv.org/pdf/2408.15996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15995v1","updated":"2024-08-28T17:59:02Z","published":"2024-08-28T17:59:02Z","title":"TEDRA: Text-based Editing of Dynamic and Photoreal Actors","summary":"  Over the past years, significant progress has been made in creating\nphotorealistic and drivable 3D avatars solely from videos of real humans.\nHowever, a core remaining challenge is the fine-grained and user-friendly\nediting of clothing styles by means of textual descriptions. To this end, we\npresent TEDRA, the first method allowing text-based edits of an avatar, which\nmaintains the avatar's high fidelity, space-time coherency, as well as\ndynamics, and enables skeletal pose and view control. We begin by training a\nmodel to create a controllable and high-fidelity digital replica of the real\nactor. Next, we personalize a pretrained generative diffusion model by\nfine-tuning it on various frames of the real character captured from different\ncamera angles, ensuring the digital representation faithfully captures the\ndynamics and movements of the real person. This two-stage process lays the\nfoundation for our approach to dynamic human avatar editing. Utilizing this\npersonalized diffusion model, we modify the dynamic avatar based on a provided\ntext prompt using our Personalized Normal Aligned Score Distillation Sampling\n(PNA-SDS) within a model-based guidance framework. Additionally, we propose a\ntime step annealing strategy to ensure high-quality edits. Our results\ndemonstrate a clear improvement over prior work in functionality and visual\nquality.\n","authors":["Basavaraj Sunagad","Heming Zhu","Mohit Mendiratta","Adam Kortylewski","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2408.15995v1.pdf","comment":"For project page, see this https://vcai.mpi-inf.mpg.de/projects/Tedra"},{"id":"http://arxiv.org/abs/2408.15994v1","updated":"2024-08-28T17:58:54Z","published":"2024-08-28T17:58:54Z","title":"Perceive-IR: Learning to Perceive Degradation Better for All-in-One\n  Image Restoration","summary":"  The limitations of task-specific and general image restoration methods for\nspecific degradation have prompted the development of all-in-one image\nrestoration techniques. However, the diversity of patterns among multiple\ndegradation, along with the significant uncertainties in mapping between\ndegraded images of different severities and their corresponding undistorted\nversions, pose significant challenges to the all-in-one restoration tasks. To\naddress these challenges, we propose Perceive-IR, an all-in-one image restorer\ndesigned to achieve fine-grained quality control that enables restored images\nto more closely resemble their undistorted counterparts, regardless of the type\nor severity of degradation. Specifically, Perceive-IR contains two stages: (1)\nprompt learning stage and (2) restoration stage. In the prompt learning stage,\nwe leverage prompt learning to acquire a fine-grained quality perceiver capable\nof distinguishing three-tier quality levels by constraining the prompt-image\nsimilarity in the CLIP perception space. Subsequently, this quality perceiver\nand difficulty-adaptive perceptual loss are integrated as a quality-aware\nlearning strategy to realize fine-grained quality control in restoration stage.\nFor the restoration stage, a semantic guidance module (SGM) and compact feature\nextraction (CFE) are proposed to further promote the restoration process by\nutilizing the robust semantic information from the pre-trained large scale\nvision models and distinguishing degradation-specific features. Extensive\nexperiments demonstrate that our Perceive-IR outperforms state-of-the-art\nmethods in all-in-one image restoration tasks and exhibit superior\ngeneralization ability when dealing with unseen tasks.\n","authors":["Xu Zhang","Jiaqi Ma","Guoli Wang","Qian Zhang","Huan Zhang","Lefei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15994v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.15993v1","updated":"2024-08-28T17:58:53Z","published":"2024-08-28T17:58:53Z","title":"ClimDetect: A Benchmark Dataset for Climate Change Detection and\n  Attribution","summary":"  Detecting and attributing temperature increases due to climate change is\ncrucial for understanding global warming and guiding adaptation strategies. The\ncomplexity of distinguishing human-induced climate signals from natural\nvariability has challenged traditional detection and attribution (D&A)\napproaches, which seek to identify specific \"fingerprints\" in climate response\nvariables. Deep learning offers potential for discerning these complex patterns\nin expansive spatial datasets. However, lack of standard protocols has hindered\nconsistent comparisons across studies. We introduce ClimDetect, a standardized\ndataset of over 816k daily climate snapshots, designed to enhance model\naccuracy in identifying climate change signals. ClimDetect integrates various\ninput and target variables used in past research, ensuring comparability and\nconsistency. We also explore the application of vision transformers (ViT) to\nclimate data, a novel and modernizing approach in this context. Our open-access\ndata and code serve as a benchmark for advancing climate science through\nimproved model evaluations. ClimDetect is publicly accessible via Huggingface\ndataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.\n","authors":["Sungduk Yu","Brian L. White","Anahita Bhiwandiwalla","Musashi Hinck","Matthew Lyle Olson","Tung Nguyen","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2408.15993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15992v1","updated":"2024-08-28T17:58:39Z","published":"2024-08-28T17:58:39Z","title":"CoGen: Learning from Feedback with Coupled Comprehension and Generation","summary":"  Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.\n","authors":["Mustafa Omer Gul","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.15992v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.15991v1","updated":"2024-08-28T17:58:17Z","published":"2024-08-28T17:58:17Z","title":"Distribution Backtracking Builds A Faster Convergence Trajectory for\n  One-step Diffusion Distillation","summary":"  Accelerating the sampling speed of diffusion models remains a significant\nchallenge. Recent score distillation methods distill a heavy teacher model into\nan one-step student generator, which is optimized by calculating the difference\nbetween the two score functions on the samples generated by the student model.\nHowever, there is a score mismatch issue in the early stage of the distillation\nprocess, because existing methods mainly focus on using the endpoint of\npre-trained diffusion models as teacher models, overlooking the importance of\nthe convergence trajectory between the student generator and the teacher model.\nTo address this issue, we extend the score distillation process by introducing\nthe entire convergence trajectory of teacher models and propose Distribution\nBacktracking Distillation (DisBack) for distilling student generators. DisBask\nis composed of two stages: Degradation Recording and Distribution Backtracking.\nDegradation Recording is designed to obtain the convergence trajectory of\nteacher models, which records the degradation path from the trained teacher\nmodel to the untrained initial student generator. The degradation path\nimplicitly represents the intermediate distributions of teacher models. Then\nDistribution Backtracking trains a student generator to backtrack the\nintermediate distributions for approximating the convergence trajectory of\nteacher models. Extensive experiments show that DisBack achieves faster and\nbetter convergence than the existing distillation method and accomplishes\ncomparable generation performance. Notably, DisBack is easy to implement and\ncan be generalized to existing distillation methods to boost performance. Our\ncode is publicly available on https://github.com/SYZhang0805/DisBack.\n","authors":["Shengyuan Zhang","Ling Yang","Zejian Li","An Zhao","Chenye Meng","Changyuan Yang","Guang Yang","Zhiyuan Yang","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2408.15991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15966v1","updated":"2024-08-28T17:38:44Z","published":"2024-08-28T17:38:44Z","title":"More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding","summary":"  Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.\n","authors":["Yuan Tang","Xu Han","Xianzhi Li","Qiao Yu","Jinfeng Xu","Yixue Hao","Long Hu","Min Chen"],"pdf_url":"https://arxiv.org/pdf/2408.15966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15958v1","updated":"2024-08-28T17:20:56Z","published":"2024-08-28T17:20:56Z","title":"Efficient Slice Anomaly Detection Network for 3D Brain MRI Volume","summary":"  Current anomaly detection methods excel with benchmark industrial data but\nstruggle with natural images and medical data due to varying definitions of\n'normal' and 'abnormal.' This makes accurate identification of deviations in\nthese fields particularly challenging. Especially for 3D brain MRI data, all\nthe state-of-the-art models are reconstruction-based with 3D convolutional\nneural networks which are memory-intensive, time-consuming and producing noisy\noutputs that require further post-processing. We propose a framework called\nSimple Slice-based Network (SimpleSliceNet), which utilizes a model pre-trained\non ImageNet and fine-tuned on a separate MRI dataset as a 2D slice feature\nextractor to reduce computational cost. We aggregate the extracted features to\nperform anomaly detection tasks on 3D brain MRI volumes. Our model integrates a\nconditional normalizing flow to calculate log likelihood of features and\nemploys the Semi-Push-Pull Mechanism to enhance anomaly detection accuracy. The\nresults indicate improved performance, showcasing our model's remarkable\nadaptability and effectiveness when addressing the challenges exists in brain\nMRI data. In addition, for the large-scale 3D brain volumes, our model\nSimpleSliceNet outperforms the state-of-the-art 2D and 3D models in terms of\naccuracy, memory usage and time consumption. Code is available at:\nhttps://anonymous.4open.science/r/SimpleSliceNet-8EA3.\n","authors":["Zeduo Zhang","Yalda Mohsenzadeh"],"pdf_url":"https://arxiv.org/pdf/2408.15958v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.13818v2","updated":"2024-08-28T17:19:34Z","published":"2024-08-25T12:22:50Z","title":"HER2 and FISH Status Prediction in Breast Biopsy H&E-Stained Images\n  Using Deep Learning","summary":"  The current standard for detecting human epidermal growth factor receptor 2\n(HER2) status in breast cancer patients relies on HER2 amplification,\nidentified through fluorescence in situ hybridization (FISH) or\nimmunohistochemistry (IHC). However, hematoxylin and eosin (H\\&E) tumor stains\nare more widely available, and accurately predicting HER2 status using H\\&E\ncould reduce costs and expedite treatment selection. Deep Learning algorithms\nfor H&E have shown effectiveness in predicting various cancer features and\nclinical outcomes, including moderate success in HER2 status prediction. In\nthis work, we employed a customized weak supervision classification technique\ncombined with MoCo-v2 contrastive learning to predict HER2 status. We trained\nour pipeline on 182 publicly available H&E Whole Slide Images (WSIs) from The\nCancer Genome Atlas (TCGA), for which annotations by the pathology team at Yale\nSchool of Medicine are publicly available. Our pipeline achieved an Area Under\nthe Curve (AUC) of 0.85 across four different test folds. Additionally, we\ntested our model on 44 H&E slides from the TCGA-BRCA dataset, which had an HER2\nscore of 2+ and included corresponding HER2 status and FISH test results. These\ncases are considered equivocal for IHC, requiring an expensive FISH test on\ntheir IHC slides for disambiguation. Our pipeline demonstrated an AUC of 0.81\non these challenging H&E slides. Reducing the need for FISH test can have\nsignificant implications in cancer treatment equity for underserved\npopulations.\n","authors":["Ardhendu Sekhar","Vrinda Goel","Garima Jain","Abhijeet Patil","Ravi Kant Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2408.13818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15956v1","updated":"2024-08-28T17:17:20Z","published":"2024-08-28T17:17:20Z","title":"Generating Binary Species Range Maps","summary":"  Accurately predicting the geographic ranges of species is crucial for\nassisting conservation efforts. Traditionally, range maps were manually created\nby experts. However, species distribution models (SDMs) and, more recently,\ndeep learning-based variants offer a potential automated alternative. Deep\nlearning-based SDMs generate a continuous probability representing the\npredicted presence of a species at a given location, which must be binarized by\nsetting per-species thresholds to obtain binary range maps. However, selecting\nappropriate per-species thresholds to binarize these predictions is non-trivial\nas different species can require distinct thresholds. In this work, we evaluate\ndifferent approaches for automatically identifying the best thresholds for\nbinarizing range maps using presence-only data. This includes approaches that\nrequire the generation of additional pseudo-absence data, along with ones that\nonly require presence data. We also propose an extension of an existing\npresence-only technique that is more robust to outliers. We perform a detailed\nevaluation of different thresholding techniques on the tasks of binary range\nestimation and large-scale fine-grained visual classification, and we\ndemonstrate improved performance over existing pseudo-absence free approaches\nusing our method.\n","authors":["Filip Dorm","Christian Lange","Scott Loarie","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2408.15956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15955v1","updated":"2024-08-28T17:14:51Z","published":"2024-08-28T17:14:51Z","title":"Fall Detection for Smart Living using YOLOv5","summary":"  This work introduces a fall detection system using the YOLOv5mu model, which\nachieved a mean average precision (mAP) of 0.995, demonstrating exceptional\naccuracy in identifying fall events within smart home environments. Enhanced by\nadvanced data augmentation techniques, the model demonstrates significant\nrobustness and adaptability across various conditions. The integration of\nYOLOv5mu offers precise, real-time fall detection, which is crucial for\nimproving safety and emergency response for residents. Future research will\nfocus on refining the system by incorporating contextual data and exploring\nmulti-sensor approaches to enhance its performance and practical applicability\nin diverse environments.\n","authors":["Gracile Astlin Pereira"],"pdf_url":"https://arxiv.org/pdf/2408.15955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15954v1","updated":"2024-08-28T17:14:21Z","published":"2024-08-28T17:14:21Z","title":"InstanSeg: an embedding-based instance segmentation algorithm optimized\n  for accurate, efficient and portable cell segmentation","summary":"  Cell and nucleus segmentation are fundamental tasks for quantitative bioimage\nanalysis. Despite progress in recent years, biologists and other domain experts\nstill require novel algorithms to handle increasingly large and complex\nreal-world datasets. These algorithms must not only achieve state-of-the-art\naccuracy, but also be optimized for efficiency, portability and\nuser-friendliness. Here, we introduce InstanSeg: a novel embedding-based\ninstance segmentation pipeline designed to identify cells and nuclei in\nmicroscopy images. Using six public cell segmentation datasets, we demonstrate\nthat InstanSeg can significantly improve accuracy when compared to the most\nwidely used alternative methods, while reducing the processing time by at least\n60%. Furthermore, InstanSeg is designed to be fully serializable as TorchScript\nand supports GPU acceleration on a range of hardware. We provide an open-source\nimplementation of InstanSeg in Python, in addition to a user-friendly,\ninteractive QuPath extension for inference written in Java. Our code and\npre-trained models are available at https://github.com/instanseg/instanseg .\n","authors":["Thibaut Goldsborough","Ben Philps","Alan O'Callaghan","Fiona Inglis","Leo Leplat","Andrew Filby","Hakan Bilen","Peter Bankhead"],"pdf_url":"https://arxiv.org/pdf/2408.15954v1.pdf","comment":"12 pages,6 figures"},{"id":"http://arxiv.org/abs/2408.15947v1","updated":"2024-08-28T17:05:38Z","published":"2024-08-28T17:05:38Z","title":"Auxiliary Input in Training: Incorporating Catheter Features into Deep\n  Learning Models for ECG-Free Dynamic Coronary Roadmapping","summary":"  Dynamic coronary roadmapping is a technology that overlays the vessel maps\n(the \"roadmap\") extracted from an offline image sequence of X-ray angiography\nonto a live stream of X-ray fluoroscopy in real-time. It aims to offer\nnavigational guidance for interventional surgeries without the need for\nrepeated contrast agent injections, thereby reducing the risks associated with\nradiation exposure and kidney failure. The precision of the roadmaps is\ncontingent upon the accurate alignment of angiographic and fluoroscopic images\nbased on their cardiac phases, as well as precise catheter tip tracking. The\nformer ensures the selection of a roadmap that closely matches the vessel shape\nin the current frame, while the latter uses catheter tips as reference points\nto adjust for translational motion between the roadmap and the present vessel\ntree. Training deep learning models for both tasks is challenging and\nunderexplored. However, incorporating catheter features into the models could\noffer substantial benefits, given humans heavily rely on catheters to complete\nthe tasks. To this end, we introduce a simple but effective method, auxiliary\ninput in training (AIT), and demonstrate that it enhances model performance\nacross both tasks, outperforming baseline methods in knowledge incorporation\nand transfer learning.\n","authors":["Yikang Liu","Lin Zhao","Eric Z. Chen","Xiao Chen","Terrence Chen","Shanhui Sun"],"pdf_url":"https://arxiv.org/pdf/2408.15947v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.15946v1","updated":"2024-08-28T17:04:56Z","published":"2024-08-28T17:04:56Z","title":"Sigma Flows for Image and Data Labeling and Learning Structured\n  Prediction","summary":"  This paper introduces the sigma flow model for the prediction of structured\nlabelings of data observed on Riemannian manifolds, including Euclidean image\ndomains as special case. The approach combines the Laplace-Beltrami framework\nfor image denoising and enhancement, introduced by Sochen, Kimmel and Malladi\nabout 25 years ago, and the assignment flow approach introduced and studied by\nthe authors.\n  The sigma flow arises as Riemannian gradient flow of generalized harmonic\nenergies and thus is governed by a nonlinear geometric PDE which determines a\nharmonic map from a closed Riemannian domain manifold to a statistical\nmanifold, equipped with the Fisher-Rao metric from information geometry. A\nspecific ingredient of the sigma flow is the mutual dependency of the\nRiemannian metric of the domain manifold on the evolving state. This makes the\napproach amenable to machine learning in a specific way, by realizing this\ndependency through a mapping with compact time-variant parametrization that can\nbe learned from data. Proof of concept experiments demonstrate the expressivity\nof the sigma flow model and prediction performance.\n  Structural similarities to transformer network architectures and networks\ngenerated by the geometric integration of sigma flows are pointed out, which\nhighlights the connection to deep learning and, conversely, may stimulate the\nuse of geometric design principles for structured prediction in other areas of\nscientific machine learning.\n","authors":["Jonas Cassel","Bastian Boll","Stefania Petra","Peter Albers","Christoph SchnÃ¶rr"],"pdf_url":"https://arxiv.org/pdf/2408.15946v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2305.12437v4","updated":"2024-08-28T16:56:02Z","published":"2023-05-21T11:51:09Z","title":"SCP: Soft Conditional Prompt Learning for Aerial Video Action\n  Recognition","summary":"  We present a new learning approach, Soft Conditional Prompt Learning (SCP),\nwhich leverages the strengths of prompt learning for aerial video action\nrecognition. Our approach is designed to predict the action of each agent by\nhelping the models focus on the descriptions or instructions associated with\nactions in the input videos for aerial/robot visual perception. Our formulation\nsupports various prompts, including learnable prompts, auxiliary visual\ninformation, and large vision models to improve the recognition performance. We\npresent a soft conditional prompt method that learns to dynamically generate\nprompts from a pool of prompt experts under different video inputs. By sharing\nthe same objective with the task, our proposed SCP can optimize prompts that\nguide the model's predictions while explicitly learning input-invariant (prompt\nexperts pool) and input-specific (data-dependent) prompt knowledge. In\npractice, we observe a 3.17-10.2% accuracy improvement on the aerial video\ndatasets (Okutama, NECDrone), which consist of scenes with single-agent and\nmulti-agent actions. We further evaluate our approach on ground camera videos\nto verify the effectiveness and generalization and achieve a 1.0-3.6%\nimprovement on dataset SSV2. We integrate our method into the ROS2 as well.\n","authors":["Xijun Wang","Ruiqi Xian","Tianrui Guan","Fuxiao Liu","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2305.12437v4.pdf","comment":"IROS2024"},{"id":"http://arxiv.org/abs/2402.09786v4","updated":"2024-08-28T16:48:06Z","published":"2024-02-15T08:34:21Z","title":"Examining Pathological Bias in a Generative Adversarial Network\n  Discriminator: A Case Study on a StyleGAN3 Model","summary":"  Generative adversarial networks (GANs) generate photorealistic faces that are\noften indistinguishable by humans from real faces. While biases in machine\nlearning models are often assumed to be due to biases in training data, we find\npathological internal color and luminance biases in the discriminator of a\npre-trained StyleGAN3-r model that are not explicable by the training data. We\nalso find that the discriminator systematically stratifies scores by both\nimage- and face-level qualities and that this disproportionately affects images\nacross gender, race, and other categories. We examine axes common in research\non stereotyping in social psychology.\n","authors":["Alvin Grissom II","Ryan F. Lei","Matt Gusdorff","Jeova Farias Sales Rocha Neto","Bailey Lin","Ryan Trotter"],"pdf_url":"https://arxiv.org/pdf/2402.09786v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15924v1","updated":"2024-08-28T16:36:23Z","published":"2024-08-28T16:36:23Z","title":"Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot\n  Learning","summary":"  Few-shot image classification is a challenging task in the field of machine\nlearning, involving the identification of new categories using a limited number\nof labeled samples. In recent years, methods based on local descriptors have\nmade significant progress in this area. However, the key to improving\nclassification accuracy lies in effectively filtering background noise and\naccurately selecting critical local descriptors highly relevant to image\ncategory information.\n  To address this challenge, we propose an innovative weighted adaptive\nthreshold filtering (WATF) strategy for local descriptors. This strategy can\ndynamically adjust based on the current task and image context, thereby\nselecting local descriptors most relevant to the image category. This enables\nthe model to better focus on category-related information while effectively\nmitigating interference from irrelevant background regions.\n  To evaluate the effectiveness of our method, we adopted the N-way K-shot\nexperimental framework. Experimental results show that our method not only\nimproves the clustering effect of selected local descriptors but also\nsignificantly enhances the discriminative ability between image categories.\nNotably, our method maintains a simple and lightweight design philosophy\nwithout introducing additional learnable parameters. This feature ensures\nconsistency in filtering capability during both training and testing phases,\nfurther enhancing the reliability and practicality of the method.\n","authors":["Bingchen Yan"],"pdf_url":"https://arxiv.org/pdf/2408.15924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15922v1","updated":"2024-08-28T16:36:09Z","published":"2024-08-28T16:36:09Z","title":"DiffAge3D: Diffusion-based 3D-aware Face Aging","summary":"  Face aging is the process of converting an individual's appearance to a\nyounger or older version of themselves. Existing face aging techniques have\nbeen limited to 2D settings, which often weaken their applications as there is\na growing demand for 3D face modeling. Moreover, existing aging methods\nstruggle to perform faithful aging, maintain identity, and retain the fine\ndetails of the input images. Given these limitations and the need for a\n3D-aware aging method, we propose DiffAge3D, the first 3D-aware aging framework\nthat not only performs faithful aging and identity preservation but also\noperates in a 3D setting. Our aging framework allows to model the aging and\ncamera pose separately by only taking a single image with a target age. Our\nframework includes a robust 3D-aware aging dataset generation pipeline by\nutilizing a pre-trained 3D GAN and the rich text embedding capabilities within\nCLIP model. Notably, we do not employ any inversion bottleneck in dataset\ngeneration. Instead, we randomly generate training samples from the latent\nspace of 3D GAN, allowing us to manipulate the rich latent space of GAN to\ngenerate ages even with large gaps. With the generated dataset, we train a\nviewpoint-aware diffusion-based aging model to control the camera pose and\nfacial age. Through quantitative and qualitative evaluations, we demonstrate\nthat DiffAge3D outperforms existing methods, particularly in\nmultiview-consistent aging and fine details preservation.\n","authors":["Junaid Wahid","Fangneng Zhan","Pramod Rao","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2408.15922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15915v1","updated":"2024-08-28T16:28:07Z","published":"2024-08-28T16:28:07Z","title":"Leveraging Open Knowledge for Advancing Task Expertise in Large Language\n  Models","summary":"  The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Codes and models\nwill be released later.\n","authors":["Yuncheng Yang","Yulei Qin","Tong Wu","Zihan Xu","Gang Li","Pengcheng Guo","Hang Shao","Yucheng Shi","Ke Li","Xing Sun","Jie Yang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2408.15915v1.pdf","comment":"28 pages, 12 tables, 10 figures"},{"id":"http://arxiv.org/abs/2408.15914v1","updated":"2024-08-28T16:27:58Z","published":"2024-08-28T16:27:58Z","title":"CoRe: Context-Regularized Text Embedding Learning for Text-to-Image\n  Personalization","summary":"  Recent advances in text-to-image personalization have enabled high-quality\nand controllable image synthesis for user-provided concepts. However, existing\nmethods still struggle to balance identity preservation with text alignment.\nOur approach is based on the fact that generating prompt-aligned images\nrequires a precise semantic understanding of the prompt, which involves\naccurately processing the interactions between the new concept and its\nsurrounding context tokens within the CLIP text encoder. To address this, we\naim to embed the new concept properly into the input embedding space of the\ntext encoder, allowing for seamless integration with existing tokens. We\nintroduce Context Regularization (CoRe), which enhances the learning of the new\nconcept's text embedding by regularizing its context tokens in the prompt. This\nis based on the insight that appropriate output vectors of the text encoder for\nthe context tokens can only be achieved if the new concept's text embedding is\ncorrectly learned. CoRe can be applied to arbitrary prompts without requiring\nthe generation of corresponding images, thus improving the generalization of\nthe learned text embedding. Additionally, CoRe can serve as a test-time\noptimization technique to further enhance the generations for specific prompts.\nComprehensive experiments demonstrate that our method outperforms several\nbaseline methods in both identity preservation and text alignment. Code will be\nmade publicly available.\n","authors":["Feize Wu","Yun Pang","Junyi Zhang","Lianyu Pang","Jian Yin","Baoquan Zhao","Qing Li","Xudong Mao"],"pdf_url":"https://arxiv.org/pdf/2408.15914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01090v3","updated":"2024-08-28T16:23:19Z","published":"2023-11-02T08:55:11Z","title":"Infusion: internal diffusion for inpainting of dynamic textures and\n  complex motion","summary":"  Video inpainting is the task of filling a region in a video in a visually\nconvincing manner. It is very challenging due to the high dimensionality of the\ndata and the temporal consistency required for obtaining convincing results.\nRecently, diffusion models have shown impressive results in modeling complex\ndata distributions, including images and videos. Such models remain nonetheless\nvery expensive to train and to perform inference with, which strongly reduce\ntheir applicability to videos, and yields unreasonable computational loads. We\nshow that in the case of video inpainting, thanks to the highly auto-similar\nnature of videos, the training data of a diffusion model can be restricted to\nthe input video and still produce very satisfying results. This leads us to\nadopt an internal learning approach, which also allows us to greatly reduce the\nneural network size by about three orders of magnitude less than current\ndiffusion models used for image inpainting. We also introduce a new method for\nefficient training and inference of diffusion models in the context of internal\nlearning, by splitting the diffusion process into different learning intervals\ncorresponding to different noise levels of the diffusion process. To the best\nof our knowledge, this is the first video inpainting method based purely on\ndiffusion. Other methods require additional components such as optical flow\nestimation, which limits their performance in the case of dynamic textures and\ncomplex motions. We show qualitative and quantitative results, demonstrating\nthat our method reaches state of the art performance in the case of dynamic\ntextures and complex dynamic backgrounds.\n","authors":["Nicolas Cherel","AndrÃ©s Almansa","Yann Gousseau","Alasdair Newson"],"pdf_url":"https://arxiv.org/pdf/2311.01090v3.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.15899v1","updated":"2024-08-28T16:12:28Z","published":"2024-08-28T16:12:28Z","title":"Gen-Swarms: Adapting Deep Generative Models to Swarms of Drones","summary":"  Gen-Swarms is an innovative method that leverages and combines the\ncapabilities of deep generative models with reactive navigation algorithms to\nautomate the creation of drone shows. Advancements in deep generative models,\nparticularly diffusion models, have demonstrated remarkable effectiveness in\ngenerating high-quality 2D images. Building on this success, various works have\nextended diffusion models to 3D point cloud generation. In contrast,\nalternative generative models such as flow matching have been proposed,\noffering a simple and intuitive transition from noise to meaningful outputs.\nHowever, the application of flow matching models to 3D point cloud generation\nremains largely unexplored. Gen-Swarms adapts these models to automatically\ngenerate drone shows. Existing 3D point cloud generative models create point\ntrajectories which are impractical for drone swarms. In contrast, our method\nnot only generates accurate 3D shapes but also guides the swarm motion,\nproducing smooth trajectories and accounting for potential collisions through a\nreactive navigation algorithm incorporated into the sampling process. For\nexample, when given a text category like Airplane, Gen-Swarms can rapidly and\ncontinuously generate numerous variations of 3D airplane shapes. Our\nexperiments demonstrate that this approach is particularly well-suited for\ndrone shows, providing feasible trajectories, creating representative final\nshapes, and significantly enhancing the overall performance of drone show\ngeneration.\n","authors":["Carlos Plou","Pablo Pueyo","Ruben Martinez-Cantin","Mac Schwager","Ana C. Murillo","Eduardo Montijano"],"pdf_url":"https://arxiv.org/pdf/2408.15899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15890v1","updated":"2024-08-28T16:03:18Z","published":"2024-08-28T16:03:18Z","title":"Disentangled Diffusion Autoencoder for Harmonization of Multi-site\n  Neuroimaging Data","summary":"  Combining neuroimaging datasets from multiple sites and scanners can help\nincrease statistical power and thus provide greater insight into subtle\nneuroanatomical effects. However, site-specific effects pose a challenge by\npotentially obscuring the biological signal and introducing unwanted variance.\nExisting harmonization techniques, which use statistical models to remove such\neffects, have been shown to incompletely remove site effects while also failing\nto preserve biological variability. More recently, generative models using GANs\nor autoencoder-based approaches, have been proposed for site adjustment.\nHowever, such methods are known for instability during training or blurry image\ngeneration. In recent years, diffusion models have become increasingly popular\nfor their ability to generate high-quality synthetic images. In this work, we\nintroduce the disentangled diffusion autoencoder (DDAE), a novel diffusion\nmodel designed for controlling specific aspects of an image. We apply the DDAE\nto the task of harmonizing MR images by generating high-quality site-adjusted\nimages that preserve biological variability. We use data from 7 different sites\nand demonstrate the DDAE's superiority in generating high-resolution,\nharmonized 2D MR images over previous approaches. As far as we are aware, this\nwork marks the first diffusion-based model for site adjustment of neuroimaging\ndata.\n","authors":["Ayodeji Ijishakin","Ana Lawry Aguila","Elizabeth Levitis","Ahmed Abdulaal","Andre Altmann","James Cole"],"pdf_url":"https://arxiv.org/pdf/2408.15890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15887v1","updated":"2024-08-28T15:59:40Z","published":"2024-08-28T15:59:40Z","title":"SpineMamba: Enhancing 3D Spinal Segmentation in Clinical Imaging through\n  Residual Visual Mamba Layers and Shape Priors","summary":"  Accurate segmentation of 3D clinical medical images is critical in the\ndiagnosis and treatment of spinal diseases. However, the inherent complexity of\nspinal anatomy and uncertainty inherent in current imaging technologies, poses\nsignificant challenges for semantic segmentation of spinal images. Although\nconvolutional neural networks (CNNs) and Transformer-based models have made\nsome progress in spinal segmentation, their limitations in handling long-range\ndependencies hinder further improvements in segmentation accuracy.To address\nthese challenges, we introduce a residual visual Mamba layer to effectively\ncapture and model the deep semantic features and long-range spatial\ndependencies of 3D spinal data. To further enhance the structural semantic\nunderstanding of the vertebrae, we also propose a novel spinal shape prior\nmodule that captures specific anatomical information of the spine from medical\nimages, significantly enhancing the model's ability to extract structural\nsemantic information of the vertebrae. Comparative and ablation experiments on\ntwo datasets demonstrate that SpineMamba outperforms existing state-of-the-art\nmodels. On the CT dataset, the average Dice similarity coefficient for\nsegmentation reaches as high as 94.40, while on the MR dataset, it reaches\n86.95. Notably, compared to the renowned nnU-Net, SpineMamba achieves superior\nsegmentation performance, exceeding it by up to 2 percentage points. This\nunderscores its accuracy, robustness, and excellent generalization\ncapabilities.\n","authors":["Zhiqing Zhang","Tianyong Liu","Guojia Fan","Bin Li","Qianjin Feng","Shoujun Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.15887v1.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.15881v1","updated":"2024-08-28T15:52:23Z","published":"2024-08-28T15:52:23Z","title":"LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation","summary":"  We introduce LLaVA-MoD, a novel framework designed to enable the efficient\ntraining of small-scale Multimodal Language Models (s-MLLM) by distilling\nknowledge from large-scale MLLM (l-MLLM). Our approach tackles two fundamental\nchallenges in MLLM distillation. First, we optimize the network structure of\ns-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into the\nlanguage model, striking a balance between computational efficiency and model\nexpressiveness. Second, we propose a progressive knowledge transfer strategy to\nensure comprehensive knowledge migration. This strategy begins with mimic\ndistillation, where we minimize the Kullback-Leibler (KL) divergence between\noutput distributions to enable the student model to emulate the teacher\nnetwork's understanding. Following this, we introduce preference distillation\nvia Direct Preference Optimization (DPO), where the key lies in treating l-MLLM\nas the reference model. During this phase, the s-MLLM's ability to discriminate\nbetween superior and inferior examples is significantly enhanced beyond l-MLLM,\nleading to a better student that surpasses its teacher, particularly in\nhallucination benchmarks. Extensive experiments demonstrate that LLaVA-MoD\noutperforms existing models across various multimodal benchmarks while\nmaintaining a minimal number of activated parameters and low computational\ncosts. Remarkably, LLaVA-MoD, with only 2B activated parameters, surpasses\nQwen-VL-Chat-7B by an average of 8.8% across benchmarks, using merely 0.3% of\nthe training data and 23% trainable parameters. These results underscore\nLLaVA-MoD's ability to effectively distill comprehensive knowledge from its\nteacher model, paving the way for the development of more efficient MLLMs. The\ncode will be available on: https://github.com/shufangxun/LLaVA-MoD.\n","authors":["Fangxun Shu","Yue Liao","Le Zhuo","Chenning Xu","Guanghao Zhang","Haonan Shi","Long Chen","Tao Zhong","Wanggui He","Siming Fu","Haoyuan Li","Bolin Li","Zhelun Yu","Si Liu","Hongsheng Li","Hao Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.15881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15876v1","updated":"2024-08-28T15:47:32Z","published":"2024-08-28T15:47:32Z","title":"Unleashing the Temporal-Spatial Reasoning Capacity of GPT for\n  Training-Free Audio and Language Referenced Video Object Segmentation","summary":"  In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2)\npipeline to explore the training-free paradigm for audio and\nlanguage-referenced video object segmentation, namely AVS and RVOS tasks. The\nintuitive solution leverages GroundingDINO to identify the target object from a\nsingle frame and SAM 2 to segment the identified object throughout the video,\nwhich is less robust to spatiotemporal variations due to a lack of video\ncontext exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel\nGPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform\ntwo-step temporal-spatial reasoning for sequentially selecting pivot frames and\npivot boxes, thereby providing SAM 2 with a high-quality initial object prompt.\nWithin GPT-PS, two task-specific Chain-of-Thought prompts are designed to\nunleash GPT's temporal-spatial reasoning capacity by guiding GPT to make\nselections based on a comprehensive understanding of video and reference\ninformation. Furthermore, we propose a Language-Binded Reference Unification\n(LBRU) module to convert audio signals into language-formatted references,\nthereby unifying the formats of AVS and RVOS tasks in the same pipeline.\nExtensive experiments on both tasks show that our training-free AL-Ref-SAM 2\npipeline achieves performances comparable to or even better than\nfully-supervised fine-tuning methods. The code is available at:\nhttps://github.com/appletea233/AL-Ref-SAM2.\n","authors":["Shaofei Huang","Rui Ling","Hongyu Li","Tianrui Hui","Zongheng Tang","Xiaoming Wei","Jizhong Han","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15868v1","updated":"2024-08-28T15:37:44Z","published":"2024-08-28T15:37:44Z","title":"GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video\n  Generative Model","summary":"  Autonomous driving training requires a diverse range of datasets encompassing\nvarious traffic conditions, weather scenarios, and road types. Traditional data\naugmentation methods often struggle to generate datasets that represent rare\noccurrences. To address this challenge, we propose GenDDS, a novel approach for\ngenerating driving scenarios generation by leveraging the capabilities of\nStable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology\ninvolves the use of descriptive prompts to guide the synthesis process, aimed\nat producing realistic and diverse driving scenarios. With the power of the\nlatest computer vision techniques, such as ControlNet and Hotshot-XL, we have\nbuilt a complete pipeline for video generation together with SDXL. We employ\nthe KITTI dataset, which includes real-world driving videos, to train the\nmodel. Through a series of experiments, we demonstrate that our model can\ngenerate high-quality driving videos that closely replicate the complexity and\nvariability of real-world driving scenarios. This research contributes to the\ndevelopment of sophisticated training data for autonomous driving systems and\nopens new avenues for creating virtual environments for simulation and\nvalidation purposes.\n","authors":["Yongjie Fu","Yunlong Li","Xuan Di"],"pdf_url":"https://arxiv.org/pdf/2408.15868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15865v1","updated":"2024-08-28T15:29:27Z","published":"2024-08-28T15:29:27Z","title":"microYOLO: Towards Single-Shot Object Detection on Microcontrollers","summary":"  This work-in-progress paper presents results on the feasibility of\nsingle-shot object detection on microcontrollers using YOLO. Single-shot object\ndetectors like YOLO are widely used, however due to their complexity mainly on\nlarger GPU-based platforms. We present microYOLO, which can be used on Cortex-M\nbased microcontrollers, such as the OpenMV H7 R2, achieving about 3.5 FPS when\nclassifying 128x128 RGB images while using less than 800 KB Flash and less than\n350 KB RAM. Furthermore, we share experimental results for three different\nobject detection tasks, analyzing the accuracy of microYOLO on them.\n","authors":["Mark Deutel","Christopher Mutschler","JÃ¼rgen Teich"],"pdf_url":"https://arxiv.org/pdf/2408.15865v1.pdf","comment":"Published at the ECML PKDD Conference 2023, at the 4th Workshop on\n  IoT, Edge, and Mobile for Embedded Machine Learning"},{"id":"http://arxiv.org/abs/2310.10835v3","updated":"2024-08-28T15:29:17Z","published":"2023-10-16T21:17:29Z","title":"Provable Probabilistic Imaging using Score-Based Generative Priors","summary":"  Estimating high-quality images while also quantifying their uncertainty are\ntwo desired features in an image reconstruction algorithm for solving ill-posed\ninverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as\na principled framework for characterizing the space of possible solutions to a\ngeneral inverse problem. PMC is able to incorporate expressive score-based\ngenerative priors for high-quality image reconstruction while also performing\nuncertainty quantification via posterior sampling. In particular, we develop\ntwo PMC algorithms that can be viewed as the sampling analogues of the\ntraditional plug-and-play priors (PnP) and regularization by denoising (RED)\nalgorithms. To improve the sampling efficiency, we introduce weighted annealing\ninto these PMC algorithms, further developing two additional annealed PMC\nalgorithms (APMC). We establish a theoretical analysis for characterizing the\nconvergence behavior of PMC algorithms. Our analysis provides non-asymptotic\nstationarity guarantees in terms of the Fisher information, fully compatible\nwith the joint presence of weighted annealing, potentially non-log-concave\nlikelihoods, and imperfect score networks. We demonstrate the performance of\nthe PMC algorithms on multiple representative inverse problems with both linear\nand nonlinear forward models. Experimental results show that PMC significantly\nimproves reconstruction quality and enables high-fidelity uncertainty\nquantification.\n","authors":["Yu Sun","Zihui Wu","Yifan Chen","Berthy T. Feng","Katherine L. Bouman"],"pdf_url":"https://arxiv.org/pdf/2310.10835v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15857v1","updated":"2024-08-28T15:18:46Z","published":"2024-08-28T15:18:46Z","title":"What is YOLOv8: An In-Depth Exploration of the Internal Features of the\n  Next-Generation Object Detector","summary":"  This study presents a detailed analysis of the YOLOv8 object detection model,\nfocusing on its architecture, training techniques, and performance improvements\nover previous iterations like YOLOv5. Key innovations, including the CSPNet\nbackbone for enhanced feature extraction, the FPN+PAN neck for superior\nmulti-scale object detection, and the transition to an anchor-free approach,\nare thoroughly examined. The paper reviews YOLOv8's performance across\nbenchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy\nand real-time capabilities across diverse hardware platforms. Additionally, the\nstudy explores YOLOv8's developer-friendly enhancements, such as its unified\nPython package and CLI, which streamline model training and deployment.\nOverall, this research positions YOLOv8 as a state-of-the-art solution in the\nevolving object detection field.\n","authors":["Muhammad Yaseen"],"pdf_url":"https://arxiv.org/pdf/2408.15857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19254v2","updated":"2024-08-28T15:13:45Z","published":"2024-03-28T09:21:00Z","title":"Imperceptible Protection against Style Imitation from Diffusion Models","summary":"  Recent progress in diffusion models has profoundly enhanced the fidelity of\nimage generation, but it has raised concerns about copyright infringements.\nWhile prior methods have introduced adversarial perturbations to prevent style\nimitation, most are accompanied by the degradation of artworks' visual quality.\nRecognizing the importance of maintaining this, we introduce a visually\nimproved protection method while preserving its protection capability. To this\nend, we devise a perceptual map to highlight areas sensitive to human eyes,\nguided by instance-aware refinement, which refines the protection intensity\naccordingly. We also introduce a difficulty-aware protection by predicting how\ndifficult the artwork is to protect and dynamically adjusting the intensity\nbased on this. Lastly, we integrate a perceptual constraints bank to further\nimprove the imperceptibility. Results show that our method substantially\nelevates the quality of the protected image without compromising on protection\nefficacy.\n","authors":["Namhyuk Ahn","Wonhyuk Ahn","KiYoon Yoo","Daesik Kim","Seung-Hun Nam"],"pdf_url":"https://arxiv.org/pdf/2403.19254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15844v1","updated":"2024-08-28T15:04:52Z","published":"2024-08-28T15:04:52Z","title":"Shot Segmentation Based on Von Neumann Entropy for Key Frame Extraction","summary":"  Video key frame extraction is important in various fields, such as video\nsummary, retrieval, and compression. Therefore, we suggest a video key frame\nextraction algorithm based on shot segmentation using Von Neumann entropy. The\nsegmentation of shots is achieved through the computation of Von Neumann\nentropy of the similarity matrix among frames within the video sequence. The\ninitial frame of each shot is selected as key frames, which combines the\ntemporal sequence information of frames. The experimental results show the\nextracted key frames can fully and accurately represent the original video\ncontent while minimizing the number of repeated frames.\n","authors":["Xueqing Zhang. Di Fu","Naihao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15844v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.15833v1","updated":"2024-08-28T14:47:34Z","published":"2024-08-28T14:47:34Z","title":"Network transferability of adversarial patches in real-time object\n  detection","summary":"  Adversarial patches in computer vision can be used, to fool deep neural\nnetworks and manipulate their decision-making process. One of the most\nprominent examples of adversarial patches are evasion attacks for object\ndetectors. By covering parts of objects of interest, these patches suppress the\ndetections and thus make the target object 'invisible' to the object detector.\nSince these patches are usually optimized on a specific network with a specific\ntrain dataset, the transferability across multiple networks and datasets is not\ngiven. This paper addresses these issues and investigates the transferability\nacross numerous object detector architectures. Our extensive evaluation across\nvarious models on two distinct datasets indicates that patches optimized with\nlarger models provide better network transferability than patches that are\noptimized with smaller models.\n","authors":["Jens Bayer","Stefan Becker","David MÃ¼nch","Michael Arens"],"pdf_url":"https://arxiv.org/pdf/2408.15833v1.pdf","comment":"7 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2408.15829v1","updated":"2024-08-28T14:44:42Z","published":"2024-08-28T14:44:42Z","title":"SITransformer: Shared Information-Guided Transformer for Extreme\n  Multimodal Summarization","summary":"  Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes an\nattractive summarization approach by integrating various types of information\nto create extremely concise yet informative summaries for individual\nmodalities. Existing methods overlook the issue that multimodal data often\ncontains more topic irrelevant information, which can mislead the model into\nproducing inaccurate summaries especially for extremely short ones. In this\npaper, we propose SITransformer, a \\textbf{S}hared \\textbf{I}nformation-guided\n\\textbf{T}ransformer for extreme multimodal summarization. It has a shared\ninformation guided pipeline which involves a cross-modal shared information\nextractor and a cross-modal interaction module. The extractor formulates\nsemantically shared salient information from different modalities by devising a\nnovel filtering process consisting of a differentiable top-k selector and a\nshared-information guided gating unit. As a result, the common, salient, and\nrelevant contents across modalities are identified. Next, a transformer with\ncross-modal attentions is developed for intra- and inter-modality learning with\nthe shared information guidance to produce the extreme summary. Comprehensive\nexperiments demonstrate that SITransformer significantly enhances the\nsummarization quality for both video and text summaries for XMSMO. Our code\nwill be publicly available at https://github.com/SichengLeoLiu/MMAsia24-XMSMO.\n","authors":["Sicheng Liu","Lintao Wang","Xiaogan Zhu","Xuequan Lu","Zhiyong Wang","Kun Hu"],"pdf_url":"https://arxiv.org/pdf/2408.15829v1.pdf","comment":"8 pages, 5 figures, submitted to ACM Multimedia Asia 2024"},{"id":"http://arxiv.org/abs/2408.15823v1","updated":"2024-08-28T14:34:45Z","published":"2024-08-28T14:34:45Z","title":"Benchmarking foundation models as feature extractors for\n  weakly-supervised computational pathology","summary":"  Advancements in artificial intelligence have driven the development of\nnumerous pathology foundation models capable of extracting clinically relevant\ninformation. However, there is currently limited literature independently\nevaluating these foundation models on truly external cohorts and\nclinically-relevant tasks to uncover adjustments for future improvements. In\nthis study, we benchmarked ten histopathology foundation models on 13 patient\ncohorts with 6,791 patients and 9,493 slides from lung, colorectal, gastric,\nand breast cancers. The models were evaluated on weakly-supervised tasks\nrelated to biomarkers, morphological properties, and prognostic outcomes. We\nshow that a vision-language foundation model, CONCH, yielded the highest\nperformance in 42% of tasks when compared to vision-only foundation models. The\nexperiments reveal that foundation models trained on distinct cohorts learn\ncomplementary features to predict the same label, and can be fused to\noutperform the current state of the art. Creating an ensemble of complementary\nfoundation models outperformed CONCH in 66% of tasks. Moreover, our findings\nsuggest that data diversity outweighs data volume for foundation models. Our\nwork highlights actionable adjustments to improve pathology foundation models.\n","authors":["Peter Neidlinger","Omar S. M. El Nahhas","Hannah Sophie Muti","Tim Lenz","Michael Hoffmeister","Hermann Brenner","Marko van Treeck","Rupert Langer","Bastian Dislich","Hans Michael Behrens","Christoph RÃ¶cken","Sebastian Foersch","Daniel Truhn","Antonio Marra","Oliver Lester Saldanha","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2408.15823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05348v4","updated":"2024-08-28T14:26:07Z","published":"2023-11-09T13:18:27Z","title":"u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model","summary":"  Recent advancements in multi-modal large language models (MLLMs) have led to\nsubstantial improvements in visual understanding, primarily driven by\nsophisticated modality alignment strategies. However, predominant approaches\nprioritize global or regional comprehension, with less focus on fine-grained,\npixel-level tasks. To address this gap, we introduce u-LLaVA, an innovative\nunifying multi-task framework that integrates pixel, regional, and global\nfeatures to refine the perceptual faculties of MLLMs. We commence by leveraging\nan efficient modality alignment approach, harnessing both image and video\ndatasets to bolster the model's foundational understanding across diverse\nvisual contexts. Subsequently, a joint instruction tuning method with\ntask-specific projectors and decoders for end-to-end downstream training is\npresented. Furthermore, this work contributes a novel mask-based multi-task\ndataset comprising 277K samples, crafted to challenge and assess the\nfine-grained perception capabilities of MLLMs. The overall framework is simple,\neffective, and achieves state-of-the-art performance across multiple\nbenchmarks. We also make our model, data, and code publicly accessible at\nhttps://github.com/OPPOMKLab/u-LLaVA.\n","authors":["Jinjin Xu","Liwu Xu","Yuzhe Yang","Xiang Li","Fanyi Wang","Yanchun Xie","Yi-Jie Huang","Yaqian Li"],"pdf_url":"https://arxiv.org/pdf/2311.05348v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15816v1","updated":"2024-08-28T14:25:35Z","published":"2024-08-28T14:25:35Z","title":"Mining Field Data for Tree Species Recognition at Scale","summary":"  Individual tree species labels are particularly hard to acquire due to the\nexpert knowledge needed and the limitations of photointerpretation. Here, we\npresent a methodology to automatically mine species labels from public forest\ninventory data, using available pretrained tree detection models. We identify\ntree instances in aerial imagery and match them with field data with close to\nzero human involvement. We conduct a series of experiments on the resulting\ndataset, and show a beneficial effect when adding noisy or even unlabeled data\npoints, highlighting a strong potential for large-scale individual species\nmapping.\n","authors":["Dimitri Gominski","Daniel Ortiz-Gonzalo","Martin Brandt","Maurice Mugabowindekwe","Rasmus Fensholt"],"pdf_url":"https://arxiv.org/pdf/2408.15816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15813v1","updated":"2024-08-28T14:14:33Z","published":"2024-08-28T14:14:33Z","title":"DQFormer: Towards Unified LiDAR Panoptic Segmentation with Decoupled\n  Queries","summary":"  LiDAR panoptic segmentation, which jointly performs instance and semantic\nsegmentation for things and stuff classes, plays a fundamental role in LiDAR\nperception tasks. While most existing methods explicitly separate these two\nsegmentation tasks and utilize different branches (i.e., semantic and instance\nbranches), some recent methods have embraced the query-based paradigm to unify\nLiDAR panoptic segmentation. However, the distinct spatial distribution and\ninherent characteristics of objects(things) and their surroundings(stuff) in 3D\nscenes lead to challenges, including the mutual competition of things/stuff and\nthe ambiguity of classification/segmentation. In this paper, we propose\ndecoupling things/stuff queries according to their intrinsic properties for\nindividual decoding and disentangling classification/segmentation to mitigate\nambiguity. To this end, we propose a novel framework dubbed DQFormer to\nimplement semantic and instance segmentation in a unified workflow.\nSpecifically, we design a decoupled query generator to propose informative\nqueries with semantics by localizing things/stuff positions and fusing\nmulti-level BEV embeddings. Moreover, a query-oriented mask decoder is\nintroduced to decode corresponding segmentation masks by performing masked\ncross-attention between queries and mask embeddings. Finally, the decoded masks\nare combined with the semantics of the queries to produce panoptic results.\nExtensive experiments on nuScenes and SemanticKITTI datasets demonstrate the\nsuperiority of our DQFormer framework.\n","authors":["Yu Yang","Jianbiao Mei","Liang Liu","Siliang Du","Yilin Xiao","Jongwon Ra","Yong Liu","Xiao Xu","Huifeng Wu"],"pdf_url":"https://arxiv.org/pdf/2408.15813v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.15810v1","updated":"2024-08-28T14:10:57Z","published":"2024-08-28T14:10:57Z","title":"Multi-view Pose Fusion for Occlusion-Aware 3D Human Pose Estimation","summary":"  Robust 3D human pose estimation is crucial to ensure safe and effective\nhuman-robot collaboration. Accurate human perception,however, is particularly\nchallenging in these scenarios due to strong occlusions and limited camera\nviewpoints. Current 3D human pose estimation approaches are rather vulnerable\nin such conditions. In this work we present a novel approach for robust 3D\nhuman pose estimation in the context of human-robot collaboration. Instead of\nrelying on noisy 2D features triangulation, we perform multi-view fusion on 3D\nskeletons provided by absolute monocular methods. Accurate 3D pose estimation\nis then obtained via reprojection error optimization, introducing limbs length\nsymmetry constraints. We evaluate our approach on the public dataset Human3.6M\nand on a novel version Human3.6M-Occluded, derived adding synthetic occlusions\non the camera views with the purpose of testing pose estimation algorithms\nunder severe occlusions. We further validate our method on real human-robot\ncollaboration workcells, in which we strongly surpass current 3D human pose\nestimation methods. Our approach outperforms state-of-the-art multi-view human\npose estimation techniques and demonstrates superior capabilities in handling\nchallenging scenarios with strong occlusions, representing a reliable and\neffective solution for real human-robot collaboration setups.\n","authors":["Laura Bragagnolo","Matteo Terreran","Davide Allegro","Stefano Ghidoni"],"pdf_url":"https://arxiv.org/pdf/2408.15810v1.pdf","comment":"ECCV workshops 2024"},{"id":"http://arxiv.org/abs/2408.15809v1","updated":"2024-08-28T14:08:24Z","published":"2024-08-28T14:08:24Z","title":"Object Detection for Vehicle Dashcams using Transformers","summary":"  The use of intelligent automation is growing significantly in the automotive\nindustry, as it assists drivers and fleet management companies, thus increasing\ntheir productivity. Dash cams are now been used for this purpose which enables\nthe instant identification and understanding of multiple objects and\noccurrences in the surroundings. In this paper, we propose a novel approach for\nobject detection in dashcams using transformers. Our system is based on the\nstate-of-the-art DEtection TRansformer (DETR), which has demonstrated strong\nperformance in a variety of conditions, including different weather and\nillumination scenarios. The use of transformers allows for the consideration of\ncontextual information in decisionmaking, improving the accuracy of object\ndetection. To validate our approach, we have trained our DETR model on a\ndataset that represents real-world conditions. Our results show that the use of\nintelligent automation through transformers can significantly enhance the\ncapabilities of dashcam systems. The model achieves an mAP of 0.95 on\ndetection.\n","authors":["Osama Mustafa","Khizer Ali","Anam Bibi","Imran Siddiqi","Momina Moetesum"],"pdf_url":"https://arxiv.org/pdf/2408.15809v1.pdf","comment":"7 Pages, and 6 Figures"},{"id":"http://arxiv.org/abs/2408.15802v1","updated":"2024-08-28T13:53:27Z","published":"2024-08-28T13:53:27Z","title":"Visual Prompt Engineering for Medical Vision Language Models in\n  Radiology","summary":"  Medical image classification in radiology faces significant challenges,\nparticularly in generalizing to unseen pathologies. In contrast, CLIP offers a\npromising solution by leveraging multimodal learning to improve zero-shot\nclassification performance. However, in the medical domain, lesions can be\nsmall and might not be well represented in the embedding space. Therefore, in\nthis paper, we explore the potential of visual prompt engineering to enhance\nthe capabilities of Vision Language Models (VLMs) in radiology. Leveraging\nBiomedCLIP, trained on extensive biomedical image-text pairs, we investigate\nthe impact of embedding visual markers directly within radiological images to\nguide the model's attention to critical regions. Our evaluation on the JSRT\ndataset, focusing on lung nodule malignancy classification, demonstrates that\nincorporating visual prompts $\\unicode{x2013}$ such as arrows, circles, and\ncontours $\\unicode{x2013}$ significantly improves classification metrics\nincluding AUROC, AUPRC, F1 score, and accuracy. Moreover, the study provides\nattention maps, showcasing enhanced model interpretability and focus on\nclinically relevant areas. These findings underscore the efficacy of visual\nprompt engineering as a straightforward yet powerful approach to advance VLM\nperformance in medical image analysis.\n","authors":["Stefan Denner","Markus Bujotzek","Dimitrios Bounias","David Zimmerer","Raphael Stock","Paul F. JÃ¤ger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2408.15802v1.pdf","comment":"Accepted at ECCV 2024 Workshop on Emergent Visual Abilities and\n  Limits of Foundation Models"},{"id":"http://arxiv.org/abs/2405.18064v2","updated":"2024-08-28T13:41:34Z","published":"2024-05-28T11:24:20Z","title":"Automated Real-World Sustainability Data Generation from Images of\n  Buildings","summary":"  When data on building features is unavailable, the task of determining how to\nimprove that building in terms of carbon emissions becomes infeasible. We show\nthat from only a set of images, a Large Language Model with appropriate prompt\nengineering and domain knowledge can successfully estimate a range of building\nfeatures relevant for sustainability calculations. We compare our novel\nimage-to-data method with a ground truth comprising real building data for 47\napartments and achieve accuracy better than a human performing the same task.\nWe also demonstrate that the method can generate tailored recommendations to\nthe owner on how best to improve their properties and discuss methods to scale\nthe approach.\n","authors":["Peter J Bentley","Soo Ling Lim","Rajat Mathur","Sid Narang"],"pdf_url":"https://arxiv.org/pdf/2405.18064v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2307.14382v2","updated":"2024-08-28T13:30:36Z","published":"2023-07-25T20:08:41Z","title":"When Multi-Task Learning Meets Partial Supervision: A Computer Vision\n  Review","summary":"  Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while\nexploiting their mutual relationships. By using shared resources to\nsimultaneously calculate multiple outputs, this learning paradigm has the\npotential to have lower memory requirements and inference times compared to the\ntraditional approach of using separate methods for each task. Previous work in\nMTL has mainly focused on fully-supervised methods, as task relationships can\nnot only be leveraged to lower the level of data-dependency of those methods\nbut they can also improve performance. However, MTL introduces a set of\nchallenges due to a complex optimisation scheme and a higher labeling\nrequirement. This review focuses on how MTL could be utilised under different\npartial supervision settings to address these challenges. First, this review\nanalyses how MTL traditionally uses different parameter sharing techniques to\ntransfer knowledge in between tasks. Second, it presents the different\nchallenges arising from such a multi-objective optimisation scheme. Third, it\nintroduces how task groupings can be achieved by analysing task relationships.\nFourth, it focuses on how partially supervised methods applied to MTL can\ntackle the aforementioned challenges. Lastly, this review presents the\navailable datasets, tools and benchmarking results of such methods.\n","authors":["Maxime Fontana","Michael Spratling","Miaojing Shi"],"pdf_url":"https://arxiv.org/pdf/2307.14382v2.pdf","comment":"Accepted by Proceedings of the IEEE"},{"id":"http://arxiv.org/abs/2408.15777v1","updated":"2024-08-28T13:15:25Z","published":"2024-08-28T13:15:25Z","title":"A Survey on Facial Expression Recognition of Static and Dynamic Emotions","summary":"  Facial expression recognition (FER) aims to analyze emotional states from\nstatic images and dynamic sequences, which is pivotal in enhancing\nanthropomorphic communication among humans, robots, and digital avatars by\nleveraging AI technologies. As the FER field evolves from controlled laboratory\nenvironments to more complex in-the-wild scenarios, advanced methods have been\nrapidly developed and new challenges and apporaches are encounted, which are\nnot well addressed in existing reviews of FER. This paper offers a\ncomprehensive survey of both image-based static FER (SFER) and video-based\ndynamic FER (DFER) methods, analyzing from model-oriented development to\nchallenge-focused categorization. We begin with a critical comparison of recent\nreviews, an introduction to common datasets and evaluation criteria, and an\nin-depth workflow on FER to establish a robust research foundation. We then\nsystematically review representative approaches addressing eight main\nchallenges in SFER (such as expression disturbance, uncertainties, compound\nemotions, and cross-domain inconsistency) as well as seven main challenges in\nDFER (such as key frame sampling, expression intensity variations, and\ncross-modal alignment). Additionally, we analyze recent advancements, benchmark\nperformances, major applications, and ethical considerations. Finally, we\npropose five promising future directions and development trends to guide\nongoing research. The project page for this paper can be found at\nhttps://github.com/wangyanckxx/SurveyFER.\n","authors":["Yan Wang","Shaoqi Yan","Yang Liu","Wei Song","Jing Liu","Yang Chang","Xinji Mai","Xiping Hu","Wenqiang Zhang","Zhongxue Gan"],"pdf_url":"https://arxiv.org/pdf/2408.15777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15769v1","updated":"2024-08-28T13:05:55Z","published":"2024-08-28T13:05:55Z","title":"A Survey on Evaluation of Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) mimic human perception and reasoning\nsystem by integrating powerful Large Language Models (LLMs) with various\nmodality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and\nvarious modality encoders as sensory organs. This framework endows MLLMs with\nhuman-like capabilities, and suggests a potential pathway towards achieving\nartificial general intelligence (AGI). With the emergence of all-round MLLMs\nlike GPT-4V and Gemini, a multitude of evaluation methods have been developed\nto assess their capabilities across different dimensions. This paper presents a\nsystematic and comprehensive review of MLLM evaluation methods, covering the\nfollowing key aspects: (1) the background of MLLMs and their evaluation; (2)\n\"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks\nbased on the capabilities assessed, including general multimodal recognition,\nperception, reasoning and trustworthiness, and domain-specific applications\nsuch as socioeconomic, natural sciences and engineering, medical usage, AI\nagent, remote sensing, video and audio processing, 3D point cloud analysis, and\nothers; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into\ngeneral and specific benchmarks; (4) \"how to evaluate\" that reviews and\nillustrates MLLM evaluation steps and metrics; Our overarching goal is to\nprovide valuable insights for researchers in the field of MLLM evaluation,\nthereby facilitating the development of more capable and reliable MLLMs. We\nemphasize that evaluation should be regarded as a critical discipline,\nessential for advancing the field of MLLMs.\n","authors":["Jiaxing Huang","Jingyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19730v5","updated":"2024-08-28T13:05:41Z","published":"2024-05-30T06:21:34Z","title":"Research on the Spatial Data Intelligent Foundation Model","summary":"  This report focuses on spatial data intelligent large models, delving into\nthe principles, methods, and cutting-edge applications of these models. It\nprovides an in-depth discussion on the definition, development history, current\nstatus, and trends of spatial data intelligent large models, as well as the\nchallenges they face. The report systematically elucidates the key technologies\nof spatial data intelligent large models and their applications in urban\nenvironments, aerospace remote sensing, geography, transportation, and other\nscenarios. Additionally, it summarizes the latest application cases of spatial\ndata intelligent large models in themes such as urban development, multimodal\nsystems, remote sensing, smart transportation, and resource environments.\nFinally, the report concludes with an overview and outlook on the development\nprospects of spatial data intelligent large models.\n","authors":["Shaohua Wang","Xing Xie","Yong Li","Danhuai Guo","Zhi Cai","Yu Liu","Yang Yue","Xiao Pan","Feng Lu","Huayi Wu","Zhipeng Gui","Zhiming Ding","Bolong Zheng","Fuzheng Zhang","Jingyuan Wang","Zhengchao Chen","Hao Lu","Jiayi Li","Peng Yue","Wenhao Yu","Yao Yao","Leilei Sun","Yong Zhang","Longbiao Chen","Xiaoping Du","Xiang Li","Xueying Zhang","Kun Qin","Zhaoya Gong","Weihua Dong","Xiaofeng Meng"],"pdf_url":"https://arxiv.org/pdf/2405.19730v5.pdf","comment":"V1 and V2 are in Chinese language, other versions are in English"},{"id":"http://arxiv.org/abs/2408.15761v1","updated":"2024-08-28T12:56:00Z","published":"2024-08-28T12:56:00Z","title":"Addressing the challenges of loop detection in agricultural environments","summary":"  While visual SLAM systems are well studied and achieve impressive results in\nindoor and urban settings, natural, outdoor and open-field environments are\nmuch less explored and still present relevant research challenges. Visual\nnavigation and local mapping have shown a relatively good performance in\nopen-field environments. However, globally consistent mapping and long-term\nlocalization still depend on the robustness of loop detection and closure, for\nwhich the literature is scarce. In this work we propose a novel method to pave\nthe way towards robust loop detection in open fields, particularly in\nagricultural settings, based on local feature search and stereo geometric\nrefinement, with a final stage of relative pose estimation. Our method\nconsistently achieves good loop detections, with a median error of 15cm. We aim\nto characterize open fields as a novel environment for loop detection,\nunderstanding the limitations and problems that arise when dealing with them.\n","authors":["NicolÃ¡s Soncini","Javier Civera","TaihÃº Pire"],"pdf_url":"https://arxiv.org/pdf/2408.15761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18006v2","updated":"2024-08-28T12:43:26Z","published":"2024-04-27T20:54:15Z","title":"FRAME: A Modular Framework for Autonomous Map Merging: Advancements in\n  the Field","summary":"  In this article, a novel approach for merging 3D point cloud maps in the\ncontext of egocentric multi-robot exploration is presented. Unlike traditional\nmethods, the proposed approach leverages state-of-the-art place recognition and\nlearned descriptors to efficiently detect overlap between maps, eliminating the\nneed for the time-consuming global feature extraction and feature matching\nprocess. The estimated overlapping regions are used to calculate a homogeneous\nrigid transform, which serves as an initial condition for the GICP point cloud\nregistration algorithm to refine the alignment between the maps. The advantages\nof this approach include faster processing time, improved accuracy, and\nincreased robustness in challenging environments. Furthermore, the\neffectiveness of the proposed framework is successfully demonstrated through\nmultiple field missions of robot exploration in a variety of different\nunderground environments.\n","authors":["Nikolaos Stathoulopoulos","BjÃ¶rn Lindqvist","Anton Koval","Ali-akbar Agha-mohammadi","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2404.18006v2.pdf","comment":"28 pages, 24 figures. Accepted to the IEEE Transactions on Field\n  Robotics"},{"id":"http://arxiv.org/abs/2312.02255v3","updated":"2024-08-28T12:43:10Z","published":"2023-12-04T18:56:08Z","title":"Re-Nerfing: Improving Novel View Synthesis through Novel View Synthesis","summary":"  Recent neural rendering and reconstruction techniques, such as NeRFs or\nGaussian Splatting, have shown remarkable novel view synthesis capabilities but\nrequire hundreds of images of the scene from diverse viewpoints to render\nhigh-quality novel views. With fewer images available, these methods start to\nfail since they can no longer correctly triangulate the underlying 3D geometry\nand converge to a non-optimal solution. These failures can manifest as floaters\nor blurry renderings in sparsely observed areas of the scene. In this paper, we\npropose Re-Nerfing, a simple and general add-on approach that leverages novel\nview synthesis itself to tackle this problem. Using an already trained NVS\nmethod, we render novel views between existing ones and augment the training\ndata to optimize a second model. This introduces additional multi-view\nconstraints and allows the second model to converge to a better solution. With\nRe-Nerfing we achieve significant improvements upon multiple pipelines based on\nNeRF and Gaussian-Splatting in sparse view settings of the mip-NeRF 360 and\nLLFF datasets. Notably, Re-Nerfing does not require prior knowledge or extra\nsupervision signals, making it a flexible and practical add-on.\n","authors":["Felix Tristram","Stefano Gasperini","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2312.02255v3.pdf","comment":"Code will be released upon acceptance"},{"id":"http://arxiv.org/abs/2408.15750v1","updated":"2024-08-28T12:33:26Z","published":"2024-08-28T12:33:26Z","title":"Str-L Pose: Integrating Point and Structured Line for Relative Pose\n  Estimation in Dual-Graph","summary":"  Relative pose estimation is crucial for various computer vision applications,\nincluding Robotic and Autonomous Driving. Current methods primarily depend on\nselecting and matching feature points prone to incorrect matches, leading to\npoor performance. Consequently, relying solely on point-matching relationships\nfor pose estimation is a huge challenge. To overcome these limitations, we\npropose a Geometric Correspondence Graph neural network that integrates point\nfeatures with extra structured line segments. This integration of matched\npoints and line segments further exploits the geometry constraints and enhances\nmodel performance across different environments. We employ the Dual-Graph\nmodule and Feature Weighted Fusion Module to aggregate geometric and visual\nfeatures effectively, facilitating complex scene understanding. We demonstrate\nour approach through extensive experiments on the DeMoN and KITTI Odometry\ndatasets. The results show that our method is competitive with state-of-the-art\ntechniques.\n","authors":["Zherong Zhang","Chunyu Lin","Shujuan Huang","Shangrong Yang","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.15750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15741v1","updated":"2024-08-28T12:08:25Z","published":"2024-08-28T12:08:25Z","title":"Segmentation-guided Layer-wise Image Vectorization with Gradient Fills","summary":"  The widespread use of vector graphics creates a significant demand for\nvectorization methods. While recent learning-based techniques have shown their\ncapability to create vector images of clear topology, filling these primitives\nwith gradients remains a challenge. In this paper, we propose a\nsegmentation-guided vectorization framework to convert raster images into\nconcise vector graphics with radial gradient fills. With the guidance of an\nembedded gradient-aware segmentation subroutine, our approach progressively\nappends gradient-filled B\\'ezier paths to the output, where primitive\nparameters are initiated with our newly designed initialization technique and\nare optimized to minimize our novel loss function. We build our method on a\ndifferentiable renderer with traditional segmentation algorithms to develop it\nas a model-free tool for raster-to-vector conversion. It is tested on various\ninputs to demonstrate its feasibility, independent of datasets, to synthesize\nvector graphics with improved visual quality and layer-wise topology compared\nto prior work.\n","authors":["Hengyu Zhou","Hui Zhang","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.15741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15740v1","updated":"2024-08-28T12:06:11Z","published":"2024-08-28T12:06:11Z","title":"MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with\n  Attention Mamba Mechanisms","summary":"  Vision Language Place Recognition (VLVPR) enhances robot localization\nperformance by incorporating natural language descriptions from images. By\nutilizing language information, VLVPR directs robot place matching, overcoming\nthe constraint of solely depending on vision. The essence of multimodal fusion\nlies in mining the complementary information between different modalities.\nHowever, general fusion methods rely on traditional neural architectures and\nare not well equipped to capture the dynamics of cross modal interactions,\nespecially in the presence of complex intra modal and inter modal correlations.\nTo this end, this paper proposes a novel coarse to fine and end to end\nconnected cross modal place recognition framework, called MambaPlace. In the\ncoarse localization stage, the text description and 3D point cloud are encoded\nby the pretrained T5 and instance encoder, respectively. They are then\nprocessed using Text Attention Mamba (TAM) and Point Clouds Mamba (PCM) for\ndata enhancement and alignment. In the subsequent fine localization stage, the\nfeatures of the text description and 3D point cloud are cross modally fused and\nfurther enhanced through cascaded Cross Attention Mamba (CCAM). Finally, we\npredict the positional offset from the fused text point cloud features,\nachieving the most accurate localization. Extensive experiments show that\nMambaPlace achieves improved localization accuracy on the KITTI360Pose dataset\ncompared to the state of the art methods.\n","authors":["Tianyi Shang","Zhenyu Li","Wenhao Pei","Pengjie Xu","ZhaoJun Deng","Fanchen Kong"],"pdf_url":"https://arxiv.org/pdf/2408.15740v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2408.14035v2","updated":"2024-08-28T12:03:50Z","published":"2024-08-26T06:01:54Z","title":"FAST-LIVO2: Fast, Direct LiDAR-Inertial-Visual Odometry","summary":"  This paper proposes FAST-LIVO2: a fast, direct LiDAR-inertial-visual odometry\nframework to achieve accurate and robust state estimation in SLAM tasks and\nprovide great potential in real-time, onboard robotic applications. FAST-LIVO2\nfuses the IMU, LiDAR and image measurements efficiently through an ESIKF. To\naddress the dimension mismatch between the heterogeneous LiDAR and image\nmeasurements, we use a sequential update strategy in the Kalman filter. To\nenhance the efficiency, we use direct methods for both the visual and LiDAR\nfusion, where the LiDAR module registers raw points without extracting edge or\nplane features and the visual module minimizes direct photometric errors\nwithout extracting ORB or FAST corner features. The fusion of both visual and\nLiDAR measurements is based on a single unified voxel map where the LiDAR\nmodule constructs the geometric structure for registering new LiDAR scans and\nthe visual module attaches image patches to the LiDAR points. To enhance the\naccuracy of image alignment, we use plane priors from the LiDAR points in the\nvoxel map (and even refine the plane prior) and update the reference patch\ndynamically after new images are aligned. Furthermore, to enhance the\nrobustness of image alignment, FAST-LIVO2 employs an on-demanding raycast\noperation and estimates the image exposure time in real time. Lastly, we detail\nthree applications of FAST-LIVO2: UAV onboard navigation demonstrating the\nsystem's computation efficiency for real-time onboard navigation, airborne\nmapping showcasing the system's mapping accuracy, and 3D model rendering\n(mesh-based and NeRF-based) underscoring the suitability of our reconstructed\ndense map for subsequent rendering tasks. We open source our code, dataset and\napplication on GitHub to benefit the robotics community.\n","authors":["Chunran Zheng","Wei Xu","Zuhao Zou","Tong Hua","Chongjian Yuan","Dongjiao He","Bingyang Zhou","Zheng Liu","Jiarong Lin","Fangcheng Zhu","Yunfan Ren","Rong Wang","Fanle Meng","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.14035v2.pdf","comment":"30 pages, 31 figures, due to the limitation that 'The abstract field\n  cannot exceed 1,920 characters', the abstract presented here is shorter than\n  the one in the PDF file"},{"id":"http://arxiv.org/abs/2408.15721v1","updated":"2024-08-28T11:36:43Z","published":"2024-08-28T11:36:43Z","title":"Defending Text-to-image Diffusion Models: Surprising Efficacy of Textual\n  Perturbations Against Backdoor Attacks","summary":"  Text-to-image diffusion models have been widely adopted in real-world\napplications due to their ability to generate realistic images from textual\ndescriptions. However, recent studies have shown that these methods are\nvulnerable to backdoor attacks. Despite the significant threat posed by\nbackdoor attacks on text-to-image diffusion models, countermeasures remain\nunder-explored. In this paper, we address this research gap by demonstrating\nthat state-of-the-art backdoor attacks against text-to-image diffusion models\ncan be effectively mitigated by a surprisingly simple defense strategy -\ntextual perturbation. Experiments show that textual perturbations are effective\nin defending against state-of-the-art backdoor attacks with minimal sacrifice\nto generation quality. We analyze the efficacy of textual perturbation from two\nangles: text embedding space and cross-attention maps. They further explain how\nbackdoor attacks have compromised text-to-image diffusion models, providing\ninsights for studying future attack and defense strategies. Our code is\navailable at https://github.com/oscarchew/t2i-backdoor-defense.\n","authors":["Oscar Chew","Po-Yi Lu","Jayden Lin","Hsuan-Tien Lin"],"pdf_url":"https://arxiv.org/pdf/2408.15721v1.pdf","comment":"ECCV 2024 Workshop The Dark Side of Generative AIs and Beyond"},{"id":"http://arxiv.org/abs/2408.15714v1","updated":"2024-08-28T11:21:23Z","published":"2024-08-28T11:21:23Z","title":"Pixels to Prose: Understanding the art of Image Captioning","summary":"  In the era of evolving artificial intelligence, machines are increasingly\nemulating human-like capabilities, including visual perception and linguistic\nexpression. Image captioning stands at the intersection of these domains,\nenabling machines to interpret visual content and generate descriptive text.\nThis paper provides a thorough review of image captioning techniques, catering\nto individuals entering the field of machine learning who seek a comprehensive\nunderstanding of available options, from foundational methods to\nstate-of-the-art approaches. Beginning with an exploration of primitive\narchitectures, the review traces the evolution of image captioning models to\nthe latest cutting-edge solutions. By dissecting the components of these\narchitectures, readers gain insights into the underlying mechanisms and can\nselect suitable approaches tailored to specific problem requirements without\nduplicating efforts. The paper also delves into the application of image\ncaptioning in the medical domain, illuminating its significance in various\nreal-world scenarios.\n  Furthermore, the review offers guidance on evaluating the performance of\nimage captioning systems, highlighting key metrics for assessment. By\nsynthesizing theoretical concepts with practical application, this paper equips\nreaders with the knowledge needed to navigate the complex landscape of image\ncaptioning and harness its potential for diverse applications in machine\nlearning and beyond.\n","authors":["Hrishikesh Singh","Aarti Sharma","Millie Pant"],"pdf_url":"https://arxiv.org/pdf/2408.15714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15708v1","updated":"2024-08-28T11:13:27Z","published":"2024-08-28T11:13:27Z","title":"Towards Realistic Example-based Modeling via 3D Gaussian Stitching","summary":"  Using parts of existing models to rebuild new models, commonly termed as\nexample-based modeling, is a classical methodology in the realm of computer\ngraphics. Previous works mostly focus on shape composition, making them very\nhard to use for realistic composition of 3D objects captured from real-world\nscenes. This leads to combining multiple NeRFs into a single 3D scene to\nachieve seamless appearance blending. However, the current SeamlessNeRF method\nstruggles to achieve interactive editing and harmonious stitching for\nreal-world scenes due to its gradient-based strategy and grid-based\nrepresentation. To this end, we present an example-based modeling method that\ncombines multiple Gaussian fields in a point-based representation using\nsample-guided synthesis. Specifically, as for composition, we create a GUI to\nsegment and transform multiple fields in real time, easily obtaining a\nsemantically meaningful composition of models represented by 3D Gaussian\nSplatting (3DGS). For texture blending, due to the discrete and irregular\nnature of 3DGS, straightforwardly applying gradient propagation as SeamlssNeRF\nis not supported. Thus, a novel sampling-based cloning method is proposed to\nharmonize the blending while preserving the original rich texture and content.\nOur workflow consists of three steps: 1) real-time segmentation and\ntransformation of a Gaussian model using a well-tailored GUI, 2) KNN analysis\nto identify boundary points in the intersecting area between the source and\ntarget models, and 3) two-phase optimization of the target model using\nsampling-based cloning and gradient constraints. Extensive experimental results\nvalidate that our approach significantly outperforms previous works in terms of\nrealistic synthesis, demonstrating its practicality. More demos are available\nat https://ingra14m.github.io/gs_stitching_website.\n","authors":["Xinyu Gao","Ziyi Yang","Bingchen Gong","Xiaoguang Han","Sipeng Yang","Xiaogang Jin"],"pdf_url":"https://arxiv.org/pdf/2408.15708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10534v2","updated":"2024-08-28T11:12:35Z","published":"2024-07-15T08:42:10Z","title":"Automated Label Unification for Multi-Dataset Semantic Segmentation with\n  GNNs","summary":"  Deep supervised models possess significant capability to assimilate extensive\ntraining data, thereby presenting an opportunity to enhance model performance\nthrough training on multiple datasets. However, conflicts arising from\ndifferent label spaces among datasets may adversely affect model performance.\nIn this paper, we propose a novel approach to automatically construct a unified\nlabel space across multiple datasets using graph neural networks. This enables\nsemantic segmentation models to be trained simultaneously on multiple datasets,\nresulting in performance improvements. Unlike existing methods, our approach\nfacilitates seamless training without the need for additional manual\nreannotation or taxonomy reconciliation. This significantly enhances the\nefficiency and effectiveness of multi-dataset segmentation model training. The\nresults demonstrate that our method significantly outperforms other\nmulti-dataset training methods when trained on seven datasets simultaneously,\nand achieves state-of-the-art performance on the WildDash 2 benchmark.\n","authors":["Rong Ma","Jie Chen","Xiangyang Xue","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2407.10534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11982v2","updated":"2024-08-28T11:01:16Z","published":"2024-08-21T20:32:45Z","title":"AIM 2024 Challenge on Compressed Video Quality Assessment: Methods and\n  Results","summary":"  Video quality assessment (VQA) is a crucial task in the development of video\ncompression standards, as it directly impacts the viewer experience. This paper\npresents the results of the Compressed Video Quality Assessment challenge, held\nin conjunction with the Advances in Image Manipulation (AIM) workshop at ECCV\n2024. The challenge aimed to evaluate the performance of VQA methods on a\ndiverse dataset of 459 videos, encoded with 14 codecs of various compression\nstandards (AVC/H.264, HEVC/H.265, AV1, and VVC/H.266) and containing a\ncomprehensive collection of compression artifacts. To measure the methods\nperformance, we employed traditional correlation coefficients between their\npredictions and subjective scores, which were collected via large-scale\ncrowdsourced pairwise human comparisons. For training purposes, participants\nwere provided with the Compressed Video Quality Assessment Dataset (CVQAD), a\npreviously developed dataset of 1022 videos. Up to 30 participating teams\nregistered for the challenge, while we report the results of 6 teams, which\nsubmitted valid final solutions and code for reproducing the results. Moreover,\nwe calculated and present the performance of state-of-the-art VQA methods on\nthe developed dataset, providing a comprehensive benchmark for future research.\nThe dataset, results, and online leaderboard are publicly available at\nhttps://challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.\n","authors":["Maksim Smirnov","Aleksandr Gushchin","Anastasia Antsiferova","Dmitry Vatolin","Radu Timofte","Ziheng Jia","Zicheng Zhang","Wei Sun","Jiaying Qian","Yuqin Cao","Yinan Sun","Yuxin Zhu","Xiongkuo Min","Guangtao Zhai","Kanjar De","Qing Luo","Ao-Xiang Zhang","Peng Zhang","Haibo Lei","Linyan Jiang","Yaqing Li","Wenhui Meng","Xiaoheng Tan","Haiqiang Wang","Xiaozhong Xu","Shan Liu","Zhenzhong Chen","Zhengxue Cheng","Jiahao Xiao","Jun Xu","Chenlong He","Qi Zheng","Ruoxi Zhu","Min Li","Yibo Fan","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2408.11982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17550v2","updated":"2024-08-28T10:52:32Z","published":"2024-03-26T09:58:06Z","title":"DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping","summary":"  Recently, significant progress has been achieved in sensing real large-scale\noutdoor 3D environments, particularly by using modern acquisition equipment\nsuch as LiDAR sensors. Unfortunately, they are fundamentally limited in their\nability to produce dense, complete 3D scenes. To address this issue, recent\nlearning-based methods integrate neural implicit representations and\noptimizable feature grids to approximate surfaces of 3D scenes. However,\nnaively fitting samples along raw LiDAR rays leads to noisy 3D mapping results\ndue to the nature of sparse, conflicting LiDAR measurements. Instead, in this\nwork we depart from fitting LiDAR data exactly, instead letting the network\noptimize a non-metric monotonic implicit field defined in 3D space. To fit our\nfield, we design a learning system integrating a monotonicity loss that enables\noptimizing neural monotonic fields and leverages recent progress in large-scale\n3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as\ncaptured by multiple quantitative and perceptual measures and visual results\nobtained for Mai City, Newer College, and KITTI benchmarks. The code of our\napproach will be made publicly available.\n","authors":["Kutay YÄ±lmaz","Matthias NieÃner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.17550v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.15695v1","updated":"2024-08-28T10:43:42Z","published":"2024-08-28T10:43:42Z","title":"G-Style: Stylized Gaussian Splatting","summary":"  We introduce G-Style, a novel algorithm designed to transfer the style of an\nimage onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting\nis a powerful 3D representation for novel view synthesis, as -- compared to\nother approaches based on Neural Radiance Fields -- it provides fast scene\nrenderings and user control over the scene. Recent pre-prints have demonstrated\nthat the style of Gaussian Splatting scenes can be modified using an image\nexemplar. However, since the scene geometry remains fixed during the\nstylization process, current solutions fall short of producing satisfactory\nresults. Our algorithm aims to address these limitations by following a\nthree-step process: In a pre-processing step, we remove undesirable Gaussians\nwith large projection areas or highly elongated shapes. Subsequently, we\ncombine several losses carefully designed to preserve different scales of the\nstyle in the image, while maintaining as much as possible the integrity of the\noriginal scene content. During the stylization process and following the\noriginal design of Gaussian Splatting, we split Gaussians where additional\ndetail is necessary within our scene by tracking the gradient of the stylized\ncolor. Our experiments demonstrate that G-Style generates high-quality\nstylizations within just a few minutes, outperforming existing methods both\nqualitatively and quantitatively.\n","authors":["Ãron Samuel KovÃ¡cs","Pedro Hermosilla","Renata G. Raidou"],"pdf_url":"https://arxiv.org/pdf/2408.15695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15693v1","updated":"2024-08-28T10:33:00Z","published":"2024-08-28T10:33:00Z","title":"Synthetic Forehead-creases Biometric Generation for Reliable User\n  Verification","summary":"  Recent studies have emphasized the potential of forehead-crease patterns as\nan alternative for face, iris, and periocular recognition, presenting\ncontactless and convenient solutions, particularly in situations where faces\nare covered by surgical masks. However, collecting forehead data presents\nchallenges, including cost and time constraints, as developing and optimizing\nforehead verification methods requires a substantial number of high-quality\nimages. To tackle these challenges, the generation of synthetic biometric data\nhas gained traction due to its ability to protect privacy while enabling\neffective training of deep learning-based biometric verification methods. In\nthis paper, we present a new framework to synthesize forehead-crease image data\nwhile maintaining important features, such as uniqueness and realism. The\nproposed framework consists of two main modules: a Subject-Specific Generation\nModule (SSGM), based on an image-to-image Brownian Bridge Diffusion Model\n(BBDM), which learns a one-to-many mapping between image pairs to generate\nidentity-aware synthetic forehead creases corresponding to real subjects, and a\nSubject-Agnostic Generation Module (SAGM), which samples new synthetic\nidentities with assistance from the SSGM. We evaluate the diversity and realism\nof the generated forehead-crease images primarily using the Fr\\'echet Inception\nDistance (FID) and the Structural Similarity Index Measure (SSIM). In addition,\nwe assess the utility of synthetically generated forehead-crease images using a\nforehead-crease verification system (FHCVS). The results indicate an\nimprovement in the verification accuracy of the FHCVS by utilizing synthetic\ndata.\n","authors":["Abhishek Tandon","Geetanjali Sharma","Gaurav Jaswal","Aditya Nigam","Raghavendra Ramachandra"],"pdf_url":"https://arxiv.org/pdf/2408.15693v1.pdf","comment":"Accepted at Generative AI for Futuristic Biometrics - IJCB'24 Special\n  Session"},{"id":"http://arxiv.org/abs/2408.15682v1","updated":"2024-08-28T10:12:44Z","published":"2024-08-28T10:12:44Z","title":"A quantitative model of takeover request time budget for conditionally\n  automated driving","summary":"  In conditional automation, the automated driving system assumes full control\nand only issues a takeover request to a human driver to resume driving in\ncritical situations. Previous studies have concluded that the time budget\nrequired by drivers to resume driving after a takeover request varies with\nsituations and different takeover variables. However, no comprehensive\ngeneralized approaches for estimating in advance the time budget required by\ndrivers to takeover have been provided. In this contribution, fixed (7 s) and\nvariable time budgets (6 s, 5 s, and 4 s) with and without visual imagery\nassistance were investigated for suitability in three takeover scenarios using\nperformance measures such as average lateral displacement. The results indicate\nthat 7 s is suitable for two of the studied scenarios based on their\ncharacteristics. Using the obtained results and known relations between\ntakeover variables, a mathematical formula for estimating takeover request time\nbudget is proposed. The proposed formula integrates individual stimulus\nresponse time, driving experience, scenario specific requirements and allows\nincreased safety for takeover maneuvers. Furthermore, the visual imagery\nresulted in increased takeover time which invariably increases the time budget.\nThus the time demand of the visualized information if applicable (such as\nvisual imagery) should be included in the time budget.\n","authors":["Foghor Tanshi","Dirk SÃ¶ffker"],"pdf_url":"https://arxiv.org/pdf/2408.15682v1.pdf","comment":"Manuscript: 12 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2408.15679v1","updated":"2024-08-28T10:08:38Z","published":"2024-08-28T10:08:38Z","title":"DEAR: Depth-Enhanced Action Recognition","summary":"  Detecting actions in videos, particularly within cluttered scenes, poses\nsignificant challenges due to the limitations of 2D frame analysis from a\ncamera perspective. Unlike human vision, which benefits from 3D understanding,\nrecognizing actions in such environments can be difficult. This research\nintroduces a novel approach integrating 3D features and depth maps alongside\nRGB features to enhance action recognition accuracy. Our method involves\nprocessing estimated depth maps through a separate branch from the RGB feature\nencoder and fusing the features to understand the scene and actions\ncomprehensively. Using the Side4Video framework and VideoMamba, which employ\nCLIP and VisionMamba for spatial feature extraction, our approach outperformed\nour implementation of the Side4Video network on the Something-Something V2\ndataset. Our code is available at: https://github.com/SadeghRahmaniB/DEAR\n","authors":["Sadegh Rahmaniboldaji","Filip Rybansky","Quoc Vuong","Frank Guerin","Andrew Gilbert"],"pdf_url":"https://arxiv.org/pdf/2408.15679v1.pdf","comment":"5 pages, 1 figure, 1 table, accepted at Human-inspired Computer\n  Vision, ECCV"},{"id":"http://arxiv.org/abs/2408.15678v1","updated":"2024-08-28T10:07:17Z","published":"2024-08-28T10:07:17Z","title":"Deep Learning Based Speckle Filtering for Polarimetric SAR Images.\n  Application to Sentinel-1","summary":"  Speckle suppression in synthetic aperture radar (SAR) images is a key\nprocessing step which continues to be a research topic. A wide variety of\nmethods, using either spatially-based approaches or transform-based strategies,\nhave been developed and have shown to provide outstanding results. However,\nrecent advances in deep learning techniques and their application to SAR image\ndespeckling have been demonstrated to offer state-of-the-art results.\nUnfortunately, they have been mostly applied to single-polarimetric images. The\nextension of a deep learning-based approach for speckle removal to polarimetric\nSAR (PolSAR) images is complicated because of the complex nature of the\nmeasured covariance matrices for every image pixel, the properties of which\nmust be preserved during filtering. In this work, we propose a complete\nframework to remove speckle in polarimetric SAR images using a convolutional\nneural network. The methodology includes a reversible transformation of the\noriginal complex covariance matrix to obtain a set of real-valued intensity\nbands which are fed to the neural network. In addition, the proposed method\nincludes a change detection strategy to avoid the neural network to learn\nerroneous features in areas strongly affected by temporal changes, so that the\nnetwork only learns the underlying speckle component present in the data. The\nmethod is implemented and tested with dual-polarimetric images acquired by\nSentinel-1. Experiments show that the proposed approach offers exceptional\nresults in both speckle reduction and resolution preservation. More\nimportantly, it is also shown that the neural network is not generating\nartifacts or introducing bias in the filtered images, making them suitable for\nfurther polarimetric processing and exploitation.\n","authors":["Alejandro Mestre-Quereda","Juan M. Lopez-Sanchez"],"pdf_url":"https://arxiv.org/pdf/2408.15678v1.pdf","comment":"23 pages, 32 figures"},{"id":"http://arxiv.org/abs/2312.03187v3","updated":"2024-08-28T10:00:01Z","published":"2023-12-05T23:33:49Z","title":"FERGI: Automatic Annotation of User Preferences for Text-to-Image\n  Generation from Spontaneous Facial Expression Reaction","summary":"  Researchers have proposed to use data of human preference feedback to\nfine-tune text-to-image generative models. However, the scalability of human\nfeedback collection has been limited by its reliance on manual annotation.\nTherefore, we develop and test a method to automatically score user preferences\nfrom their spontaneous facial expression reaction to the generated images. We\ncollect a dataset of Facial Expression Reaction to Generated Images (FERGI) and\nshow that the activations of multiple facial action units (AUs) are highly\ncorrelated with user evaluations of the generated images. We develop an FAU-Net\n(Facial Action Units Neural Network), which receives inputs from an AU\nestimation model, to automatically score user preferences for text-to-image\ngeneration based on their facial expression reactions, which is complementary\nto the pre-trained scoring models based on the input text prompts and generated\nimages. Integrating our FAU-Net valence score with the pre-trained scoring\nmodels improves their consistency with human preferences. This method of\nautomatic annotation with facial expression analysis can be potentially\ngeneralized to other generation tasks. The code is available at\nhttps://github.com/ShuangquanFeng/FERGI, and the dataset is also available at\nthe same link for research purposes.\n","authors":["Shuangquan Feng","Junhua Ma","Virginia R. de Sa"],"pdf_url":"https://arxiv.org/pdf/2312.03187v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14930v2","updated":"2024-08-28T09:50:00Z","published":"2024-08-27T10:09:17Z","title":"CMTA: Cross-Modal Temporal Alignment for Event-guided Video Deblurring","summary":"  Video deblurring aims to enhance the quality of restored results in\nmotion-blurred videos by effectively gathering information from adjacent video\nframes to compensate for the insufficient data in a single blurred frame.\nHowever, when faced with consecutively severe motion blur situations,\nframe-based video deblurring methods often fail to find accurate temporal\ncorrespondence among neighboring video frames, leading to diminished\nperformance. To address this limitation, we aim to solve the video deblurring\ntask by leveraging an event camera with micro-second temporal resolution. To\nfully exploit the dense temporal resolution of the event camera, we propose two\nmodules: 1) Intra-frame feature enhancement operates within the exposure time\nof a single blurred frame, iteratively enhancing cross-modality features in a\nrecurrent manner to better utilize the rich temporal information of events, 2)\nInter-frame temporal feature alignment gathers valuable long-range temporal\ninformation to target frames, aggregating sharp features leveraging the\nadvantages of the events. In addition, we present a novel dataset composed of\nreal-world blurred RGB videos, corresponding sharp videos, and event data. This\ndataset serves as a valuable resource for evaluating event-guided deblurring\nmethods. We demonstrate that our proposed methods outperform state-of-the-art\nframe-based and event-based motion deblurring methods through extensive\nexperiments conducted on both synthetic and real-world deblurring datasets. The\ncode and dataset are available at https://github.com/intelpro/CMTA.\n","authors":["Taewoo Kim","Hoonhee Cho","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2408.14930v2.pdf","comment":"Accepted in ECCV2024"},{"id":"http://arxiv.org/abs/2401.12471v2","updated":"2024-08-28T09:48:24Z","published":"2024-01-23T03:45:05Z","title":"Training-Free Action Recognition and Goal Inference with Dynamic Frame\n  Selection","summary":"  We introduce VidTFS, a Training-free, open-vocabulary video goal and action\ninference framework that combines the frozen vision foundational model (VFM)\nand large language model (LLM) with a novel dynamic Frame Selection module. Our\nexperiments demonstrate that the proposed frame selection module improves the\nperformance of the framework significantly. We validate the performance of the\nproposed VidTFS on four widely used video datasets, including CrossTask, COIN,\nUCF101, and ActivityNet, covering goal inference and action recognition tasks\nunder open-vocabulary settings without requiring any training or fine-tuning.\nThe results show that VidTFS outperforms pretrained and instruction-tuned\nmultimodal language models that directly stack LLM and VFM for downstream video\ninference tasks. Our VidTFS with its adaptability shows the future potential\nfor generalizing to new training-free video inference tasks.\n","authors":["Ee Yeo Keat","Zhang Hao","Alexander Matyasko","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2401.12471v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15048v2","updated":"2024-08-28T09:42:44Z","published":"2024-01-26T18:20:53Z","title":"Unrecognizable Yet Identifiable: Image Distortion with Preserved\n  Embeddings","summary":"  Biometric authentication systems play a crucial role in modern security\nsystems. However, maintaining the balance of privacy and integrity of stored\nbiometrics derivative data while achieving high recognition accuracy is often\nchallenging. Addressing this issue, we introduce an innovative image\ntransformation technique that effectively renders facial images unrecognizable\nto the eye while maintaining their identifiability by neural network models,\nwhich allows the distorted photo version to be stored for further verification.\nWhile initially intended for biometrics systems, the proposed methodology can\nbe used in various artificial intelligence applications to distort the visual\ndata and keep the derived features close. By experimenting with widely used\ndatasets LFW and MNIST, we show that it is possible to build the distortion\nthat changes the image content by more than 70% while maintaining the same\nrecognition accuracy. We compare our method with previously state-of-the-art\napproaches. We publically release the source code.\n","authors":["Dmytro Zakharov","Oleksandr Kuznetsov","Emanuele Frontoni"],"pdf_url":"https://arxiv.org/pdf/2401.15048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15667v1","updated":"2024-08-28T09:40:40Z","published":"2024-08-28T09:40:40Z","title":"Towards reliable respiratory disease diagnosis based on cough sounds and\n  vision transformers","summary":"  Recent advancements in deep learning techniques have sparked performance\nboosts in various real-world applications including disease diagnosis based on\nmulti-modal medical data. Cough sound data-based respiratory disease (e.g.,\nCOVID-19 and Chronic Obstructive Pulmonary Disease) diagnosis has also\nattracted much attention. However, existing works usually utilise traditional\nmachine learning or deep models of moderate scales. On the other hand, the\ndeveloped approaches are trained and evaluated on small-scale data due to the\ndifficulty of curating and annotating clinical data on scale. To address these\nissues in prior works, we create a unified framework to evaluate various deep\nmodels from lightweight Convolutional Neural Networks (e.g., ResNet18) to\nmodern vision transformers and compare their performance in respiratory disease\nclassification. Based on the observations from such an extensive empirical\nstudy, we propose a novel approach to cough-based disease classification based\non both self-supervised and supervised learning on a large-scale cough data\nset. Experimental results demonstrate our proposed approach outperforms prior\narts consistently on two benchmark datasets for COVID-19 diagnosis and a\nproprietary dataset for COPD/non-COPD classification with an AUROC of 92.5%.\n","authors":["Qian Wang","Zhaoyang Bu","Jiaxuan Mao","Wenyu Zhu","Jingya Zhao","Wei Du","Guochao Shi","Min Zhou","Si Chen","Jieming Qu"],"pdf_url":"https://arxiv.org/pdf/2408.15667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02044v4","updated":"2024-08-28T09:34:33Z","published":"2023-10-03T13:35:49Z","title":"How Physics and Background Attributes Impact Video Transformers in\n  Robotic Manipulation: A Case Study on Planar Pushing","summary":"  As model and dataset sizes continue to scale in robot learning, the need to\nunderstand how the composition and properties of a dataset affect model\nperformance becomes increasingly urgent to ensure cost-effective data\ncollection and model performance. In this work, we empirically investigate how\nphysics attributes (color, friction coefficient, shape) and scene background\ncharacteristics, such as the complexity and dynamics of interactions with\nbackground objects, influence the performance of Video Transformers in\npredicting planar pushing trajectories. We investigate three primary questions:\nHow do physics attributes and background scene characteristics influence model\nperformance? What kind of changes in attributes are most detrimental to model\ngeneralization? What proportion of fine-tuning data is required to adapt models\nto novel scenarios? To facilitate this research, we present\nCloudGripper-Push-1K, a large real-world vision-based robot pushing dataset\ncomprising 1278 hours and 460,000 videos of planar pushing interactions with\nobjects with different physics and background attributes. We also propose Video\nOcclusion Transformer (VOT), a generic modular video-transformer-based\ntrajectory prediction framework which features 3 choices of 2D-spatial encoders\nas the subject of our case study. The dataset and source code are available at\nhttps://cloudgripper.org.\n","authors":["Shutong Jin","Ruiyu Wang","Muhammad Zahid","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2310.02044v4.pdf","comment":"IEEE/RSJ IROS 2024"},{"id":"http://arxiv.org/abs/2408.15660v1","updated":"2024-08-28T09:22:32Z","published":"2024-08-28T09:22:32Z","title":"Merging and Splitting Diffusion Paths for Semantically Coherent\n  Panoramas","summary":"  Diffusion models have become the State-of-the-Art for text-to-image\ngeneration, and increasing research effort has been dedicated to adapting the\ninference process of pretrained diffusion models to achieve zero-shot\ncapabilities. An example is the generation of panorama images, which has been\ntackled in recent works by combining independent diffusion paths over\noverlapping latent features, which is referred to as joint diffusion, obtaining\nperceptually aligned panoramas. However, these methods often yield semantically\nincoherent outputs and trade-off diversity for uniformity. To overcome this\nlimitation, we propose the Merge-Attend-Diffuse operator, which can be plugged\ninto different types of pretrained diffusion models used in a joint diffusion\nsetting to improve the perceptual and semantical coherence of the generated\npanorama images. Specifically, we merge the diffusion paths, reprogramming\nself- and cross-attention to operate on the aggregated latent space. Extensive\nquantitative and qualitative experimental analysis, together with a user study,\ndemonstrate that our method maintains compatibility with the input prompt and\nvisual quality of the generated images while increasing their semantic\ncoherence. We release the code at https://github.com/aimagelab/MAD.\n","authors":["Fabio Quattrini","Vittorio Pippi","Silvia Cascianelli","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2408.15660v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2408.15657v1","updated":"2024-08-28T09:18:36Z","published":"2024-08-28T09:18:36Z","title":"TeFF: Tracking-enhanced Forgetting-free Few-shot 3D LiDAR Semantic\n  Segmentation","summary":"  In autonomous driving, 3D LiDAR plays a crucial role in understanding the\nvehicle's surroundings. However, the newly emerged, unannotated objects\npresents few-shot learning problem for semantic segmentation. This paper\naddresses the limitations of current few-shot semantic segmentation by\nexploiting the temporal continuity of LiDAR data. Employing a tracking model to\ngenerate pseudo-ground-truths from a sequence of LiDAR frames, our method\nsignificantly augments the dataset, enhancing the model's ability to learn on\nnovel classes. However, this approach introduces a data imbalance biased to\nnovel data that presents a new challenge of catastrophic forgetting. To\nmitigate this, we incorporate LoRA, a technique that reduces the number of\ntrainable parameters, thereby preserving the model's performance on base\nclasses while improving its adaptability to novel classes. This work represents\na significant step forward in few-shot 3D LiDAR semantic segmentation for\nautonomous driving. Our code is available at\nhttps://github.com/junbao-zhou/Track-no-forgetting.\n","authors":["Junbao Zhou","Jilin Mei","Pengze Wu","Liang Chen","Fangzhou Zhao","Xijun Zhao","Yu Hu"],"pdf_url":"https://arxiv.org/pdf/2408.15657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13123v2","updated":"2024-08-28T09:18:00Z","published":"2024-08-23T14:50:49Z","title":"Evidential Deep Partial Multi-View Classification With Discount Fusion","summary":"  Incomplete multi-view data classification poses significant challenges due to\nthe common issue of missing views in real-world scenarios. Despite\nadvancements, existing methods often fail to provide reliable predictions,\nlargely due to the uncertainty of missing views and the inconsistent quality of\nimputed data. To tackle these problems, we propose a novel framework called\nEvidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we use\nK-means imputation to address missing views, creating a complete set of\nmulti-view data. However, the potential conflicts and uncertainties within this\nimputed data can affect the reliability of downstream inferences. To manage\nthis, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), which\ndynamically adjusts based on the reliability of the evidence, ensuring\ntrustworthy discount fusion and producing reliable inference outcomes.\nComprehensive experiments on various benchmark datasets reveal EDP-MVC not only\nmatches but often surpasses the performance of state-of-the-art methods.\n","authors":["Haojian Huang","Zhe Liu","Sukumar Letchmunan","Muhammet Deveci","Mingwei Lin","Weizhong Wang"],"pdf_url":"https://arxiv.org/pdf/2408.13123v2.pdf","comment":"Ongoing work. 13 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.15656v1","updated":"2024-08-28T09:17:25Z","published":"2024-08-28T09:17:25Z","title":"Realigned Softmax Warping for Deep Metric Learning","summary":"  Deep Metric Learning (DML) loss functions traditionally aim to control the\nforces of separability and compactness within an embedding space so that the\nsame class data points are pulled together and different class ones are pushed\napart. Within the context of DML, a softmax operation will typically normalize\ndistances into a probability for optimization, thus coupling all the push/pull\nforces together. This paper proposes a potential new class of loss functions\nthat operate within a euclidean domain and aim to take full advantage of the\ncoupled forces governing embedding space formation under a softmax. These\nforces of compactness and separability can be boosted or mitigated within\ncontrolled locations at will by using a warping function. In this work, we\nprovide a simple example of a warping function and use it to achieve\ncompetitive, state-of-the-art results on various metric learning benchmarks.\n","authors":["Michael G. DeMoor","John J. Prevost"],"pdf_url":"https://arxiv.org/pdf/2408.15656v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2408.15119v2","updated":"2024-08-28T09:11:55Z","published":"2024-08-27T14:58:13Z","title":"Urdu Digital Text Word Optical Character Recognition Using Permuted Auto\n  Regressive Sequence Modeling","summary":"  This research paper presents a novel word-level Optical Character Recognition\n(OCR) model developed specifically for digital Urdu text. The model utilizes\ntransformer-based architectures and attention mechanisms to address the unique\nchallenges of recognizing Urdu script, which includes handling a diverse range\nof text styles, fonts, and variations. Trained on a comprehensive dataset of\napproximately 160,000 Urdu text images, the model incorporates a permuted\nautoregressive sequence (PARSeq) architecture. This design enables\ncontext-aware inference and iterative refinement by leveraging bidirectional\ncontext information, significantly enhancing its ability to accurately\nrecognize Urdu characters. The model achieves a character error rate (CER) of\n0.178, highlighting its effectiveness and precision in real-world applications.\nHowever, the model has some limitations, such as difficulties with blurred\nimages, non-horizontal orientations, and the presence of trailing punctuation\nmarks, which can introduce noise into the recognition process. Addressing these\nchallenges will be a key focus of future work. Future research will aim to\nfurther refine the model through advanced data augmentation techniques,\noptimization of hyperparameters, and the integration of context-aware language\nmodels, ultimately enhancing the model's performance and robustness in Urdu\ntext recognition.\n","authors":["Ahmed Mustafa","Muhammad Tahir Rafique","Muhammad Ijlal Baig","Hasan Sajid","Muhammad Jawad Khan","Karam Dad Kallu"],"pdf_url":"https://arxiv.org/pdf/2408.15119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00467v2","updated":"2024-08-28T09:11:40Z","published":"2024-03-01T11:45:29Z","title":"When ControlNet Meets Inexplicit Masks: A Case Study of ControlNet on\n  its Contour-following Ability","summary":"  ControlNet excels at creating content that closely matches precise contours\nin user-provided masks. However, when these masks contain noise, as a frequent\noccurrence with non-expert users, the output would include unwanted artifacts.\nThis paper first highlights the crucial role of controlling the impact of these\ninexplicit masks with diverse deterioration levels through in-depth analysis.\nSubsequently, to enhance controllability with inexplicit masks, an advanced\nShape-aware ControlNet consisting of a deterioration estimator and a\nshape-prior modulation block is devised. The deterioration estimator assesses\nthe deterioration factor of the provided masks. Then this factor is utilized in\nthe modulation block to adaptively modulate the model's contour-following\nability, which helps it dismiss the noise part in the inexplicit masks.\nExtensive experiments prove its effectiveness in encouraging ControlNet to\ninterpret inaccurate spatial conditions robustly rather than blindly following\nthe given contours. We showcase application scenarios like modifying shape\npriors and composable shape-controllable generation. Codes are soon available.\n","authors":["Wenjie Xuan","Yufei Xu","Shanshan Zhao","Chaoyue Wang","Juhua Liu","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2403.00467v2.pdf","comment":"Accepted by ACM-MM 2024"},{"id":"http://arxiv.org/abs/2401.11790v2","updated":"2024-08-28T09:09:34Z","published":"2024-01-22T09:40:52Z","title":"Deep Learning for Computer Vision based Activity Recognition and Fall\n  Detection of the Elderly: a Systematic Review","summary":"  As the percentage of elderly people in developed countries increases\nworldwide, the healthcare of this collective is a worrying matter, especially\nif it includes the preservation of their autonomy. In this direction, many\nstudies are being published on Ambient Assisted Living (AAL) systems, which\nhelp to reduce the preoccupations raised by the independent living of the\nelderly. In this study, a systematic review of the literature is presented on\nfall detection and Human Activity Recognition (HAR) for the elderly, as the two\nmain tasks to solve to guarantee the safety of elderly people living alone. To\naddress the current tendency to perform these two tasks, the review focuses on\nthe use of Deep Learning (DL) based approaches on computer vision data. In\naddition, different collections of data like DL models, datasets or hardware\n(e.g. depth or thermal cameras) are gathered from the reviewed studies and\nprovided for reference in future studies. Strengths and weaknesses of existing\napproaches are also discussed and, based on them, our recommendations for\nfuture works are provided.\n","authors":["F. Xavier Gaya-Morey","Cristina Manresa-Yee","Jose M. Buades-Rubio"],"pdf_url":"https://arxiv.org/pdf/2401.11790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15651v1","updated":"2024-08-28T09:07:40Z","published":"2024-08-28T09:07:40Z","title":"Online pre-training with long-form videos","summary":"  In this study, we investigate the impact of online pre-training with\ncontinuous video clips. We will examine three methods for pre-training (masked\nimage modeling, contrastive learning, and knowledge distillation), and assess\nthe performance on downstream action recognition tasks. As a result, online\npre-training with contrast learning showed the highest performance in\ndownstream tasks. Our findings suggest that learning from long-form videos can\nbe helpful for action recognition with short videos.\n","authors":["Itsuki Kato","Kodai Kamiya","Toru Tamaki"],"pdf_url":"https://arxiv.org/pdf/2408.15651v1.pdf","comment":"GCCE2024"},{"id":"http://arxiv.org/abs/2408.04957v3","updated":"2024-08-28T09:05:01Z","published":"2024-08-09T09:22:40Z","title":"LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial\n  Description","summary":"  Visual Spatial Description (VSD) aims to generate texts that describe the\nspatial relationships between objects within images. Traditional visual spatial\nrelationship classification (VSRC) methods typically output the spatial\nrelationship between two objects in an image, often neglecting world knowledge\nand lacking general language capabilities. In this paper, we propose a Large\nLanguage-and-Vision Assistant for Visual Spatial Description, named LLaVA-VSD,\nwhich is designed for the classification, description, and open-ended\ndescription of visual spatial relationships. Specifically, the model first\nconstructs a VSD instruction-following dataset using given figure-caption pairs\nfor the three tasks. It then employs LoRA to fine-tune a Large Language and\nVision Assistant for VSD, which has 13 billion parameters and supports\nhigh-resolution images. Finally, a large language model (Qwen-2) is used to\nrefine the generated sentences, enhancing their diversity and accuracy.\nLLaVA-VSD demonstrates excellent multimodal conversational capabilities and can\nfollow open-ended instructions to assist with inquiries about object\nrelationships in images.\n","authors":["Yizhang Jin","Jian Li","Jiangning Zhang","Jianlong Hu","Zhenye Gan","Xin Tan","Yong Liu","Yabiao Wang","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2408.04957v3.pdf","comment":"We have discovered a significant error in the paper that affects the\n  main conclusions. To ensure the accuracy of our research, we have decided to\n  withdraw this paper and will resubmit it after making the necessary\n  corrections"},{"id":"http://arxiv.org/abs/2408.15647v1","updated":"2024-08-28T09:01:55Z","published":"2024-08-28T09:01:55Z","title":"Leveraging Persistent Homology for Differential Diagnosis of Mild\n  Cognitive Impairment","summary":"  Mild cognitive impairment (MCI) is characterized by subtle changes in\ncognitive functions, often associated with disruptions in brain connectivity.\nThe present study introduces a novel fine-grained analysis to examine\ntopological alterations in neurodegeneration pertaining to six different brain\nnetworks of MCI subjects (Early/Late MCI). To achieve this, fMRI time series\nfrom two distinct populations are investigated: (i) the publicly accessible\nADNI dataset and (ii) our in-house dataset. The study utilizes sliding window\nembedding to convert each fMRI time series into a sequence of 3-dimensional\nvectors, facilitating the assessment of changes in regional brain topology.\nDistinct persistence diagrams are computed for Betti descriptors of\ndimension-0, 1, and 2. Wasserstein distance metric is used to quantify\ndifferences in topological characteristics. We have examined both (i)\nROI-specific inter-subject interactions and (ii) subject-specific inter-ROI\ninteractions. Further, a new deep learning model is proposed for\nclassification, achieving a maximum classification accuracy of 95% for the ADNI\ndataset and 85% for the in-house dataset. This methodology is further adapted\nfor the differential diagnosis of MCI sub-types, resulting in a peak accuracy\nof 76.5%, 91.1% and 80% in classifying HC Vs. EMCI, HC Vs. LMCI and EMCI Vs.\nLMCI, respectively. We showed that the proposed approach surpasses current\nstate-of-the-art techniques designed for classifying MCI and its sub-types\nusing fMRI.\n","authors":["Ninad Aithal","Debanjali Bhattacharya","Neelam Sinha","Thomas Gregor Issac"],"pdf_url":"https://arxiv.org/pdf/2408.15647v1.pdf","comment":"16 pages, 6 figures, 3 tables, accepted at International Conference\n  on Pattern Recognition 2024"},{"id":"http://arxiv.org/abs/2402.09066v2","updated":"2024-08-28T09:01:37Z","published":"2024-02-14T10:24:04Z","title":"Solid Waste Detection, Monitoring and Mapping in Remote Sensing Images:\n  A Survey","summary":"  The detection and characterization of illegal solid waste disposal sites are\nessential for environmental protection, particularly for mitigating pollution\nand health hazards. Improperly managed landfills contaminate soil and\ngroundwater via rainwater infiltration, posing threats to both animals and\nhumans. Traditional landfill identification approaches, such as on-site\ninspections, are time-consuming and expensive. Remote sensing is a\ncost-effective solution for the identification and monitoring of solid waste\ndisposal sites that enables broad coverage and repeated acquisitions over time.\nEarth Observation (EO) satellites, equipped with an array of sensors and\nimaging capabilities, have been providing high-resolution data for several\ndecades. Researchers proposed specialized techniques that leverage remote\nsensing imagery to perform a range of tasks such as waste site detection,\ndumping site monitoring, and assessment of suitable locations for new\nlandfills. This review aims to provide a detailed illustration of the most\nrelevant proposals for the detection and monitoring of solid waste sites by\ndescribing and comparing the approaches, the implemented techniques, and the\nemployed data. Furthermore, since the data sources are of the utmost importance\nfor developing an effective solid waste detection model, a comprehensive\noverview of the satellites and publicly available data sets is presented.\nFinally, this paper identifies the open issues in the state-of-the-art and\ndiscusses the relevant research directions for reducing the costs and improving\nthe effectiveness of novel solid waste detection methods.\n","authors":["Piero Fraternali","Luca Morandini","Sergio Luis Herrera GonzÃ¡lez"],"pdf_url":"https://arxiv.org/pdf/2402.09066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15646v1","updated":"2024-08-28T09:01:18Z","published":"2024-08-28T09:01:18Z","title":"Î¼gat: Improving Single-Page Document Parsing by Providing Multi-Page\n  Context","summary":"  Regesta are catalogs of summaries of other documents and, in some cases, are\nthe only source of information about the content of such full-length documents.\nFor this reason, they are of great interest to scholars in many social and\nhumanities fields. In this work, we focus on Regesta Pontificum Romanum, a\nlarge collection of papal registers. Regesta are visually rich documents, where\nthe layout is as important as the text content to convey the contained\ninformation through the structure, and are inherently multi-page documents.\nAmong Digital Humanities techniques that can help scholars efficiently exploit\nregesta and other documental sources in the form of scanned documents, Document\nParsing has emerged as a task to process document images and convert them into\nmachine-readable structured representations, usually markup language. However,\ncurrent models focus on scientific and business documents, and most of them\nconsider only single-paged documents. To overcome this limitation, in this\nwork, we propose {\\mu}gat, an extension of the recently proposed Document\nparsing Nougat architecture, which can handle elements spanning over the single\npage limits. Specifically, we adapt Nougat to process a larger, multi-page\ncontext, consisting of the previous and the following page, while parsing the\ncurrent page. Experimental results, both qualitative and quantitative,\ndemonstrate the effectiveness of our proposed approach also in the case of the\nchallenging Regesta Pontificum Romanorum.\n","authors":["Fabio Quattrini","Carmine Zaccagnino","Silvia Cascianelli","Laura Righi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2408.15646v1.pdf","comment":"Accepted at ECCV Workshop \"AI4DH: Artificial Intelligence for Digital\n  Humanities\""},{"id":"http://arxiv.org/abs/2408.15643v1","updated":"2024-08-28T08:53:33Z","published":"2024-08-28T08:53:33Z","title":"RIDE: Boosting 3D Object Detection for LiDAR Point Clouds via\n  Rotation-Invariant Analysis","summary":"  The rotation robustness property has drawn much attention to point cloud\nanalysis, whereas it still poses a critical challenge in 3D object detection.\nWhen subjected to arbitrary rotation, most existing detectors fail to produce\nexpected outputs due to the poor rotation robustness. In this paper, we present\nRIDE, a pioneering exploration of Rotation-Invariance for the 3D\nLiDAR-point-based object DEtector, with the key idea of designing\nrotation-invariant features from LiDAR scenes and then effectively\nincorporating them into existing 3D detectors. Specifically, we design a\nbi-feature extractor that extracts (i) object-aware features though sensitive\nto rotation but preserve geometry well, and (ii) rotation-invariant features,\nwhich lose geometric information to a certain extent but are robust to\nrotation. These two kinds of features complement each other to decode 3D\nproposals that are robust to arbitrary rotations. Particularly, our RIDE is\ncompatible and easy to plug into the existing one-stage and two-stage 3D\ndetectors, and boosts both detection performance and rotation robustness.\nExtensive experiments on the standard benchmarks showcase that the mean average\nprecision (mAP) and rotation robustness can be significantly boosted by\nintegrating with our RIDE, with +5.6% mAP and 53% rotation robustness\nimprovement on KITTI, +5.1% and 28% improvement correspondingly on nuScenes.\nThe code will be available soon.\n","authors":["Zhaoxuan Wang","Xu Han","Hongxin Liu","Xianzhi Li"],"pdf_url":"https://arxiv.org/pdf/2408.15643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15642v1","updated":"2024-08-28T08:53:20Z","published":"2024-08-28T08:53:20Z","title":"Can SAR improve RSVQA performance?","summary":"  Remote sensing visual question answering (RSVQA) has been involved in several\nresearch in recent years, leading to an increase in new methods. RSVQA\nautomatically extracts information from satellite images, so far only optical,\nand a question to automatically search for the answer in the image and provide\nit in a textual form. In our research, we study whether Synthetic Aperture\nRadar (SAR) images can be beneficial to this field. We divide our study into\nthree phases which include classification methods and VQA. In the first one, we\nexplore the classification results of SAR alone and investigate the best method\nto extract information from SAR data. Then, we study the combination of SAR and\noptical data. In the last phase, we investigate how SAR images and a\ncombination of different modalities behave in RSVQA compared to a method only\nusing optical images. We conclude that adding the SAR modality leads to\nimproved performances, although further research on using SAR data to\nautomatically answer questions is needed as well as more balanced datasets.\n","authors":["Lucrezia Tosato","Sylvain Lobry","Flora Weissgerber","Laurent Wendling"],"pdf_url":"https://arxiv.org/pdf/2408.15642v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.15641v1","updated":"2024-08-28T08:52:33Z","published":"2024-08-28T08:52:33Z","title":"MMDRFuse: Distilled Mini-Model with Dynamic Refresh for Multi-Modality\n  Image Fusion","summary":"  In recent years, Multi-Modality Image Fusion (MMIF) has been applied to many\nfields, which has attracted many scholars to endeavour to improve the fusion\nperformance. However, the prevailing focus has predominantly been on the\narchitecture design, rather than the training strategies. As a low-level vision\ntask, image fusion is supposed to quickly deliver output images for observation\nand supporting downstream tasks. Thus, superfluous computational and storage\noverheads should be avoided. In this work, a lightweight Distilled Mini-Model\nwith a Dynamic Refresh strategy (MMDRFuse) is proposed to achieve this\nobjective. To pursue model parsimony, an extremely small convolutional network\nwith a total of 113 trainable parameters (0.44 KB) is obtained by three\ncarefully designed supervisions. First, digestible distillation is constructed\nby emphasising external spatial feature consistency, delivering soft\nsupervision with balanced details and saliency for the target network. Second,\nwe develop a comprehensive loss to balance the pixel, gradient, and perception\nclues from the source images. Third, an innovative dynamic refresh training\nstrategy is used to collaborate history parameters and current supervision\nduring training, together with an adaptive adjust function to optimise the\nfusion network. Extensive experiments on several public datasets demonstrate\nthat our method exhibits promising advantages in terms of model efficiency and\ncomplexity, with superior performance in multiple image fusion tasks and\ndownstream pedestrian detection application. The code of this work is publicly\navailable at https://github.com/yanglinDeng/MMDRFuse.\n","authors":["Yanglin Deng","Tianyang Xu","Chunyang Cheng","Xiao-Jun Wu","Josef Kittler"],"pdf_url":"https://arxiv.org/pdf/2408.15641v1.pdf","comment":"10 pages, 8 figures, accpeted by ACM International Conference on\n  Multimedia 2024(Oral)"},{"id":"http://arxiv.org/abs/2407.02392v4","updated":"2024-08-28T08:49:57Z","published":"2024-07-02T16:10:55Z","title":"TokenPacker: Efficient Visual Projector for Multimodal LLM","summary":"  The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker.\n","authors":["Wentong Li","Yuqian Yuan","Jian Liu","Dongqi Tang","Song Wang","Jie Qin","Jianke Zhu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.02392v4.pdf","comment":"16 pages, Codes:https://github.com/CircleRadon/TokenPacker"},{"id":"http://arxiv.org/abs/2408.15637v1","updated":"2024-08-28T08:44:58Z","published":"2024-08-28T08:44:58Z","title":"Transfer Learning from Simulated to Real Scenes for Monocular 3D Object\n  Detection","summary":"  Accurately detecting 3D objects from monocular images in dynamic roadside\nscenarios remains a challenging problem due to varying camera perspectives and\nunpredictable scene conditions. This paper introduces a two-stage training\nstrategy to address these challenges. Our approach initially trains a model on\nthe large-scale synthetic dataset, RoadSense3D, which offers a diverse range of\nscenarios for robust feature learning. Subsequently, we fine-tune the model on\na combination of real-world datasets to enhance its adaptability to practical\nconditions. Experimental results of the Cube R-CNN model on challenging public\nbenchmarks show a remarkable improvement in detection performance, with a mean\naverage precision rising from 0.26 to 12.76 on the TUM Traffic A9 Highway\ndataset and from 2.09 to 6.60 on the DAIR-V2X-I dataset when performing\ntransfer learning. Code, data, and qualitative video results are available on\nthe project website: https://roadsense3d.github.io.\n","authors":["Sondos Mohamed","Walter Zimmer","Ross Greer","Ahmed Alaaeldin Ghita","Modesto CastrillÃ³n-Santana","Mohan Trivedi","Alois Knoll","Salvatore Mario Carta","Mirko Marras"],"pdf_url":"https://arxiv.org/pdf/2408.15637v1.pdf","comment":"18 pages. Accepted for ECVA European Conference on Computer Vision\n  2024 (ECCV'24)"},{"id":"http://arxiv.org/abs/2401.11835v2","updated":"2024-08-28T08:38:43Z","published":"2024-01-22T10:52:02Z","title":"Unveiling the Human-like Similarities of Automatic Facial Expression\n  Recognition: An Empirical Exploration through Explainable AI","summary":"  Facial expression recognition is vital for human behavior analysis, and deep\nlearning has enabled models that can outperform humans. However, it is unclear\nhow closely they mimic human processing. This study aims to explore the\nsimilarity between deep neural networks and human perception by comparing\ntwelve different networks, including both general object classifiers and\nFER-specific models. We employ an innovative global explainable AI method to\ngenerate heatmaps, revealing crucial facial regions for the twelve networks\ntrained on six facial expressions. We assess these results both quantitatively\nand qualitatively, comparing them to ground truth masks based on Friesen and\nEkman's description and among them. We use Intersection over Union (IoU) and\nnormalized correlation coefficients for comparisons. We generate 72 heatmaps to\nhighlight critical regions for each expression and architecture. Qualitatively,\nmodels with pre-trained weights show more similarity in heatmaps compared to\nthose without pre-training. Specifically, eye and nose areas influence certain\nfacial expressions, while the mouth is consistently important across all models\nand expressions. Quantitatively, we find low average IoU values (avg. 0.2702)\nacross all expressions and architectures. The best-performing architecture\naverages 0.3269, while the worst-performing one averages 0.2066. Dendrograms,\nbuilt with the normalized correlation coefficient, reveal two main clusters for\nmost expressions: models with pre-training and models without pre-training.\nFindings suggest limited alignment between human and AI facial expression\nrecognition, with network architectures influencing the similarity, as similar\narchitectures prioritize similar facial regions.\n","authors":["F. Xavier Gaya-Morey","Silvia Ramis-Guarinos","Cristina Manresa-Yee","Jose M. Buades-Rubio"],"pdf_url":"https://arxiv.org/pdf/2401.11835v2.pdf","comment":"Multimed Tools Appl (2024)"},{"id":"http://arxiv.org/abs/2408.15045v2","updated":"2024-08-28T08:32:44Z","published":"2024-08-27T13:13:38Z","title":"DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding","summary":"  Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors.\n","authors":["Wenhui Liao","Jiapeng Wang","Hongliang Li","Chengyu Wang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2408.15045v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08454v2","updated":"2024-08-28T08:31:28Z","published":"2024-08-15T23:34:04Z","title":"Beyond Uniform Query Distribution: Key-Driven Grouped Query Attention","summary":"  The Transformer architecture has revolutionized deep learning through its\nSelf-Attention mechanism, which effectively captures contextual information.\nHowever, the memory footprint of Self-Attention presents significant challenges\nfor long-sequence tasks. Grouped Query Attention (GQA) addresses this issue by\ngrouping queries and mean-pooling the corresponding key-value heads - reducing\nthe number of overall parameters and memory requirements in a flexible manner\nwithout adversely compromising model accuracy. In this work, we introduce\nenhancements to GQA, focusing on two novel approaches that deviate from the\nstatic nature of grouping: Key-Distributed GQA (KDGQA) and Dynamic\nKey-Distributed GQA (DGQA), which leverage information from the norms of the\nkey heads to inform query allocation. Specifically, KDGQA looks at the ratios\nof the norms of the key heads during each forward pass, while DGQA examines the\nratios of the norms as they evolve through training. Additionally, we present\nPerturbed GQA (PGQA) as a case-study, which introduces variability in (static)\ngroup formation via subtracting noise from the attention maps. Our experiments\nwith up-trained Vision Transformers, for Image Classification on datasets such\nas CIFAR-10, CIFAR-100, Food101, and Tiny ImageNet, demonstrate the promise of\nthese variants in improving upon the original GQA through more informed and\nadaptive grouping mechanisms: specifically ViT-L experiences accuracy gains of\nup to 8% when utilizing DGQA in comparison to GQA and other variants. We\nfurther analyze the impact of the number of Key-Value Heads on performance,\nunderscoring the importance of utilizing query-key affinities. Code is\navailable on GitHub.\n","authors":["Zohaib Khan","Muhammad Khaquan","Omer Tafveez","Burhanuddin Samiwala","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2408.08454v2.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.15063v2","updated":"2024-08-28T08:28:50Z","published":"2024-08-27T13:47:31Z","title":"Adapting Segment Anything Model to Multi-modal Salient Object Detection\n  with Semantic Feature Fusion Guidance","summary":"  Although most existing multi-modal salient object detection (SOD) methods\ndemonstrate effectiveness through training models from scratch, the limited\nmulti-modal data hinders these methods from reaching optimality. In this paper,\nwe propose a novel framework to explore and exploit the powerful feature\nrepresentation and zero-shot generalization ability of the pre-trained Segment\nAnything Model (SAM) for multi-modal SOD. Despite serving as a recent vision\nfundamental model, driving the class-agnostic SAM to comprehend and detect\nsalient objects accurately is non-trivial, especially in challenging scenes. To\nthis end, we develop \\underline{SAM} with se\\underline{m}antic\nf\\underline{e}ature fu\\underline{s}ion guidanc\\underline{e} (Sammese), which\nincorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to\nmulti-modal SOD tasks. However, it is difficult for SAM trained on single-modal\ndata to directly mine the complementary benefits of multi-modal inputs and\ncomprehensively utilize them to achieve accurate saliency prediction.To address\nthese issues, we first design a multi-modal complementary fusion module to\nextract robust multi-modal semantic features by integrating information from\nvisible and thermal or depth image pairs. Then, we feed the extracted\nmulti-modal semantic features into both the SAM image encoder and mask decoder\nfor fine-tuning and prompting, respectively. Specifically, in the image\nencoder, a multi-modal adapter is proposed to adapt the single-modal SAM to\nmulti-modal information. In the mask decoder, a semantic-geometric prompt\ngeneration strategy is proposed to produce corresponding embeddings with\nvarious saliency cues. Extensive experiments on both RGB-D and RGB-T SOD\nbenchmarks show the effectiveness of the proposed framework.\n","authors":["Kunpeng Wang","Danying Lin","Chenglong Li","Zhengzheng Tu","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2408.15063v2.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.15628v1","updated":"2024-08-28T08:27:41Z","published":"2024-08-28T08:27:41Z","title":"CSAD: Unsupervised Component Segmentation for Logical Anomaly Detection","summary":"  To improve logical anomaly detection, some previous works have integrated\nsegmentation techniques with conventional anomaly detection methods. Although\nthese methods are effective, they frequently lead to unsatisfactory\nsegmentation results and require manual annotations. To address these\ndrawbacks, we develop an unsupervised component segmentation technique that\nleverages foundation models to autonomously generate training labels for a\nlightweight segmentation network without human labeling. Integrating this new\nsegmentation technique with our proposed Patch Histogram module and the\nLocal-Global Student-Teacher (LGST) module, we achieve a detection AUROC of\n95.3% in the MVTec LOCO AD dataset, which surpasses previous SOTA methods.\nFurthermore, our proposed method provides lower latency and higher throughput\nthan most existing approaches.\n","authors":["Yu-Hsuan Hsieh","Shang-Hong Lai"],"pdf_url":"https://arxiv.org/pdf/2408.15628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15626v1","updated":"2024-08-28T08:25:41Z","published":"2024-08-28T08:25:41Z","title":"Can Visual Language Models Replace OCR-Based Visual Question Answering\n  Pipelines in Production? A Case Study in Retail","summary":"  Most production-level deployments for Visual Question Answering (VQA) tasks\nare still build as processing pipelines of independent steps including image\npre-processing, object- and text detection, Optical Character Recognition (OCR)\nand (mostly supervised) object classification. However, the recent advances in\nvision Foundation Models [25] and Vision Language Models (VLMs) [23] raise the\nquestion if these custom trained, multi-step approaches can be replaced with\npre-trained, single-step VLMs. This paper analyzes the performance and limits\nof various VLMs in the context of VQA and OCR [5, 9, 12] tasks in a\nproduction-level scenario. Using data from the Retail-786k [10] dataset, we\ninvestigate the capabilities of pre-trained VLMs to answer detailed questions\nabout advertised products in images. Our study includes two commercial models,\nGPT-4V [16] and GPT-4o [17], as well as four open-source models: InternVL [5],\nLLaVA 1.5 [12], LLaVA-NeXT [13], and CogAgent [9]. Our initial results show,\nthat there is in general no big performance gap between open-source and\ncommercial models. However, we observe a strong task dependent variance in VLM\nperformance: while most models are able to answer questions regarding the\nproduct brand and price with high accuracy, they completely fail at the same\ntime to correctly identity the specific product name or discount. This\nindicates the problem of VLMs to solve fine-grained classification tasks as\nwell to model the more abstract concept of discounts.\n","authors":["Bianca Lamm","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2408.15626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15608v1","updated":"2024-08-28T08:02:47Z","published":"2024-08-28T08:02:47Z","title":"Geometry-guided Feature Learning and Fusion for Indoor Scene\n  Reconstruction","summary":"  In addition to color and textural information, geometry provides important\ncues for 3D scene reconstruction. However, current reconstruction methods only\ninclude geometry at the feature level thus not fully exploiting the geometric\ninformation.\n  In contrast, this paper proposes a novel geometry integration mechanism for\n3D scene reconstruction. Our approach incorporates 3D geometry at three levels,\ni.e. feature learning, feature fusion, and network supervision. First,\ngeometry-guided feature learning encodes geometric priors to contain\nview-dependent information. Second, a geometry-guided adaptive feature fusion\nis introduced which utilizes the geometric priors as a guidance to adaptively\ngenerate weights for multiple views. Third, at the supervision level, taking\nthe consistency between 2D and 3D normals into account, a consistent 3D normal\nloss is designed to add local constraints.\n  Large-scale experiments are conducted on the ScanNet dataset, showing that\nvolumetric methods with our geometry integration mechanism outperform\nstate-of-the-art methods quantitatively as well as qualitatively. Volumetric\nmethods with ours also show good generalization on the 7-Scenes and TUM RGB-D\ndatasets.\n","authors":["Ruihong Yin","Sezer Karaoglu","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2408.15608v1.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2408.15605v1","updated":"2024-08-28T07:56:28Z","published":"2024-08-28T07:56:28Z","title":"ES-PTAM: Event-based Stereo Parallel Tracking and Mapping","summary":"  Visual Odometry (VO) and SLAM are fundamental components for spatial\nperception in mobile robots. Despite enormous progress in the field, current\nVO/SLAM systems are limited by their sensors' capability. Event cameras are\nnovel visual sensors that offer advantages to overcome the limitations of\nstandard cameras, enabling robots to expand their operating range to\nchallenging scenarios, such as high-speed motion and high dynamic range\nillumination. We propose a novel event-based stereo VO system by combining two\nideas: a correspondence-free mapping module that estimates depth by maximizing\nray density fusion and a tracking module that estimates camera poses by\nmaximizing edge-map alignment. We evaluate the system comprehensively on five\nreal-world datasets, spanning a variety of camera types (manufacturers and\nspatial resolutions) and scenarios (driving, flying drone, hand-held,\negocentric, etc). The quantitative and qualitative results demonstrate that our\nmethod outperforms the state of the art in majority of the test sequences by a\nmargin, e.g., trajectory error reduction of 45% on RPG dataset, 61% on DSEC\ndataset, and 21% on TUM-VIE dataset. To benefit the community and foster\nresearch on event-based perception systems, we release the source code and\nresults: https://github.com/tub-rip/ES-PTAM\n","authors":["Suman Ghosh","Valentina Cavinato","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2408.15605v1.pdf","comment":"17 pages, 7 figures, 4 tables, https://github.com/tub-rip/ES-PTAM"},{"id":"http://arxiv.org/abs/2408.08091v2","updated":"2024-08-28T07:51:34Z","published":"2024-08-15T11:34:33Z","title":"HAIR: Hypernetworks-based All-in-One Image Restoration","summary":"  Image restoration aims to recover a high-quality clean image from its\ndegraded version. Recent progress in image restoration has demonstrated the\neffectiveness of All-in-One image restoration models in addressing various\ndegradations simultaneously. However, these existing methods typically utilize\nthe same parameters to tackle images with different degradation types, thus\nforcing the model to balance the performance between different tasks and\nlimiting its performance on each task. To alleviate this issue, we propose\nHAIR, a \\textbf{H}ypernetworks-based \\textbf{A}ll-in-One \\textbf{I}mage\n\\textbf{R}estoration method that dynamically generates parameters based on\ninput images. Specifically, HAIR consists of two main components, i.e.,\nClassifier and Hyper Selecting Net (HSN). The Classifier is a simple image\nclassification network used to generate a Global Information Vector (GIV) that\ncontains the degradation information of the input image, and the HSN is a\nsimple fully-connected neural network that receives the GIV and outputs\nparameters for the corresponding modules. Extensive experiments demonstrate\nthat HAIR can significantly improve the performance of existing image\nrestoration models in a plug-and-play manner, both in single-task and\nall-in-one settings. Notably, our innovative model, Res-HAIR, which integrates\nHAIR into the well-known Restormer, can obtain superior or comparable\nperformance compared with current state-of-the-art methods. Moreover, we\ntheoretically demonstrate that our proposed HAIR requires fewer parameters in\ncontrast to the prevalent All-in-One methodologies. The code is available at\n\\textcolor{blue}{\\href{https://github.com/toummHus/HAIR}{https://github.com/toummHus/HAIR}.}\n","authors":["Jin Cao","Yi Cao","Li Pang","Deyu Meng","Xiangyong Cao"],"pdf_url":"https://arxiv.org/pdf/2408.08091v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2408.15602v1","updated":"2024-08-28T07:49:30Z","published":"2024-08-28T07:49:30Z","title":"On the Benefits of Visual Stabilization for Frame- and Event-based\n  Perception","summary":"  Vision-based perception systems are typically exposed to large orientation\nchanges in different robot applications. In such conditions, their performance\nmight be compromised due to the inherent complexity of processing data captured\nunder challenging motion. Integration of mechanical stabilizers to compensate\nfor the camera rotation is not always possible due to the robot payload\nconstraints. This paper presents a processing-based stabilization approach to\ncompensate the camera's rotational motion both on events and on frames (i.e.,\nimages). Assuming that the camera's attitude is available, we evaluate the\nbenefits of stabilization in two perception applications: feature tracking and\nestimating the translation component of the camera's ego-motion. The validation\nis performed using synthetic data and sequences from well-known event-based\nvision datasets. The experiments unveil that stabilization can improve feature\ntracking and camera ego-motion estimation accuracy in 27.37% and 34.82%,\nrespectively. Concurrently, stabilization can reduce the processing time of\ncomputing the camera's linear velocity by at least 25%. Code is available at\nhttps://github.com/tub-rip/visual_stabilization\n","authors":["Juan Pablo Rodriguez-Gomez","Jose Ramiro Martinez-de Dios","Anibal Ollero","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2408.15602v1.pdf","comment":"8 pages, 4 figures, 4 tables,\n  https://github.com/tub-rip/visual_stabilization"},{"id":"http://arxiv.org/abs/2407.10389v2","updated":"2024-08-28T07:49:14Z","published":"2024-07-15T01:58:54Z","title":"Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High\n  Quality and Efficient Rendering","summary":"  Since the introduction of NeRFs, considerable attention has been focused on\nimproving their training and inference times, leading to the development of\nFast-NeRFs models. Despite demonstrating impressive rendering speed and\nquality, the rapid convergence of such models poses challenges for further\nimproving reconstruction quality. Common strategies to improve rendering\nquality involves augmenting model parameters or increasing the number of\nsampled points. However, these computationally intensive approaches encounter\nlimitations in achieving significant quality enhancements. This study\nintroduces a model-agnostic framework inspired by Sparsely-Gated Mixture of\nExperts to enhance rendering quality without escalating computational\ncomplexity. Our approach enables specialization in rendering different scene\ncomponents by employing a mixture of experts with varying resolutions. We\npresent a novel gate formulation designed to maximize expert capabilities and\npropose a resolution-based routing technique to effectively induce sparsity and\ndecompose scenes. Our work significantly improves reconstruction quality while\nmaintaining competitive performance.\n","authors":["Francesco Di Sario","Riccardo Renzulli","Enzo Tartaglione","Marco Grangetto"],"pdf_url":"https://arxiv.org/pdf/2407.10389v2.pdf","comment":"The paper has been accepted to the ECCV 2024 conference"},{"id":"http://arxiv.org/abs/2407.20495v2","updated":"2024-08-28T07:40:18Z","published":"2024-07-30T01:39:30Z","title":"Enhancing Quantitative Image Synthesis through Pretraining and\n  Resolution Scaling for Bone Mineral Density Estimation from a Plain X-ray\n  Image","summary":"  While most vision tasks are essentially visual in nature (for recognition),\nsome important tasks, especially in the medical field, also require\nquantitative analysis (for quantification) using quantitative images. Unlike in\nvisual analysis, pixel values in quantitative images correspond to physical\nmetrics measured by specific devices (e.g., a depth image). However, recent\nwork has shown that it is sometimes possible to synthesize accurate\nquantitative values from visual ones (e.g., depth from visual cues or defocus).\nThis research aims to improve quantitative image synthesis (QIS) by exploring\npretraining and image resolution scaling. We propose a benchmark for evaluating\npretraining performance using the task of QIS-based bone mineral density (BMD)\nestimation from plain X-ray images, where the synthesized quantitative image is\nused to derive BMD. Our results show that appropriate pretraining can improve\nQIS performance, significantly raising the correlation of BMD estimation from\n0.820 to 0.898, while others do not help or even hinder it. Scaling-up the\nresolution can further boost the correlation up to 0.923, a significant\nenhancement over conventional methods. Future work will include exploring more\npretraining strategies and validating them on other image synthesis tasks.\n","authors":["Yi Gu","Yoshito Otake","Keisuke Uemura","Masaki Takao","Mazen Soufi","Seiji Okada","Nobuhiko Sugano","Hugues Talbot","Yoshinobu Sato"],"pdf_url":"https://arxiv.org/pdf/2407.20495v2.pdf","comment":"SASHIMI, 2024 (MICCAI workshop). 13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2305.14668v4","updated":"2024-08-28T07:28:15Z","published":"2023-05-24T03:20:09Z","title":"NOVUM: Neural Object Volumes for Robust Object Classification","summary":"  Discriminative models for object classification typically learn image-based\nrepresentations that do not capture the compositional and 3D nature of objects.\nIn this work, we show that explicitly integrating 3D compositional object\nrepresentations into deep networks for image classification leads to a largely\nenhanced generalization in out-of-distribution scenarios. In particular, we\nintroduce a novel architecture, referred to as NOVUM, that consists of a\nfeature extractor and a neural object volume for every target object class.\nEach neural object volume is a composition of 3D Gaussians that emit feature\nvectors. This compositional object representation allows for a highly robust\nand fast estimation of the object class by independently matching the features\nof the 3D Gaussians of each category to features extracted from an input image.\nAdditionally, the object pose can be estimated via inverse rendering of the\ncorresponding neural object volume. To enable the classification of objects,\nthe neural features at each 3D Gaussian are trained discriminatively to be\ndistinct from (i) the features of 3D Gaussians in other categories, (ii)\nfeatures of other 3D Gaussians of the same object, and (iii) the background\nfeatures. Our experiments show that NOVUM offers intriguing advantages over\nstandard architectures due to the 3D compositional structure of the object\nrepresentation, namely: (1) An exceptional robustness across a spectrum of\nreal-world and synthetic out-of-distribution shifts and (2) an enhanced human\ninterpretability compared to standard models, all while maintaining real-time\ninference and a competitive accuracy on in-distribution data.\n","authors":["Artur Jesslen","Guofeng Zhang","Angtian Wang","Wufei Ma","Alan Yuille","Adam Kortylewski"],"pdf_url":"https://arxiv.org/pdf/2305.14668v4.pdf","comment":"14 pages, 4 figures, accepted at ECCV 2024, code is accessible at\n  https://github.com/GenIntel/NOVUM"},{"id":"http://arxiv.org/abs/2405.15239v3","updated":"2024-08-28T07:07:06Z","published":"2024-05-24T06:06:11Z","title":"Brain3D: Generating 3D Objects from fMRI","summary":"  Understanding the hidden mechanisms behind human's visual perception is a\nfundamental question in neuroscience. To that end, investigating into the\nneural responses of human mind activities, such as functional Magnetic\nResonance Imaging (fMRI), has been a significant research vehicle. However,\nanalyzing fMRI signals is challenging, costly, daunting, and demanding for\nprofessional training. Despite remarkable progress in fMRI analysis, existing\napproaches are limited to generating 2D images and far away from being\nbiologically meaningful and practically useful. Under this insight, we propose\nto generate visually plausible and functionally more comprehensive 3D outputs\ndecoded from brain signals, enabling more sophisticated modeling of fMRI data.\nConceptually, we reformulate this task as a {\\em fMRI conditioned 3D object\ngeneration} problem. We design a novel 3D object representation learning\nmethod, Brain3D, that takes as input the fMRI data of a subject who was\npresented with a 2D image, and yields as output the corresponding 3D object\nimages. The key capabilities of this model include tackling the noises with\nhigh-level semantic signals and a two-stage architecture design for progressive\nhigh-level information integration. Extensive experiments validate the superior\ncapability of our model over previous state-of-the-art 3D object generation\nmethods. Importantly, we show that our model captures the distinct\nfunctionalities of each region of human vision system as well as their\nintricate interplay relationships, aligning remarkably with the established\ndiscoveries in neuroscience. Further, preliminary evaluations indicate that\nBrain3D can successfully identify the disordered brain regions in simulated\nscenarios, such as V1, V2, V3, V4, and the medial temporal lobe (MTL) within\nthe human visual system. Our data and code will be available at\nhttps://brain-3d.github.io/.\n","authors":["Yuankun Yang","Li Zhang","Ziyang Xie","Zhiyuan Yuan","Jianfeng Feng","Xiatian Zhu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2405.15239v3.pdf","comment":"20 pages, 11 figures, project page: https://brain-3d.github.io/"},{"id":"http://arxiv.org/abs/2408.15580v1","updated":"2024-08-28T07:05:46Z","published":"2024-08-28T07:05:46Z","title":"Hierarchical Visual Categories Modeling: A Joint Representation Learning\n  and Density Estimation Framework for Out-of-Distribution Detection","summary":"  Detecting out-of-distribution inputs for visual recognition models has become\ncritical in safe deep learning. This paper proposes a novel hierarchical visual\ncategory modeling scheme to separate out-of-distribution data from\nin-distribution data through joint representation learning and statistical\nmodeling. We learn a mixture of Gaussian models for each in-distribution\ncategory. There are many Gaussian mixture models to model different visual\ncategories. With these Gaussian models, we design an in-distribution score\nfunction by aggregating multiple Mahalanobis-based metrics. We don't use any\nauxiliary outlier data as training samples, which may hurt the generalization\nability of out-of-distribution detection algorithms. We split the ImageNet-1k\ndataset into ten folds randomly. We use one fold as the in-distribution dataset\nand the others as out-of-distribution datasets to evaluate the proposed method.\nWe also conduct experiments on seven popular benchmarks, including CIFAR,\niNaturalist, SUN, Places, Textures, ImageNet-O, and OpenImage-O. Extensive\nexperiments indicate that the proposed method outperforms state-of-the-art\nalgorithms clearly. Meanwhile, we find that our visual representation has a\ncompetitive performance when compared with features learned by classical\nmethods. These results demonstrate that the proposed method hasn't weakened the\ndiscriminative ability of visual recognition models and keeps high efficiency\nin detecting out-of-distribution samples.\n","authors":["Jinglun Li","Xinyu Zhou","Pinxue Guo","Yixuan Sun","Yiwen Huang","Weifeng Ge","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15580v1.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2408.13509v2","updated":"2024-08-28T06:53:46Z","published":"2024-08-24T08:09:32Z","title":"DualAnoDiff: Dual-Interrelated Diffusion Model for Few-Shot Anomaly\n  Image Generation","summary":"  The performance of anomaly inspection in industrial manufacturing is\nconstrained by the scarcity of anomaly data. To overcome this challenge,\nresearchers have started employing anomaly generation approaches to augment the\nanomaly dataset. However, existing anomaly generation methods suffer from\nlimited diversity in the generated anomalies and struggle to achieve a seamless\nblending of this anomaly with the original image. In this paper, we overcome\nthese challenges from a new perspective, simultaneously generating a pair of\nthe overall image and the corresponding anomaly part. We propose DualAnoDiff, a\nnovel diffusion-based few-shot anomaly image generation model, which can\ngenerate diverse and realistic anomaly images by using a dual-interrelated\ndiffusion model, where one of them is employed to generate the whole image\nwhile the other one generates the anomaly part. Moreover, we extract background\nand shape information to mitigate the distortion and blurriness phenomenon in\nfew-shot image generation. Extensive experiments demonstrate the superiority of\nour proposed model over state-of-the-art methods in terms of both realism and\ndiversity. Overall, our approach significantly improves the performance of\ndownstream anomaly detection tasks, including anomaly detection, anomaly\nlocalization, and anomaly classification tasks.\n","authors":["Ying Jin","Jinlong Peng","Qingdong He","Teng Hu","Hao Chen","Jiafu Wu","Wenbing Zhu","Mingmin Chi","Jun Liu","Yabiao Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.13509v2.pdf","comment":"Code: https://github.com/yinyjin/DualAnoDiff"},{"id":"http://arxiv.org/abs/2408.15569v1","updated":"2024-08-28T06:53:08Z","published":"2024-08-28T06:53:08Z","title":"Temporal Attention for Cross-View Sequential Image Localization","summary":"  This paper introduces a novel approach to enhancing cross-view localization,\nfocusing on the fine-grained, sequential localization of street-view images\nwithin a single known satellite image patch, a significant departure from\ntraditional one-to-one image retrieval methods. By expanding to sequential\nimage fine-grained localization, our model, equipped with a novel Temporal\nAttention Module (TAM), leverages contextual information to significantly\nimprove sequential image localization accuracy. Our method shows substantial\nreductions in both mean and median localization errors on the Cross-View Image\nSequence (CVIS) dataset, outperforming current state-of-the-art single-image\nlocalization techniques. Additionally, by adapting the KITTI-CVL dataset into\nsequential image sets, we not only offer a more realistic dataset for future\nresearch but also demonstrate our model's robust generalization capabilities\nacross varying times and areas, evidenced by a 75.3% reduction in mean distance\nerror in cross-view sequential image localization.\n","authors":["Dong Yuan","Frederic Maire","Feras Dayoub"],"pdf_url":"https://arxiv.org/pdf/2408.15569v1.pdf","comment":"Accepted to IROS 2024"},{"id":"http://arxiv.org/abs/2408.15566v1","updated":"2024-08-28T06:37:59Z","published":"2024-08-28T06:37:59Z","title":"TagOOD: A Novel Approach to Out-of-Distribution Detection via\n  Vision-Language Representations and Class Center Learning","summary":"  Multimodal fusion, leveraging data like vision and language, is rapidly\ngaining traction. This enriched data representation improves performance across\nvarious tasks. Existing methods for out-of-distribution (OOD) detection, a\ncritical area where AI models encounter unseen data in real-world scenarios,\nrely heavily on whole-image features. These image-level features can include\nirrelevant information that hinders the detection of OOD samples, ultimately\nlimiting overall performance. In this paper, we propose \\textbf{TagOOD}, a\nnovel approach for OOD detection that leverages vision-language representations\nto achieve label-free object feature decoupling from whole images. This\ndecomposition enables a more focused analysis of object semantics, enhancing\nOOD detection performance. Subsequently, TagOOD trains a lightweight network on\nthe extracted object features to learn representative class centers. These\ncenters capture the central tendencies of IND object classes, minimizing the\ninfluence of irrelevant image features during OOD detection. Finally, our\napproach efficiently detects OOD samples by calculating distance-based metrics\nas OOD scores between learned centers and test samples. We conduct extensive\nexperiments to evaluate TagOOD on several benchmark datasets and demonstrate\nits superior performance compared to existing OOD detection methods. This work\npresents a novel perspective for further exploration of multimodal information\nutilization in OOD detection, with potential applications across various tasks.\n","authors":["Jinglun Li","Xinyu Zhou","Kaixun Jiang","Lingyi Hong","Pinxue Guo","Zhaoyu Chen","Weifeng Ge","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15566v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.15557v1","updated":"2024-08-28T06:18:55Z","published":"2024-08-28T06:18:55Z","title":"Generalization Capabilities of Neural Cellular Automata for Medical\n  Image Segmentation: A Robust and Lightweight Approach","summary":"  In the field of medical imaging, the U-Net architecture, along with its\nvariants, has established itself as a cornerstone for image segmentation tasks,\nparticularly due to its strong performance when trained on limited datasets.\nDespite its impressive performance on identically distributed (in-domain) data,\nU-Nets exhibit a significant decline in performance when tested on data that\ndeviates from the training distribution, out-of-distribution (out-of-domain)\ndata. Current methodologies predominantly address this issue by employing\ngeneralization techniques that hinge on various forms of regularization, which\nhave demonstrated moderate success in specific scenarios. This paper, however,\nventures into uncharted territory by investigating the implications of\nutilizing models that are smaller by three orders of magnitude (i.e., x1000)\ncompared to a conventional U-Net. A reduction of this size in U-net parameters\ntypically adversely affects both in-domain and out-of-domain performance,\npossibly due to a significantly reduced receptive field. To circumvent this\nissue, we explore the concept of Neural Cellular Automata (NCA), which, despite\nits simpler model structure, can attain larger receptive fields through\nrecursive processes. Experimental results on two distinct datasets reveal that\nNCA outperforms traditional methods in terms of generalization, while still\nmaintaining a commendable IID performance.\n","authors":["Steven Korevaar","Ruwan Tennakoon","Alireza Bab-Hadiashar"],"pdf_url":"https://arxiv.org/pdf/2408.15557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13134v2","updated":"2024-08-28T06:15:04Z","published":"2023-11-22T03:41:13Z","title":"Lightweight High-Speed Photography Built on Coded Exposure and Implicit\n  Neural Representation of Videos","summary":"  The demand for compact cameras capable of recording high-speed scenes with\nhigh resolution is steadily increasing. However, achieving such capabilities\noften entails high bandwidth requirements, resulting in bulky, heavy systems\nunsuitable for low-capacity platforms. To address this challenge, leveraging a\ncoded exposure setup to encode a frame sequence into a blurry snapshot and\nsubsequently retrieve the latent sharp video presents a lightweight solution.\nNevertheless, restoring motion from blur remains a formidable challenge due to\nthe inherent ill-posedness of motion blur decomposition, the intrinsic\nambiguity in motion direction, and the diverse motions present in natural\nvideos. In this study, we propose a novel approach to address these challenges\nby combining the classical coded exposure imaging technique with the emerging\nimplicit neural representation for videos. We strategically embed motion\ndirection cues into the blurry image during the imaging process. Additionally,\nwe develop a novel implicit neural representation based blur decomposition\nnetwork to sequentially extract the latent video frames from the blurry image,\nleveraging the embedded motion direction cues. To validate the effectiveness\nand efficiency of our proposed framework, we conduct extensive experiments\nusing benchmark datasets and real-captured blurry images. The results\ndemonstrate that our approach significantly outperforms existing methods in\nterms of both quality and flexibility. The code for our work is available at\n.https://github.com/zhihongz/BDINR\n","authors":["Zhihong Zhang","Runzhao Yang","Jinli Suo","Yuxiao Cheng","Qionghai Dai"],"pdf_url":"https://arxiv.org/pdf/2311.13134v2.pdf","comment":"Accepted by IJCV"},{"id":"http://arxiv.org/abs/2408.15556v1","updated":"2024-08-28T06:09:02Z","published":"2024-08-28T06:09:02Z","title":"Divide, Conquer and Combine: A Training-Free Framework for\n  High-Resolution Image Perception in Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have experienced significant\nadvancements recently, but still struggle to recognize and interpret intricate\ndetails in high-resolution (HR) images effectively. While state-of-the-art\n(SOTA) MLLMs claim to process images at 4K resolution, existing MLLM benchmarks\nonly support up to 2K, leaving the capabilities of SOTA models on true HR\nimages largely untested. Furthermore, existing methods for enhancing HR image\nperception in MLLMs rely on computationally expensive visual instruction\ntuning. To address these limitations, we introduce HR-Bench, the first\ndeliberately designed benchmark to rigorously evaluate MLLM performance on\n4K&8K images. Through extensive experiments, we demonstrate that while\ndownsampling HR images leads to vision information loss, leveraging\ncomplementary modalities, e.g., text, can effectively compensate for this loss.\nBuilding upon this insight, we propose Divide, Conquer and Combine (DC$^2$), a\nnovel training-free framework for enhancing MLLM perception of HR images.\nDC$^2$ follows a three-staged approach: 1) Divide: recursively partitioning the\nHR image into patches and merging similar patches to minimize computational\noverhead, 2) Conquer: leveraging the MLLM to generate accurate textual\ndescriptions for each image patch, and 3) Combine: utilizing the generated text\ndescriptions to enhance the MLLM's understanding of the overall HR image.\nExtensive experiments show that: 1) the SOTA MLLM achieves 63% accuracy, which\nis markedly lower than the 87% accuracy achieved by humans on HR-Bench; 2) our\nDC$^2$ brings consistent and significant improvements (a relative increase of\n+6% on HR-Bench and +8% on general multimodal benchmarks). The benchmark and\ncode will be released to facilitate the multimodal R&D community.\n","authors":["Wenbin Wang","Liang Ding","Minyan Zeng","Xiabin Zhou","Li Shen","Yong Luo","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2408.15556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15555v1","updated":"2024-08-28T06:08:46Z","published":"2024-08-28T06:08:46Z","title":"Latent Relationship Mining of Glaucoma Biomarkers: a TRI-LSTM based Deep\n  Learning","summary":"  In recently years, a significant amount of research has been conducted on\napplying deep learning methods for glaucoma classification and detection.\nHowever, the explainability of those established machine learning models\nremains a big concern. In this research, in contrast, we learn from cognitive\nscience concept and study how ophthalmologists judge glaucoma detection.\nSimulating experts' efforts, we propose a hierarchical decision making system,\ncentered around a holistic set of carefully designed biomarker-oriented machine\nlearning models. While biomarkers represent the key indicators of how\nophthalmologists identify glaucoma, they usually exhibit latent\ninter-relations. We thus construct a time series model, named TRI-LSTM, capable\nof calculating and uncovering potential and latent relationships among various\nbiomarkers of glaucoma. Our model is among the first efforts to explore the\nintrinsic connections among glaucoma biomarkers. We monitor temporal\nrelationships in patients' disease states over time and to capture and retain\nthe progression of disease-relevant clinical information from prior visits,\nthereby enriching biomarker's potential relationships. Extensive experiments\nover real-world dataset have demonstrated the effectiveness of the proposed\nmodel.\n","authors":["Cheng Huang","Junhao Shen","Qiuyu Luo","Karanjit Kooner","Tsengdar Lee","Yishen Liu","Jia Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15555v1.pdf","comment":"9 pages, 4 images"},{"id":"http://arxiv.org/abs/2408.15548v1","updated":"2024-08-28T05:53:30Z","published":"2024-08-28T05:53:30Z","title":"ConsistencyTrack: A Robust Multi-Object Tracker with a Generation\n  Strategy of Consistency Model","summary":"  Multi-object tracking (MOT) is a critical technology in computer vision,\ndesigned to detect multiple targets in video sequences and assign each target a\nunique ID per frame. Existed MOT methods excel at accurately tracking multiple\nobjects in real-time across various scenarios. However, these methods still\nface challenges such as poor noise resistance and frequent ID switches. In this\nresearch, we propose a novel ConsistencyTrack, joint detection and\ntracking(JDT) framework that formulates detection and association as a\ndenoising diffusion process on perturbed bounding boxes. This progressive\ndenoising strategy significantly improves the model's noise resistance. During\nthe training phase, paired object boxes within two adjacent frames are diffused\nfrom ground-truth boxes to a random distribution, and then the model learns to\ndetect and track by reversing this process. In inference, the model refines\nrandomly generated boxes into detection and tracking results through minimal\ndenoising steps. ConsistencyTrack also introduces an innovative target\nassociation strategy to address target occlusion. Experiments on the MOT17 and\nDanceTrack datasets demonstrate that ConsistencyTrack outperforms other\ncompared methods, especially better than DiffusionTrack in inference speed and\nother performance metrics. Our code is available at\nhttps://github.com/Tankowa/ConsistencyTrack.\n","authors":["Lifan Jiang","Zhihui Wang","Siqi Yin","Guangxiao Ma","Peng Zhang","Boxi Wu"],"pdf_url":"https://arxiv.org/pdf/2408.15548v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2308.09905 by other authors"},{"id":"http://arxiv.org/abs/2406.18967v2","updated":"2024-08-28T05:40:55Z","published":"2024-06-27T07:59:25Z","title":"Structural Attention: Rethinking Transformer for Unpaired Medical Image\n  Synthesis","summary":"  Unpaired medical image synthesis aims to provide complementary information\nfor an accurate clinical diagnostics, and address challenges in obtaining\naligned multi-modal medical scans. Transformer-based models excel in imaging\ntranslation tasks thanks to their ability to capture long-range dependencies.\nAlthough effective in supervised training settings, their performance falters\nin unpaired image synthesis, particularly in synthesizing structural details.\nThis paper empirically demonstrates that, lacking strong inductive biases,\nTransformer can converge to non-optimal solutions in the absence of paired\ndata. To address this, we introduce UNet Structured Transformer (UNest), a\nnovel architecture incorporating structural inductive biases for unpaired\nmedical image synthesis. We leverage the foundational Segment-Anything Model to\nprecisely extract the foreground structure and perform structural attention\nwithin the main anatomy. This guides the model to learn key anatomical regions,\nthus improving structural synthesis under the lack of supervision in unpaired\ntraining. Evaluated on two public datasets, spanning three modalities, i.e.,\nMR, CT, and PET, UNest improves recent methods by up to 19.30% across six\nmedical image synthesis tasks. Our code is released at\nhttps://github.com/HieuPhan33/MICCAI2024-UNest.\n","authors":["Vu Minh Hieu Phan","Yutong Xie","Bowen Zhang","Yuankai Qi","Zhibin Liao","Antonios Perperidis","Son Lam Phung","Johan W. Verjans","Minh-Son To"],"pdf_url":"https://arxiv.org/pdf/2406.18967v2.pdf","comment":"MICCAI version before camera ready"},{"id":"http://arxiv.org/abs/2408.15542v1","updated":"2024-08-28T05:34:14Z","published":"2024-08-28T05:34:14Z","title":"Kangaroo: A Powerful Video-Language Model Supporting Long-context Video\n  Input","summary":"  Rapid advancements have been made in extending Large Language Models (LLMs)\nto Large Multi-modal Models (LMMs). However, extending input modality of LLMs\nto video data remains a challenging endeavor, especially for long videos. Due\nto insufficient access to large-scale high-quality video data and the excessive\ncompression of visual features, current methods exhibit limitations in\neffectively processing long videos. In this paper, we introduce Kangaroo, a\npowerful Video LMM aimed at addressing these challenges. Confronted with issue\nof inadequate training data, we develop a data curation system to build a\nlarge-scale dataset with high-quality annotations for vision-language\npre-training and instruction tuning. In addition, we design a curriculum\ntraining pipeline with gradually increasing resolution and number of input\nframes to accommodate long videos. Evaluation results demonstrate that, with 8B\nparameters, Kangaroo achieves state-of-the-art performance across a variety of\nvideo understanding benchmarks while exhibiting competitive results on others.\nParticularly, on benchmarks specialized for long videos, Kangaroo excels some\nlarger models with over 10B parameters and proprietary models.\n","authors":["Jiajun Liu","Yibing Wang","Hanghang Ma","Xiaoping Wu","Xiaoqi Ma","Xiaoming Wei","Jianbin Jiao","Enhua Wu","Jie Hu"],"pdf_url":"https://arxiv.org/pdf/2408.15542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08872v2","updated":"2024-08-28T05:03:34Z","published":"2024-08-16T17:57:01Z","title":"xGen-MM (BLIP-3): A Family of Open Large Multimodal Models","summary":"  This report introduces xGen-MM (also known as BLIP-3), a framework for\ndeveloping Large Multimodal Models (LMMs). The framework comprises meticulously\ncurated datasets, a training recipe, model architectures, and a resulting suite\nof LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen\ninitiative on foundation AI models. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\npre-trained base model exhibits strong in-context learning capabilities and the\ninstruction-tuned model demonstrates competitive performance among open-source\nLMMs with similar model sizes. In addition, we introduce a safety-tuned model\nwith DPO, aiming to mitigate harmful behaviors such as hallucinations and\nimprove safety. We open-source our models, curated large-scale datasets, and\nour fine-tuning codebase to facilitate further advancements in LMM research.\nAssociated resources will be available on our project page above.\n","authors":["Le Xue","Manli Shu","Anas Awadalla","Jun Wang","An Yan","Senthil Purushwalkam","Honglu Zhou","Viraj Prabhu","Yutong Dai","Michael S Ryoo","Shrikant Kendre","Jieyu Zhang","Can Qin","Shu Zhang","Chia-Chih Chen","Ning Yu","Juntao Tan","Tulika Manoj Awalgaonkar","Shelby Heinecke","Huan Wang","Yejin Choi","Ludwig Schmidt","Zeyuan Chen","Silvio Savarese","Juan Carlos Niebles","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2408.08872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19698v3","updated":"2024-08-28T04:41:09Z","published":"2024-07-29T04:43:58Z","title":"Classification Matters: Improving Video Action Detection with\n  Class-Specific Attention","summary":"  Video action detection (VAD) aims to detect actors and classify their actions\nin a video. We figure that VAD suffers more from classification rather than\nlocalization of actors. Hence, we analyze how prevailing methods form features\nfor classification and find that they prioritize actor regions, yet often\noverlooking the essential contextual information necessary for accurate\nclassification. Accordingly, we propose to reduce the bias toward actor and\nencourage paying attention to the context that is relevant to each action\nclass. By assigning a class-dedicated query to each action class, our model can\ndynamically determine where to focus for effective classification. The proposed\nmodel demonstrates superior performance on three challenging benchmarks with\nsignificantly fewer parameters and less computation.\n","authors":["Jinsung Lee","Taeoh Kim","Inwoong Lee","Minho Shim","Dongyoon Wee","Minsu Cho","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.19698v3.pdf","comment":"31 pages, accepted to ECCV 2024 (oral)"},{"id":"http://arxiv.org/abs/2208.06561v3","updated":"2024-08-28T04:36:23Z","published":"2022-08-13T03:25:50Z","title":"Drone Referring Localization: An Efficient Heterogeneous Spatial Feature\n  Interaction Method For UAV Self-Localization","summary":"  Image retrieval (IR) has emerged as a promising approach for\nself-localization in unmanned aerial vehicles (UAVs). However, IR-based methods\nface several challenges: 1) Pre- and post-processing incur significant\ncomputational and storage overhead; 2) The lack of interaction between\ndual-source features impairs precise spatial perception. In this paper, we\npropose an efficient heterogeneous spatial feature interaction method, termed\nDrone Referring Localization (DRL), which aims to localize UAV-view images\nwithin satellite imagery. Unlike conventional methods that treat different data\nsources in isolation, followed by cosine similarity computations, DRL\nfacilitates the learnable interaction of heterogeneous features. To implement\nthe proposed DRL, we design two transformer-based frameworks, Post-Fusion and\nMix-Fusion, enabling end-to-end training and inference. Furthermore, we\nintroduce random scale cropping and weight balance loss techniques to augment\npaired data and optimize the balance between positive and negative sample\nweights. Additionally, we construct a new dataset, UL14, and establish a\nbenchmark tailored to the DRL framework. Compared to traditional IR methods,\nDRL achieves superior localization accuracy (MA@20 +9.4\\%) while significantly\nreducing computational time (1/7) and storage overhead (1/3). The dataset and\ncode will be made publicly available. The dataset and code are available at\n\\url{https://github.com/Dmmm1997/DRL} .\n","authors":["Ming Dai","Enhui Zheng","Jiahao Chen","Lei Qi","Zhenhua Feng","Wankou Yang"],"pdf_url":"https://arxiv.org/pdf/2208.06561v3.pdf","comment":"15 pages, 14 figures"},{"id":"http://arxiv.org/abs/2408.15524v1","updated":"2024-08-28T04:19:14Z","published":"2024-08-28T04:19:14Z","title":"Ray-Distance Volume Rendering for Neural Scene Reconstruction","summary":"  Existing methods in neural scene reconstruction utilize the Signed Distance\nFunction (SDF) to model the density function. However, in indoor scenes, the\ndensity computed from the SDF for a sampled point may not consistently reflect\nits real importance in volume rendering, often due to the influence of\nneighboring objects. To tackle this issue, our work proposes a novel approach\nfor indoor scene reconstruction, which instead parameterizes the density\nfunction with the Signed Ray Distance Function (SRDF). Firstly, the SRDF is\npredicted by the network and transformed to a ray-conditioned density function\nfor volume rendering. We argue that the ray-specific SRDF only considers the\nsurface along the camera ray, from which the derived density function is more\nconsistent to the real occupancy than that from the SDF. Secondly, although\nSRDF and SDF represent different aspects of scene geometries, their values\nshould share the same sign indicating the underlying spatial occupancy.\nTherefore, this work introduces a SRDF-SDF consistency loss to constrain the\nsigns of the SRDF and SDF outputs. Thirdly, this work proposes a\nself-supervised visibility task, introducing the physical visibility geometry\nto the reconstruction task. The visibility task combines prior from predicted\nSRDF and SDF as pseudo labels, and contributes to generating more accurate 3D\ngeometry. Our method implemented with different representations has been\nvalidated on indoor datasets, achieving improved performance in both\nreconstruction and view synthesis.\n","authors":["Ruihong Yin","Yunlu Chen","Sezer Karaoglu","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2408.15524v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.15521v1","updated":"2024-08-28T04:14:01Z","published":"2024-08-28T04:14:01Z","title":"A Simple Baseline with Single-encoder for Referring Image Segmentation","summary":"  Referring image segmentation (RIS) requires dense vision-language\ninteractions between visual pixels and textual words to segment objects based\non a given description. However, commonly adapted dual-encoders in RIS, e.g.,\nSwin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal\ndual-encoder), lack dense multi-modal interactions during pre-training, leading\nto a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods\noften rely on multi-modal fusion modules that interact two encoders, but this\napproach leads to high computational costs. In this paper, we present a novel\nRIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of\nshared self-attention across all framework components. This enables seamless\ninteractions of two modalities from input to final prediction, producing\ngranularly aligned multi-modal features. Furthermore, we propose lightweight\nyet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which\ncontribute to the high efficiency of our model. Our simple baseline with a\nsingle encoder achieves outstanding performances on the RIS benchmark datasets\nwhile maintaining computational efficiency, compared to the most recent SoTA\nmethods based on dual-encoders.\n","authors":["Seonghoon Yu","Ilchae Jung","Byeongju Han","Taeoh Kim","Yunho Kim","Dongyoon Wee","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2408.15521v1.pdf","comment":"ArXiv pre-print"},{"id":"http://arxiv.org/abs/2408.15519v1","updated":"2024-08-28T04:12:07Z","published":"2024-08-28T04:12:07Z","title":"Depth-Weighted Detection of Behaviours of Risk in People with Dementia\n  using Cameras","summary":"  The behavioural and psychological symptoms of dementia, such as agitation and\naggression, present a significant health and safety risk in residential care\nsettings. Many care facilities have video cameras in place for digital\nmonitoring of public spaces, which can be leveraged to develop an automated\nbehaviours of risk detection system that can alert the staff to enable timely\nintervention and prevent the situation from escalating. However, one of the\nchallenges in our previous study was the presence of false alarms due to\nobstruction of view by activities happening close to the camera. To address\nthis issue, we proposed a novel depth-weighted loss function to train a\ncustomized convolutional autoencoder to enforce equivalent importance to the\nevents happening both near and far from the cameras; thus, helping to reduce\nfalse alarms and making the method more suitable for real-world deployment. The\nproposed method was trained using data from nine participants with dementia\nacross three cameras situated in a specialized dementia unit and achieved an\narea under the curve of receiver operating characteristic of $0.852$, $0.81$\nand $0.768$ for the three cameras. Ablation analysis was conducted for the\nindividual components of the proposed method and the performance of the\nproposed method was investigated for participant-specific and sex-specific\nbehaviours of risk detection. The proposed method performed reasonably well in\ndetecting behaviours of risk in people with dementia motivating further\nresearch toward the development of a behaviours of risk detection system\nsuitable for deployment in video surveillance systems in care facilities.\n","authors":["Pratik K. Mishra","Irene Ballester","Andrea Iaboni","Bing Ye","Kristine Newman","Alex Mihailidis","Shehroz S. Khan"],"pdf_url":"https://arxiv.org/pdf/2408.15519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03691v3","updated":"2024-08-28T03:57:26Z","published":"2024-03-06T13:17:41Z","title":"MolNexTR: A Generalized Deep Learning Model for Molecular Image\n  Recognition","summary":"  In the field of chemical structure recognition, the task of converting\nmolecular images into machine-readable data formats such as SMILES string\nstands as a significant challenge, primarily due to the varied drawing styles\nand conventions prevalent in chemical literature. To bridge this gap, we\nproposed MolNexTR, a novel image-to-graph deep learning model that collaborates\nto fuse the strengths of ConvNext, a powerful Convolutional Neural Network\nvariant, and Vision-TRansformer. This integration facilitates a more detailed\nextraction of both local and global features from molecular images. MolNexTR\ncan predict atoms and bonds simultaneously and understand their layout rules.\nIt also excels at flexibly integrating symbolic chemistry principles to discern\nchirality and decipher abbreviated structures. We further incorporate a series\nof advanced algorithms, including an improved data augmentation module, an\nimage contamination module, and a post-processing module for getting the final\nSMILES output. These modules cooperate to enhance the model's robustness to\ndiverse styles of molecular images found in real literature. In our test sets,\nMolNexTR has demonstrated superior performance, achieving an accuracy rate of\n81-97%, marking a significant advancement in the domain of molecular structure\nrecognition.\n","authors":["Yufan Chen","Ching Ting Leung","Yong Huang","Jianwei Sun","Hao Chen","Hanyu Gao"],"pdf_url":"https://arxiv.org/pdf/2403.03691v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15513v1","updated":"2024-08-28T03:50:04Z","published":"2024-08-28T03:50:04Z","title":"Continual-learning-based framework for structural damage recognition","summary":"  Multi-damage is common in reinforced concrete structures and leads to the\nrequirement of large number of neural networks, parameters and data storage, if\nconvolutional neural network (CNN) is used for damage recognition. In addition,\nconventional CNN experiences catastrophic forgetting and training inefficiency\nas the number of tasks increases during continual learning, leading to large\naccuracy decrease of previous learned tasks. To address these problems, this\nstudy proposes a continuallearning-based damage recognition model (CLDRM) which\nintegrates the learning without forgetting continual learning method into the\nResNet-34 architecture for the recognition of damages in RC structures as well\nas relevant structural components. Three experiments for four recognition tasks\nwere designed to validate the feasibility and effectiveness of the CLDRM\nframework. In this way, it reduces both the prediction time and data storage by\nabout 75% in four tasks of continuous learning. Three experiments for four\nrecognition tasks were designed to validate the feasibility and effectiveness\nof the CLDRM framework. By gradual feature fusion, CLDRM outperformed other\nmethods by managed to achieve high accuracy in the damage recognition and\nclassification. As the number of recognition tasks increased, CLDRM also\nexperienced smaller decrease of the previous learned tasks. Results indicate\nthat the CLDRM framework successfully performs damage recognition and\nclassification with reasonable accuracy and effectiveness.\n","authors":["Jiangpeng Shu","Jiawei Zhang","Reachsak Ly","Fangzheng Lin","Yuanfeng Duan"],"pdf_url":"https://arxiv.org/pdf/2408.15513v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2307.12622v6","updated":"2024-08-28T03:45:08Z","published":"2023-07-24T08:51:49Z","title":"Phase Matching for Out-of-Distribution Generalization","summary":"  The Fourier transform, an explicit decomposition method for visual signals,\nhas been employed to explain the out-of-distribution generalization behaviors\nof Deep Neural Networks (DNNs). Previous studies indicate that the amplitude\nspectrum is susceptible to the disturbance caused by distribution shifts,\nwhereas the phase spectrum preserves highly-structured spatial information that\nis crucial for robust visual representation learning. Inspired by this insight,\nthis paper is dedicated to clarifying the relationships between Domain\nGeneralization (DG) and the frequency components. Specifically, we provide\ndistribution analysis and empirical experiments for the frequency components.\nBased on these observations, we propose a Phase Matching approach, termed\nPhaMa, to address DG problems. To this end, PhaMa introduces perturbations on\nthe amplitude spectrum and establishes spatial relationships to match the phase\ncomponents with patch contrastive learning. Experiments on multiple benchmarks\ndemonstrate that our proposed method achieves state-of-the-art performance in\ndomain generalization and out-of-distribution robustness tasks. Beyond vanilla\nanalysis and experiments, we further clarify the relationships between the\nFourier components and DG problems by introducing a Fourier-based Structural\nCausal Model (SCM).\n","authors":["Chengming Hu","Yeqian Du","Rui Wang","Hao Chen","Congcong Zhu"],"pdf_url":"https://arxiv.org/pdf/2307.12622v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06207v5","updated":"2024-08-28T03:39:10Z","published":"2023-09-12T13:21:12Z","title":"SGNet: Salient Geometric Network for Point Cloud Registration","summary":"  Point Cloud Registration (PCR) is a critical and challenging task in computer\nvision. One of the primary difficulties in PCR is identifying salient and\nmeaningful points that exhibit consistent semantic and geometric properties\nacross different scans. Previous methods have encountered challenges with\nambiguous matching due to the similarity among patch blocks throughout the\nentire point cloud and the lack of consideration for efficient global geometric\nconsistency. To address these issues, we propose a new framework that includes\nseveral novel techniques. Firstly, we introduce a semantic-aware geometric\nencoder that combines object-level and patch-level semantic information. This\nencoder significantly improves registration recall by reducing ambiguity in\npatch-level superpoint matching. Additionally, we incorporate a prior knowledge\napproach that utilizes an intrinsic shape signature to identify salient points.\nThis enables us to extract the most salient super points and meaningful dense\npoints in the scene. Secondly, we introduce an innovative transformer that\nencodes High-Order (HO) geometric features. These features are crucial for\nidentifying salient points within initial overlap regions while considering\nglobal high-order geometric consistency. To optimize this high-order\ntransformer further, we introduce an anchor node selection strategy. By\nencoding inter-frame triangle or polyhedron consistency features based on these\nanchor nodes, we can effectively learn high-order geometric features of salient\nsuper points. These high-order features are then propagated to dense points and\nutilized by a Sinkhorn matching module to identify key correspondences for\nsuccessful registration. In our experiments conducted on well-known datasets\nsuch as 3DMatch/3DLoMatch and KITTI, our approach has shown promising results,\nhighlighting the effectiveness of our novel method.\n","authors":["Qianliang Wu","Yaqing Ding","Lei Luo","Haobo Jiang","Shuo Gu","Chuanwei Zhou","Jin Xie","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2309.06207v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09460v2","updated":"2024-08-28T03:29:42Z","published":"2024-08-18T12:48:48Z","title":"Fine-Grained Building Function Recognition from Street-View Images via\n  Geometry-Aware Semi-Supervised Learning","summary":"  In this work, we propose a geometry-aware semi-supervised method for\nfine-grained building function recognition. This method leverages the geometric\nrelationships between multi-source data to improve the accuracy of pseudo\nlabels in semi-supervised learning, extending the task's scope and making it\napplicable to cross-categorization systems of building function recognition.\nFirstly, we design an online semi-supervised pre-training stage, which\nfacilitates the precise acquisition of building facade location information in\nstreet-view images. In the second stage, we propose a geometry-aware coarse\nannotation generation module. This module effectively combines GIS data and\nstreet-view data based on the geometric relationships, improving the accuracy\nof pseudo annotations. In the third stage, we combine the newly generated\ncoarse annotations with the existing labeled dataset to achieve fine-grained\nfunctional recognition of buildings across multiple cities at a large scale.\nExtensive experiments demonstrate that our proposed framework exhibits superior\nperformance in fine-grained functional recognition of buildings. Within the\nsame categorization system, it achieves improvements of 7.6% and 4.8% compared\nto fully-supervised methods and state-of-the-art semi-supervised methods,\nrespectively. Additionally, our method also performs well in cross-city tasks,\ni.e., extending the model trained on OmniCity (New York) to new areas (i.e.,\nLos Angeles and Boston). This study provides a novel solution for the\nfine-grained function recognition of large-scale buildings across multiple\ncities, offering essential data for understanding urban infrastructure\nplanning, human activity patterns, and the interactions between humans and\nbuildings.\n","authors":["Weijia Li","Jinhua Yu","Dairong Chen","Yi Lin","Runmin Dong","Xiang Zhang","Conghui He","Haohuan Fu"],"pdf_url":"https://arxiv.org/pdf/2408.09460v2.pdf","comment":"This paper is currently under review"},{"id":"http://arxiv.org/abs/2408.15503v1","updated":"2024-08-28T03:17:40Z","published":"2024-08-28T03:17:40Z","title":"RoboSense: Large-scale Dataset and Benchmark for Multi-sensor Low-speed\n  Autonomous Driving","summary":"  Robust object detection and tracking under arbitrary sight of view is\nchallenging yet essential for the development of Autonomous Vehicle technology.\nWith the growing demand of unmanned function vehicles, near-field scene\nunderstanding becomes an important research topic in the areas of low-speed\nautonomous driving. Due to the complexity of driving conditions and diversity\nof near obstacles such as blind spots and high occlusion, the perception\ncapability of near-field environment is still inferior than its farther\ncounterpart. To further enhance the intelligent ability of unmanned vehicles,\nin this paper, we construct a multimodal data collection platform based on 3\nmain types of sensors (Camera, LiDAR and Fisheye), which supports flexible\nsensor configurations to enable dynamic sight of view for ego vehicle, either\nglobal view or local view. Meanwhile, a large-scale multi-sensor dataset is\nbuilt, named RoboSense, to facilitate near-field scene understanding. RoboSense\ncontains more than 133K synchronized data with 1.4M 3D bounding box and IDs\nannotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K\ntemporal sequences. It has $270\\times$ and $18\\times$ as many annotations of\nnear-field obstacles within 5$m$ as the previous single-vehicle datasets such\nas KITTI and nuScenes. Moreover, we define a novel matching criterion for\nnear-field 3D perception and prediction metrics. Based on RoboSense, we\nformulate 6 popular tasks to facilitate the future development of related\nresearch, where the detailed data analysis as well as benchmarks are also\nprovided accordingly.\n","authors":["Haisheng Su","Feixiang Song","Cong Ma","Panpan Cai","Wei Wu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2408.15503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02408v2","updated":"2024-08-28T02:53:22Z","published":"2024-08-05T12:09:38Z","title":"Multi-weather Cross-view Geo-localization Using Denoising Diffusion\n  Models","summary":"  Cross-view geo-localization in GNSS-denied environments aims to determine an\nunknown location by matching drone-view images with the correct geo-tagged\nsatellite-view images from a large gallery. Recent research shows that learning\ndiscriminative image representations under specific weather conditions can\nsignificantly enhance performance. However, the frequent occurrence of unseen\nextreme weather conditions hinders progress. This paper introduces MCGF, a\nMulti-weather Cross-view Geo-localization Framework designed to dynamically\nadapt to unseen weather conditions. MCGF establishes a joint optimization\nbetween image restoration and geo-localization using denoising diffusion\nmodels. For image restoration, MCGF incorporates a shared encoder and a\nlightweight restoration module to help the backbone eliminate weather-specific\ninformation. For geo-localization, MCGF uses EVA-02 as a backbone for feature\nextraction, with cross-entropy loss for training and cosine distance for\ntesting. Extensive experiments on University160k-WX demonstrate that MCGF\nachieves competitive results for geo-localization in varying weather\nconditions.\n","authors":["Tongtong Feng","Qing Li","Xin Wang","Mingzi Wang","Guangyao Li","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.02408v2.pdf","comment":"Accepted by ACM MM24 workshop"},{"id":"http://arxiv.org/abs/2408.15484v1","updated":"2024-08-28T02:17:58Z","published":"2024-08-28T02:17:58Z","title":"NAS-BNN: Neural Architecture Search for Binary Neural Networks","summary":"  Binary Neural Networks (BNNs) have gained extensive attention for their\nsuperior inferencing efficiency and compression ratio compared to traditional\nfull-precision networks. However, due to the unique characteristics of BNNs,\ndesigning a powerful binary architecture is challenging and often requires\nsignificant manpower. A promising solution is to utilize Neural Architecture\nSearch (NAS) to assist in designing BNNs, but current NAS methods for BNNs are\nrelatively straightforward and leave a performance gap between the searched\nmodels and manually designed ones. To address this gap, we propose a novel\nneural architecture search scheme for binary neural networks, named NAS-BNN. We\nfirst carefully design a search space based on the unique characteristics of\nBNNs. Then, we present three training strategies, which significantly enhance\nthe training of supernet and boost the performance of all subnets. Our\ndiscovered binary model family outperforms previous BNNs for a wide range of\noperations (OPs) from 20M to 200M. For instance, we achieve 68.20% top-1\naccuracy on ImageNet with only 57M OPs. In addition, we validate the\ntransferability of these searched BNNs on the object detection task, and our\nbinary detectors with the searched BNNs achieve a novel state-of-the-art\nresult, e.g., 31.6% mAP with 370M OPs, on MS COCO dataset. The source code and\nmodels will be released at https://github.com/VDIGPKU/NAS-BNN.\n","authors":["Zhihao Lin","Yongtao Wang","Jinhe Zhang","Xiaojie Chu","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2408.15484v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2408.14895v2","updated":"2024-08-28T01:56:33Z","published":"2024-08-27T09:18:57Z","title":"VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view\n  Videos of Daily Activities","summary":"  Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data\n(e.g., images and videos) into symbols, have attracted attention as resources\nenabling knowledge processing and machine learning across modalities. However,\nthe construction of MMKGs for videos consisting of multiple events, such as\ndaily activities, is still in the early stages. In this paper, we construct an\nMMKG based on synchronized multi-view simulated videos of daily activities.\nBesides representing the content of daily life videos as event-centric\nknowledge, our MMKG also includes frame-by-frame fine-grained changes, such as\nbounding boxes within video frames. In addition, we provide support tools for\nquerying our MMKG. As an application example, we demonstrate that our MMKG\nfacilitates benchmarking vision-language models by providing the necessary\nvision-language datasets for a tailored task.\n","authors":["Shusaku Egami","Takahiro Ugai","Swe Nwe Nwe Htun","Ken Fukuda"],"pdf_url":"https://arxiv.org/pdf/2408.14895v2.pdf","comment":"5 pages, 4 figures, accepted by CIKM2024 Resource Track"},{"id":"http://arxiv.org/abs/2402.14780v3","updated":"2024-08-28T01:13:44Z","published":"2024-02-22T18:38:48Z","title":"Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models","summary":"  Image customization has been extensively studied in text-to-image (T2I)\ndiffusion models, leading to impressive outcomes and applications. With the\nemergence of text-to-video (T2V) diffusion models, its temporal counterpart,\nmotion customization, has not yet been well investigated. To address the\nchallenge of one-shot video motion customization, we propose Customize-A-Video\nthat models the motion from a single reference video and adapts it to new\nsubjects and scenes with both spatial and temporal varieties. It leverages\nlow-rank adaptation (LoRA) on temporal attention layers to tailor the\npre-trained T2V diffusion model for specific motion modeling. To disentangle\nthe spatial and temporal information during training, we introduce a novel\nconcept of appearance absorbers that detach the original appearance from the\nreference video prior to motion learning. The proposed modules are trained in a\nstaged pipeline and inferred in a plug-and-play fashion, enabling easy\nextensions to various downstream tasks such as custom video generation and\nediting, video appearance customization and multiple motion combination. Our\nproject page can be found at https://customize-a-video.github.io.\n","authors":["Yixuan Ren","Yang Zhou","Jimei Yang","Jing Shi","Difan Liu","Feng Liu","Mingi Kwon","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2402.14780v3.pdf","comment":"Accepted by ECCV 2024. Project page:\n  https://customize-a-video.github.io"},{"id":"http://arxiv.org/abs/2408.15465v1","updated":"2024-08-28T01:06:19Z","published":"2024-08-28T01:06:19Z","title":"Dynamic Reconstruction from Neuromorphic Data","summary":"  Unlike traditional cameras which synchronously register pixel intensity,\nneuromorphic sensors only register `changes' at pixels where a change is\noccurring asynchronously. This enables neuromorphic sensors to sample at a\nmicro-second level and efficiently capture the dynamics. Since, only sequences\nof asynchronous event changes are recorded rather than brightness intensities\nover time, many traditional image processing techniques cannot be directly\napplied. Furthermore, existing approaches, including the ones recently\nintroduced by the authors, use traditional images combined with neuromorphic\nevent data to carry out reconstructions. The aim of this work is introduce an\noptimization based approach to reconstruct images and dynamics only from the\nneuromoprhic event data without any additional knowledge of the events. Each\npixel is modeled temporally. The experimental results on real data highlight\nthe efficacy of the presented approach, paving the way for efficient and\naccurate processing of neuromorphic sensor data in real-world applications.\n","authors":["Harbir Antil","Daniel Blauvelt","David Sayre"],"pdf_url":"https://arxiv.org/pdf/2408.15465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15461v1","updated":"2024-08-28T00:54:51Z","published":"2024-08-28T00:54:51Z","title":"Hand1000: Generating Realistic Hands from Text with Only 1,000 Images","summary":"  Text-to-image generation models have achieved remarkable advancements in\nrecent years, aiming to produce realistic images from textual descriptions.\nHowever, these models often struggle with generating anatomically accurate\nrepresentations of human hands. The resulting images frequently exhibit issues\nsuch as incorrect numbers of fingers, unnatural twisting or interlacing of\nfingers, or blurred and indistinct hands. These issues stem from the inherent\ncomplexity of hand structures and the difficulty in aligning textual\ndescriptions with precise visual depictions of hands. To address these\nchallenges, we propose a novel approach named Hand1000 that enables the\ngeneration of realistic hand images with target gesture using only 1,000\ntraining samples. The training of Hand1000 is divided into three stages with\nthe first stage aiming to enhance the model's understanding of hand anatomy by\nusing a pre-trained hand gesture recognition model to extract gesture\nrepresentation. The second stage further optimizes text embedding by\nincorporating the extracted hand gesture representation, to improve alignment\nbetween the textual descriptions and the generated hand images. The third stage\nutilizes the optimized embedding to fine-tune the Stable Diffusion model to\ngenerate realistic hand images. In addition, we construct the first publicly\navailable dataset specifically designed for text-to-hand image generation.\nBased on the existing hand gesture recognition dataset, we adopt advanced image\ncaptioning models and LLaMA3 to generate high-quality textual descriptions\nenriched with detailed gesture information. Extensive experiments demonstrate\nthat Hand1000 significantly outperforms existing models in producing\nanatomically correct hand images while faithfully representing other details in\nthe text, such as faces, clothing, and colors.\n","authors":["Haozhuo Zhang","Bin Zhu","Yu Cao","Yanbin Hao"],"pdf_url":"https://arxiv.org/pdf/2408.15461v1.pdf","comment":"Project page https://haozhuo-zhang.github.io/Hand1000-project-page/"},{"id":"http://arxiv.org/abs/2408.15450v1","updated":"2024-08-28T00:07:51Z","published":"2024-08-28T00:07:51Z","title":"Avoiding Generative Model Writer's Block With Embedding Nudging","summary":"  Generative image models, since introduction, have become a global phenomenon.\nFrom new arts becoming possible to new vectors of abuse, many new capabilities\nhave become available. One of the challenging issues with generative models is\ncontrolling the generation process specially to prevent specific generations\nclasses or instances . There are several reasons why one may want to control\nthe output of generative models, ranging from privacy and safety concerns to\napplication limitations or user preferences\n  To address memorization and privacy challenges, there has been considerable\nresearch dedicated to filtering prompts or filtering the outputs of these\nmodels. What all these solutions have in common is that at the end of the day\nthey stop the model from producing anything, hence limiting the usability of\nthe model. In this paper, we propose a method for addressing this usability\nissue by making it possible to steer away from unwanted concepts (when detected\nin model's output) and still generating outputs. In particular we focus on the\nlatent diffusion image generative models and how one can prevent them to\ngenerate particular images while generating similar images with limited\noverhead.\n  We focus on mitigating issues like image memorization, demonstrating our\ntechnique's effectiveness through qualitative and quantitative evaluations. Our\nmethod successfully prevents the generation of memorized training images while\nmaintaining comparable image quality and relevance to the unmodified model.\n","authors":["Ali Zand","Milad Nasr"],"pdf_url":"https://arxiv.org/pdf/2408.15450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16176v1","updated":"2024-08-28T23:53:57Z","published":"2024-08-28T23:53:57Z","title":"VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language\n  Models for Trait Discovery from Biological Images","summary":"  Images are increasingly becoming the currency for documenting biodiversity on\nthe planet, providing novel opportunities for accelerating scientific\ndiscoveries in the field of organismal biology, especially with the advent of\nlarge vision-language models (VLMs). We ask if pre-trained VLMs can aid\nscientists in answering a range of biologically relevant questions without any\nadditional fine-tuning. In this paper, we evaluate the effectiveness of 12\nstate-of-the-art (SOTA) VLMs in the field of organismal biology using a novel\ndataset, VLM4Bio, consisting of 469K question-answer pairs involving 30K images\nfrom three groups of organisms: fishes, birds, and butterflies, covering five\nbiologically relevant tasks. We also explore the effects of applying prompting\ntechniques and tests for reasoning hallucination on the performance of VLMs,\nshedding new light on the capabilities of current SOTA VLMs in answering\nbiologically relevant questions using images. The code and datasets for running\nall the analyses reported in this paper can be found at\nhttps://github.com/sammarfy/VLM4Bio.\n","authors":["M. Maruf","Arka Daw","Kazi Sajeed Mehrab","Harish Babu Manogaran","Abhilash Neog","Medha Sawhney","Mridul Khurana","James P. Balhoff","Yasin Bakis","Bahadir Altintas","Matthew J. Thompson","Elizabeth G. Campolongo","Josef C. Uyeda","Hilmar Lapp","Henry L. Bart","Paula M. Mabee","Yu Su","Wei-Lun Chao","Charles Stewart","Tanya Berger-Wolf","Wasila Dahdul","Anuj Karpatne"],"pdf_url":"https://arxiv.org/pdf/2408.16176v1.pdf","comment":"36 pages, 37 figures, 7 tables"},{"id":"http://arxiv.org/abs/2408.16154v1","updated":"2024-08-28T22:14:44Z","published":"2024-08-28T22:14:44Z","title":"Does Data-Efficient Generalization Exacerbate Bias in Foundation Models?","summary":"  Foundation models have emerged as robust models with label efficiency in\ndiverse domains. In medical imaging, these models contribute to the advancement\nof medical diagnoses due to the difficulty in obtaining labeled data. However,\nit is unclear whether using a large amount of unlabeled data, biased by the\npresence of sensitive attributes during pre-training, influences the fairness\nof the model. This research examines the bias in the Foundation model\n(RetFound) when it is applied to fine-tune the Brazilian Multilabel\nOphthalmological Dataset (BRSET), which has a different population than the\npre-training dataset. The model evaluation, in comparison with supervised\nlearning, shows that the Foundation Model has the potential to reduce the gap\nbetween the maximum AUC and minimum AUC evaluations across gender and age\ngroups. However, in a data-efficient generalization, the model increases the\nbias when the data amount decreases. These findings suggest that when deploying\na Foundation Model in real-life scenarios with limited data, the possibility of\nfairness issues should be considered.\n","authors":["Dilermando Queiroz","Anderson Carlos","MaÃ­ra Fatoretto","AndrÃ© Anjos","Lilian Berton","Luis Filipe Nakayama"],"pdf_url":"https://arxiv.org/pdf/2408.16154v1.pdf","comment":"Preprint of paper to be presented at Fairness and Ethics Towards\n  Transparent AI: Facing the Challenge through Model Debiasing (FAILED) during\n  ECCV 2024"},{"id":"http://arxiv.org/abs/2408.16150v1","updated":"2024-08-28T22:02:38Z","published":"2024-08-28T22:02:38Z","title":"Single-Photon 3D Imaging with Equi-Depth Photon Histograms","summary":"  Single-photon cameras present a promising avenue for high-resolution 3D\nimaging. They have ultra-high sensitivity -- down to individual photons -- and\ncan record photon arrival times with extremely high (sub-nanosecond)\nresolution. Single-photon 3D cameras estimate the round-trip time of a laser\npulse by forming equi-width (EW) histograms of detected photon timestamps.\nAcquiring and transferring such EW histograms requires high bandwidth and\nin-pixel memory, making SPCs less attractive in resource-constrained settings\nsuch as mobile devices and AR/VR headsets. In this work we propose a 3D sensing\ntechnique based on equi-depth (ED) histograms. ED histograms compress timestamp\ndata more efficiently than EW histograms, reducing the bandwidth requirement.\nMoreover, to reduce the in-pixel memory requirement, we propose a lightweight\nalgorithm to estimate ED histograms in an online fashion without explicitly\nstoring the photon timestamps. This algorithm is amenable to future in-pixel\nimplementations. We propose algorithms that process ED histograms to perform 3D\ncomputer-vision tasks of estimating scene distance maps and performing visual\nodometry under challenging conditions such as high ambient light. Our work\npaves the way towards lower bandwidth and reduced in-pixel memory requirements\nfor SPCs, making them attractive for resource-constrained 3D vision\napplications. Project page:\n$\\href{https://www.computational.camera/pedh}{https://www.computational.camera/pedh}$\n","authors":["Kaustubh Sadekar","David Maier","Atul Ingle"],"pdf_url":"https://arxiv.org/pdf/2408.16150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16318v2","updated":"2024-08-28T21:07:49Z","published":"2024-03-24T22:53:16Z","title":"AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans","summary":"  Recently, progress in acquisition equipment such as LiDAR sensors has enabled\nsensing increasingly spacious outdoor 3D environments. Making sense of such 3D\nacquisitions requires fine-grained scene understanding, such as constructing\ninstance-based 3D scene segmentations. Commonly, a neural network is trained\nfor this task; however, this requires access to a large, densely annotated\ndataset, which is widely known to be challenging to obtain. To address this\nissue, in this work we propose to predict instance segmentations for 3D scenes\nin an unsupervised way, without relying on ground-truth annotations. To this\nend, we construct a learning framework consisting of two components: (1) a\npseudo-annotation scheme for generating initial unsupervised pseudo-labels; and\n(2) a self-training algorithm for instance segmentation to fit robust, accurate\ninstances from initial noisy proposals. To enable generating 3D instance mask\nproposals, we construct a weighted proxy-graph by connecting 3D points with\nedges integrating multi-modal image- and point-based self-supervised features,\nand perform graph-cuts to isolate individual pseudo-instances. We then build on\na state-of-the-art point-based architecture and train a 3D instance\nsegmentation model, resulting in significant refinement of initial proposals.\nTo scale to arbitrary complexity 3D scenes, we design our algorithm to operate\non local 3D point chunks and construct a merging step to generate scene-level\ninstance segmentations. Experiments on the challenging SemanticKITTI benchmark\ndemonstrate the potential of our approach, where it attains 13.3% higher\nAverage Precision and 9.1% higher F1 score compared to the best-performing\nbaseline. The code will be made publicly available at\nhttps://github.com/artonson/autoinst.\n","authors":["Cedric Perauer","Laurenz Adrian Heidrich","Haifan Zhang","Matthias NieÃner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.16318v2.pdf","comment":"8 pages, 7 figures, to be published in IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2408.16130v1","updated":"2024-08-28T20:35:38Z","published":"2024-08-28T20:35:38Z","title":"Using Backbone Foundation Model for Evaluating Fairness in Chest\n  Radiography Without Demographic Data","summary":"  Ensuring consistent performance across diverse populations and incorporating\nfairness into machine learning models are crucial for advancing medical image\ndiagnostics and promoting equitable healthcare. However, many databases do not\nprovide protected attributes or contain unbalanced representations of\ndemographic groups, complicating the evaluation of model performance across\ndifferent demographics and the application of bias mitigation techniques that\nrely on these attributes. This study aims to investigate the effectiveness of\nusing the backbone of Foundation Models as an embedding extractor for creating\ngroups that represent protected attributes, such as gender and age. We propose\nutilizing these groups in different stages of bias mitigation, including\npre-processing, in-processing, and evaluation. Using databases in and\nout-of-distribution scenarios, it is possible to identify that the method can\ncreate groups that represent gender in both databases and reduce in 4.44% the\ndifference between the gender attribute in-distribution and 6.16% in\nout-of-distribution. However, the model lacks robustness in handling age\nattributes, underscoring the need for more fundamentally fair and robust\nFoundation models. These findings suggest a role in promoting fairness\nassessment in scenarios where we lack knowledge of attributes, contributing to\nthe development of more equitable medical diagnostics.\n","authors":["Dilermando Queiroz","AndrÃ© Anjos","Lilian Berton"],"pdf_url":"https://arxiv.org/pdf/2408.16130v1.pdf","comment":"Preprint of paper to be presented at Fairness of AI in Medical\n  Imaging (FAIMI) during MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.15113v2","updated":"2024-08-28T20:31:42Z","published":"2024-08-27T14:51:34Z","title":"AnomalousPatchCore: Exploring the Use of Anomalous Samples in Industrial\n  Anomaly Detection","summary":"  Visual inspection, or industrial anomaly detection, is one of the most common\nquality control types in manufacturing. The task is to identify the presence of\nan anomaly given an image, e.g., a missing component on an image of a circuit\nboard, for subsequent manual inspection. While industrial anomaly detection has\nseen a surge in recent years, most anomaly detection methods still utilize\nknowledge only from normal samples, failing to leverage the information from\nthe frequently available anomalous samples. Additionally, they heavily rely on\nvery general feature extractors pre-trained on common image classification\ndatasets. In this paper, we address these shortcomings and propose the new\nanomaly detection system AnomalousPatchCore~(APC) based on a feature extractor\nfine-tuned with normal and anomalous in-domain samples and a subsequent memory\nbank for identifying unusual features. To fine-tune the feature extractor in\nAPC, we propose three auxiliary tasks that address the different aspects of\nanomaly detection~(classification vs. localization) and mitigate the effect of\nthe imbalance between normal and anomalous samples. Our extensive evaluation on\nthe MVTec dataset shows that APC outperforms state-of-the-art systems in\ndetecting anomalies, which is especially important in industrial anomaly\ndetection given the subsequent manual inspection. In detailed ablation studies,\nwe further investigate the properties of our APC.\n","authors":["Mykhailo Koshil","Tilman Wegener","Detlef Mentrup","Simone Frintrop","Christian Wilms"],"pdf_url":"https://arxiv.org/pdf/2408.15113v2.pdf","comment":"Accepted at the 2nd workshop on Vision-based InduStrial InspectiON\n  (VISION) @ ECCV"},{"id":"http://arxiv.org/abs/2408.15077v2","updated":"2024-08-28T20:30:29Z","published":"2024-08-27T14:05:48Z","title":"MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of\n  Children with Autism Spectrum Disorder","summary":"  Autism spectrum disorder (ASD) is characterized by significant challenges in\nsocial interaction and comprehending communication signals. Recently,\ntherapeutic interventions for ASD have increasingly utilized Deep learning\npowered-computer vision techniques to monitor individual progress over time.\nThese models are trained on private, non-public datasets from the autism\ncommunity, creating challenges in comparing results across different models due\nto privacy-preserving data-sharing issues. This work introduces MMASD+, an\nenhanced version of the novel open-source dataset called Multimodal ASD\n(MMASD). MMASD+ consists of diverse data modalities, including 3D-Skeleton, 3D\nBody Mesh, and Optical Flow data. It integrates the capabilities of Yolov8 and\nDeep SORT algorithms to distinguish between the therapist and children,\naddressing a significant barrier in the original dataset. Additionally, a\nMultimodal Transformer framework is proposed to predict 11 action types and the\npresence of ASD. This framework achieves an accuracy of 95.03% for predicting\naction types and 96.42% for predicting ASD presence, demonstrating over a 10%\nimprovement compared to models trained on single data modalities. These\nfindings highlight the advantages of integrating multiple data modalities\nwithin the Multimodal Transformer framework.\n","authors":["Pavan Uttej Ravva","Behdokht Kiafar","Pinar Kullu","Jicheng Li","Anjana Bhat","Roghayeh Leila Barmaki"],"pdf_url":"https://arxiv.org/pdf/2408.15077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03652v2","updated":"2024-08-28T20:29:12Z","published":"2024-05-06T17:23:42Z","title":"Field-of-View Extension for Brain Diffusion MRI via Deep Generative\n  Models","summary":"  Purpose: In diffusion MRI (dMRI), the volumetric and bundle analyses of\nwhole-brain tissue microstructure and connectivity can be severely impeded by\nan incomplete field-of-view (FOV). This work aims to develop a method for\nimputing the missing slices directly from existing dMRI scans with an\nincomplete FOV. We hypothesize that the imputed image with complete FOV can\nimprove the whole-brain tractography for corrupted data with incomplete FOV.\nTherefore, our approach provides a desirable alternative to discarding the\nvaluable dMRI data, enabling subsequent tractography analyses that would\notherwise be challenging or unattainable with corrupted data. Approach: We\npropose a framework based on a deep generative model that estimates the absent\nbrain regions in dMRI scans with incomplete FOV. The model is capable of\nlearning both the diffusion characteristics in diffusion-weighted images (DWI)\nand the anatomical features evident in the corresponding structural images for\nefficiently imputing missing slices of DWI outside of incomplete FOV. Results:\nFor evaluating the imputed slices, on the WRAP dataset the proposed framework\nachieved PSNRb0=22.397, SSIMb0=0.905, PSNRb1300=22.479, SSIMb1300=0.893; on the\nNACC dataset it achieved PSNRb0=21.304, SSIMb0=0.892, PSNRb1300=21.599,\nSSIMb1300= 0.877. The proposed framework improved the tractography accuracy, as\ndemonstrated by an increased average Dice score for 72 tracts (p < 0.001) on\nboth the WRAP and NACC datasets. Conclusions: Results suggest that the proposed\nframework achieved sufficient imputation performance in dMRI data with\nincomplete FOV for improving whole-brain tractography, thereby repairing the\ncorrupted data. Our approach achieved more accurate whole-brain tractography\nresults with extended and complete FOV and reduced the uncertainty when\nanalyzing bundles associated with Alzheimer's Disease.\n","authors":["Chenyu Gao","Shunxing Bao","Michael Kim","Nancy Newlin","Praitayini Kanakaraj","Tianyuan Yao","Gaurav Rudravaram","Yuankai Huo","Daniel Moyer","Kurt Schilling","Walter Kukull","Arthur Toga","Derek Archer","Timothy Hohman","Bennett Landman","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2405.03652v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.16123v1","updated":"2024-08-28T20:22:39Z","published":"2024-08-28T20:22:39Z","title":"ChartEye: A Deep Learning Framework for Chart Information Extraction","summary":"  The widespread use of charts and infographics as a means of data\nvisualization in various domains has inspired recent research in automated\nchart understanding. However, information extraction from chart images is a\ncomplex multitasked process due to style variations and, as a consequence, it\nis challenging to design an end-to-end system. In this study, we propose a deep\nlearning-based framework that provides a solution for key steps in the chart\ninformation extraction pipeline. The proposed framework utilizes hierarchal\nvision transformers for the tasks of chart-type and text-role classification,\nwhile YOLOv7 for text detection. The detected text is then enhanced using Super\nResolution Generative Adversarial Networks to improve the recognition output of\nthe OCR. Experimental results on a benchmark dataset show that our proposed\nframework achieves excellent performance at every stage with F1-scores of 0.97\nfor chart-type classification, 0.91 for text-role classification, and a mean\nAverage Precision of 0.95 for text detection.\n","authors":["Osama Mustafa","Muhammad Khizer Ali","Momina Moetesum","Imran Siddiqi"],"pdf_url":"https://arxiv.org/pdf/2408.16123v1.pdf","comment":"8 Pages, and 11 Figures"},{"id":"http://arxiv.org/abs/2408.16117v1","updated":"2024-08-28T20:05:36Z","published":"2024-08-28T20:05:36Z","title":"Alternating Direction Method of Multipliers for Negative Binomial Model\n  with The Weighted Difference of Anisotropic and Isotropic Total Variation","summary":"  In many applications such as medical imaging, the measurement data represent\ncounts of photons hitting a detector. Such counts in low-photon settings are\noften modeled using a Poisson distribution. However, this model assumes that\nthe mean and variance of the signal's noise distribution are equal. For\noverdispersed data where the variance is greater than the mean, the negative\nbinomial distribution is a more appropriate statistical model. In this paper,\nwe propose an optimization approach for recovering images corrupted by\noverdispersed Poisson noise. In particular, we incorporate a weighted\nanisotropic-isotropic total variation regularizer, which avoids staircasing\nartifacts that are introduced by a regular total variation penalty. We use an\nalternating direction method of multipliers, where each subproblem has a\nclosed-form solution. Numerical experiments demonstrate the effectiveness of\nour proposed approach, especially in very photon-limited settings.\n","authors":["Yu Lu","Kevin Bui","Roummel F. Marcia"],"pdf_url":"https://arxiv.org/pdf/2408.16117v1.pdf","comment":"6 pages, Accepted by the IEEE International Conference on Multimedia\n  and Expo (ICME)"},{"id":"http://arxiv.org/abs/2408.03393v2","updated":"2024-08-28T19:56:19Z","published":"2024-08-06T18:38:55Z","title":"Biomedical Image Segmentation: A Systematic Literature Review of Deep\n  Learning Based Object Detection Methods","summary":"  Biomedical image segmentation plays a vital role in diagnosis of diseases\nacross various organs. Deep learning-based object detection methods are\ncommonly used for such segmentation. There exists an extensive research in this\ntopic. However, there is no standard review on this topic. Existing surveys\noften lack a standardized approach or focus on broader segmentation techniques.\nIn this paper, we conducted a systematic literature review (SLR), collected and\nanalysed 148 articles that explore deep learning object detection methods for\nbiomedical image segmentation. We critically analyzed these methods, identified\nthe key challenges, and discussed the future directions. From the selected\narticles we extracted the results including the deep learning models, targeted\nimaging modalities, targeted diseases, and the metrics for the analysis of the\nmethods. The results have been presented in tabular and/or charted forms. The\nresults are presented in three major categories including two stage detection\nmodels, one stage detection models and point-based detection models. Each\narticle is individually analyzed along with its pros and cons. Finally, we\ndiscuss open challenges, potential benefits, and future research directions.\nThis SLR aims to provide the research community with a quick yet deeper\nunderstanding of these segmentation models, ultimately facilitating the\ndevelopment of more powerful solutions for biomedical image analysis.\n","authors":["Fazli Wahid","Yingliang Ma","Dawar Khan","Muhammad Aamir","Syed U. K. Bukhari"],"pdf_url":"https://arxiv.org/pdf/2408.03393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16113v1","updated":"2024-08-28T19:43:48Z","published":"2024-08-28T19:43:48Z","title":"Negative Binomial Matrix Completion","summary":"  Matrix completion focuses on recovering missing or incomplete information in\nmatrices. This problem arises in various applications, including image\nprocessing and network analysis. Previous research proposed Poisson matrix\ncompletion for count data with noise that follows a Poisson distribution, which\nassumes that the mean and variance are equal. Since overdispersed count data,\nwhose variance is greater than the mean, is more likely to occur in realistic\nsettings, we assume that the noise follows the negative binomial (NB)\ndistribution, which can be more general than the Poisson distribution. In this\npaper, we introduce NB matrix completion by proposing a nuclear-norm\nregularized model that can be solved by proximal gradient descent. In our\nexperiments, we demonstrate that the NB model outperforms Poisson matrix\ncompletion in various noise and missing data settings on real data.\n","authors":["Yu Lu","Kevin Bui","Roummel F. Marcia"],"pdf_url":"https://arxiv.org/pdf/2408.16113v1.pdf","comment":"6 pages, Accepted by the IEEE International Workshop on Machine\n  Learning for Signal Processing (MLSP)"},{"id":"http://arxiv.org/abs/2408.13724v2","updated":"2024-08-28T18:09:49Z","published":"2024-08-25T04:56:09Z","title":"PhysPart: Physically Plausible Part Completion for Interactable Objects","summary":"  Interactable objects are ubiquitous in our daily lives. Recent advances in 3D\ngenerative models make it possible to automate the modeling of these objects,\nbenefiting a range of applications from 3D printing to the creation of robot\nsimulation environments. However, while significant progress has been made in\nmodeling 3D shapes and appearances, modeling object physics, particularly for\ninteractable objects, remains challenging due to the physical constraints\nimposed by inter-part motions. In this paper, we tackle the problem of\nphysically plausible part completion for interactable objects, aiming to\ngenerate 3D parts that not only fit precisely into the object but also allow\nsmooth part motions. To this end, we propose a diffusion-based part generation\nmodel that utilizes geometric conditioning through classifier-free guidance and\nformulates physical constraints as a set of stability and mobility losses to\nguide the sampling process. Additionally, we demonstrate the generation of\ndependent parts, paving the way toward sequential part generation for objects\nwith complex part-whole hierarchies. Experimentally, we introduce a new metric\nfor measuring physical plausibility based on motion success rates. Our model\noutperforms existing baselines over shape and physical metrics, especially\nthose that do not adequately model physical constraints. We also demonstrate\nour applications in 3D printing, robot manipulation, and sequential part\ngeneration, showing our strength in realistic tasks with the demand for high\nphysical plausibility.\n","authors":["Rundong Luo","Haoran Geng","Congyue Deng","Puhao Li","Zan Wang","Baoxiong Jia","Leonidas Guibas","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2408.13724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14028v2","updated":"2024-08-28T18:06:50Z","published":"2024-08-26T05:38:27Z","title":"SurGen: Text-Guided Diffusion Model for Surgical Video Generation","summary":"  Diffusion-based video generation models have made significant strides,\nproducing outputs with improved visual fidelity, temporal coherence, and user\ncontrol. These advancements hold great promise for improving surgical education\nby enabling more realistic, diverse, and interactive simulation environments.\nIn this study, we introduce SurGen, a text-guided diffusion model tailored for\nsurgical video synthesis, producing the highest resolution and longest duration\nvideos among existing surgical video generation models. We validate the visual\nand temporal quality of the outputs using standard image and video generation\nmetrics. Additionally, we assess their alignment to the corresponding text\nprompts through a deep learning classifier trained on surgical data. Our\nresults demonstrate the potential of diffusion models to serve as valuable\neducational tools for surgical trainees.\n","authors":["Joseph Cho","Samuel Schmidgall","Cyril Zakka","Mrudang Mathur","Rohan Shad","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2408.14028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16061v1","updated":"2024-08-28T18:01:00Z","published":"2024-08-28T18:01:00Z","title":"3D Reconstruction with Spatial Memory","summary":"  We present Spann3R, a novel approach for dense 3D reconstruction from ordered\nor unordered image collections. Built on the DUSt3R paradigm, Spann3R uses a\ntransformer-based architecture to directly regress pointmaps from images\nwithout any prior knowledge of the scene or camera parameters. Unlike DUSt3R,\nwhich predicts per image-pair pointmaps each expressed in its local coordinate\nframe, Spann3R can predict per-image pointmaps expressed in a global coordinate\nsystem, thus eliminating the need for optimization-based global alignment. The\nkey idea of Spann3R is to manage an external spatial memory that learns to keep\ntrack of all previous relevant 3D information. Spann3R then queries this\nspatial memory to predict the 3D structure of the next frame in a global\ncoordinate system. Taking advantage of DUSt3R's pre-trained weights, and\nfurther fine-tuning on a subset of datasets, Spann3R shows competitive\nperformance and generalization ability on various unseen datasets and can\nprocess ordered image collections in real time. Project page:\n\\url{https://hengyiwang.github.io/projects/spanner}\n","authors":["Hengyi Wang","Lourdes Agapito"],"pdf_url":"https://arxiv.org/pdf/2408.16061v1.pdf","comment":"Project page: \\url{https://hengyiwang.github.io/projects/spanner}"},{"id":"http://arxiv.org/abs/2011.08388v3","updated":"2024-08-28T22:05:07Z","published":"2020-11-17T02:55:16Z","title":"Interpretable Image Emotion Recognition: A Domain Adaptation Approach\n  Using Facial Expressions","summary":"  This paper proposes a feature-based domain adaptation technique for\nidentifying emotions in generic images, encompassing both facial and non-facial\nobjects, as well as non-human components. This approach addresses the challenge\nof the limited availability of pre-trained models and well-annotated datasets\nfor Image Emotion Recognition (IER). Initially, a deep-learning-based Facial\nExpression Recognition (FER) system is developed, classifying facial images\ninto discrete emotion classes. Maintaining the same network architecture, this\nFER system is then adapted to recognize emotions in generic images through the\napplication of discrepancy loss, enabling the model to effectively learn IER\nfeatures while classifying emotions into categories such as 'happy,' 'sad,'\n'hate,' and 'anger.' Additionally, a novel interpretability method, Divide and\nConquer based Shap (DnCShap), is introduced to elucidate the visual features\nmost relevant for emotion recognition. The proposed IER system demonstrated\nemotion classification accuracies of 60.98% for the IAPSa dataset, 58.86% for\nthe ArtPhoto dataset, 69.13% for the FI dataset, and 58.06% for the EMOTIC\ndataset. The system effectively identifies the important visual features\nleading to specific emotion classifications and provides detailed embedding\nplots to explain the predictions, enhancing the understanding and trust in\nAI-driven emotion recognition systems.\n","authors":["Puneet Kumar","Balasubramanian Raman"],"pdf_url":"https://arxiv.org/pdf/2011.08388v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.15953v1","updated":"2024-08-28T17:12:01Z","published":"2024-08-28T17:12:01Z","title":"Modeling and Analyzing the Influence of Non-Item Pages on Sequential\n  Next-Item Prediction","summary":"  Analyzing the sequence of historical interactions between users and items,\nsequential recommendation models learn user intent and make predictions about\nthe next item of interest. Next to these item interactions, most systems also\nhave interactions with pages not related to specific items, for example\nnavigation pages, account pages, and pages for a specific category, which may\nprovide additional insights into the user's interests. However, while there are\nseveral approaches to integrate additional information about items and users,\nthe topic of integrating non-item pages has been less explored. We use the\nhypotheses testing framework HypTrails to show that there is indeed a\nrelationship between these non-item pages and the items of interest and fill\nthis gap by proposing various approaches of representing non-item pages (e.g,\nbased on their content) to use them as an additional information source for the\ntask of sequential next-item prediction.\n  We create a synthetic dataset with non-item pages highly related to the\nsubsequent item to show that the models are generally capable of learning from\nthese interactions, and subsequently evaluate the improvements gained by\nincluding non-item pages in two real-world datasets.\n  We adapt eight popular sequential recommender models, covering CNN-, RNN- and\ntransformer-based architectures, to integrate non-item pages and investigate\nthe capabilities of these models to leverage their information for next item\nprediction. We also analyze their behavior on noisy data and compare different\nitem representation strategies.\n  Our results show that non-item pages are a valuable source of information,\nbut representing such a page well is the key to successfully leverage them. The\ninclusion of non-item pages can increase the performance for next-item\nprediction in all examined model architectures with a varying degree.\n","authors":["Elisabeth Fischer","Daniel SchlÃ¶r","Albin Zehe","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2408.15953v1.pdf","comment":"36 pages, 19 figures; Work in Progress"},{"id":"http://arxiv.org/abs/2408.15836v1","updated":"2024-08-28T14:48:37Z","published":"2024-08-28T14:48:37Z","title":"Knowledge Navigator: LLM-guided Browsing Framework for Exploratory\n  Search in Scientific Literature","summary":"  The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available.\n","authors":["Uri Katz","Mosh Levy","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2408.15836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15796v1","updated":"2024-08-28T13:42:28Z","published":"2024-08-28T13:42:28Z","title":"Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models","summary":"  This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.\n","authors":["HÃ©di Zhegidi","Ludovic Moncla"],"pdf_url":"https://arxiv.org/pdf/2408.15796v1.pdf","comment":"Github repo: https://github.com/GEODE-project/ner-llm"},{"id":"http://arxiv.org/abs/2408.15787v1","updated":"2024-08-28T13:29:59Z","published":"2024-08-28T13:29:59Z","title":"Interactive Agents: Simulating Counselor-Client Psychological Counseling\n  via Role-Playing LLM-to-LLM Interactions","summary":"  Virtual counselors powered by large language models (LLMs) aim to create\ninteractive support systems that effectively assist clients struggling with\nmental health challenges. To replicate counselor-client conversations,\nresearchers have built an online mental health platform that allows\nprofessional counselors to provide clients with text-based counseling services\nfor about an hour per session. Notwithstanding its effectiveness, challenges\nexist as human annotation is time-consuming, cost-intensive, privacy-protected,\nand not scalable. To address this issue and investigate the applicability of\nLLMs in psychological counseling conversation simulation, we propose a\nframework that employs two LLMs via role-playing for simulating\ncounselor-client interactions. Our framework involves two LLMs, one acting as a\nclient equipped with a specific and real-life user profile and the other\nplaying the role of an experienced counselor, generating professional responses\nusing integrative therapy techniques. We implement both the counselor and the\nclient by zero-shot prompting the GPT-4 model. In order to assess the\neffectiveness of LLMs in simulating counselor-client interactions and\nunderstand the disparities between LLM- and human-generated conversations, we\nevaluate the synthetic data from various perspectives. We begin by assessing\nthe client's performance through automatic evaluations. Next, we analyze and\ncompare the disparities between dialogues generated by the LLM and those\ngenerated by professional counselors. Furthermore, we conduct extensive\nexperiments to thoroughly examine the performance of our LLM-based counselor\ntrained with synthetic interactive dialogues by benchmarking against\nstate-of-the-art models for mental health.\n","authors":["Huachuan Qiu","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2408.15787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14432v2","updated":"2024-08-28T12:39:57Z","published":"2024-08-26T17:20:34Z","title":"Contextual Bandit with Herding Effects: Algorithms and Recommendation\n  Applications","summary":"  Contextual bandits serve as a fundamental algorithmic framework for\noptimizing recommendation decisions online. Though extensive attention has been\npaid to tailoring contextual bandits for recommendation applications, the\n\"herding effects\" in user feedback have been ignored. These herding effects\nbias user feedback toward historical ratings, breaking down the assumption of\nunbiased feedback inherent in contextual bandits. This paper develops a novel\nvariant of the contextual bandit that is tailored to address the feedback bias\ncaused by the herding effects. A user feedback model is formulated to capture\nthis feedback bias. We design the TS-Conf (Thompson Sampling under Conformity)\nalgorithm, which employs posterior sampling to balance the exploration and\nexploitation tradeoff. We prove an upper bound for the regret of the algorithm,\nrevealing the impact of herding effects on learning speed. Extensive\nexperiments on datasets demonstrate that TS-Conf outperforms four benchmark\nalgorithms. Analysis reveals that TS-Conf effectively mitigates the negative\nimpact of herding effects, resulting in faster learning and improved\nrecommendation accuracy.\n","authors":["Luyue Xu","Liming Wang","Hong Xie","Mingqiang Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.14432v2.pdf","comment":"Published as a conference paper at PRICAI 2024"},{"id":"http://arxiv.org/abs/2408.15688v1","updated":"2024-08-28T10:25:36Z","published":"2024-08-28T10:25:36Z","title":"PDSR: A Privacy-Preserving Diversified Service Recommendation Method on\n  Distributed Data","summary":"  The last decade has witnessed a tremendous growth of service computing, while\nefficient service recommendation methods are desired to recommend high-quality\nservices to users. It is well known that collaborative filtering is one of the\nmost popular methods for service recommendation based on QoS, and many existing\nproposals focus on improving recommendation accuracy, i.e., recommending\nhigh-quality redundant services. Nevertheless, users may have different\nrequirements on QoS, and hence diversified recommendation has been attracting\nincreasing attention in recent years to fulfill users' diverse demands and to\nexplore potential services. Unfortunately, the recommendation performances\nrelies on a large volume of data (e.g., QoS data), whereas the data may be\ndistributed across multiple platforms. Therefore, to enable data sharing across\nthe different platforms for diversified service recommendation, we propose a\nPrivacy-preserving Diversified Service Recommendation (PDSR) method.\nSpecifically, we innovate in leveraging the Locality-Sensitive Hashing (LSH)\nmechanism such that privacy-preserved data sharing across different platforms\nis enabled to construct a service similarity graph. Based on the similarity\ngraph, we propose a novel accuracy-diversity metric and design a\n$2$-approximation algorithm to select $K$ services to recommend by maximizing\nthe accuracy-diversity measure. Extensive experiments on real datasets are\nconducted to verify the efficacy of our PDSR method.\n","authors":["Lina Wang","Huan Yang","Yiran Shen","Chao Liu","Lianyong Qi","Xiuzhen Cheng","Feng Li"],"pdf_url":"https://arxiv.org/pdf/2408.15688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11245v4","updated":"2024-08-28T08:51:57Z","published":"2022-05-18T04:38:15Z","title":"PASH at TREC 2021 Deep Learning Track: Generative Enhanced Model for\n  Multi-stage Ranking","summary":"  This paper describes the PASH participation in TREC 2021 Deep Learning Track.\nIn the recall stage, we adopt a scheme combining sparse and dense retrieval\nmethod. In the multi-stage ranking phase, point-wise and pair-wise ranking\nstrategies are used one after another based on model continual pre-trained on\ngeneral knowledge and document-level data. Compared to TREC 2020 Deep Learning\nTrack, we have additionally introduced the generative model T5 to further\nenhance the performance.\n","authors":["Yixuan Qiao","Hao Chen","Jun Wang","Tuozhen Liu","Xianbin Ye","Xin Tang","Rui Fang","Peng Gao","Wenfeng Xie","Guotong Xie"],"pdf_url":"https://arxiv.org/pdf/2205.11245v4.pdf","comment":"TREC 2021"},{"id":"http://arxiv.org/abs/2408.15620v1","updated":"2024-08-28T08:21:56Z","published":"2024-08-28T08:21:56Z","title":"CAPER: Enhancing Career Trajectory Prediction using Temporal Knowledge\n  Graph and Ternary Relationship","summary":"  The problem of career trajectory prediction (CTP) aims to predict one's\nfuture employer or job position. While several CTP methods have been developed\nfor this problem, we posit that none of these methods (1) jointly considers the\nmutual ternary dependency between three key units (i.e., user, position, and\ncompany) of a career and (2) captures the characteristic shifts of key units in\ncareer over time, leading to an inaccurate understanding of the job movement\npatterns in the labor market. To address the above challenges, we propose a\nnovel solution, named as CAPER, that solves the challenges via sophisticated\ntemporal knowledge graph (TKG) modeling. It enables the utilization of a\ngraph-structured knowledge base with rich expressiveness, effectively\npreserving the changes in job movement patterns. Furthermore, we devise an\nextrapolated career reasoning task on TKG for a realistic evaluation. The\nexperiments on a real-world career trajectory dataset demonstrate that CAPER\nconsistently and significantly outperforms four baselines, two recent TKG\nreasoning methods, and five state-of-the-art CTP methods in predicting one's\nfuture companies and positions-i.e., on average, yielding 6.80% and 34.58% more\naccurate predictions, respectively.\n","authors":["Yeon-Chang Lee","JaeHyun Lee","Michiharu Yamashita","Dongwon Lee","Sang-Wook Kim"],"pdf_url":"https://arxiv.org/pdf/2408.15620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15575v1","updated":"2024-08-28T07:00:19Z","published":"2024-08-28T07:00:19Z","title":"Lyrically Speaking: Exploring the Link Between Lyrical Emotions, Themes\n  and Depression Risk","summary":"  Lyrics play a crucial role in affecting and reinforcing emotional states by\nproviding meaning and emotional connotations that interact with the acoustic\nproperties of the music. Specific lyrical themes and emotions may intensify\nexisting negative states in listeners and may lead to undesirable outcomes,\nespecially in listeners with mood disorders such as depression. Hence, it is\nimportant for such individuals to be mindful of their listening strategies. In\nthis study, we examine online music consumption of individuals at risk of\ndepression in light of lyrical themes and emotions. Lyrics obtained from the\nlistening histories of 541 Last.fm users, divided into At-Risk and No-Risk\nbased on their mental well-being scores, were analyzed using natural language\nprocessing techniques. Statistical analyses of the results revealed that\nindividuals at risk for depression prefer songs with lyrics associated with low\nvalence and low arousal. Additionally, lyrics associated with themes of denial,\nself-reference, and ambivalence were preferred. In contrast, themes such as\nliberation, familiarity, and activity are not as favored. This study opens up\nthe possibility of an approach to assessing depression risk from the digital\nfootprint of individuals and potentially developing personalized recommendation\nsystems.\n","authors":["Pavani Chowdary","Bhavyajeet Singh","Rajat Agarwal","Vinoo Alluri"],"pdf_url":"https://arxiv.org/pdf/2408.15575v1.pdf","comment":"Accepted at the 25th International Society for Music Information\n  Retrieval Conference (ISMIR) 2024, San Francisco, United States"},{"id":"http://arxiv.org/abs/2408.07611v2","updated":"2024-08-28T03:47:28Z","published":"2024-08-14T15:19:16Z","title":"WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs","summary":"  Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions.\n","authors":["Weijian Xie","Xuefeng Liang","Yuhui Liu","Kaihua Ni","Hong Cheng","Zetian Hu"],"pdf_url":"https://arxiv.org/pdf/2408.07611v2.pdf","comment":"8 pages, 2 figures, technical report for 3rd place in Task 3 of Meta\n  KDD Cup 2024 CRAG Challenge"},{"id":"http://arxiv.org/abs/2402.07926v2","updated":"2024-08-28T18:53:17Z","published":"2024-02-05T18:16:04Z","title":"From Data Creator to Data Reuser: Distance Matters","summary":"  Sharing research data is necessary, but not sufficient, for data reuse. Open\nscience policies focus more heavily on data sharing than on reuse, yet both are\ncomplex, labor-intensive, expensive, and require infrastructure investments by\nmultiple stakeholders. The value of data reuse lies in relationships between\ncreators and reusers. By addressing knowledge exchange, rather than mere\ntransactions between stakeholders, investments in data management and knowledge\ninfrastructures can be made more wisely. Drawing upon empirical studies of data\nsharing and reuse, we develop the theoretical construct of distance between\ndata creator and data reuser, identifying six distance dimensions that\ninfluence the ability to transfer knowledge effectively: domain, methods,\ncollaboration, curation, purposes, and time and temporality. We address the\nsocial and socio-technical aspects of these dimensions, exploring ways in which\nthey may decrease -- or increase -- distances between creators and reusers. Our\ntheoretical framing of the distance between data creators and prospective\nreusers leads to recommendations to four categories of stakeholders on how to\nmake data sharing and reuse more effective: data creators, data reusers, data\narchivists, and funding agencies. 'It takes a village' to share research data\n-- and a village to reuse data. Our aim is to provoke new research questions,\nnew research, and new investments in effective and efficient circulation of\nresearch data; and to identify criteria for investments at each stage of data\nand research life cycles.\n","authors":["Christine L. Borgman","Paul T. Groth"],"pdf_url":"https://arxiv.org/pdf/2402.07926v2.pdf","comment":"74 pages, double-spaced, consisting of Table of Contents, Abstract,\n  45 page narrative, 1 box, 1 figure, 1 table, 27 pages references. Original\n  work"},{"id":"http://arxiv.org/abs/2408.16036v1","updated":"2024-08-28T16:16:55Z","published":"2024-08-28T16:16:55Z","title":"Efficient $k$-NN Search in IoT Data: Overlap Optimization in Tree-Based\n  Indexing Structures","summary":"  The proliferation of interconnected devices in the Internet of Things (IoT)\nhas led to an exponential increase in data, commonly known as Big IoT Data.\nEfficient retrieval of this heterogeneous data demands a robust indexing\nmechanism for effective organization. However, a significant challenge remains:\nthe overlap in data space partitions during index construction. This overlap\nincreases node access during search and retrieval, resulting in higher resource\nconsumption, performance bottlenecks, and impedes system scalability. To\naddress this issue, we propose three innovative heuristics designed to quantify\nand strategically reduce data space partition overlap. The volume-based method\n(VBM) offers a detailed assessment by calculating the intersection volume\nbetween partitions, providing deeper insights into spatial relationships. The\ndistance-based method (DBM) enhances efficiency by using the distance between\npartition centers and radii to evaluate overlap, offering a streamlined yet\naccurate approach. Finally, the object-based method (OBM) provides a practical\nsolution by counting objects across multiple partitions, delivering an\nintuitive understanding of data space dynamics. Experimental results\ndemonstrate the effectiveness of these methods in reducing search time,\nunderscoring their potential to improve data space partitioning and enhance\noverall system performance.\n","authors":["Ala-Eddine Benrazek","Zineddine Kouahla","Brahim Farou","Hamid Seridi","Ibtissem Kemouguette"],"pdf_url":"https://arxiv.org/pdf/2408.16036v1.pdf","comment":"28 pages, 21 figures, 1 table"},{"id":"http://arxiv.org/abs/2408.16032v1","updated":"2024-08-28T10:31:50Z","published":"2024-08-28T10:31:50Z","title":"An Extremely Data-efficient and Generative LLM-based Reinforcement\n  Learning Agent for Recommenders","summary":"  Recent advancements in large language models (LLMs) have enabled\nunderstanding webpage contexts, product details, and human instructions.\nUtilizing LLMs as the foundational architecture for either reward models or\npolicies in reinforcement learning has gained popularity -- a notable\nachievement is the success of InstructGPT. RL algorithms have been instrumental\nin maximizing long-term customer satisfaction and avoiding short-term, myopic\ngoals in industrial recommender systems, which often rely on deep learning\nmodels to predict immediate clicks or purchases.\n  In this project, several RL methods are implemented and evaluated using the\nWebShop benchmark environment, data, simulator, and pre-trained model\ncheckpoints. The goal is to train an RL agent to maximize the purchase reward\ngiven a detailed human instruction describing a desired product. The RL agents\nare developed by fine-tuning a pre-trained BERT model with various objectives,\nlearning from preferences without a reward model, and employing contemporary\ntraining techniques such as Proximal Policy Optimization (PPO) as used in\nInstructGPT, and Direct Preference Optimization (DPO). This report also\nevaluates the RL agents trained using generative trajectories. Evaluations were\nconducted using Thompson sampling in the WebShop simulator environment.\n  The simulated online experiments demonstrate that agents trained on generated\ntrajectories exhibited comparable task performance to those trained using human\ntrajectories. This has demonstrated an example of an extremely low-cost\ndata-efficient way of training reinforcement learning agents. Also, with\nlimited training time (<2hours), without utilizing any images, a DPO agent\nachieved a 19% success rate after approximately 3000 steps or 30 minutes of\ntraining on T4 GPUs, compared to a PPO agent, which reached a 15% success rate.\n","authors":["Shuang Feng","Grace Feng"],"pdf_url":"https://arxiv.org/pdf/2408.16032v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.15999v1","updated":"2024-08-28T18:05:53Z","published":"2024-08-28T18:05:53Z","title":"Q-MRS: A Deep Learning Framework for Quantitative Magnetic Resonance\n  Spectra Analysis","summary":"  Magnetic resonance spectroscopy (MRS) is an established technique for\nstudying tissue metabolism, particularly in central nervous system disorders.\nWhile powerful and versatile, MRS is often limited by challenges associated\nwith data quality, processing, and quantification. Existing MRS quantification\nmethods face difficulties in balancing model complexity and reproducibility\nduring spectral modeling, often falling into the trap of either\noversimplification or over-parameterization. To address these limitations, this\nstudy introduces a deep learning (DL) framework that employs transfer learning,\nin which the model is pre-trained on simulated datasets before it undergoes\nfine-tuning on in vivo data. The proposed framework showed promising\nperformance when applied to the Philips dataset from the BIG GABA repository\nand represents an exciting advancement in MRS data analysis.\n","authors":["Christopher J. Wu","Lawrence S. Kegeles","Jia Guo"],"pdf_url":"https://arxiv.org/pdf/2408.15999v1.pdf","comment":"8 pages, 4 figures, and 3 tables for the main body; 9 pages, 4\n  figures, and 3 tables for the supplementary material"},{"id":"http://arxiv.org/abs/2408.15998v1","updated":"2024-08-28T17:59:31Z","published":"2024-08-28T17:59:31Z","title":"Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders","summary":"  The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle\n","authors":["Min Shi","Fuxiao Liu","Shihao Wang","Shijia Liao","Subhashree Radhakrishnan","De-An Huang","Hongxu Yin","Karan Sapra","Yaser Yacoob","Humphrey Shi","Bryan Catanzaro","Andrew Tao","Jan Kautz","Zhiding Yu","Guilin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15998v1.pdf","comment":"Github: https://github.com/NVlabs/Eagle, HuggingFace:\n  https://huggingface.co/NVEagle"},{"id":"http://arxiv.org/abs/2408.15997v1","updated":"2024-08-28T17:59:27Z","published":"2024-08-28T17:59:27Z","title":"Mamba or Transformer for Time Series Forecasting? Mixture of Universals\n  (MoU) Is All You Need","summary":"  Time series forecasting requires balancing short-term and long-term\ndependencies for accurate predictions. Existing methods mainly focus on\nlong-term dependency modeling, neglecting the complexities of short-term\ndynamics, which may hinder performance. Transformers are superior in modeling\nlong-term dependencies but are criticized for their quadratic computational\ncost. Mamba provides a near-linear alternative but is reported less effective\nin time series longterm forecasting due to potential information loss. Current\narchitectures fall short in offering both high efficiency and strong\nperformance for long-term dependency modeling. To address these challenges, we\nintroduce Mixture of Universals (MoU), a versatile model to capture both\nshort-term and long-term dependencies for enhancing performance in time series\nforecasting. MoU is composed of two novel designs: Mixture of Feature\nExtractors (MoF), an adaptive method designed to improve time series patch\nrepresentations for short-term dependency, and Mixture of Architectures (MoA),\nwhich hierarchically integrates Mamba, FeedForward, Convolution, and\nSelf-Attention architectures in a specialized order to model long-term\ndependency from a hybrid perspective. The proposed approach achieves\nstate-of-the-art performance while maintaining relatively low computational\ncosts. Extensive experiments on seven real-world datasets demonstrate the\nsuperiority of MoU. Code is available at https://github.com/lunaaa95/mou/.\n","authors":["Sijia Peng","Yun Xiong","Yangyong Zhu","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2408.15997v1.pdf","comment":"Code at https://github.com/lunaaa95/mou/"},{"id":"http://arxiv.org/abs/2408.15993v1","updated":"2024-08-28T17:58:53Z","published":"2024-08-28T17:58:53Z","title":"ClimDetect: A Benchmark Dataset for Climate Change Detection and\n  Attribution","summary":"  Detecting and attributing temperature increases due to climate change is\ncrucial for understanding global warming and guiding adaptation strategies. The\ncomplexity of distinguishing human-induced climate signals from natural\nvariability has challenged traditional detection and attribution (D&A)\napproaches, which seek to identify specific \"fingerprints\" in climate response\nvariables. Deep learning offers potential for discerning these complex patterns\nin expansive spatial datasets. However, lack of standard protocols has hindered\nconsistent comparisons across studies. We introduce ClimDetect, a standardized\ndataset of over 816k daily climate snapshots, designed to enhance model\naccuracy in identifying climate change signals. ClimDetect integrates various\ninput and target variables used in past research, ensuring comparability and\nconsistency. We also explore the application of vision transformers (ViT) to\nclimate data, a novel and modernizing approach in this context. Our open-access\ndata and code serve as a benchmark for advancing climate science through\nimproved model evaluations. ClimDetect is publicly accessible via Huggingface\ndataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.\n","authors":["Sungduk Yu","Brian L. White","Anahita Bhiwandiwalla","Musashi Hinck","Matthew Lyle Olson","Tung Nguyen","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2408.15993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15992v1","updated":"2024-08-28T17:58:39Z","published":"2024-08-28T17:58:39Z","title":"CoGen: Learning from Feedback with Coupled Comprehension and Generation","summary":"  Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.\n","authors":["Mustafa Omer Gul","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.15992v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.17701v5","updated":"2024-08-28T17:47:35Z","published":"2024-04-26T20:59:23Z","title":"Embedded FPGA Developments in 130nm and 28nm CMOS for Machine Learning\n  in Particle Detector Readout","summary":"  Embedded field programmable gate array (eFPGA) technology allows the\nimplementation of reconfigurable logic within the design of an\napplication-specific integrated circuit (ASIC). This approach offers the low\npower and efficiency of an ASIC along with the ease of FPGA configuration,\nparticularly beneficial for the use case of machine learning in the data\npipeline of next-generation collider experiments. An open-source framework\ncalled \"FABulous\" was used to design eFPGAs using 130 nm and 28 nm CMOS\ntechnology nodes, which were subsequently fabricated and verified through\ntesting. The capability of an eFPGA to act as a front-end readout chip was\nassessed using simulation of high energy particles passing through a silicon\npixel sensor. A machine learning-based classifier, designed for reduction of\nsensor data at the source, was synthesized and configured onto the eFPGA. A\nsuccessful proof-of-concept was demonstrated through reproduction of the\nexpected algorithm result on the eFPGA with perfect accuracy. Further\ndevelopment of the eFPGA technology and its application to collider detector\nreadout is discussed.\n","authors":["Julia Gonski","Aseem Gupta","Haoyi Jia","Hyunjoon Kim","Lorenzo Rota","Larry Ruckman","Angelo Dragone","Ryan Herbst"],"pdf_url":"https://arxiv.org/pdf/2404.17701v5.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2408.15969v1","updated":"2024-08-28T17:43:18Z","published":"2024-08-28T17:43:18Z","title":"Stability of Primal-Dual Gradient Flow Dynamics for Multi-Block Convex\n  Optimization Problems","summary":"  We examine stability properties of primal-dual gradient flow dynamics for\ncomposite convex optimization problems with multiple, possibly nonsmooth, terms\nin the objective function under the generalized consensus constraint. The\nproposed dynamics are based on the proximal augmented Lagrangian and they\nprovide a viable alternative to ADMM which faces significant challenges from\nboth analysis and implementation viewpoints in large-scale multi-block\nscenarios. In contrast to customized algorithms with individualized convergence\nguarantees, we provide a systematic approach for solving a broad class of\nchallenging composite optimization problems. We leverage various structural\nproperties to establish global (exponential) convergence guarantees for the\nproposed dynamics. Our assumptions are much weaker than those required to prove\n(exponential) stability of various primal-dual dynamics as well as (linear)\nconvergence of discrete-time methods, e.g., standard two-block and multi-block\nADMM and EXTRA algorithms. Finally, we show necessity of some of our structural\nassumptions for exponential stability and provide computational experiments to\ndemonstrate the convenience of the proposed dynamics for parallel and\ndistributed computing applications.\n","authors":["Ibrahim K. Ozaslan","Panagiotis Patrinos","Mihailo R. JovanoviÄ"],"pdf_url":"https://arxiv.org/pdf/2408.15969v1.pdf","comment":"31 pages; 4 figures"},{"id":"http://arxiv.org/abs/2406.10260v2","updated":"2024-08-28T17:26:03Z","published":"2024-06-11T01:16:10Z","title":"Flextron: Many-in-One Flexible Large Language Model","summary":"  Training modern LLMs is extremely resource intensive, and customizing them\nfor various deployment scenarios characterized by limited compute and memory\nresources through repeated training is impractical. In this paper, we introduce\nFlextron, a network architecture and post-training model optimization framework\nsupporting flexible model deployment. The Flextron architecture utilizes a\nnested elastic structure to rapidly adapt to specific user-defined latency and\naccuracy targets during inference with no additional fine-tuning required. It\nis also input-adaptive, and can automatically route tokens through its\nsub-networks for improved performance and efficiency. We present a\nsample-efficient training method and associated routing algorithms for\nsystematically transforming an existing trained LLM into a Flextron model. We\nevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate\nsuperior performance over multiple end-to-end trained variants and other\nstate-of-the-art elastic networks, all with a single pretraining run that\nconsumes a mere 7.63% tokens compared to original pretraining.\n","authors":["Ruisi Cai","Saurav Muralidharan","Greg Heinrich","Hongxu Yin","Zhangyang Wang","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2406.10260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15958v1","updated":"2024-08-28T17:20:56Z","published":"2024-08-28T17:20:56Z","title":"Efficient Slice Anomaly Detection Network for 3D Brain MRI Volume","summary":"  Current anomaly detection methods excel with benchmark industrial data but\nstruggle with natural images and medical data due to varying definitions of\n'normal' and 'abnormal.' This makes accurate identification of deviations in\nthese fields particularly challenging. Especially for 3D brain MRI data, all\nthe state-of-the-art models are reconstruction-based with 3D convolutional\nneural networks which are memory-intensive, time-consuming and producing noisy\noutputs that require further post-processing. We propose a framework called\nSimple Slice-based Network (SimpleSliceNet), which utilizes a model pre-trained\non ImageNet and fine-tuned on a separate MRI dataset as a 2D slice feature\nextractor to reduce computational cost. We aggregate the extracted features to\nperform anomaly detection tasks on 3D brain MRI volumes. Our model integrates a\nconditional normalizing flow to calculate log likelihood of features and\nemploys the Semi-Push-Pull Mechanism to enhance anomaly detection accuracy. The\nresults indicate improved performance, showcasing our model's remarkable\nadaptability and effectiveness when addressing the challenges exists in brain\nMRI data. In addition, for the large-scale 3D brain volumes, our model\nSimpleSliceNet outperforms the state-of-the-art 2D and 3D models in terms of\naccuracy, memory usage and time consumption. Code is available at:\nhttps://anonymous.4open.science/r/SimpleSliceNet-8EA3.\n","authors":["Zeduo Zhang","Yalda Mohsenzadeh"],"pdf_url":"https://arxiv.org/pdf/2408.15958v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.15956v1","updated":"2024-08-28T17:17:20Z","published":"2024-08-28T17:17:20Z","title":"Generating Binary Species Range Maps","summary":"  Accurately predicting the geographic ranges of species is crucial for\nassisting conservation efforts. Traditionally, range maps were manually created\nby experts. However, species distribution models (SDMs) and, more recently,\ndeep learning-based variants offer a potential automated alternative. Deep\nlearning-based SDMs generate a continuous probability representing the\npredicted presence of a species at a given location, which must be binarized by\nsetting per-species thresholds to obtain binary range maps. However, selecting\nappropriate per-species thresholds to binarize these predictions is non-trivial\nas different species can require distinct thresholds. In this work, we evaluate\ndifferent approaches for automatically identifying the best thresholds for\nbinarizing range maps using presence-only data. This includes approaches that\nrequire the generation of additional pseudo-absence data, along with ones that\nonly require presence data. We also propose an extension of an existing\npresence-only technique that is more robust to outliers. We perform a detailed\nevaluation of different thresholding techniques on the tasks of binary range\nestimation and large-scale fine-grained visual classification, and we\ndemonstrate improved performance over existing pseudo-absence free approaches\nusing our method.\n","authors":["Filip Dorm","Christian Lange","Scott Loarie","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2408.15956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15953v1","updated":"2024-08-28T17:12:01Z","published":"2024-08-28T17:12:01Z","title":"Modeling and Analyzing the Influence of Non-Item Pages on Sequential\n  Next-Item Prediction","summary":"  Analyzing the sequence of historical interactions between users and items,\nsequential recommendation models learn user intent and make predictions about\nthe next item of interest. Next to these item interactions, most systems also\nhave interactions with pages not related to specific items, for example\nnavigation pages, account pages, and pages for a specific category, which may\nprovide additional insights into the user's interests. However, while there are\nseveral approaches to integrate additional information about items and users,\nthe topic of integrating non-item pages has been less explored. We use the\nhypotheses testing framework HypTrails to show that there is indeed a\nrelationship between these non-item pages and the items of interest and fill\nthis gap by proposing various approaches of representing non-item pages (e.g,\nbased on their content) to use them as an additional information source for the\ntask of sequential next-item prediction.\n  We create a synthetic dataset with non-item pages highly related to the\nsubsequent item to show that the models are generally capable of learning from\nthese interactions, and subsequently evaluate the improvements gained by\nincluding non-item pages in two real-world datasets.\n  We adapt eight popular sequential recommender models, covering CNN-, RNN- and\ntransformer-based architectures, to integrate non-item pages and investigate\nthe capabilities of these models to leverage their information for next item\nprediction. We also analyze their behavior on noisy data and compare different\nitem representation strategies.\n  Our results show that non-item pages are a valuable source of information,\nbut representing such a page well is the key to successfully leverage them. The\ninclusion of non-item pages can increase the performance for next-item\nprediction in all examined model architectures with a varying degree.\n","authors":["Elisabeth Fischer","Daniel SchlÃ¶r","Albin Zehe","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2408.15953v1.pdf","comment":"36 pages, 19 figures; Work in Progress"},{"id":"http://arxiv.org/abs/2408.15946v1","updated":"2024-08-28T17:04:56Z","published":"2024-08-28T17:04:56Z","title":"Sigma Flows for Image and Data Labeling and Learning Structured\n  Prediction","summary":"  This paper introduces the sigma flow model for the prediction of structured\nlabelings of data observed on Riemannian manifolds, including Euclidean image\ndomains as special case. The approach combines the Laplace-Beltrami framework\nfor image denoising and enhancement, introduced by Sochen, Kimmel and Malladi\nabout 25 years ago, and the assignment flow approach introduced and studied by\nthe authors.\n  The sigma flow arises as Riemannian gradient flow of generalized harmonic\nenergies and thus is governed by a nonlinear geometric PDE which determines a\nharmonic map from a closed Riemannian domain manifold to a statistical\nmanifold, equipped with the Fisher-Rao metric from information geometry. A\nspecific ingredient of the sigma flow is the mutual dependency of the\nRiemannian metric of the domain manifold on the evolving state. This makes the\napproach amenable to machine learning in a specific way, by realizing this\ndependency through a mapping with compact time-variant parametrization that can\nbe learned from data. Proof of concept experiments demonstrate the expressivity\nof the sigma flow model and prediction performance.\n  Structural similarities to transformer network architectures and networks\ngenerated by the geometric integration of sigma flows are pointed out, which\nhighlights the connection to deep learning and, conversely, may stimulate the\nuse of geometric design principles for structured prediction in other areas of\nscientific machine learning.\n","authors":["Jonas Cassel","Bastian Boll","Stefania Petra","Peter Albers","Christoph SchnÃ¶rr"],"pdf_url":"https://arxiv.org/pdf/2408.15946v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2402.09786v4","updated":"2024-08-28T16:48:06Z","published":"2024-02-15T08:34:21Z","title":"Examining Pathological Bias in a Generative Adversarial Network\n  Discriminator: A Case Study on a StyleGAN3 Model","summary":"  Generative adversarial networks (GANs) generate photorealistic faces that are\noften indistinguishable by humans from real faces. While biases in machine\nlearning models are often assumed to be due to biases in training data, we find\npathological internal color and luminance biases in the discriminator of a\npre-trained StyleGAN3-r model that are not explicable by the training data. We\nalso find that the discriminator systematically stratifies scores by both\nimage- and face-level qualities and that this disproportionately affects images\nacross gender, race, and other categories. We examine axes common in research\non stereotyping in social psychology.\n","authors":["Alvin Grissom II","Ryan F. Lei","Matt Gusdorff","Jeova Farias Sales Rocha Neto","Bailey Lin","Ryan Trotter"],"pdf_url":"https://arxiv.org/pdf/2402.09786v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15923v1","updated":"2024-08-28T16:36:18Z","published":"2024-08-28T16:36:18Z","title":"Generalized Naive Bayes","summary":"  In this paper we introduce the so-called Generalized Naive Bayes structure as\nan extension of the Naive Bayes structure. We give a new greedy algorithm that\nfinds a good fitting Generalized Naive Bayes (GNB) probability distribution. We\nprove that this fits the data at least as well as the probability distribution\ndetermined by the classical Naive Bayes (NB). Then, under a not very\nrestrictive condition, we give a second algorithm for which we can prove that\nit finds the optimal GNB probability distribution, i.e. best fitting structure\nin the sense of KL divergence. Both algorithms are constructed to maximize the\ninformation content and aim to minimize redundancy. Based on these algorithms,\nnew methods for feature selection are introduced. We discuss the similarities\nand differences to other related algorithms in terms of structure, methodology,\nand complexity. Experimental results show, that the algorithms introduced\noutperform the related algorithms in many cases.\n","authors":["Edith Alice KovÃ¡cs","Anna OrszÃ¡g","DÃ¡niel Pfeifer","AndrÃ¡s BenczÃºr"],"pdf_url":"https://arxiv.org/pdf/2408.15923v1.pdf","comment":"44 pages, 19 figures"},{"id":"http://arxiv.org/abs/2408.15916v1","updated":"2024-08-28T16:30:41Z","published":"2024-08-28T16:30:41Z","title":"Multi-modal Adversarial Training for Zero-Shot Voice Cloning","summary":"  A text-to-speech (TTS) model trained to reconstruct speech given text tends\ntowards predictions that are close to the average characteristics of a dataset,\nfailing to model the variations that make human speech sound natural. This\nproblem is magnified for zero-shot voice cloning, a task that requires training\ndata with high variance in speaking styles. We build off of recent works which\nhave used Generative Advsarial Networks (GAN) by proposing a Transformer\nencoder-decoder architecture to conditionally discriminates between real and\ngenerated speech features. The discriminator is used in a training pipeline\nthat improves both the acoustic and prosodic features of a TTS model. We\nintroduce our novel adversarial training technique by applying it to a\nFastSpeech2 acoustic model and training on Libriheavy, a large multi-speaker\ndataset, for the task of zero-shot voice cloning. Our model achieves\nimprovements over the baseline in terms of speech quality and speaker\nsimilarity. Audio examples from our system are available online.\n","authors":["John Janiczek","Dading Chong","Dongyang Dai","Arlo Faria","Chao Wang","Tao Wang","Yuzong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15916v1.pdf","comment":"Accepted at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2408.15905v1","updated":"2024-08-28T16:19:35Z","published":"2024-08-28T16:19:35Z","title":"MetaGFN: Exploring Distant Modes with Adapted Metadynamics for\n  Continuous GFlowNets","summary":"  Generative Flow Networks (GFlowNets) are a class of generative models that\nsample objects in proportion to a specified reward function through a learned\npolicy. They can be trained either on-policy or off-policy, needing a balance\nbetween exploration and exploitation for fast convergence to a target\ndistribution. While exploration strategies for discrete GFlowNets have been\nstudied, exploration in the continuous case remains to be investigated, despite\nthe potential for novel exploration algorithms due to the local connectedness\nof continuous domains. Here, we introduce Adapted Metadynamics, a variant of\nmetadynamics that can be applied to arbitrary black-box reward functions on\ncontinuous domains. We use Adapted Metadynamics as an exploration strategy for\ncontinuous GFlowNets. We show three continuous domains where the resulting\nalgorithm, MetaGFN, accelerates convergence to the target distribution and\ndiscovers more distant reward modes than previous off-policy exploration\nstrategies used for GFlowNets.\n","authors":["Dominic Phillips","Flaviu Cipcigan"],"pdf_url":"https://arxiv.org/pdf/2408.15905v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2408.15901v1","updated":"2024-08-28T16:12:55Z","published":"2024-08-28T16:12:55Z","title":"Nexus: Specialization meets Adaptability for Efficiently Training\n  Mixture of Experts","summary":"  Efficiency, specialization, and adaptability to new data distributions are\nqualities that are hard to combine in current Large Language Models. The\nMixture of Experts (MoE) architecture has been the focus of significant\nresearch because its inherent conditional computation enables such desirable\nproperties. In this work, we focus on \"upcycling\" dense expert models into an\nMoE, aiming to improve specialization while also adding the ability to adapt to\nnew tasks easily. We introduce Nexus, an enhanced MoE architecture with\nadaptive routing where the model learns to project expert embeddings from\ndomain representations. This approach allows Nexus to flexibly add new experts\nafter the initial upcycling through separately trained dense models, without\nrequiring large-scale MoE training for unseen data domains. Our experiments\nshow that Nexus achieves a relative gain of up to 2.1% over the baseline for\ninitial upcycling, and a 18.8% relative gain for extending the MoE with a new\nexpert by using limited finetuning data. This flexibility of Nexus is crucial\nto enable an open-source ecosystem where every user continuously assembles\ntheir own MoE-mix according to their needs.\n","authors":["Nikolas Gritsch","Qizhen Zhang","Acyr Locatelli","Sara Hooker","Ahmet ÃstÃ¼n"],"pdf_url":"https://arxiv.org/pdf/2408.15901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15898v1","updated":"2024-08-28T16:12:16Z","published":"2024-08-28T16:12:16Z","title":"Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil\n  Generation","summary":"  The design of aerodynamic shapes, such as airfoils, has traditionally\nrequired significant computational resources and relied on predefined design\nparameters, which limit the potential for novel shape synthesis. In this work,\nwe introduce a data-driven methodology for airfoil generation using a diffusion\nmodel. Trained on a dataset of preexisting airfoils, our model can generate an\narbitrary number of new airfoils from random vectors, which can be conditioned\non specific aerodynamic performance metrics such as lift and drag, or geometric\ncriteria. Our results demonstrate that the diffusion model effectively produces\nairfoil shapes with realistic aerodynamic properties, offering substantial\nimprovements in efficiency, flexibility, and the potential for discovering\ninnovative airfoil designs. This approach significantly expands the design\nspace, facilitating the synthesis of high-performance aerodynamic shapes that\ntranscend the limitations of traditional methods.\n","authors":["Reid Graves","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2408.15898v1.pdf","comment":"12 Pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.15896v1","updated":"2024-08-28T16:06:12Z","published":"2024-08-28T16:06:12Z","title":"A New Method for Cross-Lingual-based Semantic Role Labeling","summary":"  Semantic role labeling is a crucial task in natural language processing,\nenabling better comprehension of natural language. However, the lack of\nannotated data in multiple languages has posed a challenge for researchers. To\naddress this, a deep learning algorithm based on model transfer has been\nproposed. The algorithm utilizes a dataset consisting of the English portion of\nCoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiency\nof training, only ten percent of the educational data from each language is\nused. The results of the proposed model demonstrate significant improvements\ncompared to Niksirt et al.'s model. In monolingual mode, the proposed model\nachieved a 2.05 percent improvement on F1-score, while in cross-lingual mode,\nthe improvement was even more substantial, reaching 6.23 percent. Worth noting\nis that the compared model only trained two of the four stages of semantic role\nlabeling and employed golden data for the remaining two stages. This suggests\nthat the actual superiority of the proposed model surpasses the reported\nnumbers by a significant margin. The development of cross-lingual methods for\nsemantic role labeling holds promise, particularly in addressing the scarcity\nof annotated data for various languages. These advancements pave the way for\nfurther research in understanding and processing natural language across\ndifferent linguistic contexts.\n","authors":["Mohammad Ebrahimi","Behrouz Minaei Bidgoli","Nasim Khozouei"],"pdf_url":"https://arxiv.org/pdf/2408.15896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15895v1","updated":"2024-08-28T16:05:20Z","published":"2024-08-28T16:05:20Z","title":"Bias in LLMs as Annotators: The Effect of Party Cues on Labelling\n  Decision by Large Language Models","summary":"  Human coders are biased. We test similar biases in Large Language Models\n(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and\nMeyer (2018), we find evidence that LLMs use political information, and\nspecifically party cues, to judge political statements. Not only do LLMs use\nrelevant information to contextualize whether a statement is positive,\nnegative, or neutral based on the party cue, they also reflect the biases of\nthe human-generated data upon which they have been trained. We also find that\nunlike humans, who are only biased when faced with statements from extreme\nparties, LLMs exhibit significant bias even when prompted with statements from\ncenter-left and center-right parties. The implications of our findings are\ndiscussed in the conclusion.\n","authors":["Sebastian Vallejo Vera","Hunter Driggers"],"pdf_url":"https://arxiv.org/pdf/2408.15895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15894v1","updated":"2024-08-28T16:04:40Z","published":"2024-08-28T16:04:40Z","title":"The Role of Fibration Symmetries in Geometric Deep Learning","summary":"  Geometric Deep Learning (GDL) unifies a broad class of machine learning\ntechniques from the perspectives of symmetries, offering a framework for\nintroducing problem-specific inductive biases like Graph Neural Networks\n(GNNs). However, the current formulation of GDL is limited to global symmetries\nthat are not often found in real-world problems. We propose to relax GDL to\nallow for local symmetries, specifically fibration symmetries in graphs, to\nleverage regularities of realistic instances. We show that GNNs apply the\ninductive bias of fibration symmetries and derive a tighter upper bound for\ntheir expressive power. Additionally, by identifying symmetries in networks, we\ncollapse network nodes, thereby increasing their computational efficiency\nduring both inference and training of deep neural networks. The mathematical\nextension introduced here applies beyond graphs to manifolds, bundles, and\ngrids for the development of models with inductive biases induced by local\nsymmetries that can lead to better generalization.\n","authors":["Osvaldo Velarde","Lucas Parra","Paolo Boldi","Hernan Makse"],"pdf_url":"https://arxiv.org/pdf/2408.15894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14780v2","updated":"2024-08-28T15:48:31Z","published":"2024-08-27T04:57:53Z","title":"GINN-KAN: Interpretability pipelining with applications in Physics\n  Informed Neural Networks","summary":"  Neural networks are powerful function approximators, yet their ``black-box\"\nnature often renders them opaque and difficult to interpret. While many\npost-hoc explanation methods exist, they typically fail to capture the\nunderlying reasoning processes of the networks. A truly interpretable neural\nnetwork would be trained similarly to conventional models using techniques such\nas backpropagation, but additionally provide insights into the learned\ninput-output relationships. In this work, we introduce the concept of\ninterpretability pipelineing, to incorporate multiple interpretability\ntechniques to outperform each individual technique. To this end, we first\nevaluate several architectures that promise such interpretability, with a\nparticular focus on two recent models selected for their potential to\nincorporate interpretability into standard neural network architectures while\nstill leveraging backpropagation: the Growing Interpretable Neural Network\n(GINN) and Kolmogorov Arnold Networks (KAN). We analyze the limitations and\nstrengths of each and introduce a novel interpretable neural network GINN-KAN\nthat synthesizes the advantages of both models. When tested on the Feynman\nsymbolic regression benchmark datasets, GINN-KAN outperforms both GINN and KAN.\nTo highlight the capabilities and the generalizability of this approach, we\nposition GINN-KAN as an alternative to conventional black-box networks in\nPhysics-Informed Neural Networks (PINNs). We expect this to have far-reaching\nimplications in the application of deep learning pipelines in the natural\nsciences. Our experiments with this interpretable PINN on 15 different partial\ndifferential equations demonstrate that GINN-KAN augmented PINNs outperform\nPINNs with black-box networks in solving differential equations and surpass the\ncapabilities of both GINN and KAN.\n","authors":["Nisal Ranasinghe","Yu Xia","Sachith Seneviratne","Saman Halgamuge"],"pdf_url":"https://arxiv.org/pdf/2408.14780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04600v2","updated":"2024-08-28T15:46:39Z","published":"2023-11-08T11:02:51Z","title":"A Deep Learning Based Resource Allocator for Communication Systems with\n  Dynamic User Utility Demands","summary":"  Deep learning (DL) based resource allocation (RA) has recently gained\nsignificant attention due to its performance efficiency. However, most related\nstudies assume an ideal case where the number of users and their utility\ndemands, e.g., data rate constraints, are fixed, and the designed DL-based RA\nscheme exploits a policy trained only for these fixed parameters. Consequently,\ncomputationally complex policy retraining is required whenever these parameters\nchange. In this paper, we introduce a DL-based resource allocator (ALCOR) that\nallows users to adjust their utility demands freely, such as based on their\napplication layer requirements. ALCOR employs deep neural networks (DNNs) as\nthe policy in a time-sharing problem. The underlying optimization algorithm\niteratively optimizes the on-off status of users to satisfy their utility\ndemands in expectation. The policy performs unconstrained RA (URA)--RA without\nconsidering user utility demands--among active users to maximize the sum\nutility (SU) at each time instant. Depending on the chosen URA scheme, ALCOR\ncan perform RA in either a centralized or distributed scenario. Derived\nconvergence analyses provide guarantees for ALCOR's convergence, and numerical\nexperiments corroborate its effectiveness.\n","authors":["Pourya Behmandpoor","Mark Eisen","Panagiotis Patrinos","Marc Moonen"],"pdf_url":"https://arxiv.org/pdf/2311.04600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15874v1","updated":"2024-08-28T15:44:34Z","published":"2024-08-28T15:44:34Z","title":"Robust Statistical Scaling of Outlier Scores: Improving the Quality of\n  Outlier Probabilities for Outliers (Extended Version)","summary":"  Outlier detection algorithms typically assign an outlier score to each\nobservation in a dataset, indicating the degree to which an observation is an\noutlier. However, these scores are often not comparable across algorithms and\ncan be difficult for humans to interpret. Statistical scaling addresses this\nproblem by transforming outlier scores into outlier probabilities without using\nground-truth labels, thereby improving interpretability and comparability\nacross algorithms. However, the quality of this transformation can be different\nfor outliers and inliers. Missing outliers in scenarios where they are of\nparticular interest - such as healthcare, finance, or engineering - can be\ncostly or dangerous. Thus, ensuring good probabilities for outliers is\nessential. This paper argues that statistical scaling, as commonly used in the\nliterature, does not produce equally good probabilities for outliers as for\ninliers. Therefore, we propose robust statistical scaling, which uses robust\nestimators to improve the probabilities for outliers. We evaluate several\nvariants of our method against other outlier score transformations for\nreal-world datasets and outlier detection algorithms, where it can improve the\nprobabilities for outliers.\n","authors":["Philipp RÃ¶chner","Henrique O. Marques","Ricardo J. G. B. Campello","Arthur Zimek","Franz Rothlauf"],"pdf_url":"https://arxiv.org/pdf/2408.15874v1.pdf","comment":"15 pages, 4 figures, accepted for publication in SISAP 2024"},{"id":"http://arxiv.org/abs/2403.05645v3","updated":"2024-08-28T15:39:45Z","published":"2024-03-08T19:36:20Z","title":"Geometric Neural Network based on Phase Space for BCI-EEG decoding","summary":"  Objective: The integration of Deep Learning (DL) algorithms on brain signal\nanalysis is still in its nascent stages compared to their success in fields\nlike Computer Vision. This is particularly true for BCI, where the brain\nactivity is decoded to control external devices without requiring muscle\ncontrol. Electroencephalography (EEG) is a widely adopted choice for designing\nBCI systems due to its non-invasive and cost-effective nature and excellent\ntemporal resolution. Still, it comes at the expense of limited training data,\npoor signal-to-noise, and a large variability across and within-subject\nrecordings. Finally, setting up a BCI system with many electrodes takes a long\ntime, hindering the widespread adoption of reliable DL architectures in BCIs\noutside research laboratories. To improve adoption, we need to improve user\ncomfort using, for instance, reliable algorithms that operate with few\nelectrodes. Approach: Our research aims to develop a DL algorithm that delivers\neffective results with a limited number of electrodes. Taking advantage of the\nAugmented Covariance Method and the framework of SPDNet, we propose the\nPhase-SPDNet architecture and analyze its performance and the interpretability\nof the results. The evaluation is conducted on 5-fold cross-validation, using\nonly three electrodes positioned above the Motor Cortex. The methodology was\ntested on nearly 100 subjects from several open-source datasets using the\nMother Of All BCI Benchmark (MOABB) framework. Main results: The results of our\nPhase-SPDNet demonstrate that the augmented approach combined with the SPDNet\nsignificantly outperforms all the current state-of-the-art DL architecture in\nMI decoding. Significance: This new architecture is explainable and with a low\nnumber of trainable parameters.\n","authors":["Igor Carrara","Bruno Aristimunha","Marie-Constance Corsi","Raphael Y. de Camargo","Sylvain Chevallier","ThÃ©odore Papadopoulo"],"pdf_url":"https://arxiv.org/pdf/2403.05645v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10734v2","updated":"2024-08-28T15:36:08Z","published":"2024-07-15T14:01:34Z","title":"On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M\n  Microcontrollers","summary":"  On-device training of DNNs allows models to adapt and fine-tune to newly\ncollected data or changing domains while deployed on microcontroller units\n(MCUs). However, DNN training is a resource-intensive task, making the\nimplementation and execution of DNN training algorithms on MCUs challenging due\nto low processor speeds, constrained throughput, limited floating-point\nsupport, and memory constraints. In this work, we explore on-device training of\nDNNs for Cortex-M MCUs. We present a method that enables efficient training of\nDNNs completely in place on the MCU using fully quantized training (FQT) and\ndynamic partial gradient updates. We demonstrate the feasibility of our\napproach on multiple vision and time-series datasets and provide insights into\nthe tradeoff between training accuracy, memory overhead, energy, and latency on\nreal hardware.\n","authors":["Mark Deutel","Frank Hannig","Christopher Mutschler","JÃ¼rgen Teich"],"pdf_url":"https://arxiv.org/pdf/2407.10734v2.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.15866v1","updated":"2024-08-28T15:33:47Z","published":"2024-08-28T15:33:47Z","title":"Retrieval-Augmented Instruction Tuning for Automated Process Engineering\n  Calculations : A Tool-Chaining Problem-Solving Framework with Attributable\n  Reflection","summary":"  The current technology landscape lacks a foundational AI model for solving\nprocess engineering calculations. In this work, we introduce a novel autonomous\nagent framework leveraging Retrieval-Augmented Instruction-Tuning (RAIT) to\nenhance open, customizable small code language models (SLMs) for these\ncalculations. By combining instruction tuned code SLMs with Retrieval-Augmented\nCode Generation (RACG) using external tools, the agent generates, debugs, and\noptimizes code from natural language specifications. Our approach addresses the\nlimitations of the current lack of a foundational AI model for specialized\nprocess engineering tasks and offers benefits of explainability, knowledge\nediting, and cost-effectiveness. Additionally, we curate custom datasets of\nchemical and process engineering problems and solutions to overcome data\nscarcity. Experimental results show that our framework matches the performance\nof large-scale proprietary models on benchmark datasets, proving its\neffectiveness and usability.\n","authors":["Sagar Srinivas Sakhinana","Geethan Sannidhi","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2408.15866v1.pdf","comment":"Accepted for publication at ML4CCE workshop at ECML PKDD 2024. Please\n  find the link: https://ml4cce-ecml.com/#agenda"},{"id":"http://arxiv.org/abs/2408.15865v1","updated":"2024-08-28T15:29:27Z","published":"2024-08-28T15:29:27Z","title":"microYOLO: Towards Single-Shot Object Detection on Microcontrollers","summary":"  This work-in-progress paper presents results on the feasibility of\nsingle-shot object detection on microcontrollers using YOLO. Single-shot object\ndetectors like YOLO are widely used, however due to their complexity mainly on\nlarger GPU-based platforms. We present microYOLO, which can be used on Cortex-M\nbased microcontrollers, such as the OpenMV H7 R2, achieving about 3.5 FPS when\nclassifying 128x128 RGB images while using less than 800 KB Flash and less than\n350 KB RAM. Furthermore, we share experimental results for three different\nobject detection tasks, analyzing the accuracy of microYOLO on them.\n","authors":["Mark Deutel","Christopher Mutschler","JÃ¼rgen Teich"],"pdf_url":"https://arxiv.org/pdf/2408.15865v1.pdf","comment":"Published at the ECML PKDD Conference 2023, at the 4th Workshop on\n  IoT, Edge, and Mobile for Embedded Machine Learning"},{"id":"http://arxiv.org/abs/2310.10835v3","updated":"2024-08-28T15:29:17Z","published":"2023-10-16T21:17:29Z","title":"Provable Probabilistic Imaging using Score-Based Generative Priors","summary":"  Estimating high-quality images while also quantifying their uncertainty are\ntwo desired features in an image reconstruction algorithm for solving ill-posed\ninverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as\na principled framework for characterizing the space of possible solutions to a\ngeneral inverse problem. PMC is able to incorporate expressive score-based\ngenerative priors for high-quality image reconstruction while also performing\nuncertainty quantification via posterior sampling. In particular, we develop\ntwo PMC algorithms that can be viewed as the sampling analogues of the\ntraditional plug-and-play priors (PnP) and regularization by denoising (RED)\nalgorithms. To improve the sampling efficiency, we introduce weighted annealing\ninto these PMC algorithms, further developing two additional annealed PMC\nalgorithms (APMC). We establish a theoretical analysis for characterizing the\nconvergence behavior of PMC algorithms. Our analysis provides non-asymptotic\nstationarity guarantees in terms of the Fisher information, fully compatible\nwith the joint presence of weighted annealing, potentially non-log-concave\nlikelihoods, and imperfect score networks. We demonstrate the performance of\nthe PMC algorithms on multiple representative inverse problems with both linear\nand nonlinear forward models. Experimental results show that PMC significantly\nimproves reconstruction quality and enables high-fidelity uncertainty\nquantification.\n","authors":["Yu Sun","Zihui Wu","Yifan Chen","Berthy T. Feng","Katherine L. Bouman"],"pdf_url":"https://arxiv.org/pdf/2310.10835v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15861v1","updated":"2024-08-28T15:21:10Z","published":"2024-08-28T15:21:10Z","title":"Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free\n  Backdoor Mitigation","summary":"  Backdoor attacks present a serious security threat to deep neuron networks\n(DNNs). Although numerous effective defense techniques have been proposed in\nrecent years, they inevitably rely on the availability of either clean or\npoisoned data. In contrast, data-free defense techniques have evolved slowly\nand still lag significantly in performance. To address this issue, different\nfrom the traditional approach of pruning followed by fine-tuning, we propose a\nnovel data-free defense method named Optimal Transport-based Backdoor Repairing\n(OTBR) in this work. This method, based on our findings on neuron weight\nchanges (NWCs) of random unlearning, uses optimal transport (OT)-based model\nfusion to combine the advantages of both pruned and backdoored models.\nSpecifically, we first demonstrate our findings that the NWCs of random\nunlearning are positively correlated with those of poison unlearning. Based on\nthis observation, we propose a random-unlearning NWC pruning technique to\neliminate the backdoor effect and obtain a backdoor-free pruned model. Then,\nmotivated by the OT-based model fusion, we propose the pruned-to-backdoored\nOT-based fusion technique, which fuses pruned and backdoored models to combine\nthe advantages of both, resulting in a model that demonstrates high clean\naccuracy and a low attack success rate. To our knowledge, this is the first\nwork to apply OT and model fusion techniques to backdoor defense. Extensive\nexperiments show that our method successfully defends against all seven\nbackdoor attacks across three benchmark datasets, outperforming both\nstate-of-the-art (SOTA) data-free and data-dependent methods. The code\nimplementation and Appendix are provided in the Supplementary Material.\n","authors":["Weilin Lin","Li Liu","Jianze Li","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2408.15861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16653v3","updated":"2024-08-28T15:17:44Z","published":"2022-11-30T00:47:03Z","title":"Correlation recurrent units: A novel neural architecture for improving\n  the predictive performance of time-series data","summary":"  The time-series forecasting (TSF) problem is a traditional problem in the\nfield of artificial intelligence. Models such as Recurrent Neural Network\n(RNN), Long Short Term Memory (LSTM), and GRU (Gate Recurrent Units) have\ncontributed to improving the predictive accuracy of TSF. Furthermore, model\nstructures have been proposed to combine time-series decomposition methods,\nsuch as seasonal-trend decomposition using Loess (STL) to ensure improved\npredictive accuracy. However, because this approach is learned in an\nindependent model for each component, it cannot learn the relationships between\ntime-series components. In this study, we propose a new neural architecture\ncalled a correlation recurrent unit (CRU) that can perform time series\ndecomposition within a neural cell and learn correlations (autocorrelation and\ncorrelation) between each decomposition component. The proposed neural\narchitecture was evaluated through comparative experiments with previous\nstudies using five univariate time-series datasets and four multivariate\ntime-series data. The results showed that long- and short-term predictive\nperformance was improved by more than 10%. The experimental results show that\nthe proposed CRU is an excellent method for TSF problems compared to other\nneural architectures.\n","authors":["Sunghyun Sim","Dohee Kim","Hyerim Bae"],"pdf_url":"https://arxiv.org/pdf/2211.16653v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15852v1","updated":"2024-08-28T15:14:58Z","published":"2024-08-28T15:14:58Z","title":"chemtrain: Learning Deep Potential Models via Automatic Differentiation\n  and Statistical Physics","summary":"  Neural Networks (NNs) are promising models for refining the accuracy of\nmolecular dynamics, potentially opening up new fields of application. Typically\ntrained bottom-up, atomistic NN potential models can reach first-principle\naccuracy, while coarse-grained implicit solvent NN potentials surpass classical\ncontinuum solvent models. However, overcoming the limitations of costly\ngeneration of accurate reference data and data inefficiency of common bottom-up\ntraining demands efficient incorporation of data from many sources. This paper\nintroduces the framework chemtrain to learn sophisticated NN potential models\nthrough customizable training routines and advanced training algorithms. These\nroutines can combine multiple top-down and bottom-up algorithms, e.g., to\nincorporate both experimental and simulation data or pre-train potentials with\nless costly algorithms. chemtrain provides an object-oriented high-level\ninterface to simplify the creation of custom routines. On the lower level,\nchemtrain relies on JAX to compute gradients and scale the computations to use\navailable resources. We demonstrate the simplicity and importance of combining\nmultiple algorithms in the examples of parametrizing an all-atomistic model of\ntitanium and a coarse-grained implicit solvent model of alanine dipeptide.\n","authors":["Paul Fuchs","Stephan Thaler","Sebastien RÃ¶cken","Julija Zavadlav"],"pdf_url":"https://arxiv.org/pdf/2408.15852v1.pdf","comment":"Package source code published at http://github.com/tummfm/chemtrain"},{"id":"http://arxiv.org/abs/2404.07839v2","updated":"2024-08-28T15:05:42Z","published":"2024-04-11T15:27:22Z","title":"RecurrentGemma: Moving Past Transformers for Efficient Open Language\n  Models","summary":"  We introduce RecurrentGemma, a family of open language models which uses\nGoogle's novel Griffin architecture. Griffin combines linear recurrences with\nlocal attention to achieve excellent performance on language. It has a\nfixed-sized state, which reduces memory use and enables efficient inference on\nlong sequences. We provide two sizes of models, containing 2B and 9B\nparameters, and provide pre-trained and instruction tuned variants for both.\nOur models achieve comparable performance to similarly-sized Gemma baselines\ndespite being trained on fewer tokens.\n","authors":["Aleksandar Botev","Soham De","Samuel L Smith","Anushan Fernando","George-Cristian Muraru","Ruba Haroun","Leonard Berrada","Razvan Pascanu","Pier Giuseppe Sessa","Robert Dadashi","LÃ©onard Hussenot","Johan Ferret","Sertan Girgin","Olivier Bachem","Alek Andreev","Kathleen Kenealy","Thomas Mesnard","Cassidy Hardin","Surya Bhupatiraju","Shreya Pathak","Laurent Sifre","Morgane RiviÃ¨re","Mihir Sanjay Kale","Juliette Love","Pouya Tafti","Armand Joulin","Noah Fiedel","Evan Senter","Yutian Chen","Srivatsan Srinivasan","Guillaume Desjardins","David Budden","Arnaud Doucet","Sharad Vikram","Adam Paszke","Trevor Gale","Sebastian Borgeaud","Charlie Chen","Andy Brock","Antonia Paterson","Jenny Brennan","Meg Risdal","Raj Gundluru","Nesh Devanathan","Paul Mooney","Nilay Chauhan","Phil Culliton","Luiz Gustavo Martins","Elisa Bandy","David Huntsperger","Glenn Cameron","Arthur Zucker","Tris Warkentin","Ludovic Peran","Minh Giang","Zoubin Ghahramani","ClÃ©ment Farabet","Koray Kavukcuoglu","Demis Hassabis","Raia Hadsell","Yee Whye Teh","Nando de Frietas"],"pdf_url":"https://arxiv.org/pdf/2404.07839v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01245v2","updated":"2024-08-28T15:01:04Z","published":"2024-04-01T17:03:41Z","title":"A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules","summary":"  Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.\n","authors":["Xiang Li","Feng Ruan","Huiyuan Wang","Qi Long","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2404.01245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15641v2","updated":"2024-08-28T15:00:32Z","published":"2023-10-24T08:59:40Z","title":"Guaranteed Coverage Prediction Intervals with Gaussian Process\n  Regression","summary":"  Gaussian Process Regression (GPR) is a popular regression method, which\nunlike most Machine Learning techniques, provides estimates of uncertainty for\nits predictions. These uncertainty estimates however, are based on the\nassumption that the model is well-specified, an assumption that is violated in\nmost practical applications, since the required knowledge is rarely available.\nAs a result, the produced uncertainty estimates can become very misleading; for\nexample the prediction intervals (PIs) produced for the 95% confidence level\nmay cover much less than 95% of the true labels. To address this issue, this\npaper introduces an extension of GPR based on a Machine Learning framework\ncalled, Conformal Prediction (CP). This extension guarantees the production of\nPIs with the required coverage even when the model is completely misspecified.\nThe proposed approach combines the advantages of GPR with the valid coverage\nguarantee of CP, while the performed experimental results demonstrate its\nsuperiority over existing methods.\n","authors":["Harris Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2310.15641v2.pdf","comment":"12 pages. This article has been accepted for publication in IEEE\n  Transactions on Pattern Analysis and Machine Intelligence. This is the\n  author's version which has not been fully edited and content may change prior\n  to final publication. Citation information: DOI 10.1109/TPAMI.2024.3418214"},{"id":"http://arxiv.org/abs/2307.08220v2","updated":"2024-08-28T14:47:24Z","published":"2023-07-17T03:45:00Z","title":"FRANC: A Lightweight Framework for High-Quality Code Generation","summary":"  In recent years, the use of automated source code generation utilizing\ntransformer-based generative models has expanded, and these models can generate\nfunctional code according to the requirements of the developers. However,\nrecent research revealed that these automatically generated source codes can\ncontain vulnerabilities and other quality issues. Despite researchers' and\npractitioners' attempts to enhance code generation models, retraining and\nfine-tuning large language models is time-consuming and resource-intensive.\nThus, we describe FRANC, a lightweight framework for recommending more secure\nand high-quality source code derived from transformer-based code generation\nmodels. FRANC includes a static filter to make the generated code compilable\nwith heuristics and a quality-aware ranker to sort the code snippets based on a\nquality score. Moreover, the framework uses prompt engineering to fix\npersistent quality issues. We evaluated the framework with five Python and Java\ncode generation models and six prompt datasets, including a newly created one\nin this work (SOEval). The static filter improves 9% to 46% Java suggestions\nand 10% to 43% Python suggestions regarding compilability. The average\nimprovement over the NDCG@10 score for the ranking system is 0.0763, and the\nrepairing techniques repair the highest 80% of prompts. FRANC takes, on\naverage, 1.98 seconds for Java; for Python, it takes 0.08 seconds.\n","authors":["Mohammed Latif Siddiq","Beatrice Casey","Joanna C. S. Santos"],"pdf_url":"https://arxiv.org/pdf/2307.08220v2.pdf","comment":"Accepted at the 24th IEEE International Conference on Source Code\n  Analysis and Manipulation (SCAM 2024)"},{"id":"http://arxiv.org/abs/2408.15827v1","updated":"2024-08-28T14:40:15Z","published":"2024-08-28T14:40:15Z","title":"Automatic Differential Diagnosis using Transformer-Based Multi-Label\n  Sequence Classification","summary":"  As the field of artificial intelligence progresses, assistive technologies\nare becoming more widely used across all industries. The healthcare industry is\nno different, with numerous studies being done to develop assistive tools for\nhealthcare professionals. Automatic diagnostic systems are one such beneficial\ntool that can assist with a variety of tasks, including collecting patient\ninformation, analyzing test results, and diagnosing patients. However, the idea\nof developing systems that can provide a differential diagnosis has been\nlargely overlooked in most of these research studies. In this study, we propose\na transformer-based approach for providing differential diagnoses based on a\npatient's age, sex, medical history, and symptoms. We use the DDXPlus dataset,\nwhich provides differential diagnosis information for patients based on 49\ndisease types. Firstly, we propose a method to process the tabular patient data\nfrom the dataset and engineer them into patient reports to make them suitable\nfor our research. In addition, we introduce two data modification modules to\ndiversify the training data and consequently improve the robustness of the\nmodels. We approach the task as a multi-label classification problem and\nconduct extensive experiments using four transformer models. All the models\ndisplayed promising results by achieving over 97% F1 score on the held-out test\nset. Moreover, we design additional behavioral tests to get a broader\nunderstanding of the models. In particular, for one of our test cases, we\nprepared a custom test set of 100 samples with the assistance of a doctor. The\nresults on the custom set showed that our proposed data modification modules\nimproved the model's generalization capabilities. We hope our findings will\nprovide future researchers with valuable insights and inspire them to develop\nreliable systems for automatic differential diagnosis.\n","authors":["Abu Adnan Sadi","Mohammad Ashrafuzzaman Khan","Lubaba Binte Saber"],"pdf_url":"https://arxiv.org/pdf/2408.15827v1.pdf","comment":"25 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.10155v2","updated":"2024-08-28T14:38:51Z","published":"2024-04-15T22:02:58Z","title":"The Fault in our Stars: Quality Assessment of Code Generation Benchmarks","summary":"  Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.\n","authors":["Mohammed Latif Siddiq","Simantika Dristi","Joy Saha","Joanna C. S. Santos"],"pdf_url":"https://arxiv.org/pdf/2404.10155v2.pdf","comment":"Accepted at the 24th IEEE International Conference on Source Code\n  Analysis and Manipulation(SCAM 2024)"},{"id":"http://arxiv.org/abs/2408.15819v1","updated":"2024-08-28T14:32:24Z","published":"2024-08-28T14:32:24Z","title":"Automated Mixture Analysis via Structural Evaluation","summary":"  The determination of chemical mixture components is vital to a multitude of\nscientific fields. Oftentimes spectroscopic methods are employed to decipher\nthe composition of these mixtures. However, the sheer density of spectral\nfeatures present in spectroscopic databases can make unambiguous assignment to\nindividual species challenging. Yet, components of a mixture are commonly\nchemically related due to environmental processes or shared precursor\nmolecules. Therefore, analysis of the chemical relevance of a molecule is\nimportant when determining which species are present in a mixture. In this\npaper, we combine machine-learning molecular embedding methods with a\ngraph-based ranking system to determine the likelihood of a molecule being\npresent in a mixture based on the other known species and/or chemical priors.\nBy incorporating this metric in a rotational spectroscopy mixture analysis\nalgorithm, we demonstrate that the mixture components can be identified with\nextremely high accuracy (>97%) in an efficient manner.\n","authors":["Zachary T. P. Fried","Brett A. McGuire"],"pdf_url":"https://arxiv.org/pdf/2408.15819v1.pdf","comment":"Accepted for publication in The Journal of Physical Chemistry A"},{"id":"http://arxiv.org/abs/2408.14511v2","updated":"2024-08-28T14:13:41Z","published":"2024-08-25T04:07:18Z","title":"Unveiling the Statistical Foundations of Chain-of-Thought Prompting\n  Methods","summary":"  Chain-of-Thought (CoT) prompting and its variants have gained popularity as\neffective methods for solving multi-step reasoning problems using pretrained\nlarge language models (LLMs). In this work, we analyze CoT prompting from a\nstatistical estimation perspective, providing a comprehensive characterization\nof its sample complexity. To this end, we introduce a multi-step latent\nvariable model that encapsulates the reasoning process, where the latent\nvariable encodes the task information. Under this framework, we demonstrate\nthat when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator\neffectively solves the multi-step reasoning problem by aggregating a posterior\ndistribution inferred from the demonstration examples in the prompt. Moreover,\nwe prove that the statistical error of the CoT estimator can be decomposed into\ntwo main components: (i) a prompting error, which arises from inferring the\ntrue task using CoT prompts, and (ii) the statistical error of the pretrained\nLLM. We establish that, under appropriate assumptions, the prompting error\ndecays exponentially to zero as the number of demonstrations increases.\nAdditionally, we explicitly characterize the approximation and generalization\nerrors of the pretrained LLM. Notably, we construct a transformer model that\napproximates the target distribution of the multi-step reasoning problem with\nan error that decreases exponentially in the number of transformer blocks. Our\nanalysis extends to other variants of CoT, including Self-Consistent CoT,\nTree-of-Thought, and Selection-Inference, offering a broad perspective on the\nefficacy of these methods. We also provide numerical experiments to validate\nthe theoretical findings.\n","authors":["Xinyang Hu","Fengzhuo Zhang","Siyu Chen","Zhuoran Yang"],"pdf_url":"https://arxiv.org/pdf/2408.14511v2.pdf","comment":"150 pages, 18 figures, 3 tables"},{"id":"http://arxiv.org/abs/2404.18531v2","updated":"2024-08-28T14:12:22Z","published":"2024-04-29T09:17:36Z","title":"A Framework to Model ML Engineering Processes","summary":"  The development of Machine Learning (ML) based systems is complex and\nrequires multidisciplinary teams with diverse skill sets. This may lead to\ncommunication issues or misapplication of best practices. Process models can\nalleviate these challenges by standardizing task orchestration, providing a\ncommon language to facilitate communication, and nurturing a collaborative\nenvironment. Unfortunately, current process modeling languages are not suitable\nfor describing the development of such systems. In this paper, we introduce a\nframework for modeling ML-based software development processes, built around a\ndomain-specific language and derived from an analysis of scientific and gray\nliterature. A supporting toolkit is also available.\n","authors":["Sergio Morales","Robert ClarisÃ³","Jordi Cabot"],"pdf_url":"https://arxiv.org/pdf/2404.18531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14846v4","updated":"2024-08-28T14:04:05Z","published":"2024-02-19T14:53:01Z","title":"Stick to your Role! Stability of Personal Values Expressed in Large\n  Language Models","summary":"  The standard way to study Large Language Models (LLMs) with benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLMs' highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence\n(specifically, value stability) should be studied as a specific property of\nLLMs and used as another dimension of LLM comparison (alongside others such as\ncognitive abilities, knowledge, or model size). We present a case-study on the\nstability of value expression over different contexts (simulated conversations\non different topics) as measured using a standard psychology questionnaire\n(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we\nstudy Rank-order stability on the population (interpersonal) level, and\nIpsative stability on the individual (intrapersonal) level. We consider two\nsettings (with and without instructing LLMs to simulate particular personas),\ntwo simulated populations, and three downstream tasks. We observe consistent\ntrends in the stability of models and model families - Mixtral, Mistral,\nGPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency\nof these trends implies that some models exhibit higher value stability than\nothers, and that stability can be estimated with the set of introduced\nmethodological tools. When instructed to simulate particular personas, LLMs\nexhibit low Rank-order stability, which further diminishes with conversation\nlength. This highlights the need for future research on LLMs that coherently\nsimulate different personas. This paper provides a foundational step in that\ndirection, and, to our knowledge, it is the first study of value stability in\nLLMs.\n","authors":["Grgur KovaÄ","RÃ©my Portelas","Masataka Sawayama","Peter Ford Dominey","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2402.14846v4.pdf","comment":"The project website and code are available at\n  https://sites.google.com/view/llmvaluestability Published in PLOS ONE (\n  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0309114 ),\n  and a shorter version at CogSci 24 (\n  https://escholarship.org/uc/item/7w4823c6 )"},{"id":"http://arxiv.org/abs/2405.12390v2","updated":"2024-08-28T13:48:07Z","published":"2024-05-20T21:50:19Z","title":"A Metric-based Principal Curve Approach for Learning One-dimensional\n  Manifold","summary":"  Principal curve is a well-known statistical method oriented in manifold\nlearning using concepts from differential geometry. In this paper, we propose a\nnovel metric-based principal curve (MPC) method that learns one-dimensional\nmanifold of spatial data. Synthetic datasets Real applications using MNIST\ndataset show that our method can learn the one-dimensional manifold well in\nterms of the shape.\n","authors":["Elvis Han Cui"],"pdf_url":"https://arxiv.org/pdf/2405.12390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15793v1","updated":"2024-08-28T13:37:07Z","published":"2024-08-28T13:37:07Z","title":"Language Adaptation on a Tight Academic Compute Budget: Tokenizer\n  Swapping Works and Pure bfloat16 Is Enough","summary":"  We investigate continued pretraining of LLMs for language adaptation on a\ntight academic budget: a setting in which only a few GPUs can be used in\nparallel, for a heavily constrained duration. We focus on adapting Mistral-7B\nto German or Arabic and evaluate several techniques to improve efficiency and\neffectiveness in this setting. Our German models adapted on this tight compute\nbudget underperform compared to the base Mistral-7B, while our Arabic models\noutperform several baselines, showing that for sufficiently well-represented\nlanguages, continued pretraining for specialization is not always helpful. Our\nmain findings focus on training precision and tokenizer swapping. Our results\nshow that pure bfloat16 training is a viable alternative to mixed-precision\ntraining, while being much faster when only using a few GPUs. Swapping the\ntokenizer for a specialized one yields more efficient tokenization and is\ncompetitive with the original tokenizer, which already contains some German\ntokens, but did not significantly increase performance for German. Code and\nmodel weights are available at on GitHub.\n","authors":["Konstantin Dobler","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2408.15793v1.pdf","comment":"WANT@ICML 2024"},{"id":"http://arxiv.org/abs/2408.15792v1","updated":"2024-08-28T13:35:54Z","published":"2024-08-28T13:35:54Z","title":"Efficient LLM Scheduling by Learning to Rank","summary":"  In Large Language Model (LLM) inference, the output length of an LLM request\nis typically regarded as not known a priori. Consequently, most LLM serving\nsystems employ a simple First-come-first-serve (FCFS) scheduling strategy,\nleading to Head-Of-Line (HOL) blocking and reduced throughput and service\nquality. In this paper, we reexamine this assumption -- we show that, although\npredicting the exact generation length of each request is infeasible, it is\npossible to predict the relative ranks of output lengths in a batch of\nrequests, using learning to rank. The ranking information offers valuable\nguidance for scheduling requests. Building on this insight, we develop a novel\nscheduler for LLM inference and serving that can approximate the\nshortest-job-first (SJF) schedule better than existing approaches. We integrate\nthis scheduler with the state-of-the-art LLM serving system and show\nsignificant performance improvement in several important applications: 2.8x\nlower latency in chatbot serving and 6.5x higher throughput in synthetic data\ngeneration. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git\n","authors":["Yichao Fu","Siqi Zhu","Runlong Su","Aurick Qiao","Ion Stoica","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.03469v2","updated":"2024-08-28T13:34:13Z","published":"2022-06-07T17:40:51Z","title":"Marked Neural Spatio-Temporal Point Process Involving a Dynamic Graph\n  Neural Network","summary":"  Temporal Point Processes (TPPs) have recently become increasingly interesting\nfor learning dynamics in graph data. A reason for this is that learning on\ndynamic graph data is becoming more relevant, since data from many scientific\nfields, ranging from mathematics, biology, social sciences, and physics to\ncomputer science, is naturally related and inherently dynamic. In addition,\nTPPs provide a meaningful characterization of event streams and a prediction\nmechanism for future events. Therefore, (semi-)parameterized Neural TPPs have\nbeen introduced whose characterization can be (partially) learned and, thus,\nenable the representation of more complex phenomena. However, the research on\nmodeling dynamic graphs with TPPs is relatively young, and only a few models\nfor node attribute changes or evolving edges have been proposed yet. To allow\nfor learning on fully dynamic graph streams, i.e., graphs that can change in\ntheir structure (addition/deletion of nodes/edge) and in their node/edge\nattributes, we propose a Marked Neural Spatio-Temporal Point Process (MNSTPP).\nIt leverages a Dynamic Graph Neural Network to learn a Marked TPP that handles\nattributes and spatial data to model and predict any event in a graph stream.\n","authors":["Alice Moallemy-Oureh","Silvia Beddar-Wiesing","Yannick Nagel","RÃ¼diger Nather","Josephine M. Thomas"],"pdf_url":"https://arxiv.org/pdf/2206.03469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00645v2","updated":"2024-08-28T13:32:43Z","published":"2023-08-30T13:26:49Z","title":"Analysis of Diagnostics (Part I): Prevalence, Uncertainty\n  Quantification, and Machine Learning","summary":"  Diagnostic testing provides a unique setting for studying and developing\ntools in classification theory. In such contexts, the concept of prevalence,\ni.e. the number of individuals with a given condition, is fundamental, both as\nan inherent quantity of interest and as a parameter that controls\nclassification accuracy. This manuscript is the first in a two-part series that\nstudies deeper connections between classification theory and prevalence,\nshowing how the latter establishes a more complete theory of uncertainty\nquantification (UQ) for certain types of machine learning (ML). We motivate\nthis analysis via a lemma demonstrating that general classifiers minimizing a\nprevalence-weighted error contain the same probabilistic information as\nBayes-optimal classifiers, which depend on conditional probability densities.\nThis leads us to study relative probability level-sets $B^\\star (q)$, which are\nreinterpreted as both classification boundaries and useful tools for\nquantifying uncertainty in class labels. To realize this in practice, we also\npropose a numerical, homotopy algorithm that estimates the $B^\\star (q)$ by\nminimizing a prevalence-weighted empirical error. The successes and\nshortcomings of this method motivate us to revisit properties of the level\nsets, and we deduce the corresponding classifiers obey a useful monotonicity\nproperty that stabilizes the numerics and points to important extensions to UQ\nof ML. Throughout, we validate our methods in the context of synthetic data and\na research-use-only SARS-CoV-2 enzyme-linked immunosorbent (ELISA) assay.\n","authors":["Paul N. Patrone","Raquel A. Binder","Catherine S. Forconi","Ann M. Moormann","Anthony J. Kearsley"],"pdf_url":"https://arxiv.org/pdf/2309.00645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14382v2","updated":"2024-08-28T13:30:36Z","published":"2023-07-25T20:08:41Z","title":"When Multi-Task Learning Meets Partial Supervision: A Computer Vision\n  Review","summary":"  Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while\nexploiting their mutual relationships. By using shared resources to\nsimultaneously calculate multiple outputs, this learning paradigm has the\npotential to have lower memory requirements and inference times compared to the\ntraditional approach of using separate methods for each task. Previous work in\nMTL has mainly focused on fully-supervised methods, as task relationships can\nnot only be leveraged to lower the level of data-dependency of those methods\nbut they can also improve performance. However, MTL introduces a set of\nchallenges due to a complex optimisation scheme and a higher labeling\nrequirement. This review focuses on how MTL could be utilised under different\npartial supervision settings to address these challenges. First, this review\nanalyses how MTL traditionally uses different parameter sharing techniques to\ntransfer knowledge in between tasks. Second, it presents the different\nchallenges arising from such a multi-objective optimisation scheme. Third, it\nintroduces how task groupings can be achieved by analysing task relationships.\nFourth, it focuses on how partially supervised methods applied to MTL can\ntackle the aforementioned challenges. Lastly, this review presents the\navailable datasets, tools and benchmarking results of such methods.\n","authors":["Maxime Fontana","Michael Spratling","Miaojing Shi"],"pdf_url":"https://arxiv.org/pdf/2307.14382v2.pdf","comment":"Accepted by Proceedings of the IEEE"},{"id":"http://arxiv.org/abs/2408.15784v1","updated":"2024-08-28T13:26:36Z","published":"2024-08-28T13:26:36Z","title":"Implicit Regularization Paths of Weighted Neural Representations","summary":"  We study the implicit regularization effects induced by (observation)\nweighting of pretrained features. For weight and feature matrices of bounded\noperator norms that are infinitesimally free with respect to (normalized) trace\nfunctionals, we derive equivalence paths connecting different weighting\nmatrices and ridge regularization levels. Specifically, we show that ridge\nestimators trained on weighted features along the same path are asymptotically\nequivalent when evaluated against test vectors of bounded norms. These paths\ncan be interpreted as matching the effective degrees of freedom of ridge\nestimators fitted with weighted features. For the special case of subsampling\nwithout replacement, our results apply to independently sampled random features\nand kernel features and confirm recent conjectures (Conjectures 7 and 8) of the\nauthors on the existence of such paths in Patil et al. We also present an\nadditive risk decomposition for ensembles of weighted estimators and show that\nthe risks are equivalent along the paths when the ensemble size goes to\ninfinity. As a practical consequence of the path equivalences, we develop an\nefficient cross-validation method for tuning and apply it to subsampled\npretrained representations across several models (e.g., ResNet-50) and datasets\n(e.g., CIFAR-100).\n","authors":["Jin-Hong Du","Pratik Patil"],"pdf_url":"https://arxiv.org/pdf/2408.15784v1.pdf","comment":"19 pages for main and 19 pages for appendix"},{"id":"http://arxiv.org/abs/2408.09237v2","updated":"2024-08-28T13:10:40Z","published":"2024-08-17T16:06:14Z","title":"QEDCartographer: Automating Formal Verification Using Reward-Free\n  Reinforcement Learning","summary":"  Formal verification is a promising method for producing reliable software,\nbut the difficulty of manually writing verification proofs severely limits its\nutility in practice. Recent methods have automated some proof synthesis by\nguiding a search through the proof space using a theorem prover. Unfortunately,\nthe theorem prover provides only the crudest estimate of progress, resulting in\neffectively undirected search. To address this problem, we create\nQEDCartographer, an automated proof-synthesis tool that combines supervised and\nreinforcement learning to more effectively explore the proof space.\nQEDCartographer incorporates the proofs' branching structure, enabling\nreward-free search and overcoming the sparse reward problem inherent to formal\nverification. We evaluate QEDCartographer using the CoqGym benchmark of 68.5K\ntheorems from 124 open-source Coq projects. QEDCartographer fully automatically\nproves 21.4% of the test-set theorems. Previous search-based proof-synthesis\ntools Tok, Tac, ASTactic, Passport, and Proverbot9001, which rely only on\nsupervised learning, prove 9.6%, 9.8%, 10.9%, 12.5%, and 19.8%, respectively.\nDiva, which combines 62 tools, proves 19.2%. Comparing to the most effective\nprior tool, Proverbot9001, QEDCartographer produces 26% shorter proofs 27%\nfaster, on average over the theorems both tools prove. Together,\nQEDCartographer and non-learning-based CoqHammer prove 31.8% of the theorems,\nwhile CoqHammer alone proves 26.6%. Our work demonstrates that reinforcement\nlearning is a fruitful research direction for improving proof-synthesis tools'\nsearch mechanisms.\n","authors":["Alex Sanchez-Stern","Abhishek Varghese","Zhanna Kaufman","Dylan Zhang","Talia Ringer","Yuriy Brun"],"pdf_url":"https://arxiv.org/pdf/2408.09237v2.pdf","comment":"Published in the International Conference on Software Engineering\n  (ICSE) 2025: Alex Sanchez-Stern, Abhishek Varghese, Zhanna Kaufman, Dylan\n  Zhang, Talia Ringer, and Yuriy Brun, QEDCartographer: Automating Formal\n  Verification Using Reward-Free Reinforcement Learning, in Proceedings of the\n  47th International Conference on Software Engineering (ICSE), 2025"},{"id":"http://arxiv.org/abs/2408.15771v1","updated":"2024-08-28T13:09:20Z","published":"2024-08-28T13:09:20Z","title":"wav2pos: Sound Source Localization using Masked Autoencoders","summary":"  We present a novel approach to the 3D sound source localization task for\ndistributed ad-hoc microphone arrays by formulating it as a set-to-set\nregression problem. By training a multi-modal masked autoencoder model that\noperates on audio recordings and microphone coordinates, we show that such a\nformulation allows for accurate localization of the sound source, by\nreconstructing coordinates masked in the input. Our approach is flexible in the\nsense that a single model can be used with an arbitrary number of microphones,\neven when a subset of audio recordings and microphone coordinates are missing.\nWe test our method on simulated and real-world recordings of music and speech\nin indoor environments, and demonstrate competitive performance compared to\nboth classical and other learning based localization methods.\n","authors":["Axel Berg","Jens Gulin","Mark O'Connor","Chuteng Zhou","Karl ÃstrÃ¶m","Magnus Oskarsson"],"pdf_url":"https://arxiv.org/pdf/2408.15771v1.pdf","comment":"IPIN 2024"},{"id":"http://arxiv.org/abs/2405.19730v5","updated":"2024-08-28T13:05:41Z","published":"2024-05-30T06:21:34Z","title":"Research on the Spatial Data Intelligent Foundation Model","summary":"  This report focuses on spatial data intelligent large models, delving into\nthe principles, methods, and cutting-edge applications of these models. It\nprovides an in-depth discussion on the definition, development history, current\nstatus, and trends of spatial data intelligent large models, as well as the\nchallenges they face. The report systematically elucidates the key technologies\nof spatial data intelligent large models and their applications in urban\nenvironments, aerospace remote sensing, geography, transportation, and other\nscenarios. Additionally, it summarizes the latest application cases of spatial\ndata intelligent large models in themes such as urban development, multimodal\nsystems, remote sensing, smart transportation, and resource environments.\nFinally, the report concludes with an overview and outlook on the development\nprospects of spatial data intelligent large models.\n","authors":["Shaohua Wang","Xing Xie","Yong Li","Danhuai Guo","Zhi Cai","Yu Liu","Yang Yue","Xiao Pan","Feng Lu","Huayi Wu","Zhipeng Gui","Zhiming Ding","Bolong Zheng","Fuzheng Zhang","Jingyuan Wang","Zhengchao Chen","Hao Lu","Jiayi Li","Peng Yue","Wenhao Yu","Yao Yao","Leilei Sun","Yong Zhang","Longbiao Chen","Xiaoping Du","Xiang Li","Xueying Zhang","Kun Qin","Zhaoya Gong","Weihua Dong","Xiaofeng Meng"],"pdf_url":"https://arxiv.org/pdf/2405.19730v5.pdf","comment":"V1 and V2 are in Chinese language, other versions are in English"},{"id":"http://arxiv.org/abs/2408.15766v1","updated":"2024-08-28T12:59:12Z","published":"2024-08-28T12:59:12Z","title":"Harmonized Speculative Sampling","summary":"  Speculative sampling has proven to be an effective solution to accelerate\ndecoding from large language models, where the acceptance rate significantly\ndetermines the performance. Most previous works on improving the acceptance\nrate focus on aligned training and efficient decoding, implicitly paying less\nattention to the linkage of training and decoding. In this work, we first\ninvestigate the linkage of training and decoding for speculative sampling and\nthen propose a solution named HArmonized Speculative Sampling (HASS). HASS\nimproves the acceptance rate without extra inference overhead by harmonizing\ntraining and decoding on their objectives and contexts. Experiments on three\nLLaMA models demonstrate that HASS achieves 2.81x-3.65x wall-clock time speedup\nratio averaging across three datasets, which is 8%-15% faster than EAGLE-2.\n","authors":["Lefan Zhang","Xiaodan Wang","Yanhua Huang","Ruiwen Xu"],"pdf_url":"https://arxiv.org/pdf/2408.15766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09495v2","updated":"2024-08-28T12:46:28Z","published":"2024-06-13T17:36:05Z","title":"FADE: Towards Fairness-aware Augmentation for Domain Generalization via\n  Classifier-Guided Score-based Diffusion Models","summary":"  Fairness-aware domain generalization (FairDG) has emerged as a critical\nchallenge for deploying trustworthy AI systems, particularly in scenarios\ninvolving distribution shifts. Traditional methods for addressing fairness have\nfailed in domain generalization due to their lack of consideration for\ndistribution shifts. Although disentanglement has been used to tackle FairDG,\nit is limited by its strong assumptions. To overcome these limitations, we\npropose Fairness-aware Classifier-Guided Score-based Diffusion Models (FADE) as\na novel approach to effectively address the FairDG issue. Specifically, we\nfirst pre-train a score-based diffusion model (SDM) and two classifiers to\nequip the model with strong generalization capabilities across different\ndomains. Then, we guide the SDM using these pre-trained classifiers to\neffectively eliminate sensitive information from the generated data. Finally,\nthe generated fair data is used to train downstream classifiers, ensuring\nrobust performance under new data distributions. Extensive experiments on three\nreal-world datasets demonstrate that FADE not only enhances fairness but also\nimproves accuracy in the presence of distribution shifts. Additionally, FADE\noutperforms existing methods in achieving the best accuracy-fairness\ntrade-offs.\n","authors":["Yujie Lin","Dong Li","Chen Zhao","Minglai Shao"],"pdf_url":"https://arxiv.org/pdf/2406.09495v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02255v3","updated":"2024-08-28T12:43:10Z","published":"2023-12-04T18:56:08Z","title":"Re-Nerfing: Improving Novel View Synthesis through Novel View Synthesis","summary":"  Recent neural rendering and reconstruction techniques, such as NeRFs or\nGaussian Splatting, have shown remarkable novel view synthesis capabilities but\nrequire hundreds of images of the scene from diverse viewpoints to render\nhigh-quality novel views. With fewer images available, these methods start to\nfail since they can no longer correctly triangulate the underlying 3D geometry\nand converge to a non-optimal solution. These failures can manifest as floaters\nor blurry renderings in sparsely observed areas of the scene. In this paper, we\npropose Re-Nerfing, a simple and general add-on approach that leverages novel\nview synthesis itself to tackle this problem. Using an already trained NVS\nmethod, we render novel views between existing ones and augment the training\ndata to optimize a second model. This introduces additional multi-view\nconstraints and allows the second model to converge to a better solution. With\nRe-Nerfing we achieve significant improvements upon multiple pipelines based on\nNeRF and Gaussian-Splatting in sparse view settings of the mip-NeRF 360 and\nLLFF datasets. Notably, Re-Nerfing does not require prior knowledge or extra\nsupervision signals, making it a flexible and practical add-on.\n","authors":["Felix Tristram","Stefano Gasperini","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2312.02255v3.pdf","comment":"Code will be released upon acceptance"},{"id":"http://arxiv.org/abs/2408.14432v2","updated":"2024-08-28T12:39:57Z","published":"2024-08-26T17:20:34Z","title":"Contextual Bandit with Herding Effects: Algorithms and Recommendation\n  Applications","summary":"  Contextual bandits serve as a fundamental algorithmic framework for\noptimizing recommendation decisions online. Though extensive attention has been\npaid to tailoring contextual bandits for recommendation applications, the\n\"herding effects\" in user feedback have been ignored. These herding effects\nbias user feedback toward historical ratings, breaking down the assumption of\nunbiased feedback inherent in contextual bandits. This paper develops a novel\nvariant of the contextual bandit that is tailored to address the feedback bias\ncaused by the herding effects. A user feedback model is formulated to capture\nthis feedback bias. We design the TS-Conf (Thompson Sampling under Conformity)\nalgorithm, which employs posterior sampling to balance the exploration and\nexploitation tradeoff. We prove an upper bound for the regret of the algorithm,\nrevealing the impact of herding effects on learning speed. Extensive\nexperiments on datasets demonstrate that TS-Conf outperforms four benchmark\nalgorithms. Analysis reveals that TS-Conf effectively mitigates the negative\nimpact of herding effects, resulting in faster learning and improved\nrecommendation accuracy.\n","authors":["Luyue Xu","Liming Wang","Hong Xie","Mingqiang Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.14432v2.pdf","comment":"Published as a conference paper at PRICAI 2024"},{"id":"http://arxiv.org/abs/2408.15753v1","updated":"2024-08-28T12:39:51Z","published":"2024-08-28T12:39:51Z","title":"A Neural Material Point Method for Particle-based Simulations","summary":"  Mesh-free Lagrangian methods are widely used for simulating fluids, solids,\nand their complex interactions due to their ability to handle large\ndeformations and topological changes. These physics simulators, however,\nrequire substantial computational resources for accurate simulations. To\naddress these issues, deep learning emulators promise faster and scalable\nsimulations, yet they often remain expensive and difficult to train, limiting\ntheir practical use. Inspired by the Material Point Method (MPM), we present\nNeuralMPM, a neural emulation framework for particle-based simulations.\nNeuralMPM interpolates Lagrangian particles onto a fixed-size grid, computes\nupdates on grid nodes using image-to-image neural networks, and interpolates\nback to the particles. Similarly to MPM, NeuralMPM benefits from the regular\nvoxelized representation to simplify the computation of the state dynamics,\nwhile avoiding the drawbacks of mesh-based Eulerian methods. We demonstrate the\nadvantages of NeuralMPM on several datasets, including fluid dynamics and\nfluid-solid interactions. Compared to existing methods, NeuralMPM reduces\ntraining times from days to hours, while achieving comparable or superior\nlong-term accuracy, making it a promising approach for practical forward and\ninverse problems. A project page is available at https://neuralmpm.isach.be\n","authors":["Omer Rochman Sharabi","Sacha Lewin","Gilles Louppe"],"pdf_url":"https://arxiv.org/pdf/2408.15753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11122v6","updated":"2024-08-28T12:33:52Z","published":"2023-10-17T10:14:10Z","title":"Sensitivity-Aware Amortized Bayesian Inference","summary":"  Sensitivity analyses reveal the influence of various modeling choices on the\noutcomes of statistical analyses. While theoretically appealing, they are\noverwhelmingly inefficient for complex Bayesian models. In this work, we\npropose sensitivity-aware amortized Bayesian inference (SA-ABI), a multifaceted\napproach to efficiently integrate sensitivity analyses into simulation-based\ninference with neural networks. First, we utilize weight sharing to encode the\nstructural similarities between alternative likelihood and prior specifications\nin the training process with minimal computational overhead. Second, we\nleverage the rapid inference of neural networks to assess sensitivity to data\nperturbations and preprocessing steps. In contrast to most other Bayesian\napproaches, both steps circumvent the costly bottleneck of refitting the model\nfor each choice of likelihood, prior, or data set. Finally, we propose to use\ndeep ensembles to detect sensitivity arising from unreliable approximation\n(e.g., due to model misspecification). We demonstrate the effectiveness of our\nmethod in applied modeling problems, ranging from disease outbreak dynamics and\nglobal warming thresholds to human decision-making. Our results support\nsensitivity-aware inference as a default choice for amortized Bayesian\nworkflows, automatically providing modelers with insights into otherwise hidden\ndimensions.\n","authors":["Lasse ElsemÃ¼ller","Hans OlischlÃ¤ger","Marvin Schmitt","Paul-Christian BÃ¼rkner","Ullrich KÃ¶the","Stefan T. Radev"],"pdf_url":"https://arxiv.org/pdf/2310.11122v6.pdf","comment":"Published in TMLR (2024)"},{"id":"http://arxiv.org/abs/2407.16496v2","updated":"2024-08-28T12:20:42Z","published":"2024-07-23T14:11:12Z","title":"Articulation Work and Tinkering for Fairness in Machine Learning","summary":"  The field of fair AI aims to counter biased algorithms through computational\nmodelling. However, it faces increasing criticism for perpetuating the use of\noverly technical and reductionist methods. As a result, novel approaches appear\nin the field to address more socially-oriented and interdisciplinary (SOI)\nperspectives on fair AI. In this paper, we take this dynamic as the starting\npoint to study the tension between computer science (CS) and SOI research. By\ndrawing on STS and CSCW theory, we position fair AI research as a matter of\n'organizational alignment': what makes research 'doable' is the successful\nalignment of three levels of work organization (the social world, the\nlaboratory, and the experiment). Based on qualitative interviews with CS\nresearchers, we analyze the tasks, resources, and actors required for doable\nresearch in the case of fair AI. We find that CS researchers engage with SOI\nresearch to some extent, but organizational conditions, articulation work, and\nambiguities of the social world constrain the doability of SOI research for\nthem. Based on our findings, we identify and discuss problems for aligning CS\nand SOI as fair AI continues to evolve.\n","authors":["Miriam Fahimi","Mayra Russo","Kristen M. Scott","Maria-Esther Vidal","Bettina Berendt","Katharina Kinder-Kurlanda"],"pdf_url":"https://arxiv.org/pdf/2407.16496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08459v3","updated":"2024-08-28T12:11:46Z","published":"2023-03-15T09:03:58Z","title":"Forecasting Intraday Power Output by a Set of PV Systems using Recurrent\n  Neural Networks and Physical Covariates","summary":"  Accurate intraday forecasts of the power output by PhotoVoltaic (PV) systems\nare critical to improve the operation of energy distribution grids. We describe\na neural autoregressive model that aims to perform such intraday forecasts. We\nbuild upon a physical, deterministic PV performance model, the output of which\nis used as covariates in the context of the neural model. In addition, our\napplication data relates to a geographically distributed set of PV systems. We\naddress all PV sites with a single neural model, which embeds the information\nabout the PV site in specific covariates. We use a scale-free approach which\nrelies on the explicit modeling of seasonal effects. Our proposal repurposes a\nmodel initially used in the retail sector and discloses a novel truncated\nGaussian output distribution. An ablation study and a comparison to alternative\narchitectures from the literature shows that the components in the best\nperforming proposed model variant work synergistically to reach a skill score\nof 15.72% with respect to the physical model, used as a baseline.\n","authors":["Pierrick Bruneau","David Fiorelli","Christian Braun","Daniel Koster"],"pdf_url":"https://arxiv.org/pdf/2303.08459v3.pdf","comment":"25 pages, 7 figures, Accepted for publication in Neural Computing and\n  Applications on 12/07/2024"},{"id":"http://arxiv.org/abs/2408.14398v2","updated":"2024-08-28T12:03:54Z","published":"2024-08-26T16:29:13Z","title":"Language-specific Calibration for Pruning Multilingual Language Models","summary":"  Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.\n","authors":["Simon Kurz","Jian-Jia Chen","Lucie Flek","Zhixue Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.14398v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11960v3","updated":"2024-08-28T11:48:43Z","published":"2024-03-18T16:57:16Z","title":"Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal\n  Time Series Imputation","summary":"  Spatiotemporal time series are usually collected via monitoring sensors\nplaced at different locations, which usually contain missing values due to\nvarious failures, such as mechanical damages and Internet outages. Imputing the\nmissing values is crucial for analyzing time series. When recovering a specific\ndata point, most existing methods consider all the information relevant to that\npoint regardless of the cause-and-effect relationship. During data collection,\nit is inevitable that some unknown confounders are included, e.g., background\nnoise in time series and non-causal shortcut edges in the constructed sensor\nnetwork. These confounders could open backdoor paths and establish non-causal\ncorrelations between the input and output. Over-exploiting these non-causal\ncorrelations could cause overfitting. In this paper, we first revisit\nspatiotemporal time series imputation from a causal perspective and show how to\nblock the confounders via the frontdoor adjustment. Based on the results of\nfrontdoor adjustment, we introduce a novel Causality-Aware Spatiotemporal Graph\nNeural Network (Casper), which contains a novel Prompt Based Decoder (PBD) and\na Spatiotemporal Causal Attention (SCA). PBD could reduce the impact of\nconfounders and SCA could discover the sparse causal relationships among\nembeddings. Theoretical analysis reveals that SCA discovers causal\nrelationships based on the values of gradients. We evaluate Casper on three\nreal-world datasets, and the experimental results show that Casper could\noutperform the baselines and could effectively discover causal relationships.\n","authors":["Baoyu Jing","Dawei Zhou","Kan Ren","Carl Yang"],"pdf_url":"https://arxiv.org/pdf/2403.11960v3.pdf","comment":"Accepted by CIKM'2024"},{"id":"http://arxiv.org/abs/2408.15722v1","updated":"2024-08-28T11:39:24Z","published":"2024-08-28T11:39:24Z","title":"Advanced POD-Based Performance Evaluation of Classifiers Applied to\n  Human Driver Lane Changing Prediction","summary":"  Machine learning (ML) classifiers serve as essential tools facilitating\nclassification and prediction across various domains. The performance of these\nalgorithms should be known to ensure their reliable application. In certain\nfields, receiver operating characteristic and precision-recall curves are\nfrequently employed to assess machine learning algorithms without accounting\nfor the impact of process parameters. However, it may be essential to evaluate\nthe performance of these algorithms in relation to such parameters. As a\nperformance evaluation metric capable of considering the effects of process\nparameters, this paper uses a modified probability of detection (POD) approach\nto assess the reliability of ML-based algorithms. As an example, the POD-based\napproach is employed to assess ML models used for predicting the lane changing\nbehavior of a vehicle driver. The time remaining to the predicted (and\ntherefore unknown) lane changing event is considered as process parameter. The\nhit/miss approach to POD is taken here and modified by considering the\nprobability of lane changing derived from ML algorithms at each time step, and\nobtaining the final result of the analysis accordingly. This improves the\nreliability of results compared to the standard hit/miss approach, which\nconsiders the outcome of the classifiers as either 0 or 1, while also\nsimplifying evaluation compared to the \\^a versus a approach. Performance\nevaluation results of the proposed approach are compared with those obtained\nwith the standard hit/miss approach and a pre-developed \\^a versus a approach\nto validate the effectiveness of the proposed method. The comparison shows that\nthis method provides an averaging conservative behavior with the advantage of\nenhancing the reliability of the hit/miss approach to POD while retaining its\nsimplicity.\n","authors":["Zahra Rastin","Dirk SÃ¶ffker"],"pdf_url":"https://arxiv.org/pdf/2408.15722v1.pdf","comment":"Manuscript: 8 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2305.17479v3","updated":"2024-08-28T11:27:25Z","published":"2023-05-27T13:57:26Z","title":"Inferring Individual Direct Causal Effects Under Heterogeneous Peer\n  Influence","summary":"  Causal inference in networks should account for interference, which occurs\nwhen a unit's outcome is influenced by treatments or outcomes of peers.\nHeterogeneous peer influence (HPI) occurs when a unit's outcome is influenced\ndifferently by different peers based on their attributes and relationships, or\nwhen each unit has a different susceptibility to peer influence. Existing\nsolutions to estimating direct causal effects under interference consider\neither homogeneous influence from peers or specific heterogeneous influence\nmechanisms (e.g., based on local neighborhood structure). This paper presents a\nmethodology for estimating individual direct causal effects in the presence of\nHPI where the mechanism of influence is not known a priori. We propose a\nstructural causal model for networks that can capture different possible\nassumptions about network structure, interference conditions, and causal\ndependence and enables reasoning about identifiability in the presence of HPI.\nWe find potential heterogeneous contexts using the causal model and propose a\nnovel graph neural network-based estimator to estimate individual direct causal\neffects. We show that state-of-the-art methods for individual direct effect\nestimation produce biased results in the presence of HPI, and that our proposed\nestimator is robust.\n","authors":["Shishir Adhikari","Elena Zheleva"],"pdf_url":"https://arxiv.org/pdf/2305.17479v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15715v1","updated":"2024-08-28T11:21:33Z","published":"2024-08-28T11:21:33Z","title":"Autoregressive model path dependence near Ising criticality","summary":"  Autoregressive models are a class of generative model that probabilistically\npredict the next output of a sequence based on previous inputs. The\nautoregressive sequence is by definition one-dimensional (1D), which is natural\nfor language tasks and hence an important component of modern architectures\nlike recurrent neural networks (RNNs) and transformers. However, when language\nmodels are used to predict outputs on physical systems that are not\nintrinsically 1D, the question arises of which choice of autoregressive\nsequence -- if any -- is optimal. In this paper, we study the reconstruction of\ncritical correlations in the two-dimensional (2D) Ising model, using RNNs and\ntransformers trained on binary spin data obtained near the thermal phase\ntransition. We compare the training performance for a number of different 1D\nautoregressive sequences imposed on finite-size 2D lattices. We find that paths\nwith long 1D segments are more efficient at training the autoregressive models\ncompared to space-filling curves that better preserve the 2D locality. Our\nresults illustrate the potential importance in choosing the optimal\nautoregressive sequence ordering when training modern language models for tasks\nin physics.\n","authors":["Yi Hong Teoh","Roger G. Melko"],"pdf_url":"https://arxiv.org/pdf/2408.15715v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.15714v1","updated":"2024-08-28T11:21:23Z","published":"2024-08-28T11:21:23Z","title":"Pixels to Prose: Understanding the art of Image Captioning","summary":"  In the era of evolving artificial intelligence, machines are increasingly\nemulating human-like capabilities, including visual perception and linguistic\nexpression. Image captioning stands at the intersection of these domains,\nenabling machines to interpret visual content and generate descriptive text.\nThis paper provides a thorough review of image captioning techniques, catering\nto individuals entering the field of machine learning who seek a comprehensive\nunderstanding of available options, from foundational methods to\nstate-of-the-art approaches. Beginning with an exploration of primitive\narchitectures, the review traces the evolution of image captioning models to\nthe latest cutting-edge solutions. By dissecting the components of these\narchitectures, readers gain insights into the underlying mechanisms and can\nselect suitable approaches tailored to specific problem requirements without\nduplicating efforts. The paper also delves into the application of image\ncaptioning in the medical domain, illuminating its significance in various\nreal-world scenarios.\n  Furthermore, the review offers guidance on evaluating the performance of\nimage captioning systems, highlighting key metrics for assessment. By\nsynthesizing theoretical concepts with practical application, this paper equips\nreaders with the knowledge needed to navigate the complex landscape of image\ncaptioning and harness its potential for diverse applications in machine\nlearning and beyond.\n","authors":["Hrishikesh Singh","Aarti Sharma","Millie Pant"],"pdf_url":"https://arxiv.org/pdf/2408.15714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08235v3","updated":"2024-08-28T11:18:26Z","published":"2023-04-14T07:55:07Z","title":"A Platform-Agnostic Deep Reinforcement Learning Framework for Effective\n  Sim2Real Transfer towards Autonomous Driving","summary":"  Deep Reinforcement Learning (DRL) has shown remarkable success in solving\ncomplex tasks across various research fields. However, transferring DRL agents\nto the real world is still challenging due to the significant discrepancies\nbetween simulation and reality. To address this issue, we propose a robust DRL\nframework that leverages platform-dependent perception modules to extract\ntask-relevant information and train a lane-following and overtaking agent in\nsimulation. This framework facilitates the seamless transfer of the DRL agent\nto new simulated environments and the real world with minimal effort. We\nevaluate the performance of the agent in various driving scenarios in both\nsimulation and the real world, and compare it to human players and the PID\nbaseline in simulation. Our proposed framework significantly reduces the gaps\nbetween different platforms and the Sim2Real gap, enabling the trained agent to\nachieve similar performance in both simulation and the real world, driving the\nvehicle effectively.\n","authors":["Dianzhao Li","Ostap Okhrin"],"pdf_url":"https://arxiv.org/pdf/2304.08235v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03472v3","updated":"2024-08-28T11:18:00Z","published":"2023-08-07T11:02:44Z","title":"Improving the forecast accuracy of wind power by leveraging multiple\n  hierarchical structure","summary":"  Renewable energy generation is of utmost importance for global\ndecarbonization. Forecasting renewable energies, particularly wind energy, is\nchallenging due to the inherent uncertainty in wind energy generation, which\ndepends on weather conditions. Recent advances in hierarchical forecasting\nthrough reconciliation have demonstrated a significant increase in the quality\nof wind energy forecasts for short-term periods. We leverage the\ncross-sectional and temporal hierarchical structure of turbines in wind farms\nand build cross-temporal hierarchies to further investigate how integrated\ncross-sectional and temporal dimensions can add value to forecast accuracy in\nwind farms. We found that cross-temporal reconciliation was superior to\nindividual cross-sectional reconciliation at multiple temporal aggregations.\nAdditionally, machine learning based forecasts that were cross-temporally\nreconciled demonstrated high accuracy at coarser temporal granularities, which\nmay encourage adoption for short-term wind forecasts. Empirically, we provide\ninsights for decision-makers on the best methods for forecasting high-frequency\nwind data across different forecasting horizons and levels.\n","authors":["Lucas English","Mahdi Abolghasemi"],"pdf_url":"https://arxiv.org/pdf/2308.03472v3.pdf","comment":"41 pages, 14 figures"},{"id":"http://arxiv.org/abs/2408.15702v1","updated":"2024-08-28T11:02:23Z","published":"2024-08-28T11:02:23Z","title":"Evaluating Model Robustness Using Adaptive Sparse L0 Regularization","summary":"  Deep Neural Networks have demonstrated remarkable success in various domains\nbut remain susceptible to adversarial examples, which are slightly altered\ninputs designed to induce misclassification. While adversarial attacks\ntypically optimize under Lp norm constraints, attacks based on the L0 norm,\nprioritising input sparsity, are less studied due to their complex and non\nconvex nature. These sparse adversarial examples challenge existing defenses by\naltering a minimal subset of features, potentially uncovering more subtle DNN\nweaknesses. However, the current L0 norm attack methodologies face a trade off\nbetween accuracy and efficiency either precise but computationally intense or\nexpedient but imprecise. This paper proposes a novel, scalable, and effective\napproach to generate adversarial examples based on the L0 norm, aimed at\nrefining the robustness evaluation of DNNs against such perturbations.\n","authors":["Weiyou Liu","Zhenyang Li","Weitong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.15702v1.pdf","comment":"Accepted by the 20th International Conference on Advanced Data Mining\n  and Applications (ADMA 2024)"},{"id":"http://arxiv.org/abs/2403.17550v2","updated":"2024-08-28T10:52:32Z","published":"2024-03-26T09:58:06Z","title":"DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping","summary":"  Recently, significant progress has been achieved in sensing real large-scale\noutdoor 3D environments, particularly by using modern acquisition equipment\nsuch as LiDAR sensors. Unfortunately, they are fundamentally limited in their\nability to produce dense, complete 3D scenes. To address this issue, recent\nlearning-based methods integrate neural implicit representations and\noptimizable feature grids to approximate surfaces of 3D scenes. However,\nnaively fitting samples along raw LiDAR rays leads to noisy 3D mapping results\ndue to the nature of sparse, conflicting LiDAR measurements. Instead, in this\nwork we depart from fitting LiDAR data exactly, instead letting the network\noptimize a non-metric monotonic implicit field defined in 3D space. To fit our\nfield, we design a learning system integrating a monotonicity loss that enables\noptimizing neural monotonic fields and leverages recent progress in large-scale\n3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as\ncaptured by multiple quantitative and perceptual measures and visual results\nobtained for Mai City, Newer College, and KITTI benchmarks. The code of our\napproach will be made publicly available.\n","authors":["Kutay YÄ±lmaz","Matthias NieÃner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.17550v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.03187v3","updated":"2024-08-28T10:00:01Z","published":"2023-12-05T23:33:49Z","title":"FERGI: Automatic Annotation of User Preferences for Text-to-Image\n  Generation from Spontaneous Facial Expression Reaction","summary":"  Researchers have proposed to use data of human preference feedback to\nfine-tune text-to-image generative models. However, the scalability of human\nfeedback collection has been limited by its reliance on manual annotation.\nTherefore, we develop and test a method to automatically score user preferences\nfrom their spontaneous facial expression reaction to the generated images. We\ncollect a dataset of Facial Expression Reaction to Generated Images (FERGI) and\nshow that the activations of multiple facial action units (AUs) are highly\ncorrelated with user evaluations of the generated images. We develop an FAU-Net\n(Facial Action Units Neural Network), which receives inputs from an AU\nestimation model, to automatically score user preferences for text-to-image\ngeneration based on their facial expression reactions, which is complementary\nto the pre-trained scoring models based on the input text prompts and generated\nimages. Integrating our FAU-Net valence score with the pre-trained scoring\nmodels improves their consistency with human preferences. This method of\nautomatic annotation with facial expression analysis can be potentially\ngeneralized to other generation tasks. The code is available at\nhttps://github.com/ShuangquanFeng/FERGI, and the dataset is also available at\nthe same link for research purposes.\n","authors":["Shuangquan Feng","Junhua Ma","Virginia R. de Sa"],"pdf_url":"https://arxiv.org/pdf/2312.03187v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18316v2","updated":"2024-08-28T09:42:58Z","published":"2024-06-26T12:59:37Z","title":"Trade-off between Gradient Measurement Efficiency and Expressivity in\n  Deep Quantum Neural Networks","summary":"  Quantum neural networks (QNNs) require an efficient training algorithm to\nachieve practical quantum advantages. A promising approach is the use of\ngradient-based optimization algorithms, where gradients are estimated through\nquantum measurements. However, general QNNs lack an efficient gradient\nmeasurement algorithm, which poses a fundamental and practical challenge to\nrealizing scalable QNNs. In this work, we rigorously prove a trade-off between\ngradient measurement efficiency, defined as the mean number of simultaneously\nmeasurable gradient components, and expressivity in a wide class of deep QNNs,\nelucidating the theoretical limits and possibilities of efficient gradient\nestimation. This trade-off implies that a more expressive QNN requires a higher\nmeasurement cost in gradient estimation, whereas we can increase gradient\nmeasurement efficiency by reducing the QNN expressivity to suit a given task.\nWe further propose a general QNN ansatz called the stabilizer-logical product\nansatz (SLPA), which can reach the upper limit of the trade-off inequality by\nleveraging the symmetric structure of the quantum circuit. In learning an\nunknown symmetric function, the SLPA drastically reduces the quantum resources\nrequired for training while maintaining accuracy and trainability compared to a\nwell-designed symmetric circuit based on the parameter-shift method. Our\nresults not only reveal a theoretical understanding of efficient training in\nQNNs but also provide a standard and broadly applicable efficient QNN design.\n","authors":["Koki Chinzei","Shinichiro Yamano","Quoc Hoan Tran","Yasuhiro Endo","Hirotaka Oshima"],"pdf_url":"https://arxiv.org/pdf/2406.18316v2.pdf","comment":"31 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.15667v1","updated":"2024-08-28T09:40:40Z","published":"2024-08-28T09:40:40Z","title":"Towards reliable respiratory disease diagnosis based on cough sounds and\n  vision transformers","summary":"  Recent advancements in deep learning techniques have sparked performance\nboosts in various real-world applications including disease diagnosis based on\nmulti-modal medical data. Cough sound data-based respiratory disease (e.g.,\nCOVID-19 and Chronic Obstructive Pulmonary Disease) diagnosis has also\nattracted much attention. However, existing works usually utilise traditional\nmachine learning or deep models of moderate scales. On the other hand, the\ndeveloped approaches are trained and evaluated on small-scale data due to the\ndifficulty of curating and annotating clinical data on scale. To address these\nissues in prior works, we create a unified framework to evaluate various deep\nmodels from lightweight Convolutional Neural Networks (e.g., ResNet18) to\nmodern vision transformers and compare their performance in respiratory disease\nclassification. Based on the observations from such an extensive empirical\nstudy, we propose a novel approach to cough-based disease classification based\non both self-supervised and supervised learning on a large-scale cough data\nset. Experimental results demonstrate our proposed approach outperforms prior\narts consistently on two benchmark datasets for COVID-19 diagnosis and a\nproprietary dataset for COPD/non-COPD classification with an AUROC of 92.5%.\n","authors":["Qian Wang","Zhaoyang Bu","Jiaxuan Mao","Wenyu Zhu","Jingya Zhao","Wei Du","Guochao Shi","Min Zhou","Si Chen","Jieming Qu"],"pdf_url":"https://arxiv.org/pdf/2408.15667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15664v1","updated":"2024-08-28T09:31:09Z","published":"2024-08-28T09:31:09Z","title":"Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts","summary":"  For Mixture-of-Experts (MoE) models, an unbalanced expert load will lead to\nrouting collapse or increased computational overhead. Existing methods commonly\nemploy an auxiliary loss to encourage load balance, but a large auxiliary loss\nwill introduce non-negligible interference gradients into training and thus\nimpair the model performance. In order to control load balance while not\nproducing undesired gradients during training, we propose Loss-Free Balancing,\nfeatured by an auxiliary-loss-free load balancing strategy. To be specific,\nbefore the top-K routing decision, Loss-Free Balancing will first apply an\nexpert-wise bias to the routing scores of each expert. By dynamically updating\nthe bias of each expert according to its recent load, Loss-Free Balancing can\nconsistently maintain a balanced distribution of expert load. In addition,\nsince Loss-Free Balancing does not produce any interference gradients, it also\nelevates the upper bound of model performance gained from MoE training. We\nvalidate the performance of Loss-Free Balancing on MoE models with up to 3B\nparameters trained on up to 200B tokens. Experimental results show that\nLoss-Free Balancing achieves both better performance and better load balance\ncompared with traditional auxiliary-loss-controlled load balancing strategies.\n","authors":["Lean Wang","Huazuo Gao","Chenggang Zhao","Xu Sun","Damai Dai"],"pdf_url":"https://arxiv.org/pdf/2408.15664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14951v2","updated":"2024-08-28T09:08:11Z","published":"2024-08-27T10:54:51Z","title":"Domain-decoupled Physics-informed Neural Networks with Closed-form\n  Gradients for Fast Model Learning of Dynamical Systems","summary":"  Physics-informed neural networks (PINNs) are trained using physical equations\nand can also incorporate unmodeled effects by learning from data. PINNs for\ncontrol (PINCs) of dynamical systems are gaining interest due to their\nprediction speed compared to classical numerical integration methods for\nnonlinear state-space models, making them suitable for real-time control\napplications. We introduce the domain-decoupled physics-informed neural network\n(DD-PINN) to address current limitations of PINC in handling large and complex\nnonlinear dynamical systems. The time domain is decoupled from the feed-forward\nneural network to construct an Ansatz function, allowing for calculation of\ngradients in closed form. This approach significantly reduces training times,\nespecially for large dynamical systems, compared to PINC, which relies on\ngraph-based automatic differentiation. Additionally, the DD-PINN inherently\nfulfills the initial condition and supports higher-order excitation inputs,\nsimplifying the training process and enabling improved prediction accuracy.\nValidation on three systems - a nonlinear mass-spring-damper, a\nfive-mass-chain, and a two-link robot - demonstrates that the DD-PINN achieves\nsignificantly shorter training times. In cases where the PINC's prediction\ndiverges, the DD-PINN's prediction remains stable and accurate due to higher\nphysics loss reduction or use of a higher-order excitation input. The DD-PINN\nallows for fast and accurate learning of large dynamical systems previously out\nof reach for the PINC.\n","authors":["Henrik Krauss","Tim-Lukas Habich","Max Bartholdt","Thomas Seel","Moritz Schappler"],"pdf_url":"https://arxiv.org/pdf/2408.14951v2.pdf","comment":"Accepted to International Conference on Informatics in Control,\n  Automation and Robotics (ICINCO) 2024"},{"id":"http://arxiv.org/abs/2402.09066v2","updated":"2024-08-28T09:01:37Z","published":"2024-02-14T10:24:04Z","title":"Solid Waste Detection, Monitoring and Mapping in Remote Sensing Images:\n  A Survey","summary":"  The detection and characterization of illegal solid waste disposal sites are\nessential for environmental protection, particularly for mitigating pollution\nand health hazards. Improperly managed landfills contaminate soil and\ngroundwater via rainwater infiltration, posing threats to both animals and\nhumans. Traditional landfill identification approaches, such as on-site\ninspections, are time-consuming and expensive. Remote sensing is a\ncost-effective solution for the identification and monitoring of solid waste\ndisposal sites that enables broad coverage and repeated acquisitions over time.\nEarth Observation (EO) satellites, equipped with an array of sensors and\nimaging capabilities, have been providing high-resolution data for several\ndecades. Researchers proposed specialized techniques that leverage remote\nsensing imagery to perform a range of tasks such as waste site detection,\ndumping site monitoring, and assessment of suitable locations for new\nlandfills. This review aims to provide a detailed illustration of the most\nrelevant proposals for the detection and monitoring of solid waste sites by\ndescribing and comparing the approaches, the implemented techniques, and the\nemployed data. Furthermore, since the data sources are of the utmost importance\nfor developing an effective solid waste detection model, a comprehensive\noverview of the satellites and publicly available data sets is presented.\nFinally, this paper identifies the open issues in the state-of-the-art and\ndiscusses the relevant research directions for reducing the costs and improving\nthe effectiveness of novel solid waste detection methods.\n","authors":["Piero Fraternali","Luca Morandini","Sergio Luis Herrera GonzÃ¡lez"],"pdf_url":"https://arxiv.org/pdf/2402.09066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15640v1","updated":"2024-08-28T08:52:14Z","published":"2024-08-28T08:52:14Z","title":"GANs Conditioning Methods: A Survey","summary":"  In recent years, Generative Adversarial Networks (GANs) have seen significant\nadvancements, leading to their widespread adoption across various fields. The\noriginal GAN architecture enables the generation of images without any specific\ncontrol over the content, making it an unconditional generation process.\nHowever, many practical applications require precise control over the generated\noutput, which has led to the development of conditional GANs (cGANs) that\nincorporate explicit conditioning to guide the generation process. cGANs extend\nthe original framework by incorporating additional information (conditions),\nenabling the generation of samples that adhere to that specific criteria.\nVarious conditioning methods have been proposed, each differing in how they\nintegrate the conditioning information into both the generator and the\ndiscriminator networks. In this work, we review the conditioning methods\nproposed for GANs, exploring the characteristics of each method and\nhighlighting their unique mechanisms and theoretical foundations. Furthermore,\nwe conduct a comparative analysis of these methods, evaluating their\nperformance on various image datasets. Through these analyses, we aim to\nprovide insights into the strengths and limitations of various conditioning\ntechniques, guiding future research and application in generative modeling.\n","authors":["Anis Bourou","Auguste Genovesio","ValÃ©rie Mezger"],"pdf_url":"https://arxiv.org/pdf/2408.15640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15633v1","updated":"2024-08-28T08:35:34Z","published":"2024-08-28T08:35:34Z","title":"Comparison of Model Predictive Control and Proximal Policy Optimization\n  for a 1-DOF Helicopter System","summary":"  This study conducts a comparative analysis of Model Predictive Control (MPC)\nand Proximal Policy Optimization (PPO), a Deep Reinforcement Learning (DRL)\nalgorithm, applied to a 1-Degree of Freedom (DOF) Quanser Aero 2 system.\nClassical control techniques such as MPC and Linear Quadratic Regulator (LQR)\nare widely used due to their theoretical foundation and practical\neffectiveness. However, with advancements in computational techniques and\nmachine learning, DRL approaches like PPO have gained traction in solving\noptimal control problems through environment interaction. This paper\nsystematically evaluates the dynamic response characteristics of PPO and MPC,\ncomparing their performance, computational resource consumption, and\nimplementation complexity. Experimental results show that while LQR achieves\nthe best steady-state accuracy, PPO excels in rise-time and adaptability,\nmaking it a promising approach for applications requiring rapid response and\nadaptability. Additionally, we have established a baseline for future\nRL-related research on this specific testbed. We also discuss the strengths and\nlimitations of each control strategy, providing recommendations for selecting\nappropriate controllers for real-world scenarios.\n","authors":["Georg SchÃ¤fer","Jakob Rehrl","Stefan Huber","Simon Hirlaender"],"pdf_url":"https://arxiv.org/pdf/2408.15633v1.pdf","comment":"Accepted at INDIN2024"},{"id":"http://arxiv.org/abs/2408.08454v2","updated":"2024-08-28T08:31:28Z","published":"2024-08-15T23:34:04Z","title":"Beyond Uniform Query Distribution: Key-Driven Grouped Query Attention","summary":"  The Transformer architecture has revolutionized deep learning through its\nSelf-Attention mechanism, which effectively captures contextual information.\nHowever, the memory footprint of Self-Attention presents significant challenges\nfor long-sequence tasks. Grouped Query Attention (GQA) addresses this issue by\ngrouping queries and mean-pooling the corresponding key-value heads - reducing\nthe number of overall parameters and memory requirements in a flexible manner\nwithout adversely compromising model accuracy. In this work, we introduce\nenhancements to GQA, focusing on two novel approaches that deviate from the\nstatic nature of grouping: Key-Distributed GQA (KDGQA) and Dynamic\nKey-Distributed GQA (DGQA), which leverage information from the norms of the\nkey heads to inform query allocation. Specifically, KDGQA looks at the ratios\nof the norms of the key heads during each forward pass, while DGQA examines the\nratios of the norms as they evolve through training. Additionally, we present\nPerturbed GQA (PGQA) as a case-study, which introduces variability in (static)\ngroup formation via subtracting noise from the attention maps. Our experiments\nwith up-trained Vision Transformers, for Image Classification on datasets such\nas CIFAR-10, CIFAR-100, Food101, and Tiny ImageNet, demonstrate the promise of\nthese variants in improving upon the original GQA through more informed and\nadaptive grouping mechanisms: specifically ViT-L experiences accuracy gains of\nup to 8% when utilizing DGQA in comparison to GQA and other variants. We\nfurther analyze the impact of the number of Key-Value Heads on performance,\nunderscoring the importance of utilizing query-key affinities. Code is\navailable on GitHub.\n","authors":["Zohaib Khan","Muhammad Khaquan","Omer Tafveez","Burhanuddin Samiwala","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2408.08454v2.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.15621v1","updated":"2024-08-28T08:22:21Z","published":"2024-08-28T08:22:21Z","title":"Convergent Differential Privacy Analysis for General Federated Learning:\n  the f-DP Perspective","summary":"  Federated learning (FL) is an efficient collaborative training paradigm\nextensively developed with a focus on local privacy protection, and\ndifferential privacy (DP) is a classical approach to capture and ensure the\nreliability of local privacy. The powerful cooperation of FL and DP provides a\npromising learning framework for large-scale private clients, juggling both\nprivacy securing and trustworthy learning. As the predominant algorithm of DP,\nthe noisy perturbation has been widely studied and incorporated into various\nfederated algorithms, theoretically proven to offer significant privacy\nprotections. However, existing analyses in noisy FL-DP mostly rely on the\ncomposition theorem and cannot tightly quantify the privacy leakage challenges,\nwhich is nearly tight for small numbers of communication rounds but yields an\narbitrarily loose and divergent bound under the large communication rounds.\nThis implies a counterintuitive judgment, suggesting that FL may not provide\nadequate privacy protection during long-term training. To further investigate\nthe convergent privacy and reliability of the FL-DP framework, in this paper,\nwe comprehensively evaluate the worst privacy of two classical methods under\nthe non-convex and smooth objectives based on the f-DP analysis, i.e.\nNoisy-FedAvg and Noisy-FedProx methods. With the aid of the\nshifted-interpolation technique, we successfully prove that the worst privacy\nof the Noisy-FedAvg method achieves a tight convergent lower bound. Moreover,\nin the Noisy-FedProx method, with the regularization of the proxy term, the\nworst privacy has a stable constant lower bound. Our analysis further provides\na solid theoretical foundation for the reliability of privacy protection in\nFL-DP. Meanwhile, our conclusions can also be losslessly converted to other\nclassical DP analytical frameworks, e.g. $(\\epsilon,\\delta)$-DP and\nR$\\acute{\\text{e}}$nyi-DP (RDP).\n","authors":["Yan Sun","Li Shen","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2408.15621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15620v1","updated":"2024-08-28T08:21:56Z","published":"2024-08-28T08:21:56Z","title":"CAPER: Enhancing Career Trajectory Prediction using Temporal Knowledge\n  Graph and Ternary Relationship","summary":"  The problem of career trajectory prediction (CTP) aims to predict one's\nfuture employer or job position. While several CTP methods have been developed\nfor this problem, we posit that none of these methods (1) jointly considers the\nmutual ternary dependency between three key units (i.e., user, position, and\ncompany) of a career and (2) captures the characteristic shifts of key units in\ncareer over time, leading to an inaccurate understanding of the job movement\npatterns in the labor market. To address the above challenges, we propose a\nnovel solution, named as CAPER, that solves the challenges via sophisticated\ntemporal knowledge graph (TKG) modeling. It enables the utilization of a\ngraph-structured knowledge base with rich expressiveness, effectively\npreserving the changes in job movement patterns. Furthermore, we devise an\nextrapolated career reasoning task on TKG for a realistic evaluation. The\nexperiments on a real-world career trajectory dataset demonstrate that CAPER\nconsistently and significantly outperforms four baselines, two recent TKG\nreasoning methods, and five state-of-the-art CTP methods in predicting one's\nfuture companies and positions-i.e., on average, yielding 6.80% and 34.58% more\naccurate predictions, respectively.\n","authors":["Yeon-Chang Lee","JaeHyun Lee","Michiharu Yamashita","Dongwon Lee","Sang-Wook Kim"],"pdf_url":"https://arxiv.org/pdf/2408.15620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15619v1","updated":"2024-08-28T08:20:05Z","published":"2024-08-28T08:20:05Z","title":"Large-Scale Demand Prediction in Urban Rail using Multi-Graph Inductive\n  Representation Learning","summary":"  With the expansion of cities over time, URT (Urban Rail Transit) networks\nhave also grown significantly. Demand prediction plays an important role in\nsupporting planning, scheduling, fleet management, and other operational\ndecisions. In this study, we propose an Origin-Destination (OD) demand\nprediction model called Multi-Graph Inductive Representation Learning\n(mGraphSAGE) for large-scale URT networks under operational uncertainties. Our\nmain contributions are twofold: we enhance prediction results while ensuring\nscalability for large networks by relying simultaneously on multiple graphs,\nwhere each OD pair is a node on a graph and distinct OD relationships, such as\ntemporal and spatial correlations; we show the importance of including\noperational uncertainties such as train delays and cancellations as inputs in\ndemand prediction for daily operations. The model is validated on three\ndifferent scales of the URT network in Copenhagen, Denmark. Experimental\nresults show that by leveraging information from neighboring ODs and learning\nnode representations via sampling and aggregation, mGraphSAGE is particularly\nsuitable for OD demand prediction in large-scale URT networks, outperforming\nreference machine learning methods. Furthermore, during periods with train\ncancellations and delays, the performance gap between mGraphSAGE and other\nmethods improves compared to normal operating conditions, demonstrating its\nability to leverage system reliability information for predicting OD demand\nunder uncertainty.\n","authors":["Dang Viet Anh Nguyen","J. Victor Flensburg","Fabrizio Cerreto","Bianca Pascariu","Paola Pellegrini","Carlos Lima Azevedo","Filipe Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2408.15619v1.pdf","comment":"18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.12961v3","updated":"2024-08-28T08:15:18Z","published":"2024-08-23T10:12:08Z","title":"Symplectic Bregman divergences","summary":"  We present a generalization of Bregman divergences in symplectic vector\nspaces that we term symplectic Bregman divergences. Symplectic Bregman\ndivergences are derived from a symplectic generalization of the Fenchel-Young\ninequality which relies on the notion of symplectic subdifferentials. The\nsymplectic Fenchel-Young inequality is obtained using the symplectic Fenchel\ntransform which is defined with respect to the symplectic form. Since\nsymplectic forms can be generically built from pairings of dual systems, we get\na generalization of Bregman divergences in dual systems obtained by equivalent\nsymplectic Bregman divergences. In particular, when the symplectic form is\nderived from an inner product, we show that the corresponding symplectic\nBregman divergences amount to ordinary Bregman divergences with respect to\ncomposite inner products. Some potential applications of symplectic divergences\nin geometric mechanics, information geometry, and learning dynamics in machine\nlearning are touched upon.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2408.12961v3.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.15609v1","updated":"2024-08-28T08:03:04Z","published":"2024-08-28T08:03:04Z","title":"Statistical QoS Provision in Business-Centric Networks","summary":"  More refined resource management and Quality of Service (QoS) provisioning is\na critical goal of wireless communication technologies. In this paper, we\npropose a novel Business-Centric Network (BCN) aimed at enabling scalable QoS\nprovisioning, based on a cross-layer framework that captures the relationship\nbetween application, transport parameters, and channels. We investigate both\ncontinuous flow and event-driven flow models, presenting key QoS metrics such\nas throughput, delay, and reliability. By jointly considering power and\nbandwidth allocation, transmission parameters, and AP network topology across\nlayers, we optimize weighted resource efficiency with statistical QoS\nprovisioning. To address the coupling among parameters, we propose a novel deep\nreinforcement learning (DRL) framework, which is Collaborative Optimization\namong Heterogeneous Actors with Experience Sharing (COHA-ES). Power and\nsub-channel (SC) Actors representing multiple APs are jointly optimized under\nthe unified guidance of a common critic. Additionally, we introduce a novel\nmultithreaded experience-sharing mechanism to accelerate training and enhance\nrewards. Extensive comparative experiments validate the effectiveness of our\nDRL framework in terms of convergence and efficiency. Moreover, comparative\nanalyses demonstrate the comprehensive advantages of the BCN structure in\nenhancing both spectral and energy efficiency.\n","authors":["Chang Wu","Yuang Chen","Hancheng Lu"],"pdf_url":"https://arxiv.org/pdf/2408.15609v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2408.15601v1","updated":"2024-08-28T07:49:29Z","published":"2024-08-28T07:49:29Z","title":"Grand canonical generative diffusion model for crystalline phases and\n  grain boundaries","summary":"  The diffusion model has emerged as a powerful tool for generating atomic\nstructures for materials science. This work calls attention to the deficiency\nof current particle-based diffusion models, which represent atoms as a point\ncloud, in generating even the simplest ordered crystalline structures. The\nproblem is attributed to particles being trapped in local minima during the\nscore-driven simulated annealing of the diffusion process, similar to the\nphysical process of force-driven simulated annealing. We develop a solution,\nthe grand canonical diffusion model, which adopts an alternative voxel-based\nrepresentation with continuous rather than fixed number of particles. The\nmethod is applied towards generation of several common crystalline phases as\nwell as the technologically important and challenging problem of grain boundary\nstructures.\n","authors":["Bo Lei","Enze Chen","Hyuna Kwon","Tim Hsu","Babak Sadigh","Vincenzo Lordi","Timofey Frolov","Fei Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.15601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15600v1","updated":"2024-08-28T07:48:39Z","published":"2024-08-28T07:48:39Z","title":"Exploring Selective Layer Fine-Tuning in Federated Learning","summary":"  Federated learning (FL) has emerged as a promising paradigm for fine-tuning\nfoundation models using distributed data in a privacy-preserving manner. Under\nlimited computational resources, clients often find it more practical to\nfine-tune a selected subset of layers, rather than the entire model, based on\ntheir task-specific data. In this study, we provide a thorough theoretical\nexploration of selective layer fine-tuning in FL, emphasizing a flexible\napproach that allows the clients to adjust their selected layers according to\ntheir local data and resources. We theoretically demonstrate that the layer\nselection strategy has a significant impact on model convergence in two\ncritical aspects: the importance of selected layers and the heterogeneous\nchoices across clients. Drawing from these insights, we further propose a\nstrategic layer selection method that utilizes local gradients and regulates\nlayer selections across clients. The extensive experiments on both image and\ntext datasets demonstrate the effectiveness of the proposed strategy compared\nwith several baselines, highlighting its advances in identifying critical\nlayers that adapt to the client heterogeneity and training dynamics in FL.\n","authors":["Yuchang Sun","Yuexiang Xie","Bolin Ding","Yaliang Li","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15593v1","updated":"2024-08-28T07:36:20Z","published":"2024-08-28T07:36:20Z","title":"Skills Regularized Task Decomposition for Multi-task Offline\n  Reinforcement Learning","summary":"  Reinforcement learning (RL) with diverse offline datasets can have the\nadvantage of leveraging the relation of multiple tasks and the common skills\nlearned across those tasks, hence allowing us to deal with real-world complex\nproblems efficiently in a data-driven way. In offline RL where only offline\ndata is used and online interaction with the environment is restricted, it is\nyet difficult to achieve the optimal policy for multiple tasks, especially when\nthe data quality varies for the tasks. In this paper, we present a skill-based\nmulti-task RL technique on heterogeneous datasets that are generated by\nbehavior policies of different quality. To learn the shareable knowledge across\nthose datasets effectively, we employ a task decomposition method for which\ncommon skills are jointly learned and used as guidance to reformulate a task in\nshared and achievable subtasks. In this joint learning, we use Wasserstein\nauto-encoder (WAE) to represent both skills and tasks on the same latent space\nand use the quality-weighted loss as a regularization term to induce tasks to\nbe decomposed into subtasks that are more consistent with high-quality skills\nthan others. To improve the performance of offline RL agents learned on the\nlatent space, we also augment datasets with imaginary trajectories relevant to\nhigh-quality skills for each task. Through experiments, we show that our\nmulti-task offline RL approach is robust to the mixed configurations of\ndifferent-quality datasets and it outperforms other state-of-the-art algorithms\nfor several robotic manipulation tasks and drone navigation tasks.\n","authors":["Minjong Yoo","Sangwoo Cho","Honguk Woo"],"pdf_url":"https://arxiv.org/pdf/2408.15593v1.pdf","comment":"12 pages, 5 figures, acceepted in NeurIPS 2022"},{"id":"http://arxiv.org/abs/2408.15591v1","updated":"2024-08-28T07:31:32Z","published":"2024-08-28T07:31:32Z","title":"VFLIP: A Backdoor Defense for Vertical Federated Learning via\n  Identification and Purification","summary":"  Vertical Federated Learning (VFL) focuses on handling vertically partitioned\ndata over FL participants. Recent studies have discovered a significant\nvulnerability in VFL to backdoor attacks which specifically target the distinct\ncharacteristics of VFL. Therefore, these attacks may neutralize existing\ndefense mechanisms designed primarily for Horizontal Federated Learning (HFL)\nand deep neural networks. In this paper, we present the first backdoor defense,\ncalled VFLIP, specialized for VFL. VFLIP employs the identification and\npurification techniques that operate at the inference stage, consequently\nimproving the robustness against backdoor attacks to a great extent. VFLIP\nfirst identifies backdoor-triggered embeddings by adopting a participant-wise\nanomaly detection approach. Subsequently, VFLIP conducts purification which\nremoves the embeddings identified as malicious and reconstructs all the\nembeddings based on the remaining embeddings. We conduct extensive experiments\non CIFAR10, CINIC10, Imagenette, NUS-WIDE, and BankMarketing to demonstrate\nthat VFLIP can effectively mitigate backdoor attacks in VFL.\nhttps://github.com/blingcho/VFLIP-esorics24\n","authors":["Yungi Cho","Woorim Han","Miseon Yu","Ho Bae","Yunheung Paek"],"pdf_url":"https://arxiv.org/pdf/2408.15591v1.pdf","comment":"Accepted by 29th European Symposium on Research in Computer Security\n  (ESORICS 2024)"},{"id":"http://arxiv.org/abs/2408.15590v1","updated":"2024-08-28T07:26:30Z","published":"2024-08-28T07:26:30Z","title":"Bayesian optimization of atomic structures with prior probabilities from\n  universal interatomic potentials","summary":"  The optimization of atomic structures plays a pivotal role in understanding\nand designing materials with desired properties. However, conventional methods\noften struggle with the formidable task of navigating the vast potential energy\nsurface, especially in high-dimensional spaces with numerous local minima.\nRecent advancements in machine learning-driven surrogate models offer a\npromising avenue for alleviating this computational burden. In this study, we\npropose a novel approach that combines the strengths of universal machine\nlearning potentials with a Bayesian approach of the GOFEE/BEACON framework. By\nleveraging the comprehensive chemical knowledge encoded in pretrained universal\nmachine learning potentials as a prior estimate of energy and forces, we enable\nthe Gaussian process to focus solely on capturing the intricate nuances of the\npotential energy surface. We demonstrate the efficacy of our approach through\ncomparative analyses across diverse systems, including periodic bulk materials,\nsurface structures, and a cluster.\n","authors":["Peder Lyngby","Casper Larsen","Karsten Wedel Jacobsen"],"pdf_url":"https://arxiv.org/pdf/2408.15590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15562v1","updated":"2024-08-28T06:28:01Z","published":"2024-08-28T06:28:01Z","title":"Boosting Lossless Speculative Decoding via Feature Sampling and Partial\n  Alignment Distillation","summary":"  Lossless speculative decoding accelerates target large language model (LLM)\ninference by employing a lightweight draft model for generating tree-structured\ncandidates, which are subsequently verified in parallel by the target LLM.\nCurrently, effective approaches leverage feature-level rather than token-level\nautoregression within the draft model to facilitate more straightforward\npredictions and enhanced knowledge distillation. In this paper, we reassess\nthese approaches and propose FSPAD (Feature Sampling and Partial Alignment\nDistillation for Lossless Speculative Decoding), which introduces two\nstraightforward and effective components within the existing framework to boost\nlossless speculative decoding. Firstly, FSPAD utilizes token embeddings to\nsample features of the target LLM in high-dimensional space before feeding them\ninto the draft model, due to the inherent uncertainty of the features\npreventing the draft model from obtaining the specific token output by the\ntarget LLM. Secondly, FSPAD introduces partial alignment distillation to weaken\nthe draft model's connection between features and logits, aiming to reduce the\nconflict between feature alignment and logit confidence during training. Our\nexperiments include both greedy and non-greedy decoding on the largest and\nsmallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in\nmulti-turn conversation, translation, summarization, question answering,\nmathematical reasoning, and retrieval-augmented generation. The results show\nthat FSPAD outperforms the state-of-the-art method across all the\naforementioned tasks and target LLMs.\n","authors":["Lujun Gui","Bin Xiao","Lei Su","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.15562v1.pdf","comment":"The work was not submitted to AAAI 2025"},{"id":"http://arxiv.org/abs/2405.07626v2","updated":"2024-08-28T06:18:28Z","published":"2024-05-13T10:37:50Z","title":"AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using\n  Large Language Models","summary":"  Detecting anomaly edges for dynamic graphs aims to identify edges\nsignificantly deviating from the normal pattern and can be applied in various\ndomains, such as cybersecurity, financial transactions and AIOps. With the\nevolving of time, the types of anomaly edges are emerging and the labeled\nanomaly samples are few for each type. Current methods are either designed to\ndetect randomly inserted edges or require sufficient labeled data for model\ntraining, which harms their applicability for real-world applications. In this\npaper, we study this problem by cooperating with the rich knowledge encoded in\nlarge language models(LLMs) and propose a method, namely AnomalyLLM. To align\nthe dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to\ngenerate the representations of edges and reprograms the edges using the\nprototypes of word embeddings. Along with the encoder, we design an in-context\nlearning framework that integrates the information of a few labeled samples to\nachieve few-shot anomaly detection. Experiments on four datasets reveal that\nAnomalyLLM can not only significantly improve the performance of few-shot\nanomaly detection, but also achieve superior results on new anomalies without\nany update of model parameters.\n","authors":["Shuo Liu","Di Yao","Lanting Fang","Zhetao Li","Wenbin Li","Kaiyu Feng","XiaoWen Ji","Jingping Bi"],"pdf_url":"https://arxiv.org/pdf/2405.07626v2.pdf","comment":"13pages"},{"id":"http://arxiv.org/abs/2408.15555v1","updated":"2024-08-28T06:08:46Z","published":"2024-08-28T06:08:46Z","title":"Latent Relationship Mining of Glaucoma Biomarkers: a TRI-LSTM based Deep\n  Learning","summary":"  In recently years, a significant amount of research has been conducted on\napplying deep learning methods for glaucoma classification and detection.\nHowever, the explainability of those established machine learning models\nremains a big concern. In this research, in contrast, we learn from cognitive\nscience concept and study how ophthalmologists judge glaucoma detection.\nSimulating experts' efforts, we propose a hierarchical decision making system,\ncentered around a holistic set of carefully designed biomarker-oriented machine\nlearning models. While biomarkers represent the key indicators of how\nophthalmologists identify glaucoma, they usually exhibit latent\ninter-relations. We thus construct a time series model, named TRI-LSTM, capable\nof calculating and uncovering potential and latent relationships among various\nbiomarkers of glaucoma. Our model is among the first efforts to explore the\nintrinsic connections among glaucoma biomarkers. We monitor temporal\nrelationships in patients' disease states over time and to capture and retain\nthe progression of disease-relevant clinical information from prior visits,\nthereby enriching biomarker's potential relationships. Extensive experiments\nover real-world dataset have demonstrated the effectiveness of the proposed\nmodel.\n","authors":["Cheng Huang","Junhao Shen","Qiuyu Luo","Karanjit Kooner","Tsengdar Lee","Yishen Liu","Jia Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15555v1.pdf","comment":"9 pages, 4 images"},{"id":"http://arxiv.org/abs/2408.15554v1","updated":"2024-08-28T06:07:58Z","published":"2024-08-28T06:07:58Z","title":"A Novel Denoising Technique and Deep Learning Based Hybrid Wind Speed\n  Forecasting Model for Variable Terrain Conditions","summary":"  Wind flow can be highly unpredictable and can suffer substantial fluctuations\nin speed and direction due to the shape and height of hills, mountains, and\nvalleys, making accurate wind speed (WS) forecasting essential in complex\nterrain. This paper presents a novel and adaptive model for short-term\nforecasting of WS. The paper's key contributions are as follows: (a) The\nPartial Auto Correlation Function (PACF) is utilised to minimise the dimension\nof the set of Intrinsic Mode Functions (IMF), hence reducing training time; (b)\nThe sample entropy (SampEn) was used to calculate the complexity of the reduced\nset of IMFs. The proposed technique is adaptive since a specific Deep Learning\n(DL) model-feature combination was chosen based on complexity; (c) A novel\nbidirectional feature-LSTM framework for complicated IMFs has been suggested,\nresulting in improved forecasting accuracy; (d) The proposed model shows\nsuperior forecasting performance compared to the persistence, hybrid, Ensemble\nempirical mode decomposition (EEMD), and Variational Mode Decomposition\n(VMD)-based deep learning models. It has achieved the lowest variance in terms\nof forecasting accuracy between simple and complex terrain conditions 0.70%.\nDimension reduction of IMF's and complexity-based model-feature selection helps\nreduce the training time by 68.77% and improve forecasting quality by 58.58% on\naverage.\n","authors":["Sourav Malakar","Saptarsi Goswami","Amlan Chakrabarti","Bhaswati Ganguli"],"pdf_url":"https://arxiv.org/pdf/2408.15554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15545v1","updated":"2024-08-28T05:41:52Z","published":"2024-08-28T05:41:52Z","title":"SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding","summary":"  Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.\n","authors":["Sihang Li","Jian Huang","Jiaxi Zhuang","Yaorui Shi","Xiaochen Cai","Mingjun Xu","Xiang Wang","Linfeng Zhang","Guolin Ke","Hengxing Cai"],"pdf_url":"https://arxiv.org/pdf/2408.15545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15535v1","updated":"2024-08-28T04:56:06Z","published":"2024-08-28T04:56:06Z","title":"Improving Thompson Sampling via Information Relaxation for Budgeted\n  Multi-armed Bandits","summary":"  We consider a Bayesian budgeted multi-armed bandit problem, in which each arm\nconsumes a different amount of resources when selected and there is a budget\nconstraint on the total amount of resources that can be used. Budgeted Thompson\nSampling (BTS) offers a very effective heuristic to this problem, but its\narm-selection rule does not take into account the remaining budget information.\nWe adopt \\textit{Information Relaxation Sampling} framework that generalizes\nThompson Sampling for classical $K$-armed bandit problems, and propose a series\nof algorithms that are randomized like BTS but more carefully optimize their\ndecisions with respect to the budget constraint. In a one-to-one correspondence\nwith these algorithms, a series of performance benchmarks that improve the\nconventional benchmark are also suggested. Our theoretical analysis and\nsimulation results show that our algorithms (and our benchmarks) make\nincremental improvements over BTS (respectively, the conventional benchmark)\nacross various settings including a real-world example.\n","authors":["Woojin Jeong","Seungki Min"],"pdf_url":"https://arxiv.org/pdf/2408.15535v1.pdf","comment":"accepted"},{"id":"http://arxiv.org/abs/2408.11293v2","updated":"2024-08-28T04:09:33Z","published":"2024-08-21T02:48:42Z","title":"ViIK: Flow-based Vision Inverse Kinematics Solver with Fusing Collision\n  Checking","summary":"  Inverse Kinematics (IK) is to find the robot's configurations that satisfy\nthe target pose of the end effector. In motion planning, diverse configurations\nwere required in case a feasible trajectory was not found. Meanwhile, collision\nchecking (CC), e.g. Oriented bounding box (OBB), Discrete Oriented Polytope\n(DOP), and Quickhull \\cite{quickhull}, needs to be done for each configuration\nprovided by the IK solver to ensure every goal configuration for motion\nplanning is available. This means the classical IK solver and CC algorithm\nshould be executed repeatedly for every configuration. Thus, the preparation\ntime is long when the required number of goal configurations is large, e.g.\nmotion planning in cluster environments. Moreover, structured maps, which might\nbe difficult to obtain, were required by classical collision-checking\nalgorithms. To sidestep such two issues, we propose a flow-based vision method\nthat can output diverse available configurations by fusing inverse kinematics\nand collision checking, named Vision Inverse Kinematics solver (ViIK).\nMoreover, ViIK uses RGB images as the perception of environments. ViIK can\noutput 1000 configurations within 40 ms, and the accuracy is about 3\nmillimeters and 1.5 degrees. The higher accuracy can be obtained by being\nrefined by the classical IK solver within a few iterations. The self-collision\nrates can be lower than 2%. The collision-with-env rates can be lower than 10%\nin most scenes. The code is available at: https://github.com/AdamQLMeng/ViIK.\n","authors":["Qinglong Meng","Chongkun Xia","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2408.11293v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15510v1","updated":"2024-08-28T03:45:49Z","published":"2024-08-28T03:45:49Z","title":"Measuring the Reliability of Causal Probing Methods: Tradeoffs,\n  Limitations, and the Plight of Nullifying Interventions","summary":"  Causal probing is an approach to interpreting foundation models, such as\nlarge language models, by training probes to recognize latent properties of\ninterest from embeddings, intervening on probes to modify this representation,\nand analyzing the resulting changes in the model's behavior. While some recent\nworks have cast doubt on the theoretical basis of several leading causal\nprobing intervention methods, it has been unclear how to systematically and\nempirically evaluate their effectiveness in practice. To address this problem,\nwe propose a general empirical analysis framework to evaluate the reliability\nof causal probing interventions, formally defining and quantifying two key\ncausal probing desiderata: completeness (fully transforming the representation\nof the target property) and selectivity (minimally impacting other properties).\nOur formalism allows us to make the first direct comparisons between different\nfamilies of causal probing methods (e.g., linear vs. nonlinear or\ncounterfactual vs. nullifying interventions). We conduct extensive experiments\nacross several leading methods, finding that (1) there is an inherent tradeoff\nbetween these criteria, and no method is able to consistently satisfy both at\nonce; and (2) across the board, nullifying interventions are always far less\ncomplete than counterfactual interventions, indicating that nullifying methods\nmay not be an effective approach to causal probing.\n","authors":["Marc Canby","Adam Davies","Chirag Rastogi","Julia Hockenmaier"],"pdf_url":"https://arxiv.org/pdf/2408.15510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14502v2","updated":"2024-08-28T03:22:53Z","published":"2024-08-24T02:04:12Z","title":"Physics-Informed Neural Network for Concrete Manufacturing Process\n  Optimization","summary":"  Concrete manufacturing projects are one of the most common ones for\nconsulting agencies. Because of the highly non-linear dependency of input\nmaterials like ash, water, cement, superplastic, etc; with the resultant\nstrength of concrete, it gets difficult for machine learning models to\nsuccessfully capture this relation and perform cost optimizations. This paper\nhighlights how PINNs (Physics Informed Neural Networks) can be useful in the\ngiven situation. This state-of-the-art model shall also get compared with\ntraditional models like Linear Regression, Random Forest, Gradient Boosting,\nand Deep Neural Network. Results of the research highlights how well PINNs\nperformed even with reduced dataset, thus resolving one of the biggest issues\nof limited data availability for ML models. On an average, PINN got the loss\nvalue reduced by 26.3% even with 40% lesser data compared to the Deep Neural\nNetwork. In addition to predicting strength of the concrete given the quantity\nof raw materials, the paper also highlights the use of heuristic optimization\nmethod like Particle Swarm Optimization (PSO) in predicting quantity of raw\nmaterials required to manufacture concrete of given strength with least cost.\n","authors":["Sam Varghese","Rahul Anand","Dr. Gaurav Paliwal"],"pdf_url":"https://arxiv.org/pdf/2408.14502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15501v1","updated":"2024-08-28T03:10:45Z","published":"2024-08-28T03:10:45Z","title":"MODULI: Unlocking Preference Generalization via Diffusion Models for\n  Offline Multi-Objective Reinforcement Learning","summary":"  Multi-objective Reinforcement Learning (MORL) seeks to develop policies that\nsimultaneously optimize multiple conflicting objectives, but it requires\nextensive online interactions. Offline MORL provides a promising solution by\ntraining on pre-collected datasets to generalize to any preference upon\ndeployment. However, real-world offline datasets are often conservatively and\nnarrowly distributed, failing to comprehensively cover preferences, leading to\nthe emergence of out-of-distribution (OOD) preference areas. Existing offline\nMORL algorithms exhibit poor generalization to OOD preferences, resulting in\npolicies that do not align with preferences. Leveraging the excellent\nexpressive and generalization capabilities of diffusion models, we propose\nMODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs\na preference-conditioned diffusion model as a planner to generate trajectories\nthat align with various preferences and derive action for decision-making. To\nachieve accurate generation, MODULI introduces two return normalization methods\nunder diverse preferences for refining guidance. To further enhance\ngeneralization to OOD preferences, MODULI proposes a novel sliding guidance\nmechanism, which involves training an additional slider adapter to capture the\ndirection of preference changes. Incorporating the slider, it transitions from\nin-distribution (ID) preferences to generating OOD preferences, patching, and\nextending the incomplete Pareto front. Extensive experiments on the D4MORL\nbenchmark demonstrate that our algorithm outperforms state-of-the-art Offline\nMORL baselines, exhibiting excellent generalization to OOD preferences.\n","authors":["Yifu Yuan","Zhenrui Zheng","Zibin Dong","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2408.15501v1.pdf","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.15498v1","updated":"2024-08-28T03:00:43Z","published":"2024-08-28T03:00:43Z","title":"Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single\n  Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep\n  Feed-forward Neural Network","summary":"  While machine learning has advanced in medicine, its widespread use in\nclinical applications, especially in predicting breast cancer metastasis, is\nstill limited. We have been dedicated to constructing a DFNN model to predict\nbreast cancer metastasis n years in advance. However, the challenge lies in\nefficiently identifying optimal hyperparameter values through grid search,\ngiven the constraints of time and resources. Issues such as the infinite\npossibilities for continuous hyperparameters like l1 and l2, as well as the\ntime-consuming and costly process, further complicate the task. To address\nthese challenges, we developed Single Hyperparameter Grid Search (SHGS)\nstrategy, serving as a preselection method before grid search. Our experiments\nwith SHGS applied to DFNN models for breast cancer metastasis prediction focus\non analyzing eight target hyperparameters: epochs, batch size, dropout, L1, L2,\nlearning rate, decay, and momentum. We created three figures, each depicting\nthe experiment results obtained from three LSM-I-10-Plus-year datasets. These\nfigures illustrate the relationship between model performance and the target\nhyperparameter values. For each hyperparameter, we analyzed whether changes in\nthis hyperparameter would affect model performance, examined if there were\nspecific patterns, and explored how to choose values for the particular\nhyperparameter. Our experimental findings reveal that the optimal value of a\nhyperparameter is not only dependent on the dataset but is also significantly\ninfluenced by the settings of other hyperparameters. Additionally, our\nexperiments suggested some reduced range of values for a target hyperparameter,\nwhich may be helpful for low-budget grid search. This approach serves as a\nprior experience and foundation for subsequent use of grid search to enhance\nmodel performance.\n","authors":["Yijun Zhou","Om Arora-Jain","Xia Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.15498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15495v1","updated":"2024-08-28T02:45:41Z","published":"2024-08-28T02:45:41Z","title":"Remove Symmetries to Control Model Expressivity","summary":"  When symmetry is present in the loss function, the model is likely to be\ntrapped in a low-capacity state that is sometimes known as a \"collapse.\" Being\ntrapped in these low-capacity states can be a major obstacle to training across\nmany scenarios where deep learning technology is applied. We first prove two\nconcrete mechanisms through which symmetries lead to reduced capacities and\nignored features during training. We then propose a simple and theoretically\njustified algorithm, syre, to remove almost all symmetry-induced low-capacity\nstates in neural networks. The proposed method is shown to improve the training\nof neural networks in scenarios when this type of entrapment is especially a\nconcern. A remarkable merit of the proposed method is that it is model-agnostic\nand does not require any knowledge of the symmetry.\n","authors":["Liu Ziyin","Yizhou Xu","Isaac Chuang"],"pdf_url":"https://arxiv.org/pdf/2408.15495v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2402.16905v2","updated":"2024-08-28T02:37:08Z","published":"2024-02-24T21:36:26Z","title":"Procedural Adherence and Interpretability Through Neuro-Symbolic\n  Generative Agents","summary":"  The surge in popularity of large language models (LLMs) has opened doors for\nnew approaches to the creation of interactive agents. However, managing and\ninterpreting the temporal behavior of such agents over the course of a\npotentially infinite interaction remain challenging. The stateful, long-term\nhorizon reasoning required for coherent agent behavior does not fit well into\nthe LLM paradigm. We propose a combination of formal logic-based program\nsynthesis and LLM content generation to bring guarantees of procedural\nadherence and interpretability to generative agent behavior. To illustrate the\nbenefit of procedural adherence and interpretability, we use Temporal Stream\nLogic (TSL) to generate an automaton that enforces an interpretable, high-level\ntemporal structure on an agent. With the automaton tracking the context of the\ninteraction and making decisions to guide the conversation accordingly, we can\ndrive content generation in a way that allows the LLM to focus on a shorter\ncontext window. We evaluated our approach on different tasks involved in\ncreating an interactive agent specialized for generating\nchoose-your-own-adventure games. We found that over all of the tasks, an\nautomaton-enhanced agent with procedural guarantees achieves at least 96%\nadherence to its temporal constraints, whereas a purely LLM-based agent\ndemonstrates as low as 14.67% adherence.\n","authors":["Raven Rothkopf","Hannah Tongxin Zeng","Mark Santolucito"],"pdf_url":"https://arxiv.org/pdf/2402.16905v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2408.06452v2","updated":"2024-08-28T01:46:48Z","published":"2024-08-12T19:01:49Z","title":"Wireless Channel Aware Data Augmentation Methods for Deep Learning-Based\n  Indoor Localization","summary":"  Indoor localization is a challenging problem that - unlike outdoor\nlocalization - lacks a universal and robust solution. Machine Learning (ML),\nparticularly Deep Learning (DL), methods have been investigated as a promising\napproach. Although such methods bring remarkable localization accuracy, they\nheavily depend on the training data collected from the environment. The data\ncollection is usually a laborious and time-consuming task, but Data\nAugmentation (DA) can be used to alleviate this issue. In this paper, different\nfrom previously used DA, we propose methods that utilize the domain knowledge\nabout wireless propagation channels and devices. The methods exploit the\ntypical hardware component drift in the transceivers and/or the statistical\nbehavior of the channel, in combination with the measured Power Delay Profile\n(PDP). We comprehensively evaluate the proposed methods to demonstrate their\neffectiveness. This investigation mainly focuses on the impact of factors such\nas the number of measurements, augmentation proportion, and the environment of\ninterest impact the effectiveness of the different DA methods. We show that in\nthe low-data regime (few actual measurements available), localization accuracy\nincreases up to 50%, matching non-augmented results in the high-data regime. In\naddition, the proposed methods may outperform the measurement-only high-data\nperformance by up to 33% using only 1/4 of the amount of measured data. We also\nexhibit the effect of different training data distribution and quality on the\neffectiveness of DA. Finally, we demonstrate the power of the proposed methods\nwhen employed along with Transfer Learning (TL) to address the data scarcity in\ntarget and/or source environments.\n","authors":["Omer Gokalp Serbetci","Daoud Burghal","Andreas F. Molisch"],"pdf_url":"https://arxiv.org/pdf/2408.06452v2.pdf","comment":"13 pages, 14 figures"},{"id":"http://arxiv.org/abs/2210.17230v4","updated":"2024-08-28T01:37:40Z","published":"2022-10-31T11:15:48Z","title":"Lipschitz-regularized gradient flows and generative particle algorithms\n  for high-dimensional scarce data","summary":"  We build a new class of generative algorithms capable of efficiently learning\nan arbitrary target distribution from possibly scarce, high-dimensional data\nand subsequently generate new samples. These generative algorithms are\nparticle-based and are constructed as gradient flows of Lipschitz-regularized\nKullback-Leibler or other $f$-divergences, where data from a source\ndistribution can be stably transported as particles, towards the vicinity of\nthe target distribution. As a highlighted result in data integration, we\ndemonstrate that the proposed algorithms correctly transport gene expression\ndata points with dimension exceeding 54K, while the sample size is typically\nonly in the hundreds.\n","authors":["Hyemin Gu","Panagiota Birmpa","Yannis Pantazis","Luc Rey-Bellet","Markos A. Katsoulakis"],"pdf_url":"https://arxiv.org/pdf/2210.17230v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15462v1","updated":"2024-08-28T00:56:03Z","published":"2024-08-28T00:56:03Z","title":"CTRQNets & LQNets: Continuous Time Recurrent and Liquid Quantum Neural\n  Networks","summary":"  Neural networks have continued to gain prevalence in the modern era for their\nability to model complex data through pattern recognition and behavior\nremodeling. However, the static construction of traditional neural networks\ninhibits dynamic intelligence. This makes them inflexible to temporal changes\nin data and unfit to capture complex dependencies. With the advent of quantum\ntechnology, there has been significant progress in creating quantum algorithms.\nIn recent years, researchers have developed quantum neural networks that\nleverage the capabilities of qubits to outperform classical networks. However,\ntheir current formulation exhibits a static construction limiting the system's\ndynamic intelligence. To address these weaknesses, we develop a Liquid Quantum\nNeural Network (LQNet) and a Continuous Time Recurrent Quantum Neural Network\n(CTRQNet). Both models demonstrate a significant improvement in accuracy\ncompared to existing quantum neural networks (QNNs), achieving accuracy\nincreases as high as 40\\% on CIFAR 10 through binary classification. We propose\nLQNets and CTRQNets might shine a light on quantum machine learning's black\nbox.\n","authors":["Alejandro Mayorga","Alexander Yuan","Andrew Yuan","Tyler Wooldridge","Xiaodi Wang"],"pdf_url":"https://arxiv.org/pdf/2408.15462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15458v1","updated":"2024-08-28T00:47:55Z","published":"2024-08-28T00:47:55Z","title":"PersonalizedUS: Interpretable Breast Cancer Risk Assessment with Local\n  Coverage Uncertainty Quantification","summary":"  Correctly assessing the malignancy of breast lesions identified during\nultrasound examinations is crucial for effective clinical decision-making.\nHowever, the current \"golden standard\" relies on manual BI-RADS scoring by\nclinicians, often leading to unnecessary biopsies and a significant mental\nhealth burden on patients and their families. In this paper, we introduce\nPersonalizedUS, an interpretable machine learning system that leverages recent\nadvances in conformal prediction to provide precise and personalized risk\nestimates with local coverage guarantees and sensitivity, specificity, and\npredictive values above 0.9 across various threshold levels. In particular, we\nidentify meaningful lesion subgroups where distribution-free, model-agnostic\nconditional coverage holds, with approximately 90% of our prediction sets\ncontaining only the ground truth in most lesion subgroups, thus explicitly\ncharacterizing for which patients the model is most suitably applied. Moreover,\nwe make available a curated tabular dataset of 1936 biopsied breast lesions\nfrom a recent observational multicenter study and benchmark the performance of\nseveral state-of-the-art learning algorithms. We also report a successful case\nstudy of the deployed system in the same multicenter context. Concrete clinical\nbenefits include up to a 65% reduction in requested biopsies among BI-RADS 4a\nand 4b lesions, with minimal to no missed cancer cases.\n","authors":["Alek FrÃ¶hlich","Thiago Ramos","Gustavo Cabello","Isabela Buzatto","Rafael Izbicki","Daniel Tiezzi"],"pdf_url":"https://arxiv.org/pdf/2408.15458v1.pdf","comment":"9 pages, 5 figure, 2 tables"},{"id":"http://arxiv.org/abs/2303.11789v8","updated":"2024-08-28T00:28:46Z","published":"2023-03-20T08:37:08Z","title":"Decentralized Online Learning for Random Inverse Problems Over Graphs","summary":"  We propose a decentralized online learning algorithm for distributed random\ninverse problems over network graphs with online measurements, and unifies the\ndistributed parameter estimation in Hilbert spaces and the least mean square\nproblem in reproducing kernel Hilbert spaces (RKHS-LMS). We transform the\nconvergence of the algorithm into the asymptotic stability of a class of\ninhomogeneous random difference equations in Hilbert spaces with\n$L_{2}$-bounded martingale difference terms and develop the $L_2$-asymptotic\nstability theory in Hilbert spaces. We show that if the network graph is\nconnected and the sequence of forward operators satisfies the\ninfinite-dimensional spatio-temporal persistence of excitation condition, then\nthe estimates of all nodes are mean square and almost surely strongly\nconsistent. Moreover, we propose a decentralized online learning algorithm in\nRKHS based on non-stationary online data streams, and prove that the algorithm\nis mean square and almost surely strongly consistent if the operators induced\nby the random input data satisfy the infinite-dimensional spatio-temporal\npersistence of excitation condition.\n","authors":["Tao Li","Xiwei Zhang","Yan Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11789v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13479v2","updated":"2024-08-28T00:19:50Z","published":"2024-08-24T05:38:31Z","title":"Quantum-machine-assisted Drug Discovery: Survey and Perspective","summary":"  Drug discovery and development is a highly complex and costly endeavor,\ntypically requiring over a decade and substantial financial investment to bring\na new drug to market. Traditional computer-aided drug design (CADD) has made\nsignificant progress in accelerating this process, but the development of\nquantum computing offers potential due to its unique capabilities. This paper\ndiscusses the integration of quantum computing into drug discovery and\ndevelopment, focusing on how quantum technologies might accelerate and enhance\nvarious stages of the drug development cycle. Specifically, we explore the\napplication of quantum computing in addressing challenges related to drug\ndiscovery, such as molecular simulation and the prediction of drug-target\ninteractions, as well as the optimization of clinical trial outcomes. By\nleveraging the inherent capabilities of quantum computing, we might be able to\nreduce the time and cost associated with bringing new drugs to market,\nultimately benefiting public health.\n","authors":["Yidong Zhou","Jintai Chen","Jinglei Cheng","Gopal Karemore","Marinka Zitnik","Frederic T. Chong","Junyu Liu","Tianfan Fu","Zhiding Liang"],"pdf_url":"https://arxiv.org/pdf/2408.13479v2.pdf","comment":"27 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.15451v1","updated":"2024-08-28T00:14:09Z","published":"2024-08-28T00:14:09Z","title":"Certified Causal Defense with Generalizable Robustness","summary":"  While machine learning models have proven effective across various scenarios,\nit is widely acknowledged that many models are vulnerable to adversarial\nattacks. Recently, there have emerged numerous efforts in adversarial defense.\nAmong them, certified defense is well known for its theoretical guarantees\nagainst arbitrary adversarial perturbations on input within a certain range\n(e.g., $l_2$ ball). However, most existing works in this line struggle to\ngeneralize their certified robustness in other data domains with distribution\nshifts. This issue is rooted in the difficulty of eliminating the negative\nimpact of spurious correlations on robustness in different domains. To address\nthis problem, in this work, we propose a novel certified defense framework\nGLEAN, which incorporates a causal perspective into the generalization problem\nin certified defense. More specifically, our framework integrates a certifiable\ncausal factor learning component to disentangle the causal relations and\nspurious correlations between input and label, and thereby exclude the negative\neffect of spurious correlations on defense. On top of that, we design a\ncausally certified defense strategy to handle adversarial attacks on latent\ncausal factors. In this way, our framework is not only robust against malicious\nnoises on data in the training distribution but also can generalize its\nrobustness across domains with distribution shifts. Extensive experiments on\nbenchmark datasets validate the superiority of our framework in certified\nrobustness generalization in different data domains. Code is available in the\nsupplementary materials.\n","authors":["Yiran Qiao","Yu Yin","Chen Chen","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2408.15451v1.pdf","comment":"Submitted to AAAI"},{"id":"http://arxiv.org/abs/2408.15450v1","updated":"2024-08-28T00:07:51Z","published":"2024-08-28T00:07:51Z","title":"Avoiding Generative Model Writer's Block With Embedding Nudging","summary":"  Generative image models, since introduction, have become a global phenomenon.\nFrom new arts becoming possible to new vectors of abuse, many new capabilities\nhave become available. One of the challenging issues with generative models is\ncontrolling the generation process specially to prevent specific generations\nclasses or instances . There are several reasons why one may want to control\nthe output of generative models, ranging from privacy and safety concerns to\napplication limitations or user preferences\n  To address memorization and privacy challenges, there has been considerable\nresearch dedicated to filtering prompts or filtering the outputs of these\nmodels. What all these solutions have in common is that at the end of the day\nthey stop the model from producing anything, hence limiting the usability of\nthe model. In this paper, we propose a method for addressing this usability\nissue by making it possible to steer away from unwanted concepts (when detected\nin model's output) and still generating outputs. In particular we focus on the\nlatent diffusion image generative models and how one can prevent them to\ngenerate particular images while generating similar images with limited\noverhead.\n  We focus on mitigating issues like image memorization, demonstrating our\ntechnique's effectiveness through qualitative and quantitative evaluations. Our\nmethod successfully prevents the generation of memorized training images while\nmaintaining comparable image quality and relevance to the unmodified model.\n","authors":["Ali Zand","Milad Nasr"],"pdf_url":"https://arxiv.org/pdf/2408.15450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16170v1","updated":"2024-08-28T23:25:25Z","published":"2024-08-28T23:25:25Z","title":"CardBench: A Benchmark for Learned Cardinality Estimation in Relational\n  Databases","summary":"  Cardinality estimation is crucial for enabling high query performance in\nrelational databases. Recently learned cardinality estimation models have been\nproposed to improve accuracy but there is no systematic benchmark or datasets\nwhich allows researchers to evaluate the progress made by new learned\napproaches and even systematically develop new learned approaches. In this\npaper, we are releasing a benchmark, containing thousands of queries over 20\ndistinct real-world databases for learned cardinality estimation. In contrast\nto other initial benchmarks, our benchmark is much more diverse and can be used\nfor training and testing learned models systematically. Using this benchmark,\nwe explored whether learned cardinality estimation can be transferred to an\nunseen dataset in a zero-shot manner. We trained GNN-based and\ntransformer-based models to study the problem in three setups: 1-)\ninstance-based, 2-) zero-shot, and 3-) fine-tuned. Our results show that while\nwe get promising results for zero-shot cardinality estimation on simple single\ntable queries; as soon as we add joins, the accuracy drops. However, we show\nthat with fine-tuning, we can still utilize pre-trained models for cardinality\nestimation, significantly reducing training overheads compared to instance\nspecific models. We are open sourcing our scripts to collect statistics,\ngenerate queries and training datasets to foster more extensive research, also\nfrom the ML community on the important problem of cardinality estimation and in\nparticular improve on recent directions such as pre-trained cardinality\nestimation.\n","authors":["Yannis Chronis","Yawen Wang","Yu Gan","Sami Abu-El-Haija","Chelsea Lin","Carsten Binnig","Fatma Ãzcan"],"pdf_url":"https://arxiv.org/pdf/2408.16170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16169v1","updated":"2024-08-28T23:20:17Z","published":"2024-08-28T23:20:17Z","title":"Simulating realistic short tandem repeat capillary electrophoretic\n  signal using a generative adversarial network","summary":"  DNA profiles are made up from multiple series of electrophoretic signal\nmeasuring fluorescence over time. Typically, human DNA analysts 'read' DNA\nprofiles using their experience to distinguish instrument noise, artefactual\nsignal, and signal corresponding to DNA fragments of interest. Recent work has\ndeveloped an artificial neural network, ANN, to carry out the task of\nclassifying fluorescence types into categories in DNA profile electrophoretic\nsignal. But the creation of the necessarily large amount of labelled training\ndata for the ANN is time consuming and expensive, and a limiting factor in the\nability to robustly train the ANN. If realistic, prelabelled, training data\ncould be simulated then this would remove the barrier to training an ANN with\nhigh efficacy. Here we develop a generative adversarial network, GAN, modified\nfrom the pix2pix GAN to achieve this task. With 1078 DNA profiles we train the\nGAN and achieve the ability to simulate DNA profile information, and then use\nthe generator from the GAN as a 'realism filter' that applies the noise and\nartefact elements exhibited in typical electrophoretic signal.\n","authors":["Duncan Taylor","Melissa Humphries"],"pdf_url":"https://arxiv.org/pdf/2408.16169v1.pdf","comment":"29 pages, 9 Figures"},{"id":"http://arxiv.org/abs/2408.16168v1","updated":"2024-08-28T23:20:03Z","published":"2024-08-28T23:20:03Z","title":"LeMON: Learning to Learn Multi-Operator Networks","summary":"  Single-operator learning involves training a deep neural network to learn a\nspecific operator, whereas recent work in multi-operator learning uses an\noperator embedding structure to train a single neural network on data from\nmultiple operators. Thus, multi-operator learning is capable of predicting a\nrange of operators within one model. In this work, we propose pretraining and\nfine-tuning strategies for solving PDEs using multi-operator learning. One key\naspect is that by increasing the number of families of operators used in\npretraining, a PDE foundation model can be fine-tuned to downstream tasks\ninvolving new PDEs with a limited number of samples, thus outperforming single\noperator neural networks. Specifically, a multi-operator learning model\npre-trained with data from diverse PDE families can predict unseen operators\nafter fine-tuning with only a limited number of operators from the new family,\nenabling them to serve as a data-free PDE solver. We also show that the\nproposed training and fine-tuning method is able to predict new operators in\nzero-shot prediction without samples. Additionally, we introduce a PDE-agnostic\nmeta-learning algorithm to improve the adaptability of the model to various\nPDEs by providing a better parameter initialization process. To address the\nneeds of applications with limited computing resources, we explore low-rank\nadaptation methods that reduce computational costs while enhancing solver\naccuracy. Lastly, by examining the scaling law with respect to the number of\noperator families, we establish and highlight its potential for broad\nadaptation in PDE-solving tasks.\n","authors":["Jingmin Sun","Zecheng Zhang","Hayden Schaeffer"],"pdf_url":"https://arxiv.org/pdf/2408.16168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16167v1","updated":"2024-08-28T23:15:46Z","published":"2024-08-28T23:15:46Z","title":"Free Lunch in the Forest: Functionally-Identical Pruning of Boosted Tree\n  Ensembles","summary":"  Tree ensembles, including boosting methods, are highly effective and widely\nused for tabular data. However, large ensembles lack interpretability and\nrequire longer inference times. We introduce a method to prune a tree ensemble\ninto a reduced version that is \"functionally identical\" to the original model.\nIn other words, our method guarantees that the prediction function stays\nunchanged for any possible input. As a consequence, this pruning algorithm is\nlossless for any aggregated metric. We formalize the problem of functionally\nidentical pruning on ensembles, introduce an exact optimization model, and\nprovide a fast yet highly effective method to prune large ensembles. Our\nalgorithm iteratively prunes considering a finite set of points, which is\nincrementally augmented using an adversarial model. In multiple computational\nexperiments, we show that our approach is a \"free lunch\", significantly\nreducing the ensemble size without altering the model's behavior. Thus, we can\npreserve state-of-the-art performance at a fraction of the original model's\nsize.\n","authors":["Youssouf Emine","Alexandre Forel","Idriss Malek","Thibaut Vidal"],"pdf_url":"https://arxiv.org/pdf/2408.16167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02325v2","updated":"2024-08-28T22:54:15Z","published":"2024-04-02T21:51:39Z","title":"Heat Death of Generative Models in Closed-Loop Learning","summary":"  Improvement and adoption of generative machine learning models is rapidly\naccelerating, as exemplified by the popularity of LLMs (Large Language Models)\nfor text, and diffusion models for image generation. As generative models\nbecome widespread, data they generate is incorporated into shared content\nthrough the public web. This opens the question of what happens when data\ngenerated by a model is fed back to the model in subsequent training campaigns.\nThis is a question about the stability of the training process, whether the\ndistribution of publicly accessible content, which we refer to as \"knowledge\",\nremains stable or collapses.\n  Small scale empirical experiments reported in the literature show that this\nclosed-loop training process is prone to degenerating. Models may start\nproducing gibberish data, or sample from only a small subset of the desired\ndata distribution (a phenomenon referred to as mode collapse). So far there has\nbeen only limited theoretical understanding of this process, in part due to the\ncomplexity of the deep networks underlying these generative models.\n  The aim of this paper is to provide insights into this process (that we refer\nto as \"generative closed-loop learning\") by studying the learning dynamics of\ngenerative models that are fed back their own produced content in addition to\ntheir original training dataset. The sampling of many of these models can be\ncontrolled via a \"temperature\" parameter. Using dynamical systems tools, we\nshow that, unless a sufficient amount of external data is introduced at each\niteration, any non-trivial temperature leads the model to asymptotically\ndegenerate. In fact, either the generative distribution collapses to a small\nset of outputs or becomes uniform over a large set of outputs.\n","authors":["Matteo Marchi","Stefano Soatto","Pratik Chaudhari","Paulo Tabuada"],"pdf_url":"https://arxiv.org/pdf/2404.02325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16160v1","updated":"2024-08-28T22:45:15Z","published":"2024-08-28T22:45:15Z","title":"CLPNets: Coupled Lie-Poisson Neural Networks for Multi-Part Hamiltonian\n  Systems with Symmetries","summary":"  To accurately compute data-based prediction of Hamiltonian systems,\nespecially the long-term evolution of such systems, it is essential to utilize\nmethods that preserve the structure of the equations over time. We consider a\ncase that is particularly challenging for data-based methods: systems with\ninteracting parts that do not reduce to pure momentum evolution. Such systems\nare essential in scientific computations. For example, any discretization of a\ncontinuum elastic rod can be viewed as interacting elements that can move and\nrotate in space, with each discrete element moving on the group of rotations\nand translations $SE(3)$.\n  We develop a novel method of data-based computation and complete phase space\nlearning of such systems. We follow the original framework of \\emph{SympNets}\n(Jin et al, 2020) building the neural network from canonical phase space\nmappings, and transformations that preserve the Lie-Poisson structure\n(\\emph{LPNets}) as in (Eldred et al, 2024). We derive a novel system of\nmappings that are built into neural networks for coupled systems. We call such\nnetworks Coupled Lie-Poisson Neural Networks, or \\emph{CLPNets}. We consider\nincreasingly complex examples for the applications of CLPNets: rotation of two\nrigid bodies about a common axis, the free rotation of two rigid bodies, and\nfinally the evolution of two connected and interacting $SE(3)$ components. Our\nmethod preserves all Casimir invariants of each system to machine precision,\nirrespective of the quality of the training data, and preserves energy to high\naccuracy. Our method also shows good resistance to the curse of dimensionality,\nrequiring only a few thousand data points for all cases studied, with the\neffective dimension varying from three to eighteen. Additionally, the method is\nhighly economical in memory requirements, requiring only about 200 parameters\nfor the most complex case considered.\n","authors":["Christopher Eldred","FranÃ§ois Gay-Balmaz","Vakhtang Putkaradze"],"pdf_url":"https://arxiv.org/pdf/2408.16160v1.pdf","comment":"52 pages, 9 figures"},{"id":"http://arxiv.org/abs/2305.10994v2","updated":"2024-08-28T22:22:29Z","published":"2023-05-18T14:14:42Z","title":"Graphical vs. Deep Generative Models: Measuring the Impact of\n  Differentially Private Mechanisms and Budgets on Utility","summary":"  Generative models trained with Differential Privacy (DP) can produce\nsynthetic data while reducing privacy risks. However, navigating their\nprivacy-utility tradeoffs makes finding the best models for specific\nsettings/tasks challenging. This paper bridges this gap by profiling how DP\ngenerative models for tabular data distribute privacy budgets across rows and\ncolumns, which is one of the primary sources of utility degradation. We compare\ngraphical and deep generative models, focusing on the key factors contributing\nto how privacy budgets are spent, i.e., underlying modeling techniques, DP\nmechanisms, and data dimensionality.\n  Through our measurement study, we shed light on the characteristics that make\ndifferent models suitable for various settings and tasks. For instance, we find\nthat graphical models distribute privacy budgets horizontally and thus cannot\nhandle relatively wide datasets for a fixed training time; also, the\nperformance on the task they were optimized for monotonically increases with\nmore data but could also overfit. Deep generative models spend their budgets\nper iteration, so their behavior is less predictable with varying dataset\ndimensions, but are more flexible as they could perform better if trained on\nmore features. Moreover, low levels of privacy ($\\epsilon\\geq100$) could help\nsome models generalize, achieving better results than without applying DP. We\nbelieve our work will aid the deployment of DP synthetic data techniques by\nnavigating through the best candidate models vis-a-vis the dataset features,\ndesired privacy levels, and downstream tasks.\n","authors":["Georgi Ganev","Kai Xu","Emiliano De Cristofaro"],"pdf_url":"https://arxiv.org/pdf/2305.10994v2.pdf","comment":"A shorter version of this paper appears in the Proceedings of the\n  31st ACM Conference on Computer and Communications Security (ACM CCS 2024).\n  This is the full version"},{"id":"http://arxiv.org/abs/2408.16154v1","updated":"2024-08-28T22:14:44Z","published":"2024-08-28T22:14:44Z","title":"Does Data-Efficient Generalization Exacerbate Bias in Foundation Models?","summary":"  Foundation models have emerged as robust models with label efficiency in\ndiverse domains. In medical imaging, these models contribute to the advancement\nof medical diagnoses due to the difficulty in obtaining labeled data. However,\nit is unclear whether using a large amount of unlabeled data, biased by the\npresence of sensitive attributes during pre-training, influences the fairness\nof the model. This research examines the bias in the Foundation model\n(RetFound) when it is applied to fine-tune the Brazilian Multilabel\nOphthalmological Dataset (BRSET), which has a different population than the\npre-training dataset. The model evaluation, in comparison with supervised\nlearning, shows that the Foundation Model has the potential to reduce the gap\nbetween the maximum AUC and minimum AUC evaluations across gender and age\ngroups. However, in a data-efficient generalization, the model increases the\nbias when the data amount decreases. These findings suggest that when deploying\na Foundation Model in real-life scenarios with limited data, the possibility of\nfairness issues should be considered.\n","authors":["Dilermando Queiroz","Anderson Carlos","MaÃ­ra Fatoretto","AndrÃ© Anjos","Lilian Berton","Luis Filipe Nakayama"],"pdf_url":"https://arxiv.org/pdf/2408.16154v1.pdf","comment":"Preprint of paper to be presented at Fairness and Ethics Towards\n  Transparent AI: Facing the Challenge through Model Debiasing (FAILED) during\n  ECCV 2024"},{"id":"http://arxiv.org/abs/2408.16147v1","updated":"2024-08-28T21:28:45Z","published":"2024-08-28T21:28:45Z","title":"Improving the Prediction of Individual Engagement in Recommendations\n  Using Cognitive Models","summary":"  For public health programs with limited resources, the ability to predict how\nbehaviors change over time and in response to interventions is crucial for\ndeciding when and to whom interventions should be allocated. Using data from a\nreal-world maternal health program, we demonstrate how a cognitive model based\non Instance-Based Learning (IBL) Theory can augment existing purely\ncomputational approaches. Our findings show that, compared to general\ntime-series forecasters (e.g., LSTMs), IBL models, which reflect human\ndecision-making processes, better predict the dynamics of individuals' states.\nAdditionally, IBL provides estimates of the volatility in individuals' states\nand their sensitivity to interventions, which can improve the efficiency of\ntraining of other time series models.\n","authors":["Roderick Seow","Yunfan Zhao","Duncan Wood","Milind Tambe","Cleotilde Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2408.16147v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.14485v6","updated":"2024-08-28T17:08:55Z","published":"2024-06-20T16:48:14Z","title":"Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)","summary":"  This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.\n","authors":["Nick Bryan-Kinns","Corey Ford","Shuoyang Zheng","Helen Kennedy","Alan Chamberlain","Makayla Lewis","Drew Hemment","Zijin Li","Qiong Wu","Lanxi Xiao","Gus Xia","Jeba Rezwana","Michael Clemens","Gabriel Vigliensoni"],"pdf_url":"https://arxiv.org/pdf/2406.14485v6.pdf","comment":"Proceedings of The second international workshop on eXplainable AI\n  for the Arts (XAIxArts)"},{"id":"http://arxiv.org/abs/2407.19976v2","updated":"2024-08-28T13:01:06Z","published":"2024-07-29T13:09:26Z","title":"MambaGesture: Enhancing Co-Speech Gesture Generation with Mamba and\n  Disentangled Multi-Modality Fusion","summary":"  Co-speech gesture generation is crucial for producing synchronized and\nrealistic human gestures that accompany speech, enhancing the animation of\nlifelike avatars in virtual environments. While diffusion models have shown\nimpressive capabilities, current approaches often overlook a wide range of\nmodalities and their interactions, resulting in less dynamic and contextually\nvaried gestures. To address these challenges, we present MambaGesture, a novel\nframework integrating a Mamba-based attention block, MambaAttn, with a\nmulti-modality feature fusion module, SEAD. The MambaAttn block combines the\nsequential data processing strengths of the Mamba model with the contextual\nrichness of attention mechanisms, enhancing the temporal coherence of generated\ngestures. SEAD adeptly fuses audio, text, style, and emotion modalities,\nemploying disentanglement to deepen the fusion process and yield gestures with\ngreater realism and diversity. Our approach, rigorously evaluated on the\nmulti-modal BEAT dataset, demonstrates significant improvements in Fr\\'echet\nGesture Distance (FGD), diversity scores, and beat alignment, achieving\nstate-of-the-art performance in co-speech gesture generation. Project website:\n$\\href{https://fcchit.github.io/mambagesture/}{\\textit{https://fcchit.github.io/mambagesture/}}$.\n","authors":["Chencan Fu","Yabiao Wang","Jiangning Zhang","Zhengkai Jiang","Xiaofeng Mao","Jiafu Wu","Weijian Cao","Chengjie Wang","Yanhao Ge","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2407.19976v2.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.11982v2","updated":"2024-08-28T11:01:16Z","published":"2024-08-21T20:32:45Z","title":"AIM 2024 Challenge on Compressed Video Quality Assessment: Methods and\n  Results","summary":"  Video quality assessment (VQA) is a crucial task in the development of video\ncompression standards, as it directly impacts the viewer experience. This paper\npresents the results of the Compressed Video Quality Assessment challenge, held\nin conjunction with the Advances in Image Manipulation (AIM) workshop at ECCV\n2024. The challenge aimed to evaluate the performance of VQA methods on a\ndiverse dataset of 459 videos, encoded with 14 codecs of various compression\nstandards (AVC/H.264, HEVC/H.265, AV1, and VVC/H.266) and containing a\ncomprehensive collection of compression artifacts. To measure the methods\nperformance, we employed traditional correlation coefficients between their\npredictions and subjective scores, which were collected via large-scale\ncrowdsourced pairwise human comparisons. For training purposes, participants\nwere provided with the Compressed Video Quality Assessment Dataset (CVQAD), a\npreviously developed dataset of 1022 videos. Up to 30 participating teams\nregistered for the challenge, while we report the results of 6 teams, which\nsubmitted valid final solutions and code for reproducing the results. Moreover,\nwe calculated and present the performance of state-of-the-art VQA methods on\nthe developed dataset, providing a comprehensive benchmark for future research.\nThe dataset, results, and online leaderboard are publicly available at\nhttps://challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.\n","authors":["Maksim Smirnov","Aleksandr Gushchin","Anastasia Antsiferova","Dmitry Vatolin","Radu Timofte","Ziheng Jia","Zicheng Zhang","Wei Sun","Jiaying Qian","Yuqin Cao","Yinan Sun","Yuxin Zhu","Xiongkuo Min","Guangtao Zhai","Kanjar De","Qing Luo","Ao-Xiang Zhang","Peng Zhang","Haibo Lei","Linyan Jiang","Yaqing Li","Wenhui Meng","Xiaoheng Tan","Haiqiang Wang","Xiaozhong Xu","Shan Liu","Zhenzhong Chen","Zhengxue Cheng","Jiahao Xiao","Jun Xu","Chenlong He","Qi Zheng","Ruoxi Zhu","Min Li","Yibo Fan","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2408.11982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15542v1","updated":"2024-08-28T05:34:14Z","published":"2024-08-28T05:34:14Z","title":"Kangaroo: A Powerful Video-Language Model Supporting Long-context Video\n  Input","summary":"  Rapid advancements have been made in extending Large Language Models (LLMs)\nto Large Multi-modal Models (LMMs). However, extending input modality of LLMs\nto video data remains a challenging endeavor, especially for long videos. Due\nto insufficient access to large-scale high-quality video data and the excessive\ncompression of visual features, current methods exhibit limitations in\neffectively processing long videos. In this paper, we introduce Kangaroo, a\npowerful Video LMM aimed at addressing these challenges. Confronted with issue\nof inadequate training data, we develop a data curation system to build a\nlarge-scale dataset with high-quality annotations for vision-language\npre-training and instruction tuning. In addition, we design a curriculum\ntraining pipeline with gradually increasing resolution and number of input\nframes to accommodate long videos. Evaluation results demonstrate that, with 8B\nparameters, Kangaroo achieves state-of-the-art performance across a variety of\nvideo understanding benchmarks while exhibiting competitive results on others.\nParticularly, on benchmarks specialized for long videos, Kangaroo excels some\nlarger models with over 10B parameters and proprietary models.\n","authors":["Jiajun Liu","Yibing Wang","Hanghang Ma","Xiaoping Wu","Xiaoqi Ma","Xiaoming Wei","Jianbin Jiao","Enhua Wu","Jie Hu"],"pdf_url":"https://arxiv.org/pdf/2408.15542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15521v1","updated":"2024-08-28T04:14:01Z","published":"2024-08-28T04:14:01Z","title":"A Simple Baseline with Single-encoder for Referring Image Segmentation","summary":"  Referring image segmentation (RIS) requires dense vision-language\ninteractions between visual pixels and textual words to segment objects based\non a given description. However, commonly adapted dual-encoders in RIS, e.g.,\nSwin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal\ndual-encoder), lack dense multi-modal interactions during pre-training, leading\nto a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods\noften rely on multi-modal fusion modules that interact two encoders, but this\napproach leads to high computational costs. In this paper, we present a novel\nRIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of\nshared self-attention across all framework components. This enables seamless\ninteractions of two modalities from input to final prediction, producing\ngranularly aligned multi-modal features. Furthermore, we propose lightweight\nyet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which\ncontribute to the high efficiency of our model. Our simple baseline with a\nsingle encoder achieves outstanding performances on the RIS benchmark datasets\nwhile maintaining computational efficiency, compared to the most recent SoTA\nmethods based on dual-encoders.\n","authors":["Seonghoon Yu","Ilchae Jung","Byeongju Han","Taeoh Kim","Yunho Kim","Dongyoon Wee","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2408.15521v1.pdf","comment":"ArXiv pre-print"},{"id":"http://arxiv.org/abs/2408.15461v1","updated":"2024-08-28T00:54:51Z","published":"2024-08-28T00:54:51Z","title":"Hand1000: Generating Realistic Hands from Text with Only 1,000 Images","summary":"  Text-to-image generation models have achieved remarkable advancements in\nrecent years, aiming to produce realistic images from textual descriptions.\nHowever, these models often struggle with generating anatomically accurate\nrepresentations of human hands. The resulting images frequently exhibit issues\nsuch as incorrect numbers of fingers, unnatural twisting or interlacing of\nfingers, or blurred and indistinct hands. These issues stem from the inherent\ncomplexity of hand structures and the difficulty in aligning textual\ndescriptions with precise visual depictions of hands. To address these\nchallenges, we propose a novel approach named Hand1000 that enables the\ngeneration of realistic hand images with target gesture using only 1,000\ntraining samples. The training of Hand1000 is divided into three stages with\nthe first stage aiming to enhance the model's understanding of hand anatomy by\nusing a pre-trained hand gesture recognition model to extract gesture\nrepresentation. The second stage further optimizes text embedding by\nincorporating the extracted hand gesture representation, to improve alignment\nbetween the textual descriptions and the generated hand images. The third stage\nutilizes the optimized embedding to fine-tune the Stable Diffusion model to\ngenerate realistic hand images. In addition, we construct the first publicly\navailable dataset specifically designed for text-to-hand image generation.\nBased on the existing hand gesture recognition dataset, we adopt advanced image\ncaptioning models and LLaMA3 to generate high-quality textual descriptions\nenriched with detailed gesture information. Extensive experiments demonstrate\nthat Hand1000 significantly outperforms existing models in producing\nanatomically correct hand images while faithfully representing other details in\nthe text, such as faces, clothing, and colors.\n","authors":["Haozhuo Zhang","Bin Zhu","Yu Cao","Yanbin Hao"],"pdf_url":"https://arxiv.org/pdf/2408.15461v1.pdf","comment":"Project page https://haozhuo-zhang.github.io/Hand1000-project-page/"},{"id":"http://arxiv.org/abs/2408.16132v1","updated":"2024-08-28T20:48:04Z","published":"2024-08-28T20:48:04Z","title":"SVDD 2024: The Inaugural Singing Voice Deepfake Detection Challenge","summary":"  With the advancements in singing voice generation and the growing presence of\nAI singers on media platforms, the inaugural Singing Voice Deepfake Detection\n(SVDD) Challenge aims to advance research in identifying AI-generated singing\nvoices from authentic singers. This challenge features two tracks: a controlled\nsetting track (CtrSVDD) and an in-the-wild scenario track (WildSVDD). The\nCtrSVDD track utilizes publicly available singing vocal data to generate\ndeepfakes using state-of-the-art singing voice synthesis and conversion\nsystems. Meanwhile, the WildSVDD track expands upon the existing SingFake\ndataset, which includes data sourced from popular user-generated content\nwebsites. For the CtrSVDD track, we received submissions from 47 teams, with 37\nsurpassing our baselines and the top team achieving a 1.65% equal error rate.\nFor the WildSVDD track, we benchmarked the baselines. This paper reviews these\nresults, discusses key findings, and outlines future directions for SVDD\nresearch.\n","authors":["You Zhang","Yongyi Zang","Jiatong Shi","Ryuichi Yamamoto","Tomoki Toda","Zhiyao Duan"],"pdf_url":"https://arxiv.org/pdf/2408.16132v1.pdf","comment":null}]},"2024-08-29T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2408.16768v1","updated":"2024-08-29T17:59:45Z","published":"2024-08-29T17:59:45Z","title":"SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners","summary":"  We introduce SAM2Point, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point\ninterprets any 3D data as a series of multi-directional videos, and leverages\nSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.\nOur framework supports various prompt types, including 3D points, boxes, and\nmasks, and can generalize across diverse scenarios, such as 3D objects, indoor\nscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple\n3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight\nthe robust generalization capabilities of SAM2Point. To our best knowledge, we\npresent the most faithful implementation of SAM in 3D, which may serve as a\nstarting point for future research in promptable 3D segmentation. Online Demo:\nhttps://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\nhttps://github.com/ZiyuGuo99/SAM2Point .\n","authors":["Ziyu Guo","Renrui Zhang","Xiangyang Zhu","Chengzhuo Tong","Peng Gao","Chunyuan Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2408.16768v1.pdf","comment":"Work in progress. Online Demo:\n  https://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\n  https://github.com/ZiyuGuo99/SAM2Point"},{"id":"http://arxiv.org/abs/2408.16756v1","updated":"2024-08-29T17:54:14Z","published":"2024-08-29T17:54:14Z","title":"How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of\n  Large Language Models","summary":"  The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment.\n","authors":["Jiyue Jiang","Liheng Chen","Pengan Chen","Sheng Wang","Qinghang Bao","Lingpeng Kong","Yu Li","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16753v1","updated":"2024-08-29T17:49:18Z","published":"2024-08-29T17:49:18Z","title":"Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning\n  of Large Language Models","summary":"  Reinforcement learning is used to align language models with human preference\nsignals after first pre-training the model to predict the next token of text\nwithin a large corpus using likelihood maximization. Before being deployed in a\nspecific domain, models are often further fine-tuned on task specific data.\nSince human preferences are often unavailable for the last step, it is\nperformed using likelihood maximization as that is the typical default method.\nHowever, reinforcement learning has other advantages besides facilitating\nalignment to a human derived reward function. For one, whereas likelihood\nmaximization is a form of imitation learning in which the model is trained on\nwhat to do under ideal conditions, reinforcement learning is not limited to\ndemonstrating actions just for optimally reached states and trains a model what\nto do under a range of scenarios as it explores the policy space. In addition,\nit also trains a model what not to do, suppressing competitive but poor\nactions. This work develops a framework for last-mile fine-tuning using\nreinforcement learning and tests whether it garners performance gains. The\nexperiments center on abstractive summarization, but the framework is general\nand broadly applicable. Use of the procedure produced significantly better\nresults than likelihood maximization when comparing raw predictions. For the\nspecific data tested, the gap could be bridged by employing post-processing of\nthe maximum likelihood outputs. Nonetheless, the framework offers a new avenue\nfor model optimization in situations where post-processing may be less\nstraightforward or effective, and it can be extended to include more complex\nclasses of undesirable outputs to penalize and train against, such as\nhallucinations.\n","authors":["Alec Solway"],"pdf_url":"https://arxiv.org/pdf/2408.16753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16751v1","updated":"2024-08-29T17:46:18Z","published":"2024-08-29T17:46:18Z","title":"A Gradient Analysis Framework for Rewarding Good and Penalizing Bad\n  Examples in Language Models","summary":"  Beyond maximum likelihood estimation (MLE), the standard objective of a\nlanguage model (LM) that optimizes good examples probabilities, many studies\nhave explored ways that also penalize bad examples for enhancing the quality of\noutput distribution, including unlikelihood training, exponential maximizing\naverage treatment effect (ExMATE), and direct preference optimization (DPO). To\nsystematically compare these methods and further provide a unified recipe for\nLM optimization, in this paper, we present a unique angle of gradient analysis\nof loss functions that simultaneously reward good examples and penalize bad\nones in LMs. Through both mathematical results and experiments on\nCausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functional\ncharacteristics among these methods. We find that ExMATE serves as a superior\nsurrogate for MLE, and that combining DPO with ExMATE instead of MLE further\nenhances both the statistical (5-7%) and generative (+18% win rate)\nperformance.\n","authors":["Yi-Lin Tuan","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16749v1","updated":"2024-08-29T17:43:03Z","published":"2024-08-29T17:43:03Z","title":"Assessing Large Language Models for Online Extremism Research:\n  Identification, Explanation, and New Knowledge","summary":"  The United States has experienced a significant increase in violent\nextremism, prompting the need for automated tools to detect and limit the\nspread of extremist ideology online. This study evaluates the performance of\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformers (GPT) in detecting and classifying online domestic\nextremist posts. We collected social media posts containing \"far-right\" and\n\"far-left\" ideological keywords and manually labeled them as extremist or\nnon-extremist. Extremist posts were further classified into one or more of five\ncontributing elements of extremism based on a working definitional framework.\nThe BERT model's performance was evaluated based on training data size and\nknowledge transfer between categories. We also compared the performance of GPT\n3.5 and GPT 4 models using different prompts: na\\\"ive, layperson-definition,\nrole-playing, and professional-definition. Results showed that the best\nperforming GPT models outperformed the best performing BERT models, with more\ndetailed prompts generally yielding better results. However, overly complex\nprompts may impair performance. Different versions of GPT have unique\nsensitives to what they consider extremist. GPT 3.5 performed better at\nclassifying far-left extremist posts, while GPT 4 performed better at\nclassifying far-right extremist posts. Large language models, represented by\nGPT models, hold significant potential for online extremism classification\ntasks, surpassing traditional BERT models in a zero-shot setting. Future\nresearch should explore human-computer interactions in optimizing GPT models\nfor extremist detection and classification tasks to develop more efficient\n(e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes)\nmethods for identifying extremist content.\n","authors":["Beidi Dong","Jin R. Lee","Ziwei Zhu","Balassubramanian Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2408.16749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16740v1","updated":"2024-08-29T17:34:10Z","published":"2024-08-29T17:34:10Z","title":"Theoretical and Methodological Framework for Studying Texts Produced by\n  Large Language Models","summary":"  This paper addresses the conceptual, methodological and technical challenges\nin studying large language models (LLMs) and the texts they produce from a\nquantitative linguistics perspective. It builds on a theoretical framework that\ndistinguishes between the LLM as a substrate and the entities the model\nsimulates. The paper advocates for a strictly non-anthropomorphic approach to\nmodels while cautiously applying methodologies used in studying human\nlinguistic behavior to the simulated entities. While natural language\nprocessing researchers focus on the models themselves, their architecture,\nevaluation, and methods for improving performance, we as quantitative linguists\nshould strive to build a robust theory concerning the characteristics of texts\nproduced by LLMs, how they differ from human-produced texts, and the properties\nof simulated entities. Additionally, we should explore the potential of LLMs as\nan instrument for studying human culture, of which language is an integral\npart.\n","authors":["JiÅÃ­ MiliÄka"],"pdf_url":"https://arxiv.org/pdf/2408.16740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16737v1","updated":"2024-08-29T17:32:35Z","published":"2024-08-29T17:32:35Z","title":"Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal\n  Sampling","summary":"  Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners.\n","authors":["Hritik Bansal","Arian Hosseini","Rishabh Agarwal","Vinh Q. Tran","Mehran Kazemi"],"pdf_url":"https://arxiv.org/pdf/2408.16737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16725v1","updated":"2024-08-29T17:18:53Z","published":"2024-08-29T17:18:53Z","title":"Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming","summary":"  Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.\n","authors":["Zhifei Xie","Changqiao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16725v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2408.15409v2","updated":"2024-08-29T17:00:24Z","published":"2024-08-27T21:19:37Z","title":"Awes, Laws, and Flaws From Today's LLM Research","summary":"  We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works based on criteria typical of what is considered good research\n(e.g. presence of statistical tests and reproducibility) and cross-validate it\nwith arguments that are at the centre of controversy (e.g., claims of emergent\nbehaviour, the use of LLMs as evaluators). We find multiple trends, such as\ndeclines in claims of emergent behaviour and ethics disclaimers; the rise of\nLLMs as evaluators in spite of a lack of consensus from the community about\ntheir useability; and an increase of claims of LLM reasoning abilities,\ntypically without leveraging human evaluation. This paper underscores the need\nfor more scrutiny and rigour by and from this field to live up to the\nfundamentals of a responsible scientific method that is ethical, reproducible,\nsystematic, and open to criticism.\n","authors":["Adrian de Wynter"],"pdf_url":"https://arxiv.org/pdf/2408.15409v2.pdf","comment":"Under review -- v1 was an old draft with an unrevised abstract (oops)"},{"id":"http://arxiv.org/abs/2405.11039v3","updated":"2024-08-29T16:57:38Z","published":"2024-05-17T18:31:26Z","title":"CC-GPX: Extracting High-Quality Annotated Geospatial Data from Common\n  Crawl","summary":"  The Common Crawl (CC) corpus is the largest open web crawl dataset containing\n9.5+ petabytes of data captured since 2008. The dataset is instrumental in\ntraining large language models, and as such it has been studied for\n(un)desirable content, and distilled for smaller, domain-specific datasets.\nHowever, to our knowledge, no research has been dedicated to using CC as a\nsource of annotated geospatial data. In this paper, we introduce an efficient\npipeline to extract annotated user-generated tracks from GPX files found in CC,\nand the resulting multimodal dataset with 1,416 pairings of human-written\ndescriptions and MultiLineString vector data from the 6 most recent CC\nreleases. The dataset can be used to study people's outdoor activity patterns,\nthe way people talk about their outdoor experiences, as well as for developing\ntrajectory generation or track annotation models, or for various other problems\nin place of synthetically generated routes. Our reproducible code is available\non GitHub: https://github.com/ilyankou/cc-gpx\n","authors":["Ilya Ilyankou","Meihui Wang","Stefano Cavazzi","James Haworth"],"pdf_url":"https://arxiv.org/pdf/2405.11039v3.pdf","comment":"Accepted as a poster to ACM SIGSPATIAL 2024"},{"id":"http://arxiv.org/abs/2406.04952v2","updated":"2024-08-29T16:49:29Z","published":"2024-06-07T14:16:37Z","title":"Quantifying Geospatial in the Common Crawl Corpus","summary":"  Large language models (LLMs) exhibit emerging geospatial capabilities,\nstemming from their pre-training on vast unlabelled text datasets that are\noften derived from the Common Crawl (CC) corpus. However, the geospatial\ncontent within CC remains largely unexplored, impacting our understanding of\nLLMs' spatial reasoning. This paper investigates the prevalence of geospatial\ndata in recent Common Crawl releases using Gemini 1.5, a powerful language\nmodel. By analyzing a sample of documents and manually revising the results, we\nestimate that 18.7% of web documents in CC contain geospatial information such\nas coordinates and addresses. We find little difference in prevalence between\nEnlgish- and non-English-language documents. Our findings provide quantitative\ninsights into the nature and extent of geospatial data in CC, and lay the\ngroundwork for future studies of geospatial biases of LLMs.\n","authors":["Ilya Ilyankou","Meihui Wang","Stefano Cavazzi","James Haworth"],"pdf_url":"https://arxiv.org/pdf/2406.04952v2.pdf","comment":"Accepted as a poster to ACM SIGSPATIAL 2024"},{"id":"http://arxiv.org/abs/2403.05527v3","updated":"2024-08-29T16:48:58Z","published":"2024-03-08T18:48:30Z","title":"GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM","summary":"  Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.\n","authors":["Hao Kang","Qingru Zhang","Souvik Kundu","Geonhwa Jeong","Zaoxing Liu","Tushar Krishna","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.05527v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16672v1","updated":"2024-08-29T16:21:00Z","published":"2024-08-29T16:21:00Z","title":"Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever","summary":"  Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis paper, we introduce several improvements to the ColBERT model architecture\nand training pipeline, leveraging techniques successful in the more established\nsingle-vector embedding model paradigm, particularly those suited for\nheterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates\nstrong performance across a range of English and multilingual retrieval tasks,\nwhile also cutting storage requirements by up to 50% compared to previous\nmodels.\n","authors":["Rohan Jha","Bo Wang","Michael GÃ¼nther","Saba Sturua","Mohammad Kalim Akram","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.16672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16667v1","updated":"2024-08-29T16:15:01Z","published":"2024-08-29T16:15:01Z","title":"Iterative Graph Alignment","summary":"  By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment.\n","authors":["Fangyuan Yu","Hardeep Singh Arora","Matt Johnson"],"pdf_url":"https://arxiv.org/pdf/2408.16667v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.04559v2","updated":"2024-08-29T15:58:09Z","published":"2024-07-05T14:48:15Z","title":"Not (yet) the whole story: Evaluating Visual Storytelling Requires More\n  than Measuring Coherence, Grounding, and Repetition","summary":"  Visual storytelling consists in generating a natural language story given a\ntemporally ordered sequence of images. This task is not only challenging for\nmodels, but also very difficult to evaluate with automatic metrics since there\nis no consensus about what makes a story 'good'. In this paper, we introduce a\nnovel method that measures story quality in terms of human likeness regarding\nthree key aspects highlighted in previous work: visual grounding, coherence,\nand repetitiveness. We then use this method to evaluate the stories generated\nby several models, showing that the foundation model LLaVA obtains the best\nresult, but only slightly so compared to TAPM, a 50-times smaller visual\nstorytelling model. Upgrading the visual and language components of TAPM\nresults in a model that yields competitive performance with a relatively low\nnumber of parameters. Finally, we carry out a human evaluation study, whose\nresults suggest that a 'good' story may require more than a human-like level of\nvisual grounding, coherence, and repetition.\n","authors":["Aditya K Surikuchi","Raquel FernÃ¡ndez","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2407.04559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14698v2","updated":"2024-08-29T15:14:48Z","published":"2024-08-26T23:52:27Z","title":"Smart Multi-Modal Search: Contextual Sparse and Dense Embedding\n  Integration in Adobe Express","summary":"  As user content and queries become increasingly multi-modal, the need for\neffective multi-modal search systems has grown. Traditional search systems\noften rely on textual and metadata annotations for indexed images, while\nmulti-modal embeddings like CLIP enable direct search using text and image\nembeddings. However, embedding-based approaches face challenges in integrating\ncontextual features such as user locale and recency. Building a scalable\nmulti-modal search system requires fine-tuning several components. This paper\npresents a multi-modal search architecture and a series of AB tests that\noptimize embeddings and multi-modal technologies in Adobe Express template\nsearch. We address considerations such as embedding model selection, the roles\nof embeddings in matching and ranking, and the balance between dense and sparse\nembeddings. Our iterative approach demonstrates how utilizing sparse, dense,\nand contextual features enhances short and long query search, significantly\nreduces null rates (over 70\\%), and increases click-through rates (CTR). Our\nfindings provide insights into developing robust multi-modal search systems,\nthereby enhancing relevance for complex queries.\n","authors":["Cherag Aroraa","Tracy Holloway King","Jayant Kumar","Yi Lu","Sanat Sharma","Arvind Srikantan","David Uvalle","Josep Valls-Vargas","Harsha Vardhan"],"pdf_url":"https://arxiv.org/pdf/2408.14698v2.pdf","comment":"CIKM 2024 (International Conference on Information and Knowledge\n  Management), Multimodal Search and Recommendations Workshop"},{"id":"http://arxiv.org/abs/2405.05418v2","updated":"2024-08-29T14:50:10Z","published":"2024-05-08T20:39:54Z","title":"Mitigating Exaggerated Safety in Large Language Models","summary":"  As the popularity of Large Language Models (LLMs) grow, combining model\nsafety with utility becomes increasingly important. The challenge is making\nsure that LLMs can recognize and decline dangerous prompts without sacrificing\ntheir ability to be helpful. The problem of \"exaggerated safety\" demonstrates\nhow difficult this can be. To reduce excessive safety behaviours -- which was\ndiscovered to be 26.1% of safe prompts being misclassified as dangerous and\nrefused -- we use a combination of XSTest dataset prompts as well as\ninteractive, contextual, and few-shot prompting to examine the decision bounds\nof LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot\nprompting works best for Llama2, interactive prompting works best Gemma, and\ncontextual prompting works best for Command R+ and Phi-3. Using a combination\nof these prompting strategies, we are able to mitigate exaggerated safety\nbehaviors by an overall 92.9% across all LLMs. Our work presents a multiple\nprompting strategies to jailbreak LLMs' decision-making processes, allowing\nthem to navigate the tight line between refusing unsafe prompts and remaining\nhelpful.\n","authors":["Ruchira Ray","Ruchi Bhalani"],"pdf_url":"https://arxiv.org/pdf/2405.05418v2.pdf","comment":"17 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.16586v1","updated":"2024-08-29T14:49:13Z","published":"2024-08-29T14:49:13Z","title":"Enhancing Dialogue Generation in Werewolf Game Through Situation\n  Analysis and Persuasion Strategies","summary":"  Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions.\n","authors":["Zhiyang Qi","Michimasa Inaba"],"pdf_url":"https://arxiv.org/pdf/2408.16586v1.pdf","comment":"Accepted to the AIWolfDial2024 workshop at INLG 2024"},{"id":"http://arxiv.org/abs/2406.11455v2","updated":"2024-08-29T14:48:10Z","published":"2024-06-17T12:11:01Z","title":"Adaptive Reinforcement Learning Planning: Harnessing Large Language\n  Models for Complex Information Extraction","summary":"  Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs.\n","authors":["Zepeng Ding","Ruiyang Ke","Wenhao Huang","Guochao Jiang","Yanda Li","Deqing Yang","Jiaqing Liang"],"pdf_url":"https://arxiv.org/pdf/2406.11455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15710v2","updated":"2024-08-29T14:47:37Z","published":"2024-08-28T11:18:06Z","title":"Conan-embedding: General Text Embedding with More and Better Negative\n  Samples","summary":"  With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark\n","authors":["Shiyu Li","Yang Tang","Shizhe Chen","Xi Chen"],"pdf_url":"https://arxiv.org/pdf/2408.15710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16570v1","updated":"2024-08-29T14:37:05Z","published":"2024-08-29T14:37:05Z","title":"Predictability maximization and the origins of word order harmony","summary":"  We address the linguistic problem of the sequential arrangement of a head and\nits dependents from an information theoretic perspective. In particular, we\nconsider the optimal placement of a head that maximizes the predictability of\nthe sequence. We assume that dependents are statistically independent given a\nhead, in line with the open-choice principle and the core assumptions of\ndependency grammar. We demonstrate the optimality of harmonic order, i.e.,\nplacing the head last maximizes the predictability of the head whereas placing\nthe head first maximizes the predictability of dependents. We also show that\npostponing the head is the optimal strategy to maximize its predictability\nwhile bringing it forward is the optimal strategy to maximize the\npredictability of dependents. We unravel the advantages of the strategy of\nmaximizing the predictability of the head over maximizing the predictability of\ndependents. Our findings shed light on the placements of the head adopted by\nreal languages or emerging in different kinds of experiments.\n","authors":["Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2408.16570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17844v2","updated":"2024-08-29T14:06:57Z","published":"2024-07-25T07:58:19Z","title":"Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease\n  Classification: A Systematic Review","summary":"  Parkinson's disease (PD), the second most prevalent neurodegenerative\ndisorder worldwide, frequently presents with early-stage speech impairments.\nRecent advancements in Artificial Intelligence (AI), particularly deep learning\n(DL), have significantly enhanced PD diagnosis through the analysis of speech\ndata. Nevertheless, the progress of research is restricted by the limited\navailability of publicly accessible speech-based PD datasets, primarily due to\nprivacy concerns. The goal of this systematic review is to explore the current\nlandscape of speech-based DL approaches for PD classification, based on 33\nscientific works published between 2020 and March 2024. We discuss their\navailable resources, capabilities, potential limitations, and issues related to\nbias, explainability, and privacy. Furthermore, this review provides an\noverview of publicly accessible speech-based datasets and open-source material\nfor PD. The DL approaches are categorized into end-to-end (E2E) learning,\ntransfer learning (TL) and deep acoustic features extraction (DAFE) approaches.\nAmong E2E approaches, Convolutional Neural Networks (CNNs) are prevalent,\nthough Transformers are increasingly popular. E2E approaches face challenges\nsuch as limited data and computational resources, especially with Transformers.\nTL addresses these issues by providing more robust PD diagnosis and better\ngeneralizability across languages. DAFE aims to improve the explainability and\ninterpretability of results by examining the specific effects of deep features\non both other DL approaches and more traditional machine learning (ML) methods.\nHowever, it often underperforms compared to E2E and TL approaches.\n","authors":["Lisanne van Gelderen","Cristian Tejedor-GarcÃ­a"],"pdf_url":"https://arxiv.org/pdf/2407.17844v2.pdf","comment":"Submitted in Applied Sciences - peer reviewed Open Access journal.\n  This research was funded by the NWO research programme AiNed Fellowship\n  Grants under the project Responsible AI for Voice Diagnostics (RAIVD) - grant\n  number NGF.1607.22.013"},{"id":"http://arxiv.org/abs/2402.01805v4","updated":"2024-08-29T14:05:44Z","published":"2024-02-02T09:45:33Z","title":"Can LLMs perform structured graph reasoning?","summary":"  Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT).\n","authors":["Palaash Agrawal","Shavak Vasania","Cheston Tan"],"pdf_url":"https://arxiv.org/pdf/2402.01805v4.pdf","comment":"International Conference on Pattern Recognition (ICPR), 2024"},{"id":"http://arxiv.org/abs/2408.16542v1","updated":"2024-08-29T14:00:57Z","published":"2024-08-29T14:00:57Z","title":"SALSA: Speedy ASR-LLM Synchronous Aggregation","summary":"  Harnessing pre-trained LLMs to improve ASR systems, particularly for\nlow-resource languages, is now an emerging area of research. Existing methods\nrange from using LLMs for ASR error correction to tightly coupled systems that\nreplace the ASR decoder with the LLM. These approaches either increase decoding\ntime or require expensive training of the cross-attention layers. We propose\nSALSA, which couples the decoder layers of the ASR to the LLM decoder, while\nsynchronously advancing both decoders. Such coupling is performed with a simple\nprojection of the last decoder state, and is thus significantly more training\nefficient than earlier approaches. A challenge of our proposed coupling is\nhandling the mismatch between the tokenizers of the LLM and ASR systems. We\nhandle this mismatch using cascading tokenization with respect to the LLM and\nASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS\nbenchmark, yielding substantial WER reductions of up to 38%.\n","authors":["Ashish Mittal","Darshan Prabhu","Sunita Sarawagi","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2408.16542v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.19307v2","updated":"2024-08-29T13:51:34Z","published":"2024-06-27T16:30:50Z","title":"The Odyssey of Commonsense Causality: From Foundational Benchmarks to\n  Cutting-Edge Reasoning","summary":"  Understanding commonsense causality is a unique mark of intelligence for\nhumans. It helps people understand the principles of the real world better and\nbenefits the decision-making process related to causation. For instance,\ncommonsense causality is crucial in judging whether a defendant's action causes\nthe plaintiff's loss in determining legal liability. Despite its significance,\na systematic exploration of this topic is notably lacking. Our comprehensive\nsurvey bridges this gap by focusing on taxonomies, benchmarks, acquisition\nmethods, qualitative reasoning, and quantitative measurements in commonsense\ncausality, synthesizing insights from over 200 representative articles. Our\nwork aims to provide a systematic overview, update scholars on recent\nadvancements, provide a pragmatic guide for beginners, and highlight promising\nfuture research directions in this vital field.\n","authors":["Shaobo Cui","Zhijing Jin","Bernhard SchÃ¶lkopf","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2406.19307v2.pdf","comment":"42 pages"},{"id":"http://arxiv.org/abs/2408.14874v2","updated":"2024-08-29T13:49:40Z","published":"2024-08-27T08:43:32Z","title":"Inverse-Q*: Token Level Reinforcement Learning for Aligning Large\n  Language Models Without Preference Data","summary":"  Reinforcement Learning from Human Feedback (RLHF) has proven effective in\naligning large language models with human intentions, yet it often relies on\ncomplex methodologies like Proximal Policy Optimization (PPO) that require\nextensive hyper-parameter tuning and present challenges in sample efficiency\nand stability. In this paper, we introduce Inverse-Q*, an innovative framework\nthat transcends traditional RL methods by optimizing token-level reinforcement\nlearning without the need for additional reward or value models. Inverse-Q*\nleverages direct preference optimization techniques but extends them by\nestimating the conditionally optimal policy directly from the model's\nresponses, facilitating more granular and flexible policy shaping. Our approach\nreduces reliance on human annotation and external supervision, making it\nespecially suitable for low-resource settings. We present extensive\nexperimental results demonstrating that Inverse-Q* not only matches but\npotentially exceeds the effectiveness of PPO in terms of convergence speed and\nthe alignment of model responses with human preferences. Our findings suggest\nthat Inverse-Q* offers a practical and robust alternative to conventional RLHF\napproaches, paving the way for more efficient and adaptable model training\napproaches.\n","authors":["Han Xia","Songyang Gao","Qiming Ge","Zhiheng Xi","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2408.14874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16518v1","updated":"2024-08-29T13:28:52Z","published":"2024-08-29T13:28:52Z","title":"CNIMA: A Universal Evaluation Framework and Automated Approach for\n  Assessing Second Language Dialogues","summary":"  We develop CNIMA (Chinese Non-Native Interactivity Measurement and\nAutomation), a Chinese-as-a-second-language labelled dataset with 10K\ndialogues. We annotate CNIMA using an evaluation framework -- originally\nintroduced for English-as-a-second-language dialogues -- that assesses\nmicro-level features (e.g.\\ backchannels) and macro-level interactivity labels\n(e.g.\\ topic management) and test the framework's transferability from English\nto Chinese. We found the framework robust across languages and revealed\nuniversal and language-specific relationships between micro-level and\nmacro-level features. Next, we propose an approach to automate the evaluation\nand find strong performance, creating a new tool for automated second language\nassessment. Our system can be adapted to other languages easily as it uses\nlarge language models and as such does not require large-scale annotated\ntraining data.\n","authors":["Rena Gao","Jingxuan Wu","Carsten Roever","Xuetong Wu","Jing Wu","Long Lv","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2408.16518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16502v1","updated":"2024-08-29T13:01:42Z","published":"2024-08-29T13:01:42Z","title":"LLMs vs Established Text Augmentation Techniques for Classification:\n  When do the Benefits Outweight the Costs?","summary":"  The generative large language models (LLMs) are increasingly being used for\ndata augmentation tasks, where text samples are LLM-paraphrased and then used\nfor classifier fine-tuning. However, a research that would confirm a clear\ncost-benefit advantage of LLMs over more established augmentation methods is\nlargely missing. To study if (and when) is the LLM-based augmentation\nadvantageous, we compared the effects of recent LLM augmentation methods with\nestablished ones on 6 datasets, 3 classifiers and 2 fine-tuning methods. We\nalso varied the number of seeds and collected samples to better explore the\ndownstream model accuracy space. Finally, we performed a cost-benefit analysis\nand show that LLM-based methods are worthy of deployment only when very small\nnumber of seeds is used. Moreover, in many cases, established methods lead to\nsimilar or better model accuracies.\n","authors":["Jan Cegin","Jakub Simko","Peter Brusilovsky"],"pdf_url":"https://arxiv.org/pdf/2408.16502v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.16493v1","updated":"2024-08-29T12:44:01Z","published":"2024-08-29T12:44:01Z","title":"Learning from Negative Samples in Generative Biomedical Entity Linking","summary":"  Generative models have become widely used in biomedical entity linking\n(BioEL) due to their excellent performance and efficient memory usage. However,\nthese models are usually trained only with positive samples--entities that\nmatch the input mention's identifier--and do not explicitly learn from hard\nnegative samples, which are entities that look similar but have different\nmeanings. To address this limitation, we introduce ANGEL (Learning from\nNegative Samples in Generative Biomedical Entity Linking), the first framework\nthat trains generative BioEL models using negative samples. Specifically, a\ngenerative model is initially trained to generate positive samples from the\nknowledge base for given input entities. Subsequently, both correct and\nincorrect outputs are gathered from the model's top-k predictions. The model is\nthen updated to prioritize the correct predictions through direct preference\noptimization. Our models fine-tuned with ANGEL outperform the previous best\nbaseline models by up to an average top-1 accuracy of 1.4% on five benchmarks.\nWhen incorporating our framework into pre-training, the performance improvement\nfurther increases to 1.7%, demonstrating its effectiveness in both the\npre-training and fine-tuning stages. Our code is available at\nhttps://github.com/dmis-lab/ANGEL.\n","authors":["Chanhwi Kim","Hyunjae Kim","Sihyeon Park","Jiwoo Lee","Mujeen Sung","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2408.16493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11512v2","updated":"2024-08-29T12:25:14Z","published":"2024-08-21T10:44:10Z","title":"IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation","summary":"  This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.\n","authors":["Baohao Liao","Christian Herold","Shahram Khadivi","Christof Monz"],"pdf_url":"https://arxiv.org/pdf/2408.11512v2.pdf","comment":"typo: 120K -> 12K vocabulary size"},{"id":"http://arxiv.org/abs/2408.16482v1","updated":"2024-08-29T12:18:04Z","published":"2024-08-29T12:18:04Z","title":"Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning","summary":"  Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries.\n","authors":["Rochelle Choenni","Ekaterina Shutova"],"pdf_url":"https://arxiv.org/pdf/2408.16482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16446v1","updated":"2024-08-29T11:19:57Z","published":"2024-08-29T11:19:57Z","title":"Is text normalization relevant for classifying medieval charters?","summary":"  This study examines the impact of historical text normalization on the\nclassification of medieval charters, specifically focusing on document dating\nand locating. Using a data set of Middle High German charters from a digital\narchive, we evaluate various classifiers, including traditional and\ntransformer-based models, with and without normalization. Our results indicate\nthat the given normalization minimally improves locating tasks but reduces\naccuracy for dating, implying that original texts contain crucial features that\nnormalization may obscure. We find that support vector machines and gradient\nboosting outperform other models, questioning the efficiency of transformers\nfor this use case. Results suggest a selective approach to historical text\nnormalization, emphasizing the significance of preserving some textual\ncharacteristics that are critical for classification tasks in document\nanalysis.\n","authors":["Florian Atzenhofer-Baumgartner","TamÃ¡s KovÃ¡cs"],"pdf_url":"https://arxiv.org/pdf/2408.16446v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections"},{"id":"http://arxiv.org/abs/2408.16444v1","updated":"2024-08-29T11:13:23Z","published":"2024-08-29T11:13:23Z","title":"SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a\n  Survey Section","summary":"  Document summarization is a task to shorten texts into concise and\ninformative summaries. This paper introduces a novel dataset designed for\nsummarizing multiple scientific articles into a section of a survey. Our\ncontributions are: (1) SurveySum, a new dataset addressing the gap in\ndomain-specific summarization tools; (2) two specific pipelines to summarize\nscientific articles into a section of a survey; and (3) the evaluation of these\npipelines using multiple metrics to compare their performance. Our results\nhighlight the importance of high-quality retrieval stages and the impact of\ndifferent configurations on the quality of generated summaries.\n","authors":["Leandro CarÃ­sio Fernandes","Gustavo Bartz Guedes","Thiago Soares Laitz","Thales Sales Almeida","Rodrigo Nogueira","Roberto Lotufo","Jayr Pereira"],"pdf_url":"https://arxiv.org/pdf/2408.16444v1.pdf","comment":"15 pages, 6 figures, 1 table. Submitted to BRACIS 2024"},{"id":"http://arxiv.org/abs/2408.16440v1","updated":"2024-08-29T11:05:54Z","published":"2024-08-29T11:05:54Z","title":"Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain","summary":"  Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics.\n","authors":["Miguel Rios"],"pdf_url":"https://arxiv.org/pdf/2408.16440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15496v2","updated":"2024-08-29T10:35:52Z","published":"2024-08-28T02:47:27Z","title":"ReMamba: Equip Mamba with Effective Long-Sequence Modeling","summary":"  While the Mamba architecture demonstrates superior inference efficiency and\ncompetitive performance on short-context natural language processing (NLP)\ntasks, empirical evidence suggests its capacity to comprehend long contexts is\nlimited compared to transformer-based models. In this study, we investigate the\nlong-context efficiency issues of the Mamba models and propose ReMamba, which\nenhances Mamba's ability to comprehend long contexts. ReMamba incorporates\nselective compression and adaptation techniques within a two-stage re-forward\nprocess, incurring minimal additional inference costs overhead. Experimental\nresults on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy,\nimproving over the baselines by 3.2 and 1.6 points, respectively, and attaining\nperformance almost on par with same-size transformer models.\n","authors":["Danlong Yuan","Jiahao Liu","Bei Li","Huishuai Zhang","Jingang Wang","Xunliang Cai","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.15496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11288v2","updated":"2024-08-29T10:10:55Z","published":"2024-04-17T11:52:47Z","title":"A Preference-driven Paradigm for Enhanced Translation with Large\n  Language Models","summary":"  Recent research has shown that large language models (LLMs) can achieve\nremarkable translation performance through supervised fine-tuning (SFT) using\nonly a small amount of parallel data. However, SFT simply instructs the model\nto imitate the reference translations at the token level, making it vulnerable\nto the noise present in the references. Hence, the assistance from SFT often\nreaches a plateau once the LLMs have achieved a certain level of translation\ncapability, and further increasing the size of parallel data does not provide\nadditional benefits. To overcome this plateau associated with imitation-based\nSFT, we propose a preference-based approach built upon the Plackett-Luce model.\nThe objective is to steer LLMs towards a more nuanced understanding of\ntranslation preferences from a holistic view, while also being more resilient\nin the absence of gold translations. We further build a dataset named MAPLE to\nverify the effectiveness of our approach, which includes multiple translations\nof varying quality for each source sentence. Extensive experiments demonstrate\nthe superiority of our approach in \"breaking the plateau\" across diverse LLMs\nand test settings. Our in-depth analysis underscores the pivotal role of\ndiverse translations and accurate preference scores in the success of our\napproach.\n","authors":["Dawei Zhu","Sony Trenous","Xiaoyu Shen","Dietrich Klakow","Bill Byrne","Eva Hasler"],"pdf_url":"https://arxiv.org/pdf/2404.11288v2.pdf","comment":"Accepted to NAACL 2024 (long, main)"},{"id":"http://arxiv.org/abs/2408.16390v1","updated":"2024-08-29T09:52:01Z","published":"2024-08-29T09:52:01Z","title":"MQM-Chat: Multidimensional Quality Metrics for Chat Translation","summary":"  The complexities of chats pose significant challenges for machine translation\nmodels. Recognizing the need for a precise evaluation metric to address the\nissues of chat translation, this study introduces Multidimensional Quality\nMetrics for Chat Translation (MQM-Chat). Through the experiments of five models\nusing MQM-Chat, we observed that all models generated certain fundamental\nerrors, while each of them has different shortcomings, such as omission, overly\ncorrecting ambiguous source content, and buzzword issues, resulting in the loss\nof stylized information. Our findings underscore the effectiveness of MQM-Chat\nin evaluating chat translation, emphasizing the importance of stylized content\nand dialogue consistency for future studies.\n","authors":["Yunmeng Li","Jun Suzuki","Makoto Morishita","Kaori Abe","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2408.16390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19097v2","updated":"2024-08-29T09:35:24Z","published":"2024-02-29T12:25:45Z","title":"TEncDM: Understanding the Properties of Diffusion Model in the Space of\n  Language Model Encodings","summary":"  This paper presents the Text Encoding Diffusion Model (TEncDM), a novel\napproach to diffusion modeling that operates in the space of pre-trained\nlanguage model encodings. In contrast to traditionally used embeddings,\nencodings integrate contextual information. In our approach, we also employ a\ntransformer-based decoder, specifically designed to incorporate context in the\ntoken prediction process. We conduct a comprehensive examination of the\ninfluence of the encoder, decoder, noise scheduler, and self-conditioning on\nzero-shot generation. Furthermore, we compare TEncDM with previous approaches\non three conditional text generation tasks: QQP, XSum, and Wiki-Auto. The\nresults show that TEncDM exhibits superior performance compared to existing\nnon-autoregressive diffusion models.\n","authors":["Alexander Shabalin","Viacheslav Meshchaninov","Egor Chimbulatov","Vladislav Lapikov","Roman Kim","Grigory Bartosh","Dmitry Molchanov","Sergey Markov","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2402.19097v2.pdf","comment":"14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.01602v2","updated":"2024-08-29T08:49:14Z","published":"2024-04-02T02:46:18Z","title":"Helmsman of the Masses? Evaluate the Opinion Leadership of Large\n  Language Models in the Werewolf Game","summary":"  Large language models (LLMs) have exhibited memorable strategic behaviors in\nsocial deductive games. However, the significance of opinion leadership\nexhibited by LLM-based agents has been largely overlooked, which is crucial for\npractical applications in multi-agent and human-AI interaction settings.\nOpinion leaders are individuals who have a noticeable impact on the beliefs and\nbehaviors of others within a social group. In this work, we employ the Werewolf\ngame as a simulation platform to assess the opinion leadership of LLMs. The\ngame includes the role of the Sheriff, tasked with summarizing arguments and\nrecommending decision options, and therefore serves as a credible proxy for an\nopinion leader. We develop a framework integrating the Sheriff role and devise\ntwo novel metrics based on the critical characteristics of opinion leaders. The\nfirst metric measures the reliability of the opinion leader, and the second\nassesses the influence of the opinion leader on other players' decisions. We\nconduct extensive experiments to evaluate LLMs of different scales. In\naddition, we collect a Werewolf question-answering dataset (WWQA) to assess and\nenhance LLM's grasp of the game rules, and we also incorporate human\nparticipants for further analysis. The results suggest that the Werewolf game\nis a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs\npossess the capacity for opinion leadership.\n","authors":["Silin Du","Xiaowei Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.01602v2.pdf","comment":"Published as a conference paper at COLM 2024. 37 pages, 6 figures, 27\n  tables"},{"id":"http://arxiv.org/abs/2407.09816v4","updated":"2024-08-29T08:45:58Z","published":"2024-07-13T09:22:33Z","title":"MaskMoE: Boosting Token-Level Learning via Routing Mask in\n  Mixture-of-Experts","summary":"  Scaling the size of a model enhances its capabilities but significantly\nincreases computation complexity. Mixture-of-Experts models (MoE) address the\nissue by allowing model size to scale up without substantially increasing\ntraining or inference costs. In MoE, there is an important module called the\nrouter, which is used to distribute each token to the experts. Currently, the\nmainstream routing methods include dynamic routing and fixed routing. Despite\ntheir promising results, MoE models encounter several challenges. Primarily,\nfor dynamic routing methods, the dispersion of training tokens across multiple\nexperts can lead to underfitting, particularly for infrequent tokens.\nAdditionally, though fixed routing methods can mitigate that issue, they\ncompromise on the diversity of representations. In this paper, we propose\n\\textbf{MaskMoE}, a method designed to enhance token-level learning by\nemploying a routing \\textbf{mask}ing technique within the\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xperts model. MaskMoE is capable of\nmaintaining representation diversity while achieving more comprehensive\ntraining. Experimental results demonstrate that our method outperforms previous\ndominant Mixture-of-Experts models in terms of both perplexity (PPL) and\ndownstream task performance.\n","authors":["Zhenpeng Su","Zijia Lin","Xue Bai","Xing Wu","Yizhe Xiong","Haoran Lian","Guangyuan Ma","Hui Chen","Guiguang Ding","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2407.09816v4.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2408.15533v2","updated":"2024-08-29T08:45:30Z","published":"2024-08-28T04:44:43Z","title":"LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation","summary":"  Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.\n","authors":["Haichuan Hu","Yuhan Sun","Quanjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16345v1","updated":"2024-08-29T08:30:33Z","published":"2024-08-29T08:30:33Z","title":"The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text\n  Memorization","summary":"  This work analyses the text memorization behavior of large language models\n(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like\nnucleus sampling are typically applied to overcome issues such as monotonous\nand repetitive text generation, which are often observed with\nmaximization-based decoding techniques. We hypothesize that nucleus sampling\nmight also reduce the occurrence of memorization patterns, because it could\nlead to the selection of tokens outside the memorized sequence. To test this\nhypothesis we create a diagnostic dataset with a known distribution of\nduplicates that gives us some control over the likelihood of memorization of\ncertain parts of the training data. Our analysis of two GPT-Neo models\nfine-tuned on this dataset interestingly shows that (i) an increase of the\nnucleus size reduces memorization only modestly, and (ii) even when models do\nnot engage in \"hard\" memorization -- a verbatim reproduction of training\nsamples -- they may still display \"soft\" memorization whereby they generate\noutputs that echo the training data but without a complete one-by-one\nresemblance.\n","authors":["Luka Borec","Philipp Sadler","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2408.16345v1.pdf","comment":"9 pages, Accepted at INLG 2024 (International Natural Language\n  Generation Conference)"},{"id":"http://arxiv.org/abs/2402.12326v2","updated":"2024-08-29T08:27:27Z","published":"2024-02-19T18:00:30Z","title":"PsychoGAT: A Novel Psychological Measurement Paradigm through\n  Interactive Fiction Games with LLM Agents","summary":"  Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction.\n","authors":["Qisen Yang","Zekun Wang","Honghui Chen","Shenzhi Wang","Yifan Pu","Xin Gao","Wenhao Huang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.12326v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2407.14507v2","updated":"2024-08-29T08:24:42Z","published":"2024-07-19T17:59:03Z","title":"Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey","summary":"  Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we summarize a theoretical framework, Internal Consistency,\noffering explanations for reasoning deficiencies and hallucinations. Internal\nConsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce another effective theoretical framework capable of mining Internal\nConsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures Internal Consistency\nSignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey.\n","authors":["Xun Liang","Shichao Song","Zifan Zheng","Hanyu Wang","Qingchen Yu","Xunkai Li","Rong-Hua Li","Peng Cheng","Zhonghao Wang","Feiyu Xiong","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2407.14507v2.pdf","comment":"24 pages, 9 figures, 7 tables, 14 equations"},{"id":"http://arxiv.org/abs/2408.16326v1","updated":"2024-08-29T08:02:09Z","published":"2024-08-29T08:02:09Z","title":"Critic-CoT: Boosting the reasoning abilities of large language model via\n  Chain-of-thoughts Critic","summary":"  Self-critic has become an important mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nwithout further training, which tend to be over-simplified, leading to limited\naccuracy.Moreover, there is a lack of in-depth investigation of the\nrelationship between LLM's ability to criticism and its task-solving\nperformance.To address these issues, we propose Critic-CoT, a novel framework\nthat pushes LLMs toward System-2-like critic capability, via step-wise CoT\nreasoning format and distant-supervision data construction, without the need\nfor human annotation. Experiments on GSM8K and MATH show that via filtering out\ninvalid solutions or iterative refinement, our enhanced model boosts\ntask-solving performance, which demonstrates the effectiveness of our method.\nFurther, we find that training on critique and refinement alone improves the\ngeneration. We hope our work could shed light on future research on improving\nthe reasoning and critic ability of LLMs.\n","authors":["Xin Zheng","Jie Lou","Boxi Cao","Xueru Wen","Yuqiu Ji","Hongyu Lin","Yaojie Lu","Xianpei Han","Debing Zhang","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2408.16326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16293v1","updated":"2024-08-29T06:49:20Z","published":"2024-08-29T06:49:20Z","title":"Physics of Language Models: Part 2.2, How to Learn From Mistakes on\n  Grade-School Math Problems","summary":"  Language models have demonstrated remarkable performance in solving reasoning\ntasks; however, even the strongest models still occasionally make reasoning\nmistakes. Recently, there has been active research aimed at improving reasoning\naccuracy, particularly by using pretrained language models to \"self-correct\"\ntheir mistakes via multi-round prompting. In this paper, we follow this line of\nwork but focus on understanding the usefulness of incorporating\n\"error-correction\" data directly into the pretraining stage. This data consists\nof erroneous solution steps immediately followed by their corrections. Using a\nsynthetic math dataset, we show promising results: this type of pretrain data\ncan help language models achieve higher reasoning accuracy directly (i.e.,\nthrough simple auto-regression, without multi-round prompting) compared to\npretraining on the same amount of error-free data. We also delve into many\ndetails, such as (1) how this approach differs from beam search, (2) how such\ndata can be prepared, (3) whether masking is needed on the erroneous tokens,\n(4) the amount of error required, (5) whether such data can be deferred to the\nfine-tuning stage, and many others.\n","authors":["Tian Ye","Zicheng Xu","Yuanzhi Li","Zeyuan Allen-Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.16293v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.20311"},{"id":"http://arxiv.org/abs/2408.16287v1","updated":"2024-08-29T06:38:55Z","published":"2024-08-29T06:38:55Z","title":"Measuring the Accuracy of Automatic Speech Recognition Solutions","summary":"  For d/Deaf and hard of hearing (DHH) people, captioning is an essential\naccessibility tool. Significant developments in artificial intelligence (AI)\nmean that Automatic Speech Recognition (ASR) is now a part of many popular\napplications. This makes creating captions easy and broadly available - but\ntranscription needs high levels of accuracy to be accessible. Scientific\npublications and industry report very low error rates, claiming AI has reached\nhuman parity or even outperforms manual transcription. At the same time the DHH\ncommunity reports serious issues with the accuracy and reliability of ASR.\nThere seems to be a mismatch between technical innovations and the real-life\nexperience for people who depend on transcription. Independent and\ncomprehensive data is needed to capture the state of ASR. We measured the\nperformance of eleven common ASR services with recordings of Higher Education\nlectures. We evaluated the influence of technical conditions like streaming,\nthe use of vocabularies, and differences between languages. Our results show\nthat accuracy ranges widely between vendors and for the individual audio\nsamples. We also measured a significant lower quality for streaming ASR, which\nis used for live events. Our study shows that despite the recent improvements\nof ASR, common services lack reliability in accuracy.\n","authors":["Korbinian Kuhn","Verena Kersken","Benedikt Reuter","Niklas Egger","Gottfried Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2408.16287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16276v1","updated":"2024-08-29T05:47:14Z","published":"2024-08-29T05:47:14Z","title":"Enhancing AI-Driven Psychological Consultation: Layered Prompts with\n  Large Language Models","summary":"  Psychological consultation is essential for improving mental health and\nwell-being, yet challenges such as the shortage of qualified professionals and\nscalability issues limit its accessibility. To address these challenges, we\nexplore the use of large language models (LLMs) like GPT-4 to augment\npsychological consultation services. Our approach introduces a novel layered\nprompting system that dynamically adapts to user input, enabling comprehensive\nand relevant information gathering. We also develop empathy-driven and\nscenario-based prompts to enhance the LLM's emotional intelligence and\ncontextual understanding in therapeutic settings. We validated our approach\nthrough experiments using a newly collected dataset of psychological\nconsultation dialogues, demonstrating significant improvements in response\nquality. The results highlight the potential of our prompt engineering\ntechniques to enhance AI-driven psychological consultation, offering a scalable\nand accessible solution to meet the growing demand for mental health support.\n","authors":["Rafael Souza","Jia-Hao Lim","Alexander Davis"],"pdf_url":"https://arxiv.org/pdf/2408.16276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11911v6","updated":"2024-08-29T05:14:36Z","published":"2023-09-21T09:22:07Z","title":"InstructERC: Reforming Emotion Recognition in Conversation with\n  Multi-task Retrieval-Augmented Large Language Models","summary":"  The field of emotion recognition of conversation (ERC) has been focusing on\nseparating sentence feature encoding and context modeling, lacking exploration\nin generative paradigms based on unified designs. In this study, we propose a\nnovel approach, InstructERC, to reformulate the ERC task from a discriminative\nframework to a generative framework based on Large Language Models (LLMs).\nInstructERC makes three significant contributions: (1) it introduces a simple\nyet effective retrieval template module, which helps the model explicitly\nintegrate multi-granularity dialogue supervision information. (2) We introduce\ntwo additional emotion alignment tasks, namely speaker identification and\nemotion prediction tasks, to implicitly model the dialogue role relationships\nand future emotional tendencies in conversations. (3) Pioneeringly, we unify\nemotion labels across benchmarks through the feeling wheel to fit real\napplication scenarios. InstructERC still perform impressively on this unified\ndataset. Our LLM-based plugin framework significantly outperforms all previous\nmodels and achieves comprehensive SOTA on three commonly used ERC datasets.\nExtensive analysis of parameter-efficient and data-scaling experiments provides\nempirical guidance for applying it in practical scenarios.\n","authors":["Shanglin Lei","Guanting Dong","Xiaoping Wang","Keheng Wang","Runqi Qiao","Sirui Wang"],"pdf_url":"https://arxiv.org/pdf/2309.11911v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16264v1","updated":"2024-08-29T05:02:52Z","published":"2024-08-29T05:02:52Z","title":"LoraMap: Harnessing the Power of LoRA Connections","summary":"  Large Language Models (LLMs) can benefit from mitigating hallucinations\nthrough fact-checking and overcoming substantial computational overhead with\nparameter-efficient techniques such as Low-Rank Adaptation (LoRA). While some\nstudies have explored the parallel integration of multiple LoRAs, these\napproaches need attention to the connections between them. This paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results on the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting LoRA composition method. LoraMap also outperforms with significantly\nfewer parameters than LoraConcat, which concatenates LoRAs and further\nfine-tunes them.\n","authors":["Hyeryun Park","Jeongwon Kwak","Dongsuk Jang","Sumin Park","Jinwook Choi"],"pdf_url":"https://arxiv.org/pdf/2408.16264v1.pdf","comment":"13 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2408.16241v1","updated":"2024-08-29T03:50:24Z","published":"2024-08-29T03:50:24Z","title":"Making the Most of your Model: Methods for Finetuning and Applying\n  Pretrained Transformers","summary":"  This thesis provides methods and analysis of models which make progress on\nthis goal. The techniques outlined are task agnostic, and should provide\nbenefit when used with nearly any transformer LM. We introduce two new\nfinetuning methods which add new capabilities to the models they are used on.\nThe first adds a recurrence mechanism, which removes the fixed-window sized\nconstraint and improves the efficiency of a transformer decoder. The second\nallows masked language models (MLMs) to be used for initialization of both the\nencoder and decoder of a non-autoregressive sequence-to-sequence transformer,\nopening up generative applications of models which were previously only used\nfor natural language understanding tasks.\n  We also introduce two new techniques for improving the quality of predictions\nof any transformer decoder without additional finetuning. One, hidden state\noptimization, can be applied to any transformer decoder to improve the quality\nof predictions at inference time, especially for few-shot classification. The\nother, conditional beam search, allows practitioners to search for natural\nlanguage generation (NLG) model outputs with high likelihood while conditioning\non the event that the output is not degenerate (e.g. empty, repetitive, etc.).\n  Finally, we provide theoretical and empirical insights on the divergence of\nmodel-likelihood and output quality which has widely been observed in prior\nwork. These insights apply to any model which represents a distribution over\ntext, and apply to language models which are not transformers or even\nautoregressive. We argue that the NLP community has, to some extent,\nmisunderstood the implications of these findings, and encourage a point of view\nwhich has more nuance.\n","authors":["Davis Yoshida"],"pdf_url":"https://arxiv.org/pdf/2408.16241v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2408.13985v2","updated":"2024-08-29T02:40:12Z","published":"2024-08-26T02:35:37Z","title":"TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models","summary":"  With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies.\n","authors":["Zelin Li","Kehai Chen","Xuefeng Bai","Lemao Liu","Mingming Yang","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.13985v2.pdf","comment":"14 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2305.17440 by other authors"},{"id":"http://arxiv.org/abs/2408.10903v5","updated":"2024-08-29T02:38:05Z","published":"2024-08-20T14:47:38Z","title":"BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model","summary":"  The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.\n","authors":["Yeyong Yu","Runsheng Yu","Haojie Wei","Zhanqiu Zhang","Quan Qian"],"pdf_url":"https://arxiv.org/pdf/2408.10903v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16221v1","updated":"2024-08-29T02:35:53Z","published":"2024-08-29T02:35:53Z","title":"SSDM: Scalable Speech Dysfluency Modeling","summary":"  Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions suffer from poor scalability. Second, there is a\nlack of a large-scale dysfluency corpus. Third, there is not an effective\nlearning framework. In this paper, we propose \\textit{SSDM: Scalable Speech\nDysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced\nalignment; (2) introduces connectionist subsequence aligner (CSA) to achieve\ndysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus\ncalled Libri-Dys; and (4) develops an end-to-end system by leveraging the power\nof large language models (LLMs). We expect SSDM to serve as a standard in the\narea of dysfluency modeling. Demo is available at\n\\url{https://eureka235.github.io}.\n","authors":["Jiachen Lian","Xuanru Zhou","Zoe Ezzes","Jet Vonk","Brittany Morin","David Baquirin","Zachary Mille","Maria Luisa Gorno Tempini","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2408.16221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21191v2","updated":"2024-08-29T02:27:19Z","published":"2024-07-30T20:58:36Z","title":"GenRec: Generative Sequential Recommendation with Large Language Models","summary":"  Sequential recommendation is a task to capture hidden user preferences from\nhistorical user item interaction data and recommend next items for the user.\nSignificant progress has been made in this domain by leveraging classification\nbased learning methods. Inspired by the recent paradigm of 'pretrain, prompt\nand predict' in NLP, we consider sequential recommendation as a sequence to\nsequence generation task and propose a novel model named Generative\nRecommendation (GenRec). Unlike classification based models that learn explicit\nuser and item representations, GenRec utilizes the sequence modeling capability\nof Transformer and adopts the masked item prediction objective to effectively\nlearn the hidden bidirectional sequential patterns. Different from existing\ngenerative sequential recommendation models, GenRec does not rely on manually\ndesigned hard prompts. The input to GenRec is textual user item sequence and\nthe output is top ranked next items. Moreover, GenRec is lightweight and\nrequires only a few hours to train effectively in low-resource settings, making\nit highly applicable to real-world scenarios and helping to democratize large\nlanguage models in the sequential recommendation domain. Our extensive\nexperiments have demonstrated that GenRec generalizes on various public\nreal-world datasets and achieves state-of-the-art results. Our experiments also\nvalidate the effectiveness of the the proposed masked item prediction objective\nthat improves the model performance by a large margin.\n","authors":["Panfeng Cao","Pietro Lio"],"pdf_url":"https://arxiv.org/pdf/2407.21191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16213v1","updated":"2024-08-29T02:12:58Z","published":"2024-08-29T02:12:58Z","title":"M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language\n  Models for Chest X-ray Interpretation","summary":"  The rapid evolution of artificial intelligence, especially in large language\nmodels (LLMs), has significantly impacted various domains, including\nhealthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,\nbut with limitations: either underutilizing the multi-tasking capabilities of\nLLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM\ndesigned to enhance CXR interpretation. The model is trained on a visual\ninstruction-following dataset that integrates various task-specific datasets in\na conversational format. As a result, the model supports multiple tasks such as\nmedical report generation (MRG), visual grounding, and visual question\nanswering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by\nemploying a chain-of-thought prompting strategy, in which it identifies\nfindings in CXR images and subsequently generates corresponding reports. The\nmodel is adaptable to various MRG scenarios depending on the available inputs,\nsuch as single-image, multi-image, and multi-study contexts. In addition to\nMRG, M4CXR performs visual grounding at a level comparable to specialized\nmodels and also demonstrates outstanding performance in VQA. Both quantitative\nand qualitative assessments reveal M4CXR's versatility in MRG, visual\ngrounding, and VQA, while consistently maintaining clinical accuracy.\n","authors":["Jonggwon Park","Soobum Kim","Byungmu Yoon","Jihun Hyun","Kyoyun Choi"],"pdf_url":"https://arxiv.org/pdf/2408.16213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16209v1","updated":"2024-08-29T02:05:39Z","published":"2024-08-29T02:05:39Z","title":"From cart to truck: meaning shift through words in English in the last\n  two centuries","summary":"  This onomasiological study uses diachronic word embeddings to explore how\ndifferent words represented the same concepts over time, using historical word\ndata from 1800 to 2000. We identify shifts in energy, transport, entertainment,\nand computing domains, revealing connections between language and societal\nchanges.\n  Our approach consisted in using diachronic word embeddings trained using\nword2vec with skipgram and aligning them using orthogonal Procrustes. We\ndiscuss possible difficulties linked to the relationships the method\nidentifies. Moreover, we look at the ethical aspects of interpreting results,\nhighlighting the need for expert insights to understand the method's\nsignificance.\n","authors":["Esteban RodrÃ­guez Betancourt","Edgar Casasola Murillo"],"pdf_url":"https://arxiv.org/pdf/2408.16209v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.16208v1","updated":"2024-08-29T02:03:05Z","published":"2024-08-29T02:03:05Z","title":"ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology\n  Report Generation Metrics","summary":"  Given the rapidly expanding capabilities of generative AI models for\nradiology, there is a need for robust metrics that can accurately measure the\nquality of AI-generated radiology reports across diverse hospitals. We develop\nReXamine-Global, a LLM-powered, multi-site framework that tests metrics across\ndifferent writing styles and patient populations, exposing gaps in their\ngeneralization. First, our method tests whether a metric is undesirably\nsensitive to reporting style, providing different scores depending on whether\nAI-generated reports are stylistically similar to ground-truth reports or not.\nSecond, our method measures whether a metric reliably agrees with experts, or\nwhether metric and expert scores of AI-generated report quality diverge for\nsome sites. Using 240 reports from 6 hospitals around the world, we apply\nReXamine-Global to 7 established report evaluation metrics and uncover serious\ngaps in their generalizability. Developers can apply ReXamine-Global when\ndesigning new report evaluation metrics, ensuring their robustness across\nsites. Additionally, our analysis of existing metrics can guide users of those\nmetrics towards evaluation procedures that work reliably at their sites of\ninterest.\n","authors":["Oishi Banerjee","Agustina Saenz","Kay Wu","Warren Clements","Adil Zia","Dominic Buensalido","Helen Kavnoudias","Alain S. Abi-Ghanem","Nour El Ghawi","Cibele Luna","Patricia Castillo","Khaled Al-Surimi","Rayyan A. Daghistani","Yuh-Min Chen","Heng-sheng Chao","Lars Heiliger","Moon Kim","Johannes Haubold","Frederic Jonske","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2408.16208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16180v1","updated":"2024-08-29T00:18:12Z","published":"2024-08-29T00:18:12Z","title":"Benchmarking Japanese Speech Recognition on ASR-LLM Setups with\n  Multi-Pass Augmented Generative Error Correction","summary":"  With the strong representational power of large language models (LLMs),\ngenerative error correction (GER) for automatic speech recognition (ASR) aims\nto provide semantic and phonetic refinements to address ASR errors. This work\nexplores how LLM-based GER can enhance and expand the capabilities of Japanese\nlanguage processing, presenting the first GER benchmark for Japanese ASR with\n0.9-2.6k text utterances. We also introduce a new multi-pass augmented\ngenerative error correction (MPA GER) by integrating multiple system hypotheses\non the input side with corrections from multiple LLMs on the output side and\nthen merging them. To the best of our knowledge, this is the first\ninvestigation of the use of LLMs for Japanese GER, which involves second-pass\nlanguage modeling on the output transcriptions generated by the ASR system\n(e.g., N-best hypotheses). Our experiments demonstrated performance improvement\nin the proposed methods of ASR quality and generalization both in SPREDS-U1-ja\nand CSJ data.\n","authors":["Yuka Ko","Sheng Li","Chao-Han Huck Yang","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2408.16180v1.pdf","comment":"submitted to SLT2024"},{"id":"http://arxiv.org/abs/2408.13985v2","updated":"2024-08-29T02:40:12Z","published":"2024-08-26T02:35:37Z","title":"TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models","summary":"  With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies.\n","authors":["Zelin Li","Kehai Chen","Xuefeng Bai","Lemao Liu","Mingming Yang","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.13985v2.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.11897v2","updated":"2024-08-29T23:45:48Z","published":"2024-05-20T09:30:03Z","title":"CReMa: Crisis Response through Computational Identification and Matching\n  of Cross-Lingual Requests and Offers Shared on Social Media","summary":"  During times of crisis, social media platforms play a crucial role in\nfacilitating communication and coordinating resources. In the midst of chaos\nand uncertainty, communities often rely on these platforms to share urgent\npleas for help, extend support, and organize relief efforts. However, the\noverwhelming volume of conversations during such periods can escalate to\nunprecedented levels, necessitating the automated identification and matching\nof requests and offers to streamline relief operations. Additionally, there is\na notable absence of studies conducted in multi-lingual settings, despite the\nfact that any geographical area can have a diverse linguistic population.\nTherefore, we propose CReMa (Crisis Response Matcher), a systematic approach\nthat integrates textual, temporal, and spatial features to address the\nchallenges of effectively identifying and matching requests and offers on\nsocial media platforms during emergencies. Our approach utilizes a\ncrisis-specific pre-trained model and a multi-lingual embedding space. We\nemulate human decision-making to compute temporal and spatial features and\nnon-linearly weigh the textual features. The results from our experiments are\npromising, outperforming strong baselines. Additionally, we introduce a novel\nmulti-lingual dataset simulating help-seeking and offering assistance on social\nmedia in 16 languages and conduct comprehensive cross-lingual experiments.\nFurthermore, we analyze a million-scale geotagged global dataset to understand\npatterns in seeking help and offering assistance on social media. Overall,\nthese contributions advance the field of crisis informatics and provide\nbenchmarks for future research in the area.\n","authors":["Rabindra Lamsal","Maria Rodriguez Read","Shanika Karunasekera","Muhammad Imran"],"pdf_url":"https://arxiv.org/pdf/2405.11897v2.pdf","comment":"\\copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2408.16942v1","updated":"2024-08-29T23:39:11Z","published":"2024-08-29T23:39:11Z","title":"A longitudinal sentiment analysis of Sinophobia during COVID-19 using\n  large language models","summary":"  The COVID-19 pandemic has exacerbated xenophobia, particularly Sinophobia,\nleading to widespread discrimination against individuals of Chinese descent.\nLarge language models (LLMs) are pre-trained deep learning models used for\nnatural language processing (NLP) tasks. The ability of LLMs to understand and\ngenerate human-like text makes them particularly useful for analysing social\nmedia data to detect and evaluate sentiments. We present a sentiment analysis\nframework utilising LLMs for longitudinal sentiment analysis of the Sinophobic\nsentiments expressed in X (Twitter) during the COVID-19 pandemic. The results\nshow a significant correlation between the spikes in Sinophobic tweets,\nSinophobic sentiments and surges in COVID-19 cases, revealing that the\nevolution of the pandemic influenced public sentiment and the prevalence of\nSinophobic discourse. Furthermore, the sentiment analysis revealed a\npredominant presence of negative sentiments, such as annoyance and denial,\nwhich underscores the impact of political narratives and misinformation shaping\npublic opinion. The lack of empathetic sentiment which was present in previous\nstudies related to COVID-19 highlights the way the political narratives in\nmedia viewed the pandemic and how it blamed the Chinese community. Our study\nhighlights the importance of transparent communication in mitigating xenophobic\nsentiments during global crises.\n","authors":["Chen Wang","Rohitash Chandra"],"pdf_url":"https://arxiv.org/pdf/2408.16942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18470v2","updated":"2024-08-29T23:13:56Z","published":"2024-04-29T07:11:39Z","title":"ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\n  using Large Language Model for Stock Performance Prediction","summary":"  In the realm of financial analytics, leveraging unstructured data, such as\nearnings conference calls (ECCs), to forecast stock volatility is a critical\nchallenge that has attracted both academics and investors. While previous\nstudies have used multimodal deep learning-based models to obtain a general\nview of ECCs for volatility predicting, they often fail to capture detailed,\ncomplex information. Our research introduces a novel framework: \\textbf{ECC\nAnalyzer}, which utilizes large language models (LLMs) to extract richer, more\npredictive content from ECCs to aid the model's prediction performance. We use\nthe pre-trained large models to extract textual and audio features from ECCs\nand implement a hierarchical information extraction strategy to extract more\nfine-grained information. This strategy first extracts paragraph-level general\ninformation by summarizing the text and then extracts fine-grained focus\nsentences using Retrieval-Augmented Generation (RAG). These features are then\nfused through multimodal feature fusion to perform volatility prediction.\nExperimental results demonstrate that our model outperforms traditional\nanalytical benchmarks, confirming the effectiveness of advanced LLM techniques\nin financial analysis.\n","authors":["Yupeng Cao","Zhi Chen","Qingyun Pei","Nathan Jinseok Lee","K. P. Subbalakshmi","Papa Momar Ndiaye"],"pdf_url":"https://arxiv.org/pdf/2404.18470v2.pdf","comment":"9 pages, 1 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.16937v1","updated":"2024-08-29T23:13:45Z","published":"2024-08-29T23:13:45Z","title":"Plausible-Parrots @ MSP2023: Enhancing Semantic Plausibility Modeling\n  using Entity and Event Knowledge","summary":"  In this work, we investigate the effectiveness of injecting external\nknowledge to a large language model (LLM) to identify semantic plausibility of\nsimple events. Specifically, we enhance the LLM with fine-grained entity types,\nevent types and their definitions extracted from an external knowledge base.\nThese knowledge are injected into our system via designed templates. We also\naugment the data to balance the label distribution and adapt the task setting\nto real world scenarios in which event mentions are expressed as natural\nlanguage sentences. The experimental results show the effectiveness of the\ninjected knowledge on modeling semantic plausibility of events. An error\nanalysis further emphasizes the importance of identifying non-trivial entity\nand event types.\n","authors":["Chong Shen","Chenyue Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.16937v1.pdf","comment":"10 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.04785v2","updated":"2024-08-29T22:18:08Z","published":"2024-03-02T22:33:17Z","title":"Large Language Multimodal Models for 5-Year Chronic Disease Cohort\n  Prediction Using EHR Data","summary":"  Chronic diseases such as diabetes are the leading causes of morbidity and\nmortality worldwide. Numerous research studies have been attempted with various\ndeep learning models in diagnosis. However, most previous studies had certain\nlimitations, including using publicly available datasets (e.g. MIMIC), and\nimbalanced data. In this study, we collected five-year electronic health\nrecords (EHRs) from the Taiwan hospital database, including 1,420,596 clinical\nnotes, 387,392 laboratory test results, and more than 1,505 laboratory test\nitems, focusing on research pre-training large language models. We proposed a\nnovel Large Language Multimodal Models (LLMMs) framework incorporating\nmultimodal data from clinical notes and laboratory test results for the\nprediction of chronic disease risk. Our method combined a text embedding\nencoder and multi-head attention layer to learn laboratory test values,\nutilizing a deep neural network (DNN) module to merge blood features with\nchronic disease semantics into a latent space. In our experiments, we observe\nthat clinicalBERT and PubMed-BERT, when combined with attention fusion, can\nachieve an accuracy of 73% in multiclass chronic diseases and diabetes\nprediction. By transforming laboratory test values into textual descriptions\nand employing the Flan T-5 model, we achieved a 76% Area Under the ROC Curve\n(AUROC), demonstrating the effectiveness of leveraging numerical text data for\ntraining and inference in language models. This approach significantly improves\nthe accuracy of early-stage diabetes prediction.\n","authors":["Jun-En Ding","Phan Nguyen Minh Thao","Wen-Chih Peng","Jian-Zhe Wang","Chun-Cheng Chug","Min-Chen Hsieh","Yun-Chien Tseng","Ling Chen","Dongsheng Luo","Chi-Te Wang","Pei-fu Chen","Feng Liu","Fang-Ming Hung"],"pdf_url":"https://arxiv.org/pdf/2403.04785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16932v1","updated":"2024-08-29T22:14:21Z","published":"2024-08-29T22:14:21Z","title":"Event Extraction for Portuguese: A QA-driven Approach using ACE-2005","summary":"  Event extraction is an Information Retrieval task that commonly consists of\nidentifying the central word for the event (trigger) and the event's arguments.\nThis task has been extensively studied for English but lags behind for\nPortuguese, partly due to the lack of task-specific annotated corpora. This\npaper proposes a framework in which two separated BERT-based models were\nfine-tuned to identify and classify events in Portuguese documents. We\ndecompose this task into two sub-tasks. Firstly, we use a token classification\nmodel to detect event triggers. To extract event arguments, we train a Question\nAnswering model that queries the triggers about their corresponding event\nargument roles. Given the lack of event annotated corpora in Portuguese, we\ntranslated the original version of the ACE-2005 dataset (a reference in the\nfield) into Portuguese, producing a new corpus for Portuguese event extraction.\nTo accomplish this, we developed an automatic translation pipeline. Our\nframework obtains F1 marks of 64.4 for trigger classification and 46.7 for\nargument classification setting, thus a new state-of-the-art reference for\nthese tasks in Portuguese.\n","authors":["LuÃ­s Filipe Cunha","Ricardo Campos","AlÃ­pio Jorge"],"pdf_url":"https://arxiv.org/pdf/2408.16932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16928v1","updated":"2024-08-29T22:05:08Z","published":"2024-08-29T22:05:08Z","title":"ACE-2005-PT: Corpus for Event Extraction in Portuguese","summary":"  Event extraction is an NLP task that commonly involves identifying the\ncentral word (trigger) for an event and its associated arguments in text.\nACE-2005 is widely recognised as the standard corpus in this field. While other\ncorpora, like PropBank, primarily focus on annotating predicate-argument\nstructure, ACE-2005 provides comprehensive information about the overall event\nstructure and semantics. However, its limited language coverage restricts its\nusability. This paper introduces ACE-2005-PT, a corpus created by translating\nACE-2005 into Portuguese, with European and Brazilian variants. To speed up the\nprocess of obtaining ACE-2005-PT, we rely on automatic translators. This,\nhowever, poses some challenges related to automatically identifying the correct\nalignments between multi-word annotations in the original text and in the\ncorresponding translated sentence. To achieve this, we developed an alignment\npipeline that incorporates several alignment techniques: lemmatization, fuzzy\nmatching, synonym matching, multiple translations and a BERT-based word\naligner. To measure the alignment effectiveness, a subset of annotations from\nthe ACE-2005-PT corpus was manually aligned by a linguist expert. This subset\nwas then compared against our pipeline results which achieved exact and relaxed\nmatch scores of 70.55\\% and 87.55\\% respectively. As a result, we successfully\ngenerated a Portuguese version of the ACE-2005 corpus, which has been accepted\nfor publication by LDC.\n","authors":["LuÃ­s Filipe Cunha","PurificaÃ§Ã£o Silvano","Ricardo Campos","AlÃ­pio Jorge"],"pdf_url":"https://arxiv.org/pdf/2408.16928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05893v4","updated":"2024-08-29T21:34:22Z","published":"2024-04-08T22:29:53Z","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models","summary":"  Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base\n","authors":["Sowmya S. Sundaram","Benjamin Solomon","Avani Khatri","Anisha Laumas","Purvesh Khatri","Mark A. Musen"],"pdf_url":"https://arxiv.org/pdf/2404.05893v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16893v1","updated":"2024-08-29T20:27:05Z","published":"2024-08-29T20:27:05Z","title":"Exploring Multiple Strategies to Improve Multilingual Coreference\n  Resolution in CorefUD","summary":"  Coreference resolution, the task of identifying expressions in text that\nrefer to the same entity, is a critical component in various natural language\nprocessing (NLP) applications. This paper presents our end-to-end neural\ncoreference resolution system, utilizing the CorefUD 1.1 dataset, which spans\n17 datasets across 12 languages. We first establish strong baseline models,\nincluding monolingual and cross-lingual variations, and then propose several\nextensions to enhance performance across diverse linguistic contexts. These\nextensions include cross-lingual training, incorporation of syntactic\ninformation, a Span2Head model for optimized headword prediction, and advanced\nsingleton modeling. We also experiment with headword span representation and\nlong-documents modeling through overlapping segments. The proposed extensions,\nparticularly the heads-only approach, singleton modeling, and long document\nprediction significantly improve performance across most datasets. We also\nperform zero-shot cross-lingual experiments, highlighting the potential and\nlimitations of cross-lingual transfer in coreference resolution. Our findings\ncontribute to the development of robust and scalable coreference systems for\nmultilingual coreference resolution. Finally, we evaluate our model on CorefUD\n1.1 test set and surpass the best model from CRAC 2023 shared task of a\ncomparable size by a large margin. Our nodel is available on GitHub:\n\\url{https://github.com/ondfa/coref-multiling}\n","authors":["OndÅej PraÅ¾Ã¡k","Miloslav KonopÃ­k"],"pdf_url":"https://arxiv.org/pdf/2408.16893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06266v3","updated":"2024-08-29T20:26:19Z","published":"2024-08-12T16:24:51Z","title":"Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment","summary":"  Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.\n","authors":["Karel D'Oosterlinck","Winnie Xu","Chris Develder","Thomas Demeester","Amanpreet Singh","Christopher Potts","Douwe Kiela","Shikib Mehri"],"pdf_url":"https://arxiv.org/pdf/2408.06266v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16889v1","updated":"2024-08-29T20:20:49Z","published":"2024-08-29T20:20:49Z","title":"LLaVA-Chef: A Multi-modal Generative Model for Food Recipes","summary":"  In the rapidly evolving landscape of online recipe sharing within a\nglobalized context, there has been a notable surge in research towards\ncomprehending and generating food recipes. Recent advancements in large\nlanguage models (LLMs) like GPT-2 and LLaVA have paved the way for Natural\nLanguage Processing (NLP) approaches to delve deeper into various facets of\nfood-related tasks, encompassing ingredient recognition and comprehensive\nrecipe generation. Despite impressive performance and multi-modal adaptability\nof LLMs, domain-specific training remains paramount for their effective\napplication. This work evaluates existing LLMs for recipe generation and\nproposes LLaVA-Chef, a novel model trained on a curated dataset of diverse\nrecipe prompts in a multi-stage approach. First, we refine the mapping of\nvisual food image embeddings to the language space. Second, we adapt LLaVA to\nthe food domain by fine-tuning it on relevant recipe data. Third, we utilize\ndiverse prompts to enhance the model's recipe comprehension. Finally, we\nimprove the linguistic quality of generated recipes by penalizing the model\nwith a custom loss function. LLaVA-Chef demonstrates impressive improvements\nover pretrained LLMs and prior works. A detailed qualitative analysis reveals\nthat LLaVA-Chef generates more detailed recipes with precise ingredient\nmentions, compared to existing approaches.\n","authors":["Fnu Mohbat","Mohammed J. Zaki"],"pdf_url":"https://arxiv.org/pdf/2408.16889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15604v3","updated":"2024-08-29T20:05:27Z","published":"2024-05-24T14:38:11Z","title":"Text Generation: A Systematic Literature Review of Tasks, Evaluation,\n  and Challenges","summary":"  Text generation has become more accessible than ever, and the increasing\ninterest in these systems, especially those using large language models, has\nspurred an increasing number of related publications. We provide a systematic\nliterature review comprising 244 selected papers between 2017 and 2024. This\nreview categorizes works in text generation into five main tasks: open-ended\ntext generation, summarization, translation, paraphrasing, and question\nanswering. For each task, we review their relevant characteristics, sub-tasks,\nand specific challenges (e.g., missing datasets for multi-document\nsummarization, coherence in story generation, and complex reasoning for\nquestion answering). Additionally, we assess current approaches for evaluating\ntext generation systems and ascertain problems with current metrics. Our\ninvestigation shows nine prominent challenges common to all tasks and sub-tasks\nin recent text generation publications: bias, reasoning, hallucinations,\nmisuse, privacy, interpretability, transparency, datasets, and computing. We\nprovide a detailed analysis of these challenges, their potential solutions, and\nwhich gaps still require further engagement from the community. This systematic\nliterature review targets two main audiences: early career researchers in\nnatural language processing looking for an overview of the field and promising\nresearch directions, as well as experienced researchers seeking a detailed view\nof tasks, evaluation methodologies, open challenges, and recent mitigation\nstrategies.\n","authors":["Jonas Becker","Jan Philip Wahle","Bela Gipp","Terry Ruas"],"pdf_url":"https://arxiv.org/pdf/2405.15604v3.pdf","comment":"35 pages, 2 figures, 2 tables, Under review"},{"id":"http://arxiv.org/abs/2408.14772v2","updated":"2024-08-29T19:50:33Z","published":"2024-08-27T04:20:10Z","title":"A global AI community requires language-diverse publishing","summary":"  In this provocation, we discuss the English dominance of the AI research\ncommunity, arguing that the requirement for English language publishing upholds\nand reinforces broader regimes of extraction in AI. While large language models\nand machine translation have been celebrated as a way to break down barriers,\nwe regard their use as a symptom of linguistic exclusion of scientists and\npotential readers. We propose alternative futures for a healthier publishing\nculture, organized around three themes: administering conferences in the\nlanguages of the country in which they are held, instructing peer reviewers not\nto adjudicate the language appropriateness of papers, and offering\nopportunities to publish and present in multiple languages. We welcome new\ntranslations of this piece. Please contact the authors if you would like to\ncontribute one.\n","authors":["Haley Lepp","Parth Sarin"],"pdf_url":"https://arxiv.org/pdf/2408.14772v2.pdf","comment":"Translations by Tianyu M. Fang (Mandarin Chinese), Michael Hardy\n  (Guarani), Vandana Sarin and Vivek Sarin (Hindi), Roshna Omer Abdulrahman\n  (Soran\\^i Kurdish), Gabriel Poesia (Portuguese), and Mat\\'ias Grinberg\n  (Spanish). In the proceedings of the Global AI Cultures Workshop at the\n  Twelfth International Conference on Learning Representations (ICLR) 2024,\n  Vienna, Austria, May 7-11, 2024"},{"id":"http://arxiv.org/abs/2211.09944v3","updated":"2024-08-29T19:25:59Z","published":"2022-11-17T23:38:29Z","title":"MelHuBERT: A simplified HuBERT on Mel spectrograms","summary":"  Self-supervised models have had great success in learning speech\nrepresentations that can generalize to various downstream tasks. However, most\nself-supervised models require a large amount of compute and multiple GPUs to\ntrain, significantly hampering the development of self-supervised learning. In\nan attempt to reduce the computation of training, we revisit the training of\nHuBERT, a highly successful self-supervised model. We improve and simplify\nseveral key components, including the loss function, input representation, and\ntraining in multiple stages. Our model, MelHuBERT, is able to achieve favorable\nperformance on phone recognition, speaker identification, and automatic speech\nrecognition against HuBERT, while saving 31.2% of the pre-training time, or\nequivalently 33.5% MACs per one second speech. The code and pre-trained models\nare available in https://github.com/nervjack2/MelHuBERT.\n","authors":["Tzu-Quan Lin","Hung-yi Lee","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2211.09944v3.pdf","comment":"ASRU 2023"},{"id":"http://arxiv.org/abs/2406.11402v2","updated":"2024-08-29T19:24:29Z","published":"2024-06-17T10:45:36Z","title":"Are Small Language Models Ready to Compete with Large Language Models\n  for Practical Applications?","summary":"  The rapid rise of Language Models (LMs) has expanded their use in several\napplications. Yet, due to constraints of model size, associated cost, or\nproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always\nfeasible. With open, smaller LMs emerging, more applications can leverage their\ncapabilities, but selecting the right LM can be challenging as smaller LMs\ndon't perform well universally. This work tries to bridge this gap by proposing\na framework to experimentally evaluate small, open LMs in practical settings\nthrough measuring semantic correctness of outputs across three practical\naspects: task types, application domains and reasoning types, using diverse\nprompt styles. It also conducts an in-depth comparison of 10 small, open LMs to\nidentify best LM and prompt style depending on specific application requirement\nusing the proposed framework. We also show that if selected appropriately, they\ncan outperform SOTA LLMs like DeepSeek-v2, GPT-4o-mini, Gemini-1.5-Pro, and\neven compete with GPT-4o.\n","authors":["Neelabh Sinha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2406.11402v2.pdf","comment":"Submitted to ARR"},{"id":"http://arxiv.org/abs/2310.12404v2","updated":"2024-08-29T19:08:54Z","published":"2023-10-19T01:20:12Z","title":"Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative\n  Editing","summary":"  Creating music is iterative, requiring varied methods at each stage. However,\nexisting AI music systems fall short in orchestrating multiple subsystems for\ndiverse needs. To address this gap, we introduce Loop Copilot, a novel system\nthat enables users to generate and iteratively refine music through an\ninteractive, multi-round dialogue interface. The system uses a large language\nmodel to interpret user intentions and select appropriate AI models for task\nexecution. Each backend model is specialized for a specific task, and their\noutputs are aggregated to meet the user's requirements. To ensure musical\ncoherence, essential attributes are maintained in a centralized table. We\nevaluate the effectiveness of the proposed system through semi-structured\ninterviews and questionnaires, highlighting its utility not only in\nfacilitating music creation but also its potential for broader applications.\n","authors":["Yixiao Zhang","Akira Maezawa","Gus Xia","Kazuhiko Yamamoto","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2310.12404v2.pdf","comment":"Source code and demo video are available at\n  \\url{https://sites.google.com/view/loop-copilot}"},{"id":"http://arxiv.org/abs/2408.16857v1","updated":"2024-08-29T18:47:41Z","published":"2024-08-29T18:47:41Z","title":"Modeling offensive content detection for TikTok","summary":"  The advent of social media transformed interpersonal communication and\ninformation consumption processes. This digital landscape accommodates user\nintentions, also resulting in an increase of offensive language and harmful\nbehavior. Concurrently, social media platforms collect vast datasets comprising\nuser-generated content and behavioral information. These datasets are\ninstrumental for platforms deploying machine learning and data-driven\nstrategies, facilitating customer insights and countermeasures against social\nmanipulation mechanisms like disinformation and offensive content.\nNevertheless, the availability of such datasets, along with the application of\nvarious machine learning techniques, to researchers and practitioners, for\nspecific social media platforms regarding particular events, is limited. In\nparticular for TikTok, which offers unique tools for personalized content\ncreation and sharing, the existing body of knowledge would benefit from having\ndiverse comprehensive datasets and associated data analytics solutions on\noffensive content. While efforts from social media platforms, research, and\npractitioner communities are seen on this behalf, such content continues to\nproliferate. This translates to an essential need to make datasets publicly\navailable and build corresponding intelligent solutions. On this behalf, this\nresearch undertakes the collection and analysis of TikTok data containing\noffensive content, building a series of machine learning and deep learning\nmodels for offensive content detection. This is done aiming at answering the\nfollowing research question: \"How to develop a series of computational models\nto detect offensive content on TikTok?\". To this end, a Data Science\nmethodological approach is considered, 120.423 TikTok comments are collected,\nand on a balanced, binary classification approach, F1 score performance results\nof 0.863 is obtained.\n","authors":["Kasper Cools","Gideon Mailette de Buy Wenniger","Clara Maathuis"],"pdf_url":"https://arxiv.org/pdf/2408.16857v1.pdf","comment":"Accepted as a conference paper at DPSH 2024, 8 pages"},{"id":"http://arxiv.org/abs/2408.16809v1","updated":"2024-08-29T17:59:57Z","published":"2024-08-29T17:59:57Z","title":"See or Guess: Counterfactually Regularized Image Captioning","summary":"  Image captioning, which generates natural language descriptions of the visual\ninformation in an image, is a crucial task in vision-language research.\nPrevious models have typically addressed this task by aligning the generative\ncapabilities of machines with human intelligence through statistical fitting of\nexisting datasets. While effective for normal images, they may struggle to\naccurately describe those where certain parts of the image are obscured or\nedited, unlike humans who excel in such cases. These weaknesses they exhibit,\nincluding hallucinations and limited interpretability, often hinder performance\nin scenarios with shifted association patterns. In this paper, we present a\ngeneric image captioning framework that employs causal inference to make\nexisting models more capable of interventional tasks, and counterfactually\nexplainable. Our approach includes two variants leveraging either total effect\nor natural direct effect. Integrating them into the training process enables\nmodels to handle counterfactual scenarios, increasing their generalizability.\nExtensive experiments on various datasets show that our method effectively\nreduces hallucinations and improves the model's faithfulness to images,\ndemonstrating high portability across both small-scale and large-scale\nimage-to-text models. The code is available at\nhttps://github.com/Aman-4-Real/See-or-Guess.\n","authors":["Qian Cao","Xu Chen","Ruihua Song","Xiting Wang","Xinting Huang","Yuchen Ren"],"pdf_url":"https://arxiv.org/pdf/2408.16809v1.pdf","comment":"Accepted by ACM MM 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.16770v1","updated":"2024-08-29T17:59:54Z","published":"2024-08-29T17:59:54Z","title":"3D Whole-body Grasp Synthesis with Directional Controllability","summary":"  Synthesizing 3D whole-bodies that realistically grasp objects is useful for\nanimation, mixed reality, and robotics. This is challenging, because the hands\nand body need to look natural w.r.t. each other, the grasped object, as well as\nthe local scene (i.e., a receptacle supporting the object). Only recent work\ntackles this, with a divide-and-conquer approach; it first generates a\n\"guiding\" right-hand grasp, and then searches for bodies that match this.\nHowever, the guiding-hand synthesis lacks controllability and receptacle\nawareness, so it likely has an implausible direction (i.e., a body can't match\nthis without penetrating the receptacle) and needs corrections through major\npost-processing. Moreover, the body search needs exhaustive sampling and is\nexpensive. These are strong limitations. We tackle these with a novel method\ncalled CWGrasp. Our key idea is that performing geometry-based reasoning \"early\non,\" instead of \"too late,\" provides rich \"control\" signals for inference. To\nthis end, CWGrasp first samples a plausible reaching-direction vector (used\nlater for both the arm and hand) from a probabilistic model built via\nraycasting from the object and collision checking. Then, it generates a\nreaching body with a desired arm direction, as well as a \"guiding\" grasping\nhand with a desired palm direction that complies with the arm's one.\nEventually, CWGrasp refines the body to match the \"guiding\" hand, while\nplausibly contacting the scene. Notably, generating already-compatible \"parts\"\ngreatly simplifies the \"whole.\" Moreover, CWGrasp uniquely tackles both right-\nand left-hand grasps. We evaluate on the GRAB and ReplicaGrasp datasets.\nCWGrasp outperforms baselines, at lower runtime and budget, while all\ncomponents help performance. Code and models will be released.\n","authors":["Georgios Paschalidis","Romana Wilschut","Dimitrije AntiÄ","Omid Taheri","Dimitrios Tzionas"],"pdf_url":"https://arxiv.org/pdf/2408.16770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16768v1","updated":"2024-08-29T17:59:45Z","published":"2024-08-29T17:59:45Z","title":"SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners","summary":"  We introduce SAM2Point, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point\ninterprets any 3D data as a series of multi-directional videos, and leverages\nSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.\nOur framework supports various prompt types, including 3D points, boxes, and\nmasks, and can generalize across diverse scenarios, such as 3D objects, indoor\nscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple\n3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight\nthe robust generalization capabilities of SAM2Point. To our best knowledge, we\npresent the most faithful implementation of SAM in 3D, which may serve as a\nstarting point for future research in promptable 3D segmentation. Online Demo:\nhttps://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\nhttps://github.com/ZiyuGuo99/SAM2Point .\n","authors":["Ziyu Guo","Renrui Zhang","Xiangyang Zhu","Chengzhuo Tong","Peng Gao","Chunyuan Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2408.16768v1.pdf","comment":"Work in progress. Online Demo:\n  https://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\n  https://github.com/ZiyuGuo99/SAM2Point"},{"id":"http://arxiv.org/abs/2408.16769v1","updated":"2024-08-29T17:59:45Z","published":"2024-08-29T17:59:45Z","title":"PromptSmooth: Certifying Robustness of Medical Vision-Language Models\n  via Prompt Learning","summary":"  Medical vision-language models (Med-VLMs) trained on large datasets of\nmedical image-text pairs and later fine-tuned for specific tasks have emerged\nas a mainstream paradigm in medical image analysis. However, recent studies\nhave highlighted the susceptibility of these Med-VLMs to adversarial attacks,\nraising concerns about their safety and robustness. Randomized smoothing is a\nwell-known technique for turning any classifier into a model that is\ncertifiably robust to adversarial perturbations. However, this approach\nrequires retraining the Med-VLM-based classifier so that it classifies well\nunder Gaussian noise, which is often infeasible in practice. In this paper, we\npropose a novel framework called PromptSmooth to achieve efficient certified\nrobustness of Med-VLMs by leveraging the concept of prompt learning. Given any\npre-trained Med-VLM, PromptSmooth adapts it to handle Gaussian noise by\nlearning textual prompts in a zero-shot or few-shot manner, achieving a\ndelicate balance between accuracy and robustness, while minimizing the\ncomputational overhead. Moreover, PromptSmooth requires only a single model to\nhandle multiple noise levels, which substantially reduces the computational\ncost compared to traditional methods that rely on training a separate model for\neach noise level. Comprehensive experiments based on three Med-VLMs and across\nsix downstream datasets of various imaging modalities demonstrate the efficacy\nof PromptSmooth. Our code and models are available at\nhttps://github.com/nhussein/promptsmooth.\n","authors":["Noor Hussein","Fahad Shamshad","Muzammal Naseer","Karthik Nandakumar"],"pdf_url":"https://arxiv.org/pdf/2408.16769v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.16767v1","updated":"2024-08-29T17:59:40Z","published":"2024-08-29T17:59:40Z","title":"ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion\n  Model","summary":"  Advancements in 3D scene reconstruction have transformed 2D images from the\nreal world into 3D models, producing realistic 3D results from hundreds of\ninput photos. Despite great success in dense-view reconstruction scenarios,\nrendering a detailed scene from insufficient captured views is still an\nill-posed optimization problem, often resulting in artifacts and distortions in\nunseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction\nparadigm that reframes the ambiguous reconstruction challenge as a temporal\ngeneration task. The key insight is to unleash the strong generative prior of\nlarge pre-trained video diffusion models for sparse-view reconstruction.\nHowever, 3D view consistency struggles to be accurately preserved in directly\ngenerated video frames from pre-trained models. To address this, given limited\ninput views, the proposed ReconX first constructs a global point cloud and\nencodes it into a contextual space as the 3D structure condition. Guided by the\ncondition, the video diffusion model then synthesizes video frames that are\nboth detail-preserved and exhibit a high degree of 3D consistency, ensuring the\ncoherence of the scene from various perspectives. Finally, we recover the 3D\nscene from the generated video through a confidence-aware 3D Gaussian Splatting\noptimization scheme. Extensive experiments on various real-world datasets show\nthe superiority of our ReconX over state-of-the-art methods in terms of quality\nand generalizability.\n","authors":["Fangfu Liu","Wenqiang Sun","Hanyang Wang","Yikai Wang","Haowen Sun","Junliang Ye","Jun Zhang","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2408.16767v1.pdf","comment":"Project page: https://liuff19.github.io/ReconX"},{"id":"http://arxiv.org/abs/2408.16766v1","updated":"2024-08-29T17:59:30Z","published":"2024-08-29T17:59:30Z","title":"CSGO: Content-Style Composition in Text-to-Image Generation","summary":"  The diffusion model has shown exceptional capabilities in controlled image\ngeneration, which has further fueled interest in image style transfer. Existing\nworks mainly focus on training free-based methods (e.g., image inversion) due\nto the scarcity of specific data. In this study, we present a data construction\npipeline for content-style-stylized image triplets that generates and\nautomatically cleanses stylized data triplets. Based on this pipeline, we\nconstruct a dataset IMAGStyle, the first large-scale style transfer dataset\ncontaining 210k image triplets, available for the community to explore and\nresearch. Equipped with IMAGStyle, we propose CSGO, a style transfer model\nbased on end-to-end training, which explicitly decouples content and style\nfeatures employing independent feature injection. The unified CSGO implements\nimage-driven style transfer, text-driven stylized synthesis, and text\nediting-driven stylized synthesis. Extensive experiments demonstrate the\neffectiveness of our approach in enhancing style control capabilities in image\ngeneration. Additional visualization and access to the source code can be\nlocated on the project page: \\url{https://csgo-gen.github.io/}.\n","authors":["Peng Xing","Haofan Wang","Yanpeng Sun","Qixun Wang","Xu Bai","Hao Ai","Renyuan Huang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2408.16766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16762v1","updated":"2024-08-29T17:57:05Z","published":"2024-08-29T17:57:05Z","title":"UV-free Texture Generation with Denoising and Geodesic Heat Diffusions","summary":"  Seams, distortions, wasted UV space, vertex-duplication, and varying\nresolution over the surface are the most prominent issues of the standard\nUV-based texturing of meshes. These issues are particularly acute when\nautomatic UV-unwrapping techniques are used. For this reason, instead of\ngenerating textures in automatically generated UV-planes like most\nstate-of-the-art methods, we propose to represent textures as coloured\npoint-clouds whose colours are generated by a denoising diffusion probabilistic\nmodel constrained to operate on the surface of 3D objects. Our sampling and\nresolution agnostic generative model heavily relies on heat diffusion over the\nsurface of the meshes for spatial communication between points. To enable\nprocessing of arbitrarily sampled point-cloud textures and ensure long-distance\ntexture consistency we introduce a fast re-sampling of the mesh spectral\nproperties used during the heat diffusion and introduce a novel\nheat-diffusion-based self-attention mechanism. Our code and pre-trained models\nare available at github.com/simofoti/UV3-TeD.\n","authors":["Simone Foti","Stefanos Zafeiriou","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2408.16762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16760v1","updated":"2024-08-29T17:56:33Z","published":"2024-08-29T17:56:33Z","title":"OmniRe: Omni Urban Scene Reconstruction","summary":"  We introduce OmniRe, a holistic approach for efficiently reconstructing\nhigh-fidelity dynamic urban scenes from on-device logs. Recent methods for\nmodeling driving sequences using neural radiance fields or Gaussian Splatting\nhave demonstrated the potential of reconstructing challenging dynamic scenes,\nbut often overlook pedestrians and other non-vehicle dynamic actors, hindering\na complete pipeline for dynamic urban scene reconstruction. To that end, we\npropose a comprehensive 3DGS framework for driving scenes, named OmniRe, that\nallows for accurate, full-length reconstruction of diverse dynamic objects in a\ndriving log. OmniRe builds dynamic neural scene graphs based on Gaussian\nrepresentations and constructs multiple local canonical spaces that model\nvarious dynamic actors, including vehicles, pedestrians, and cyclists, among\nmany others. This capability is unmatched by existing methods. OmniRe allows us\nto holistically reconstruct different objects present in the scene,\nsubsequently enabling the simulation of reconstructed scenarios with all actors\nparticipating in real-time (~60Hz). Extensive evaluations on the Waymo dataset\nshow that our approach outperforms prior state-of-the-art methods\nquantitatively and qualitatively by a large margin. We believe our work fills a\ncritical gap in driving reconstruction.\n","authors":["Ziyu Chen","Jiawei Yang","Jiahui Huang","Riccardo de Lutio","Janick Martinez Esturo","Boris Ivanovic","Or Litany","Zan Gojcic","Sanja Fidler","Marco Pavone","Li Song","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16760v1.pdf","comment":"See the project page for code, video results and demos:\n  https://ziyc.github.io/omnire/"},{"id":"http://arxiv.org/abs/2407.10972v2","updated":"2024-08-29T17:55:52Z","published":"2024-07-15T17:59:55Z","title":"VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation","summary":"  In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io.\n","authors":["Bocheng Zou","Mu Cai","Jianrui Zhang","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2407.10972v2.pdf","comment":"Project Page: https://vgbench.github.io"},{"id":"http://arxiv.org/abs/2408.16757v1","updated":"2024-08-29T17:55:07Z","published":"2024-08-29T17:55:07Z","title":"Dissecting Out-of-Distribution Detection and Open-Set Recognition: A\n  Critical Analysis of Methods and Benchmarks","summary":"  Detecting test-time distribution shift has emerged as a key capability for\nsafely deployed machine learning models, with the question being tackled under\nvarious guises in recent years. In this paper, we aim to provide a consolidated\nview of the two largest sub-fields within the community: out-of-distribution\n(OOD) detection and open-set recognition (OSR). In particular, we aim to\nprovide rigorous empirical analysis of different methods across settings and\nprovide actionable takeaways for practitioners and researchers. Concretely, we\nmake the following contributions: (i) We perform rigorous cross-evaluation\nbetween state-of-the-art methods in the OOD detection and OSR settings and\nidentify a strong correlation between the performances of methods for them;\n(ii) We propose a new, large-scale benchmark setting which we suggest better\ndisentangles the problem tackled by OOD detection and OSR, re-evaluating\nstate-of-the-art OOD detection and OSR methods in this setting; (iii) We\nsurprisingly find that the best performing method on standard benchmarks\n(Outlier Exposure) struggles when tested at scale, while scoring rules which\nare sensitive to the deep feature magnitude consistently show promise; and (iv)\nWe conduct empirical analysis to explain these phenomena and highlight\ndirections for future research. Code:\n\\url{https://github.com/Visual-AI/Dissect-OOD-OSR}\n","authors":["Hongjun Wang","Sagar Vaze","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2408.16757v1.pdf","comment":"Accepted to IJCV, preprint version"},{"id":"http://arxiv.org/abs/2408.11817v2","updated":"2024-08-29T17:47:47Z","published":"2024-08-21T17:59:32Z","title":"GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models","summary":"  Large multimodal models (LMMs) have exhibited proficiencies across many\nvisual tasks. Although numerous well-known benchmarks exist to evaluate model\nperformance, they increasingly have insufficient headroom. As such, there is a\npressing need for a new generation of benchmarks challenging enough for the\nnext generation of LMMs. One area that LMMs show potential is graph analysis,\nspecifically, the tasks an analyst might typically perform when interpreting\nfigures such as estimating the mean, intercepts or correlations of functions\nand data series. In this work, we introduce GRAB, a graph analysis benchmark,\nfit for current and future frontier LMMs. Our benchmark is entirely synthetic,\nensuring high-quality, noise-free questions. GRAB is comprised of 2170\nquestions, covering four tasks and 23 graph properties. We evaluate 20 LMMs on\nGRAB, finding it to be a challenging benchmark, with the highest performing\nmodel attaining a score of just 21.7%. Finally, we conduct various ablations to\ninvestigate where the models succeed and struggle. We release GRAB to encourage\nprogress in this important, growing domain.\n","authors":["Jonathan Roberts","Kai Han","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2408.11817v2.pdf","comment":"V2: Fixed references formatting"},{"id":"http://arxiv.org/abs/2408.16730v1","updated":"2024-08-29T17:21:58Z","published":"2024-08-29T17:21:58Z","title":"VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation","summary":"  A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.\n","authors":["Shiwei Wu","Joya Chen","Kevin Qinghong Lin","Qimeng Wang","Yan Gao","Qianli Xu","Tong Xu","Yao Hu","Enhong Chen","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2408.16730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09359v4","updated":"2024-08-29T17:21:27Z","published":"2024-04-14T21:14:47Z","title":"Evaluation Framework for Feedback Generation Methods in Skeletal\n  Movement Assessment","summary":"  The application of machine-learning solutions to movement assessment from\nskeleton videos has attracted significant research attention in recent years.\nThis advancement has made rehabilitation at home more accessible, utilizing\nmovement assessment algorithms that can operate on affordable equipment for\nhuman pose detection and analysis from 2D or 3D videos. While the primary\nobjective of automatic assessment tasks is to score movements, the automatic\ngeneration of feedback highlighting key movement issues has the potential to\nsignificantly enhance and accelerate the rehabilitation process. While numerous\nresearch works exist in the field of automatic movement assessment, only a\nhandful address feedback generation. In this study, we propose terminology and\ncriteria for the classification, evaluation, and comparison of feedback\ngeneration solutions. We discuss the challenges associated with each feedback\ngeneration approach and use our proposed criteria to classify existing\nsolutions. To our knowledge, this is the first work that formulates feedback\ngeneration in skeletal movement assessment.\n","authors":["Tal Hakim"],"pdf_url":"https://arxiv.org/pdf/2404.09359v4.pdf","comment":"Accepted to xAI4Biometrics 2024 at ECCV 2024"},{"id":"http://arxiv.org/abs/2408.16729v1","updated":"2024-08-29T17:20:59Z","published":"2024-08-29T17:20:59Z","title":"Prediction-Feedback DETR for Temporal Action Detection","summary":"  Temporal Action Detection (TAD) is fundamental yet challenging for real-world\nvideo applications. Leveraging the unique benefits of transformers, various\nDETR-based approaches have been adopted in TAD. However, it has recently been\nidentified that the attention collapse in self-attention causes the performance\ndegradation of DETR for TAD. Building upon previous research, this paper newly\naddresses the attention collapse problem in cross-attention within DETR-based\nTAD methods. Moreover, our findings reveal that cross-attention exhibits\npatterns distinct from predictions, indicating a short-cut phenomenon. To\nresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),\nwhich utilizes predictions to restore the collapse and align the cross- and\nself-attention with predictions. Specifically, we devise novel\nprediction-feedback objectives using guidance from the relations of the\npredictions. As a result, Pred-DETR significantly alleviates the collapse and\nachieves state-of-the-art performance among DETR-based methods on various\nchallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and\nFineAction.\n","authors":["Jihwan Kim","Miso Lee","Cheol-Ho Cho","Jihyun Lee","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2408.16729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11933v2","updated":"2024-08-29T17:16:13Z","published":"2024-06-17T15:41:57Z","title":"OpticalRS-4M: Scaling Efficient Masked Autoencoder Learning on Large\n  Remote Sensing Dataset","summary":"  Masked Image Modeling (MIM) has become an essential method for building\nfoundational visual models in remote sensing (RS). However, the limitations in\nsize and diversity of existing RS datasets restrict the ability of MIM methods\nto learn generalizable representations. Additionally, conventional MIM\ntechniques, which require reconstructing all tokens, introduce unnecessary\ncomputational overhead. To address these issues, we present a new pre-training\npipeline for RS models, featuring the creation of a large-scale RS dataset and\nan efficient MIM approach. We curated a high-quality dataset named OpticalRS-4M\nby collecting publicly available RS datasets and processing them through\nexclusion, slicing, and deduplication. OpticalRS-4M comprises 4 million optical\nimages covering various RS tasks, such as object detection and pixel\nsegmentation. To enhance efficiency, we propose SelectiveMAE, a pre-training\nmethod that dynamically encodes and reconstructs semantically rich patch\ntokens, thereby reducing the inefficiencies of traditional MIM models caused by\nredundant background pixels in RS images. Extensive experiments demonstrate\nthat OpticalRS-4M significantly improves classification, detection, and\nsegmentation performance, while SelectiveMAE increases training efficiency over\n2 times. This highlights the effectiveness and scalability of our pipeline in\ndeveloping RS foundational models.\n","authors":["Fengxiang Wang","Hongzhen Wang","Di Wang","Zonghao Guo","Zhenyu Zhong","Long Lan","Jing Zhang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2406.11933v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16719v1","updated":"2024-08-29T17:11:38Z","published":"2024-08-29T17:11:38Z","title":"H-SGANet: Hybrid Sparse Graph Attention Network for Deformable Medical\n  Image Registration","summary":"  The integration of Convolutional Neural Network (ConvNet) and Transformer has\nemerged as a strong candidate for image registration, leveraging the strengths\nof both models and a large parameter space. However, this hybrid model,\ntreating brain MRI volumes as grid or sequence structures, faces challenges in\naccurately representing anatomical connectivity, diverse brain regions, and\nvital connections contributing to the brain's internal architecture. Concerns\nalso arise regarding the computational expense and GPU memory usage associated\nwith this model. To tackle these issues, a lightweight hybrid sparse graph\nattention network (H-SGANet) has been developed. This network incorporates a\ncentral mechanism, Sparse Graph Attention (SGA), based on a Vision Graph Neural\nNetwork (ViG) with predetermined anatomical connections. The SGA module expands\nthe model's receptive field and seamlessly integrates into the network. To\nfurther amplify the advantages of the hybrid network, the Separable\nSelf-Attention (SSA) is employed as an enhanced token mixer, integrated with\ndepth-wise convolution to constitute SSAFormer. This strategic integration is\ndesigned to more effectively extract long-range dependencies. As a hybrid\nConvNet-ViG-Transformer model, H-SGANet offers threefold benefits for\nvolumetric medical image registration. It optimizes fixed and moving images\nconcurrently through a hybrid feature fusion layer and an end-to-end learning\nframework. Compared to VoxelMorph, a model with a similar parameter count,\nH-SGANet demonstrates significant performance enhancements of 3.5% and 1.5% in\nDice score on the OASIS dataset and LPBA40 dataset, respectively.\n","authors":["Yufeng Zhou","Wenming Cao"],"pdf_url":"https://arxiv.org/pdf/2408.16719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16704v1","updated":"2024-08-29T16:58:10Z","published":"2024-08-29T16:58:10Z","title":"One-Shot Learning Meets Depth Diffusion in Multi-Object Videos","summary":"  Creating editable videos that depict complex interactions between multiple\nobjects in various artistic styles has long been a challenging task in\nfilmmaking. Progress is often hampered by the scarcity of data sets that\ncontain paired text descriptions and corresponding videos that showcase these\ninteractions. This paper introduces a novel depth-conditioning approach that\nsignificantly advances this field by enabling the generation of coherent and\ndiverse videos from just a single text-video pair using a pre-trained\ndepth-aware Text-to-Image (T2I) model. Our method fine-tunes the pre-trained\nmodel to capture continuous motion by employing custom-designed spatial and\ntemporal attention mechanisms. During inference, we use the DDIM inversion to\nprovide structural guidance for video generation. This innovative technique\nallows for continuously controllable depth in videos, facilitating the\ngeneration of multiobject interactions while maintaining the concept generation\nand compositional strengths of the original T2I model across various artistic\nstyles, such as photorealism, animation, and impressionism.\n","authors":["Anisha Jain"],"pdf_url":"https://arxiv.org/pdf/2408.16704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16700v1","updated":"2024-08-29T16:51:07Z","published":"2024-08-29T16:51:07Z","title":"GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative\n  Models","summary":"  Recent progress in Text-to-Image (T2I) generative models has enabled\nhigh-quality image generation. As performance and accessibility increase, these\nmodels are gaining significant attraction and popularity: ensuring their\nfairness and safety is a priority to prevent the dissemination and perpetuation\nof biases. However, existing studies in bias detection focus on closed sets of\npredefined biases (e.g., gender, ethnicity). In this paper, we propose a\ngeneral framework to identify, quantify, and explain biases in an open set\nsetting, i.e. without requiring a predefined set. This pipeline leverages a\nLarge Language Model (LLM) to propose biases starting from a set of captions.\nNext, these captions are used by the target generative model for generating a\nset of images. Finally, Vision Question Answering (VQA) is leveraged for bias\nevaluation. We show two variations of this framework: OpenBias and GradBias.\nOpenBias detects and quantifies biases, while GradBias determines the\ncontribution of individual prompt words on biases. OpenBias effectively detects\nboth well-known and novel biases related to people, objects, and animals and\nhighly aligns with existing closed-set bias detection methods and human\njudgment. GradBias shows that neutral words can significantly influence biases\nand it outperforms several baselines, including state-of-the-art foundation\nmodels. Code available here: https://github.com/Moreno98/GradBias.\n","authors":["Moreno D'IncÃ ","Elia Peruzzo","Massimiliano Mancini","Xingqian Xu","Humphrey Shi","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2408.16700v1.pdf","comment":"Under review. Code: https://github.com/Moreno98/GradBias"},{"id":"http://arxiv.org/abs/2408.16690v1","updated":"2024-08-29T16:37:58Z","published":"2024-08-29T16:37:58Z","title":"Generic Objects as Pose Probes for Few-Shot View Synthesis","summary":"  Radiance fields including NeRFs and 3D Gaussians demonstrate great potential\nin high-fidelity rendering and scene reconstruction, while they require a\nsubstantial number of posed images as inputs. COLMAP is frequently employed for\npreprocessing to estimate poses, while it necessitates a large number of\nfeature matches to operate effectively, and it struggles with scenes\ncharacterized by sparse features, large baselines between images, or a limited\nnumber of input images. We aim to tackle few-view NeRF reconstruction using\nonly 3 to 6 unposed scene images. Traditional methods often use calibration\nboards but they are not common in images. We propose a novel idea of utilizing\neveryday objects, commonly found in both images and real life, as \"pose\nprobes\". The probe object is automatically segmented by SAM, whose shape is\ninitialized from a cube. We apply a dual-branch volume rendering optimization\n(object NeRF and scene NeRF) to constrain the pose optimization and jointly\nrefine the geometry. Specifically, object poses of two views are first\nestimated by PnP matching in an SDF representation, which serves as initial\nposes. PnP matching, requiring only a few features, is suitable for\nfeature-sparse scenes. Additional views are incrementally incorporated to\nrefine poses from preceding views. In experiments, PoseProbe achieves\nstate-of-the-art performance in both pose estimation and novel view synthesis\nacross multiple datasets. We demonstrate its effectiveness, particularly in\nfew-view and large-baseline scenes where COLMAP struggles. In ablations, using\ndifferent objects in a scene yields comparable performance.\n","authors":["Zhirui Gao","Renjiao Yi","Chenyang Zhu","Ke Zhuang","Wei Chen","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2408.16690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16684v1","updated":"2024-08-29T16:31:05Z","published":"2024-08-29T16:31:05Z","title":"PartFormer: Awakening Latent Diverse Representation from Vision\n  Transformer for Object Re-Identification","summary":"  Extracting robust feature representation is critical for object\nre-identification to accurately identify objects across non-overlapping\ncameras. Although having a strong representation ability, the Vision\nTransformer (ViT) tends to overfit on most distinct regions of training data,\nlimiting its generalizability and attention to holistic object features.\nMeanwhile, due to the structural difference between CNN and ViT, fine-grained\nstrategies that effectively address this issue in CNN do not continue to be\nsuccessful in ViT. To address this issue, by observing the latent diverse\nrepresentation hidden behind the multi-head attention, we present PartFormer,\nan innovative adaptation of ViT designed to overcome the granularity\nlimitations in object Re-ID tasks. The PartFormer integrates a Head\nDisentangling Block (HDB) that awakens the diverse representation of multi-head\nself-attention without the typical loss of feature richness induced by\nconcatenation and FFN layers post-attention. To avoid the homogenization of\nattention heads and promote robust part-based feature learning, two head\ndiversity constraints are imposed: attention diversity constraint and\ncorrelation diversity constraint. These constraints enable the model to exploit\ndiverse and discriminative feature representations from different attention\nheads. Comprehensive experiments on various object Re-ID benchmarks demonstrate\nthe superiority of the PartFormer. Specifically, our framework significantly\noutperforms state-of-the-art by 2.4\\% mAP scores on the most challenging MSMT17\ndataset.\n","authors":["Lei Tan","Pingyang Dai","Jie Chen","Liujuan Cao","Yongjian Wu","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.16684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18915v3","updated":"2024-08-29T16:07:30Z","published":"2024-06-27T06:12:01Z","title":"Manipulate-Anything: Automating Real-World Robots using Vision-Language\n  Models","summary":"  Large-scale endeavors like and widespread community efforts such as\nOpen-X-Embodiment have contributed to growing the scale of robot demonstration\ndata. However, there is still an opportunity to improve the quality, quantity,\nand diversity of robot demonstration data. Although vision-language models have\nbeen shown to automatically generate demonstration data, their utility has been\nlimited to environments with privileged state information, they require\nhand-designed skills, and are limited to interactions with few object\ninstances. We propose Manipulate-Anything, a scalable automated generation\nmethod for real-world robotic manipulation. Unlike prior work, our method can\noperate in real-world environments without any privileged state information,\nhand-designed skills, and can manipulate any static object. We evaluate our\nmethod using two setups. First, Manipulate-Anything successfully generates\ntrajectories for all 7 real-world and 14 simulation tasks, significantly\noutperforming existing methods like VoxPoser. Second, Manipulate-Anything's\ndemonstrations can train more robust behavior cloning policies than training\nwith human demonstrations, or from data generated by VoxPoser, Scaling-up, and\nCode-As-Policies. We believe Manipulate-Anything can be a scalable method for\nboth generating data for robotics and solving novel tasks in a zero-shot\nsetting. Project page: https://robot-ma.github.io/.\n","authors":["Jiafei Duan","Wentao Yuan","Wilbert Pumacay","Yi Ru Wang","Kiana Ehsani","Dieter Fox","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.18915v3.pdf","comment":"Project page: https://robot-ma.github.io/. All supplementary\n  material, prompts and code can be found on the project page"},{"id":"http://arxiv.org/abs/2408.16662v1","updated":"2024-08-29T16:05:22Z","published":"2024-08-29T16:05:22Z","title":"Space3D-Bench: Spatial 3D Question Answering Benchmark","summary":"  Answering questions about the spatial properties of the environment poses\nchallenges for existing language and vision foundation models due to a lack of\nunderstanding of the 3D world notably in terms of relationships between\nobjects. To push the field forward, multiple 3D Q&A datasets were proposed\nwhich, overall, provide a variety of questions, but they individually focus on\nparticular aspects of 3D reasoning or are limited in terms of data modalities.\nTo address this, we present Space3D-Bench - a collection of 1000 general\nspatial questions and answers related to scenes of the Replica dataset which\noffers a variety of data modalities: point clouds, posed RGB-D images,\nnavigation meshes and 3D object detections. To ensure that the questions cover\na wide range of 3D objectives, we propose an indoor spatial questions taxonomy\ninspired by geographic information systems and use it to balance the dataset\naccordingly. Moreover, we provide an assessment system that grades natural\nlanguage responses based on predefined ground-truth answers by leveraging a\nVision Language Model's comprehension of both text and images to compare the\nresponses with ground-truth textual information or relevant visual data.\nFinally, we introduce a baseline called RAG3D-Chat integrating the world\nunderstanding of foundation models with rich context retrieval, achieving an\naccuracy of 67% on the proposed dataset.\n","authors":["Emilia Szymanska","Mihai Dusmanu","Jan-Willem Buurlage","Mahdi Rad","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2408.16662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16661v1","updated":"2024-08-29T16:05:05Z","published":"2024-08-29T16:05:05Z","title":"Eigen-Cluster VIS: Improving Weakly-supervised Video Instance\n  Segmentation by Leveraging Spatio-temporal Consistency","summary":"  The performance of Video Instance Segmentation (VIS) methods has improved\nsignificantly with the advent of transformer networks. However, these networks\noften face challenges in training due to the high annotation cost. To address\nthis, unsupervised and weakly-supervised methods have been developed to reduce\nthe dependency on annotations. This work introduces a novel weakly-supervised\nmethod called Eigen-cluster VIS that, without requiring any mask annotations,\nachieves competitive accuracy compared to other VIS approaches. This method is\nbased on two key innovations: a Temporal Eigenvalue Loss (TEL) and a clip-level\nQuality Cluster Coefficient (QCC). The TEL ensures temporal coherence by\nleveraging the eigenvalues of the Laplacian matrix derived from graph adjacency\nmatrices. By minimizing the mean absolute error (MAE) between the eigenvalues\nof adjacent frames, this loss function promotes smooth transitions and stable\nsegmentation boundaries over time, reducing temporal discontinuities and\nimproving overall segmentation quality. The QCC employs the K-means method to\nensure the quality of spatio-temporal clusters without relying on ground truth\nmasks. Using the Davies-Bouldin score, the QCC provides an unsupervised measure\nof feature discrimination, allowing the model to self-evaluate and adapt to\nvarying object distributions, enhancing robustness during the testing phase.\nThese enhancements are computationally efficient and straightforward, offering\nsignificant performance gains without additional annotated data. The proposed\nEigen-Cluster VIS method is evaluated on the YouTube-VIS 2019/2021 and OVIS\ndatasets, demonstrating that it effectively narrows the performance gap between\nthe fully-supervised and weakly-supervised VIS approaches. The code is\navailable on: https://github.com/farnooshar/EigenClusterVIS\n","authors":["Farnoosh Arefi","Amir M. Mansourian","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2408.16661v1.pdf","comment":"12 pages, 6 Figures, 5 tabels"},{"id":"http://arxiv.org/abs/2407.04559v2","updated":"2024-08-29T15:58:09Z","published":"2024-07-05T14:48:15Z","title":"Not (yet) the whole story: Evaluating Visual Storytelling Requires More\n  than Measuring Coherence, Grounding, and Repetition","summary":"  Visual storytelling consists in generating a natural language story given a\ntemporally ordered sequence of images. This task is not only challenging for\nmodels, but also very difficult to evaluate with automatic metrics since there\nis no consensus about what makes a story 'good'. In this paper, we introduce a\nnovel method that measures story quality in terms of human likeness regarding\nthree key aspects highlighted in previous work: visual grounding, coherence,\nand repetitiveness. We then use this method to evaluate the stories generated\nby several models, showing that the foundation model LLaVA obtains the best\nresult, but only slightly so compared to TAPM, a 50-times smaller visual\nstorytelling model. Upgrading the visual and language components of TAPM\nresults in a model that yields competitive performance with a relatively low\nnumber of parameters. Finally, we carry out a human evaluation study, whose\nresults suggest that a 'good' story may require more than a human-like level of\nvisual grounding, coherence, and repetition.\n","authors":["Aditya K Surikuchi","Raquel FernÃ¡ndez","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2407.04559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16647v1","updated":"2024-08-29T15:52:56Z","published":"2024-08-29T15:52:56Z","title":"DriveGenVLM: Real-world Video Generation for Vision Language Model based\n  Autonomous Driving","summary":"  The advancement of autonomous driving technologies necessitates increasingly\nsophisticated methods for understanding and predicting real-world scenarios.\nVision language models (VLMs) are emerging as revolutionary tools with\nsignificant potential to influence autonomous driving. In this paper, we\npropose the DriveGenVLM framework to generate driving videos and use VLMs to\nunderstand them. To achieve this, we employ a video generation framework\ngrounded in denoising diffusion probabilistic models (DDPM) aimed at predicting\nreal-world video sequences. We then explore the adequacy of our generated\nvideos for use in VLMs by employing a pre-trained model known as Efficient\nIn-context Learning on Egocentric Videos (EILEV). The diffusion model is\ntrained with the Waymo open dataset and evaluated using the Fr\\'echet Video\nDistance (FVD) score to ensure the quality and realism of the generated videos.\nCorresponding narrations are provided by EILEV for these generated videos,\nwhich may be beneficial in the autonomous driving domain. These narrations can\nenhance traffic scene understanding, aid in navigation, and improve planning\ncapabilities. The integration of video generation with VLMs in the DriveGenVLM\nframework represents a significant step forward in leveraging advanced AI\nmodels to address complex challenges in autonomous driving.\n","authors":["Yongjie Fu","Anmol Jain","Xuan Di","Xu Chen","Zhaobin Mo"],"pdf_url":"https://arxiv.org/pdf/2408.16647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16645v1","updated":"2024-08-29T15:51:06Z","published":"2024-08-29T15:51:06Z","title":"SODAWideNet++: Combining Attention and Convolutions for Salient Object\n  Detection","summary":"  Salient Object Detection (SOD) has traditionally relied on feature refinement\nmodules that utilize the features of an ImageNet pre-trained backbone. However,\nthis approach limits the possibility of pre-training the entire network because\nof the distinct nature of SOD and image classification. Additionally, the\narchitecture of these backbones originally built for Image classification is\nsub-optimal for a dense prediction task like SOD. To address these issues, we\npropose a novel encoder-decoder-style neural network called SODAWideNet++ that\nis designed explicitly for SOD. Inspired by the vision transformers ability to\nattain a global receptive field from the initial stages, we introduce the\nAttention Guided Long Range Feature Extraction (AGLRFE) module, which combines\nlarge dilated convolutions and self-attention. Specifically, we use attention\nfeatures to guide long-range information extracted by multiple dilated\nconvolutions, thus taking advantage of the inductive biases of a convolution\noperation and the input dependency brought by self-attention. In contrast to\nthe current paradigm of ImageNet pre-training, we modify 118K annotated images\nfrom the COCO semantic segmentation dataset by binarizing the annotations to\npre-train the proposed model end-to-end. Further, we supervise the background\npredictions along with the foreground to push our model to generate accurate\nsaliency predictions. SODAWideNet++ performs competitively on five different\ndatasets while only containing 35% of the trainable parameters compared to the\nstate-of-the-art models. The code and pre-computed saliency maps are provided\nat https://github.com/VimsLab/SODAWideNetPlusPlus.\n","authors":["Rohit Venkata Sai Dulam","Chandra Kambhamettu"],"pdf_url":"https://arxiv.org/pdf/2408.16645v1.pdf","comment":"Accepted at ICPR 2024"},{"id":"http://arxiv.org/abs/2408.16638v1","updated":"2024-08-29T15:42:06Z","published":"2024-08-29T15:42:06Z","title":"3D Pose-Based Temporal Action Segmentation for Figure Skating: A\n  Fine-Grained and Jump Procedure-Aware Annotation Approach","summary":"  Understanding human actions from videos is essential in many domains,\nincluding sports. In figure skating, technical judgments are performed by\nwatching skaters' 3D movements, and its part of the judging procedure can be\nregarded as a Temporal Action Segmentation (TAS) task. TAS tasks in figure\nskating that automatically assign temporal semantics to video are actively\nresearched. However, there is a lack of datasets and effective methods for TAS\ntasks requiring 3D pose data. In this study, we first created the FS-Jump3D\ndataset of complex and dynamic figure skating jumps using optical markerless\nmotion capture. We also propose a new fine-grained figure skating jump TAS\ndataset annotation method with which TAS models can learn jump procedures. In\nthe experimental results, we validated the usefulness of 3D pose features as\ninput and the fine-grained dataset for the TAS model in figure skating.\nFS-Jump3D Dataset is available at https://github.com/ryota-skating/FS-Jump3D.\n","authors":["Ryota Tanaka","Tomohiro Suzuki","Keisuke Fujii"],"pdf_url":"https://arxiv.org/pdf/2408.16638v1.pdf","comment":"10 pages, 7th ACM International Workshop on Multimedia Content\n  Analysis in Sports"},{"id":"http://arxiv.org/abs/2405.20743v2","updated":"2024-08-29T15:31:58Z","published":"2024-05-31T10:13:17Z","title":"Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent\n  Codes","summary":"  Trajectory forecasting is crucial for video surveillance analytics, as it\nenables the anticipation of future movements for a set of agents, e.g.\nbasketball players engaged in intricate interactions with long-term intentions.\nDeep generative models offer a natural learning approach for trajectory\nforecasting, yet they encounter difficulties in achieving an optimal balance\nbetween sampling fidelity and diversity. We address this challenge by\nleveraging Vector Quantized Variational Autoencoders (VQ-VAEs), which utilize a\ndiscrete latent space to tackle the issue of posterior collapse. Specifically,\nwe introduce an instance-based codebook that allows tailored latent\nrepresentations for each example. In a nutshell, the rows of the codebook are\ndynamically adjusted to reflect contextual information (i.e., past motion\npatterns extracted from the observed trajectories). In this way, the\ndiscretization process gains flexibility, leading to improved reconstructions.\nNotably, instance-level dynamics are injected into the codebook through\nlow-rank updates, which restrict the customization of the codebook to a lower\ndimension space. The resulting discrete space serves as the basis of the\nsubsequent step, which regards the training of a diffusion-based predictive\nmodel. We show that such a two-fold framework, augmented with instance-level\ndiscretization, leads to accurate and diverse forecasts, yielding\nstate-of-the-art performance on three established benchmarks.\n","authors":["Riccardo Benaglia","Angelo Porrello","Pietro Buzzega","Simone Calderara","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2405.20743v2.pdf","comment":"15 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2408.16623v1","updated":"2024-08-29T15:31:51Z","published":"2024-08-29T15:31:51Z","title":"Turbulence Strength $C_n^2$ Estimation from Video using Physics-based\n  Deep Learning","summary":"  Images captured from a long distance suffer from dynamic image distortion due\nto turbulent flow of air cells with random temperatures, and thus refractive\nindices. This phenomenon, known as image dancing, is commonly characterized by\nits refractive-index structure constant $C_n^2$ as a measure of the turbulence\nstrength. For many applications such as atmospheric forecast model,\nlong-range/astronomy imaging, and aviation safety, optical communication\ntechnology, $C_n^2$ estimation is critical for accurately sensing the turbulent\nenvironment. Previous methods for $C_n^2$ estimation include estimation from\nmeteorological data (temperature, relative humidity, wind shear, etc.) for\nsingle-point measurements, two-ended pathlength measurements from optical\nscintillometer for path-averaged $C_n^2$, and more recently estimating $C_n^2$\nfrom passive video cameras for low cost and hardware complexity. In this paper,\nwe present a comparative analysis of classical image gradient methods for\n$C_n^2$ estimation and modern deep learning-based methods leveraging\nconvolutional neural networks. To enable this, we collect a dataset of video\ncapture along with reference scintillometer measurements for ground truth, and\nwe release this unique dataset to the scientific community. We observe that\ndeep learning methods can achieve higher accuracy when trained on similar data,\nbut suffer from generalization errors to other, unseen imagery as compared to\nclassical methods. To overcome this trade-off, we present a novel physics-based\nnetwork architecture that combines learned convolutional layers with a\ndifferentiable image gradient method that maintains high accuracy while being\ngeneralizable across image datasets.\n","authors":["Ripon Kumar Saha","Esen Salcin","Jihoo Kim","Joseph Smith","Suren Jayasuriya"],"pdf_url":"https://arxiv.org/pdf/2408.16623v1.pdf","comment":"Code Available: https://github.com/Riponcs/Cn2Estimation"},{"id":"http://arxiv.org/abs/2408.16622v1","updated":"2024-08-29T15:31:43Z","published":"2024-08-29T15:31:43Z","title":"Sparse Signal Reconstruction for Overdispersed Low-photon Count\n  Biomedical Imaging Using $\\ell_p$ Total Variation","summary":"  The negative binomial model, which generalizes the Poisson distribution\nmodel, can be found in applications involving low-photon signal recovery,\nincluding medical imaging. Recent studies have explored several regularization\nterms for the negative binomial model, such as the $\\ell_p$ quasi-norm with $0\n< p < 1$, $\\ell_1$ norm, and the total variation (TV) quasi-seminorm for\npromoting sparsity in signal recovery. These penalty terms have been shown to\nimprove image reconstruction outcomes. In this paper, we investigate the\n$\\ell_p$ quasi-seminorm, both isotropic and anisotropic $\\ell_p$ TV\nquasi-seminorms, within the framework of the negative binomial statistical\nmodel. This problem can be formulated as an optimization problem, which we\nsolve using a gradient-based approach. We present comparisons between the\nnegative binomial and Poisson statistical models using the $\\ell_p$ TV\nquasi-seminorm as well as common penalty terms. Our experimental results\nhighlight the efficacy of the proposed method.\n","authors":["Yu Lu","Roummel F. Marcia"],"pdf_url":"https://arxiv.org/pdf/2408.16622v1.pdf","comment":"5 pages, Accepted by the IEEE International Symposium on Biomedical\n  Imaging (ISBI)"},{"id":"http://arxiv.org/abs/2408.13140v2","updated":"2024-08-29T15:31:35Z","published":"2024-08-23T15:02:09Z","title":"Verification of Geometric Robustness of Neural Networks via Piecewise\n  Linear Approximation and Lipschitz Optimisation","summary":"  We address the problem of verifying neural networks against geometric\ntransformations of the input image, including rotation, scaling, shearing, and\ntranslation. The proposed method computes provably sound piecewise linear\nconstraints for the pixel values by using sampling and linear approximations in\ncombination with branch-and-bound Lipschitz optimisation. The method obtains\nprovably tighter over-approximations of the perturbation region than the\npresent state-of-the-art. We report results from experiments on a comprehensive\nset of verification benchmarks on MNIST and CIFAR10. We show that our proposed\nimplementation resolves up to 32% more verification cases than present\napproaches.\n","authors":["Ben Batten","Yang Zheng","Alessandro De Palma","Panagiotis Kouvaros","Alessio Lomuscio"],"pdf_url":"https://arxiv.org/pdf/2408.13140v2.pdf","comment":"ECAI 2024"},{"id":"http://arxiv.org/abs/2408.16621v1","updated":"2024-08-29T15:28:42Z","published":"2024-08-29T15:28:42Z","title":"Towards Infusing Auxiliary Knowledge for Distracted Driver Detection","summary":"  Distracted driving is a leading cause of road accidents globally.\nIdentification of distracted driving involves reliably detecting and\nclassifying various forms of driver distraction (e.g., texting, eating, or\nusing in-car devices) from in-vehicle camera feeds to enhance road safety. This\ntask is challenging due to the need for robust models that can generalize to a\ndiverse set of driver behaviors without requiring extensive annotated datasets.\nIn this paper, we propose KiD3, a novel method for distracted driver detection\n(DDD) by infusing auxiliary knowledge about semantic relations between entities\nin a scene and the structural configuration of the driver's pose. Specifically,\nwe construct a unified framework that integrates the scene graphs, and driver\npose information with the visual cues in video frames to create a holistic\nrepresentation of the driver's actions.Our results indicate that KiD3 achieves\na 13.64% accuracy improvement over the vision-only baseline by incorporating\nsuch auxiliary knowledge with visual information.\n","authors":["Ishwar B Balappanawar","Ashmit Chamoli","Ruwan Wickramarachchi","Aditya Mishra","Ponnurangam Kumaraguru","Amit P. Sheth"],"pdf_url":"https://arxiv.org/pdf/2408.16621v1.pdf","comment":"Accepted at KiL 2024: Workshop on Knowledge-infused Learning\n  co-located with 30th ACM KDD Conference"},{"id":"http://arxiv.org/abs/2408.14698v2","updated":"2024-08-29T15:14:48Z","published":"2024-08-26T23:52:27Z","title":"Smart Multi-Modal Search: Contextual Sparse and Dense Embedding\n  Integration in Adobe Express","summary":"  As user content and queries become increasingly multi-modal, the need for\neffective multi-modal search systems has grown. Traditional search systems\noften rely on textual and metadata annotations for indexed images, while\nmulti-modal embeddings like CLIP enable direct search using text and image\nembeddings. However, embedding-based approaches face challenges in integrating\ncontextual features such as user locale and recency. Building a scalable\nmulti-modal search system requires fine-tuning several components. This paper\npresents a multi-modal search architecture and a series of AB tests that\noptimize embeddings and multi-modal technologies in Adobe Express template\nsearch. We address considerations such as embedding model selection, the roles\nof embeddings in matching and ranking, and the balance between dense and sparse\nembeddings. Our iterative approach demonstrates how utilizing sparse, dense,\nand contextual features enhances short and long query search, significantly\nreduces null rates (over 70\\%), and increases click-through rates (CTR). Our\nfindings provide insights into developing robust multi-modal search systems,\nthereby enhancing relevance for complex queries.\n","authors":["Cherag Aroraa","Tracy Holloway King","Jayant Kumar","Yi Lu","Sanat Sharma","Arvind Srikantan","David Uvalle","Josep Valls-Vargas","Harsha Vardhan"],"pdf_url":"https://arxiv.org/pdf/2408.14698v2.pdf","comment":"CIKM 2024 (International Conference on Information and Knowledge\n  Management), Multimodal Search and Recommendations Workshop"},{"id":"http://arxiv.org/abs/2401.12972v3","updated":"2024-08-29T15:11:29Z","published":"2024-01-23T18:58:35Z","title":"On the Efficacy of Text-Based Input Modalities for Action Anticipation","summary":"  Anticipating future actions is a highly challenging task due to the diversity\nand scale of potential future actions; yet, information from different\nmodalities help narrow down plausible action choices. Each modality can provide\ndiverse and often complementary context for the model to learn from. While\nprevious multi-modal methods leverage information from modalities such as video\nand audio, we primarily explore how text descriptions of actions and objects\ncan also lead to more accurate action anticipation by providing additional\ncontextual cues, e.g., about the environment and its contents. We propose a\nMulti-modal Contrastive Anticipative Transformer (M-CAT), a video transformer\narchitecture that jointly learns from multi-modal features and text\ndescriptions of actions and objects. We train our model in two stages, where\nthe model first learns to align video clips with descriptions of future\nactions, and is subsequently fine-tuned to predict future actions. Compared to\nexisting methods, M-CAT has the advantage of learning additional context from\ntwo types of text inputs: rich descriptions of future actions during\npre-training, and, text descriptions for detected objects and actions during\nmodality feature fusion. Through extensive experimental evaluation, we\ndemonstrate that our model outperforms previous methods on the EpicKitchens\ndatasets, and show that using simple text descriptions of actions and objects\naid in more effective action anticipation. In addition, we examine the impact\nof object and action information obtained via text, and perform extensive\nablations.\n","authors":["Apoorva Beedu","Harish Haresamudram","Karan Samel","Irfan Essa"],"pdf_url":"https://arxiv.org/pdf/2401.12972v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16582v1","updated":"2024-08-29T14:48:00Z","published":"2024-08-29T14:48:00Z","title":"FastForensics: Efficient Two-Stream Design for Real-Time Image\n  Manipulation Detection","summary":"  With the rise in popularity of portable devices, the spread of falsified\nmedia on social platforms has become rampant. This necessitates the timely\nidentification of authentic content. However, most advanced detection methods\nare computationally heavy, hindering their real-time application. In this\npaper, we describe an efficient two-stream architecture for real-time image\nmanipulation detection. Our method consists of two-stream branches targeting\nthe cognitive and inspective perspectives. In the cognitive branch, we propose\nefficient wavelet-guided Transformer blocks to capture the global manipulation\ntraces related to frequency. This block contains an interactive wavelet-guided\nself-attention module that integrates wavelet transformation with efficient\nattention design, interacting with the knowledge from the inspective branch.\nThe inspective branch consists of simple convolutions that capture fine-grained\ntraces and interact bidirectionally with Transformer blocks to provide mutual\nsupport. Our method is lightweight ($\\sim$ 8M) but achieves competitive\nperformance compared to many other counterparts, demonstrating its efficacy in\nimage manipulation detection and its potential for portable integration.\n","authors":["Yangxiang Zhang","Yuezun Li","Ao Luo","Jiaran Zhou","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2408.16582v1.pdf","comment":"BMVC 2024"},{"id":"http://arxiv.org/abs/2404.11054v3","updated":"2024-08-29T14:43:25Z","published":"2024-04-17T03:56:28Z","title":"Mumpy: Multilateral Temporal-view Pyramid Transformer for Video\n  Inpainting Detection","summary":"  The task of video inpainting detection is to expose the pixel-level inpainted\nregions within a video sequence. Existing methods usually focus on leveraging\nspatial and temporal inconsistencies. However, these methods typically employ\nfixed operations to combine spatial and temporal clues, limiting their\napplicability in different scenarios. In this paper, we introduce a novel\nMultilateral Temporal-view Pyramid Transformer ({\\em MumPy}) that collaborates\nspatial-temporal clues flexibly. Our method utilizes a newly designed\nmultilateral temporal-view encoder to extract various collaborations of\nspatial-temporal clues and introduces a deformable window-based temporal-view\ninteraction module to enhance the diversity of these collaborations.\nSubsequently, we develop a multi-pyramid decoder to aggregate the various types\nof features and generate detection maps. By adjusting the contribution strength\nof spatial and temporal clues, our method can effectively identify inpainted\nregions. We validate our method on existing datasets and also introduce a new\nchallenging and large-scale Video Inpainting dataset based on the YouTube-VOS\ndataset, which employs several more recent inpainting methods. The results\ndemonstrate the superiority of our method in both in-domain and cross-domain\nevaluation scenarios.\n","authors":["Ying Zhang","Yuezun Li","Bo Peng","Jiaran Zhou","Huiyu Zhou","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2404.11054v3.pdf","comment":"BMVC 2024"},{"id":"http://arxiv.org/abs/2408.14810v2","updated":"2024-08-29T14:38:22Z","published":"2024-08-27T06:49:21Z","title":"Generalist Segmentation Algorithm for Photoreceptors Analysis in\n  Adaptive Optics Imaging","summary":"  Analyzing the cone photoreceptor pattern in images obtained from the living\nhuman retina using quantitative methods can be crucial for the early detection\nand management of various eye conditions. Confocal adaptive optics scanning\nlight ophthalmoscope (AOSLO) imaging enables visualization of the cones from\nreflections of waveguiding cone photoreceptors. While there have been\nsignificant improvements in automated algorithms for segmenting cones in\nconfocal AOSLO images, the process of labelling data remains labor-intensive\nand manual. This paper introduces a method based on deep learning (DL) for\ndetecting and segmenting cones in AOSLO images. The models were trained on a\nsemi-automatically labelled dataset of 20 AOSLO batches of images of 18\nparticipants for 0$^{\\circ}$, 1$^{\\circ}$, and 2$^{\\circ}$ from the foveal\ncenter. F1 scores were 0.968, 0.958, and 0.954 for 0$^{\\circ}$, 1$^{\\circ}$,\nand 2$^{\\circ}$, respectively, which is better than previously reported DL\napproaches. Our method minimizes the need for labelled data by only\nnecessitating a fraction of labelled cones, which is especially beneficial in\nthe field of ophthalmology, where labelled data can often be limited.\n","authors":["Mikhail Kulyabin","Aline Sindel","Hilde Pedersen","Stuart Gilson","Rigmor Baraas","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2408.14810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16563v1","updated":"2024-08-29T14:30:45Z","published":"2024-08-29T14:30:45Z","title":"MST-KD: Multiple Specialized Teachers Knowledge Distillation for Fair\n  Face Recognition","summary":"  As in school, one teacher to cover all subjects is insufficient to distill\nequally robust information to a student. Hence, each subject is taught by a\nhighly specialised teacher. Following a similar philosophy, we propose a\nmultiple specialized teacher framework to distill knowledge to a student\nnetwork. In our approach, directed at face recognition use cases, we train four\nteachers on one specific ethnicity, leading to four highly specialized and\nbiased teachers. Our strategy learns a project of these four teachers into a\ncommon space and distill that information to a student network. Our results\nhighlighted increased performance and reduced bias for all our experiments. In\naddition, we further show that having biased/specialized teachers is crucial by\nshowing that our approach achieves better results than when knowledge is\ndistilled from four teachers trained on balanced datasets. Our approach\nrepresents a step forward to the understanding of the importance of\nethnicity-specific features.\n","authors":["Eduarda Caldeira","Jaime S. Cardoso","Ana F. Sequeira","Pedro C. Neto"],"pdf_url":"https://arxiv.org/pdf/2408.16563v1.pdf","comment":"Accepted at ECCV 2024 ABAW"},{"id":"http://arxiv.org/abs/2408.16547v1","updated":"2024-08-29T14:10:14Z","published":"2024-08-29T14:10:14Z","title":"OP-Align: Object-level and Part-level Alignment for Self-supervised\n  Category-level Articulated Object Pose Estimation","summary":"  Category-level articulated object pose estimation focuses on the pose\nestimation of unknown articulated objects within known categories. Despite its\nsignificance, this task remains challenging due to the varying shapes and poses\nof objects, expensive dataset annotation costs, and complex real-world\nenvironments. In this paper, we propose a novel self-supervised approach that\nleverages a single-frame point cloud to solve this task. Our model consistently\ngenerates reconstruction with a canonical pose and joint state for the entire\ninput object, and it estimates object-level poses that reduce overall pose\nvariance and part-level poses that align each part of the input with its\ncorresponding part of the reconstruction. Experimental results demonstrate that\nour approach significantly outperforms previous self-supervised methods and is\ncomparable to the state-of-the-art supervised methods. To assess the\nperformance of our model in real-world scenarios, we also introduce a new\nreal-world articulated object benchmark dataset.\n","authors":["Yuchen Che","Ryo Furukawa","Asako Kanezaki"],"pdf_url":"https://arxiv.org/pdf/2408.16547v1.pdf","comment":"to be published in ECCV2024"},{"id":"http://arxiv.org/abs/2408.16544v1","updated":"2024-08-29T14:02:47Z","published":"2024-08-29T14:02:47Z","title":"Spurfies: Sparse Surface Reconstruction using Local Geometry Priors","summary":"  We introduce Spurfies, a novel method for sparse-view surface reconstruction\nthat disentangles appearance and geometry information to utilize local geometry\npriors trained on synthetic data. Recent research heavily focuses on 3D\nreconstruction using dense multi-view setups, typically requiring hundreds of\nimages. However, these methods often struggle with few-view scenarios. Existing\nsparse-view reconstruction techniques often rely on multi-view stereo networks\nthat need to learn joint priors for geometry and appearance from a large amount\nof data. In contrast, we introduce a neural point representation that\ndisentangles geometry and appearance to train a local geometry prior using a\nsubset of the synthetic ShapeNet dataset only. During inference, we utilize\nthis surface prior as additional constraint for surface and appearance\nreconstruction from sparse input views via differentiable volume rendering,\nrestricting the space of possible solutions. We validate the effectiveness of\nour method on the DTU dataset and demonstrate that it outperforms previous\nstate of the art by 35% in surface quality while achieving competitive novel\nview synthesis quality. Moreover, in contrast to previous works, our method can\nbe applied to larger, unbounded scenes, such as Mip-NeRF 360.\n","authors":["Kevin Raj","Christopher Wewer","Raza Yunus","Eddy Ilg","Jan Eric Lenssen"],"pdf_url":"https://arxiv.org/pdf/2408.16544v1.pdf","comment":"https://geometric-rl.mpi-inf.mpg.de/spurfies/"},{"id":"http://arxiv.org/abs/2408.16540v1","updated":"2024-08-29T13:58:34Z","published":"2024-08-29T13:58:34Z","title":"GRPose: Learning Graph Relations for Human Image Generation with Pose\n  Priors","summary":"  Recent methods using diffusion models have made significant progress in human\nimage generation with various additional controls such as pose priors. However,\nexisting approaches still struggle to generate high-quality images with\nconsistent pose alignment, resulting in unsatisfactory outputs. In this paper,\nwe propose a framework delving into the graph relations of pose priors to\nprovide control information for human image generation. The main idea is to\nestablish a graph topological structure between the pose priors and latent\nrepresentation of diffusion models to capture the intrinsic associations\nbetween different pose parts. A Progressive Graph Integrator (PGI) is designed\nto learn the spatial relationships of the pose priors with the graph structure,\nadopting a hierarchical strategy within an Adapter to gradually propagate\ninformation across different pose parts. A pose perception loss is further\nintroduced based on a pretrained pose estimation network to minimize the pose\ndifferences. Extensive qualitative and quantitative experiments conducted on\nthe Human-Art and LAION-Human datasets demonstrate that our model achieves\nsuperior performance, with a 9.98% increase in pose average precision compared\nto the latest benchmark model. The code is released on *******.\n","authors":["Xiangchen Yin","Donglin Di","Lei Fan","Hao Li","Chen Wei","Xiaofei Gou","Yang Song","Xiao Sun","Xun Yang"],"pdf_url":"https://arxiv.org/pdf/2408.16540v1.pdf","comment":"The code will be released at https://github.com/XiangchenYin/GRPose"},{"id":"http://arxiv.org/abs/2405.04964v2","updated":"2024-08-29T13:44:20Z","published":"2024-05-08T11:09:24Z","title":"Frequency-Assisted Mamba for Remote Sensing Image Super-Resolution","summary":"  Recent progress in remote sensing image (RSI) super-resolution (SR) has\nexhibited remarkable performance using deep neural networks, e.g.,\nConvolutional Neural Networks and Transformers. However, existing SR methods\noften suffer from either a limited receptive field or quadratic computational\noverhead, resulting in sub-optimal global representation and unacceptable\ncomputational costs in large-scale RSI. To alleviate these issues, we develop\nthe first attempt to integrate the Vision State Space Model (Mamba) for RSI-SR,\nwhich specializes in processing large-scale RSI by capturing long-range\ndependency with linear complexity. To achieve better SR reconstruction,\nbuilding upon Mamba, we devise a Frequency-assisted Mamba framework, dubbed\nFMSR, to explore the spatial and frequent correlations. In particular, our FMSR\nfeatures a multi-level fusion architecture equipped with the Frequency\nSelection Module (FSM), Vision State Space Module (VSSM), and Hybrid Gate\nModule (HGM) to grasp their merits for effective spatial-frequency fusion.\nConsidering that global and local dependencies are complementary and both\nbeneficial for SR, we further recalibrate these multi-level features for\naccurate feature fusion via learnable scaling adaptors. Extensive experiments\non AID, DOTA, and DIOR benchmarks demonstrate that our FMSR outperforms\nstate-of-the-art Transformer-based methods HAT-L in terms of PSNR by 0.11 dB on\naverage, while consuming only 28.05% and 19.08% of its memory consumption and\ncomplexity, respectively. Code will be available at\nhttps://github.com/XY-boy/FreMamba\n","authors":["Yi Xiao","Qiangqiang Yuan","Kui Jiang","Yuzeng Chen","Qiang Zhang","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2405.04964v2.pdf","comment":"Accepted by IEEE TMM"},{"id":"http://arxiv.org/abs/2408.16520v1","updated":"2024-08-29T13:31:15Z","published":"2024-08-29T13:31:15Z","title":"Towards Modality-agnostic Label-efficient Segmentation with\n  Entropy-Regularized Distribution Alignment","summary":"  Label-efficient segmentation aims to perform effective segmentation on input\ndata using only sparse and limited ground-truth labels for training. This topic\nis widely studied in 3D point cloud segmentation due to the difficulty of\nannotating point clouds densely, while it is also essential for cost-effective\nsegmentation on 2D images. Until recently, pseudo-labels have been widely\nemployed to facilitate training with limited ground-truth labels, and promising\nprogress has been witnessed in both the 2D and 3D segmentation. However,\nexisting pseudo-labeling approaches could suffer heavily from the noises and\nvariations in unlabelled data, which would result in significant discrepancies\nbetween generated pseudo-labels and current model predictions during training.\nWe analyze that this can further confuse and affect the model learning process,\nwhich shows to be a shared problem in label-efficient learning across both 2D\nand 3D modalities. To address this issue, we propose a novel learning strategy\nto regularize the pseudo-labels generated for training, thus effectively\nnarrowing the gaps between pseudo-labels and model predictions. More\nspecifically, our method introduces an Entropy Regularization loss and a\nDistribution Alignment loss for label-efficient learning, resulting in an ERDA\nlearning strategy. Interestingly, by using KL distance to formulate the\ndistribution alignment loss, ERDA reduces to a deceptively simple\ncross-entropy-based loss which optimizes both the pseudo-label generation\nmodule and the segmentation model simultaneously. In addition, we innovate in\nthe pseudo-label generation to make our ERDA consistently effective across both\n2D and 3D data modalities for segmentation. Enjoying simplicity and more\nmodality-agnostic pseudo-label generation, our method has shown outstanding\nperformance in fully utilizing all unlabeled data points for training across\n...\n","authors":["Liyao Tang","Zhe Chen","Shanshan Zhao","Chaoyue Wang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2408.16520v1.pdf","comment":"Extended version of arXiv:2305.15832; Code at\n  https://github.com/LiyaoTang/ERDA"},{"id":"http://arxiv.org/abs/2408.02674v2","updated":"2024-08-29T13:29:36Z","published":"2024-07-22T06:13:22Z","title":"On Feasibility of Intent Obfuscating Attacks","summary":"  Intent obfuscation is a common tactic in adversarial situations, enabling the\nattacker to both manipulate the target system and avoid culpability.\nSurprisingly, it has rarely been implemented in adversarial attacks on machine\nlearning systems. We are the first to propose using intent obfuscation to\ngenerate adversarial examples for object detectors: by perturbing another\nnon-overlapping object to disrupt the target object, the attacker hides their\nintended target. We conduct a randomized experiment on 5 prominent detectors --\nYOLOv3, SSD, RetinaNet, Faster R-CNN, and Cascade R-CNN -- using both targeted\nand untargeted attacks and achieve success on all models and attacks. We\nanalyze the success factors characterizing intent obfuscating attacks,\nincluding target object confidence and perturb object sizes. We then\ndemonstrate that the attacker can exploit these success factors to increase\nsuccess rates for all models and attacks. Finally, we discuss main takeaways\nand legal repercussions.\n","authors":["Zhaobin Li","Patrick Shafto"],"pdf_url":"https://arxiv.org/pdf/2408.02674v2.pdf","comment":"33 pages, 21 Figures. Includes technical appendix. To appear in AIES\n  2024"},{"id":"http://arxiv.org/abs/2406.19006v2","updated":"2024-08-29T13:23:45Z","published":"2024-06-27T08:45:31Z","title":"VideoMambaPro: A Leap Forward for Mamba in Video Understanding","summary":"  Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to computer vision tasks, including those in\nvideo analysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba\nbackbone. VideoMambaPro shows state-of-the-art video action recognition\nperformance compared to transformer models, and surpasses VideoMamba by clear\nmargins: 7.9% and 8.1% top-1 on Kinetics-400 and Something-Something V2,\nrespectively. Our VideoMambaPro-M model achieves 91.9% top-1 on Kinetics-400,\nonly 0.2% below InternVideo2-6B but with only 1.2% of its parameters. The\ncombination of high performance and efficiency makes VideoMambaPro an\ninteresting alternative for transformer models.\n","authors":["Hui Lu","Albert Ali Salah","Ronald Poppe"],"pdf_url":"https://arxiv.org/pdf/2406.19006v2.pdf","comment":"Model weights are lost due to management error, will re-calculate and\n  update the results"},{"id":"http://arxiv.org/abs/2212.12130v6","updated":"2024-08-29T13:08:14Z","published":"2022-12-23T03:54:59Z","title":"Learning to Detect and Segment for Open Vocabulary Object Detection","summary":"  Open vocabulary object detection has been greatly advanced by the recent\ndevelopment of vision-language pretrained model, which helps recognize novel\nobjects with only semantic categories. The prior works mainly focus on\nknowledge transferring to the object proposal classification and employ\nclass-agnostic box and mask prediction. In this work, we propose CondHead, a\nprincipled dynamic network design to better generalize the box regression and\nmask segmentation for open vocabulary setting. The core idea is to\nconditionally parameterize the network heads on semantic embedding and thus the\nmodel is guided with class-specific knowledge to better detect novel\ncategories. Specifically, CondHead is composed of two streams of network heads,\nthe dynamically aggregated head and the dynamically generated head. The former\nis instantiated with a set of static heads that are conditionally aggregated,\nthese heads are optimized as experts and are expected to learn sophisticated\nprediction. The latter is instantiated with dynamically generated parameters\nand encodes general class-specific information. With such a conditional design,\nthe detection model is bridged by the semantic embedding to offer strongly\ngeneralizable class-wise box and mask prediction. Our method brings significant\nimprovement to the state-of-the-art open vocabulary object detection methods\nwith very minor overhead, e.g., it surpasses a RegionClip model by 3.0\ndetection AP on novel categories, with only 1.1% more computation.\n","authors":["Tao Wang","Nan Li"],"pdf_url":"https://arxiv.org/pdf/2212.12130v6.pdf","comment":"Accepted to CVPR2023, code will be available later"},{"id":"http://arxiv.org/abs/2408.16506v1","updated":"2024-08-29T13:08:12Z","published":"2024-08-29T13:08:12Z","title":"Alignment is All You Need: A Training-free Augmentation Strategy for\n  Pose-guided Video Generation","summary":"  Character animation is a transformative field in computer graphics and\nvision, enabling dynamic and realistic video animations from static images.\nDespite advancements, maintaining appearance consistency in animations remains\na challenge. Our approach addresses this by introducing a training-free\nframework that ensures the generated video sequence preserves the reference\nimage's subtleties, such as physique and proportions, through a dual alignment\nstrategy. We decouple skeletal and motion priors from pose information,\nenabling precise control over animation generation. Our method also improves\npixel-level alignment for conditional control from the reference character,\nenhancing the temporal consistency and visual cohesion of animations. Our\nmethod significantly enhances the quality of video generation without the need\nfor large datasets or expensive computational resources.\n","authors":["Xiaoyu Jin","Zunnan Xu","Mingwen Ou","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2408.16506v1.pdf","comment":"CVG@ICML 2024"},{"id":"http://arxiv.org/abs/2408.16504v1","updated":"2024-08-29T13:02:12Z","published":"2024-08-29T13:02:12Z","title":"A Simple and Generalist Approach for Panoptic Segmentation","summary":"  Generalist vision models aim for one and the same architecture for a variety\nof vision tasks. While such shared architecture may seem attractive, generalist\nmodels tend to be outperformed by their bespoken counterparts, especially in\nthe case of panoptic segmentation. We address this problem by introducing two\nkey contributions, without compromising the desirable properties of generalist\nmodels. These contributions are: (i) a positional-embedding (PE) based loss for\nimproved centroid regressions; (ii) Edge Distance Sampling (EDS) for the better\nseparation of instance boundaries. The PE-based loss facilitates a better\nper-pixel regression of the associated instance's centroid, whereas EDS\ncontributes by carefully handling the void regions (caused by missing labels)\nand smaller instances. These two simple yet effective modifications\nsignificantly improve established baselines, while achieving state-of-the-art\nresults among all generalist solutions. More specifically, our method achieves\na panoptic quality(PQ) of 52.5 on the COCO dataset, which is an improvement of\n10 points over the best model with similar approach (Painter), and is superior\nby 2 to the best performing diffusion-based method Pix2Seq-$\\mathcal{D}$.\nFurthermore, we provide insights into and an in-depth analysis of our\ncontributions through exhaustive experiments. Our source code and model weights\nwill be made publicly available.\n","authors":["Nedyalko Prisadnikov","Wouter Van Gansbeke","Danda Pani Paudel","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2408.16504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16503v1","updated":"2024-08-29T13:02:01Z","published":"2024-08-29T13:02:01Z","title":"Locally Grouped and Scale-Guided Attention for Dense Pest Counting","summary":"  This study introduces a new dense pest counting problem to predict densely\ndistributed pests captured by digital traps. Unlike traditional detection-based\ncounting models for sparsely distributed objects, trap-based pest counting must\ndeal with dense pest distributions that pose challenges such as severe\nocclusion, wide pose variation, and similar appearances in colors and textures.\nTo address these problems, it is essential to incorporate the local attention\nmechanism, which identifies locally important and unimportant areas to learn\nlocally grouped features, thereby enhancing discriminative performance.\nAccordingly, this study presents a novel design that integrates locally grouped\nand scale-guided attention into a multiscale CenterNet framework. To group\nlocal features with similar attributes, a straightforward method is introduced\nusing the heatmap predicted by the first hourglass containing pest centroid\ninformation, which eliminates the need for complex clustering models. To\nenhance attentiveness, the pixel attention module transforms the heatmap into a\nlearnable map. Subsequently, scale-guided attention is deployed to make the\nobject and background features more discriminative, achieving multiscale\nfeature fusion. Through experiments, the proposed model is verified to enhance\nobject features based on local grouping and discriminative feature attention\nlearning. Additionally, the proposed model is highly effective in overcoming\nocclusion and pose variation problems, making it more suitable for dense pest\ncounting. In particular, the proposed model outperforms state-of-the-art models\nby a large margin, with a remarkable contribution to dense pest counting.\n","authors":["Chang-Hwan Son"],"pdf_url":"https://arxiv.org/pdf/2408.16503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16501v1","updated":"2024-08-29T13:00:37Z","published":"2024-08-29T13:00:37Z","title":"UAV-Based Human Body Detector Selection and Fusion for Geolocated\n  Saliency Map Generation","summary":"  The problem of reliably detecting and geolocating objects of different\nclasses in soft real-time is essential in many application areas, such as\nSearch and Rescue performed using Unmanned Aerial Vehicles (UAVs). This\nresearch addresses the complementary problems of system contextual vision-based\ndetector selection, allocation, and execution, in addition to the fusion of\ndetection results from teams of UAVs for the purpose of accurately and reliably\ngeolocating objects of interest in a timely manner. In an offline step, an\napplication-independent evaluation of vision-based detectors from a system\nperspective is first performed. Based on this evaluation, the most appropriate\nalgorithms for online object detection for each platform are selected\nautomatically before a mission, taking into account a number of practical\nsystem considerations, such as the available communication links, video\ncompression used, and the available computational resources. The detection\nresults are fused using a method for building maps of salient locations which\ntakes advantage of a novel sensor model for vision-based detections for both\npositive and negative observations. A number of simulated and real flight\nexperiments are also presented, validating the proposed method.\n","authors":["Piotr Rudol","Patrick Doherty","Mariusz Wzorek","Chattrakul Sombattheera"],"pdf_url":"https://arxiv.org/pdf/2408.16501v1.pdf","comment":"42 pages, 19 figures"},{"id":"http://arxiv.org/abs/2408.16500v1","updated":"2024-08-29T12:59:12Z","published":"2024-08-29T12:59:12Z","title":"CogVLM2: Visual Language Models for Image and Video Understanding","summary":"  Beginning with VisualGLM and CogVLM, we are continuously exploring VLMs in\npursuit of enhanced vision-language fusion, efficient higher-resolution\narchitecture, and broader modalities and applications. Here we propose the\nCogVLM2 family, a new generation of visual language models for image and video\nunderstanding including CogVLM2, CogVLM2-Video and GLM-4V. As an image\nunderstanding model, CogVLM2 inherits the visual expert architecture with\nimproved training recipes in both pre-training and post-training stages,\nsupporting input resolution up to $1344 \\times 1344$ pixels. As a video\nunderstanding model, CogVLM2-Video integrates multi-frame input with timestamps\nand proposes automated temporal grounding data construction. Notably, CogVLM2\nfamily has achieved state-of-the-art results on benchmarks like MMBench,\nMM-Vet, TextVQA, MVBench and VCGBench. All models are open-sourced in\nhttps://github.com/THUDM/CogVLM2 and https://github.com/THUDM/GLM-4,\ncontributing to the advancement of the field.\n","authors":["Wenyi Hong","Weihan Wang","Ming Ding","Wenmeng Yu","Qingsong Lv","Yan Wang","Yean Cheng","Shiyu Huang","Junhui Ji","Zhao Xue","Lei Zhao","Zhuoyi Yang","Xiaotao Gu","Xiaohan Zhang","Guanyu Feng","Da Yin","Zihan Wang","Ji Qi","Xixuan Song","Peng Zhang","Debing Liu","Bin Xu","Juanzi Li","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2408.16500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16486v1","updated":"2024-08-29T12:34:01Z","published":"2024-08-29T12:34:01Z","title":"Adapting Vision-Language Models to Open Classes via Test-Time Prompt\n  Tuning","summary":"  Adapting pre-trained models to open classes is a challenging problem in\nmachine learning. Vision-language models fully explore the knowledge of text\nmodality, demonstrating strong zero-shot recognition performance, which is\nnaturally suited for various open-set problems. More recently, some research\nfocuses on fine-tuning such models to downstream tasks. Prompt tuning methods\nachieved huge improvements by learning context vectors on few-shot data.\nHowever, through the evaluation under open-set adaptation setting with the test\ndata including new classes, we find that there exists a dilemma that learned\nprompts have worse generalization abilities than hand-crafted prompts. In this\npaper, we consider combining the advantages of both and come up with a\ntest-time prompt tuning approach, which leverages the maximum concept matching\n(MCM) scores as dynamic weights to generate an input-conditioned prompt for\neach image during test. Through extensive experiments on 11 different datasets,\nwe show that our proposed method outperforms all comparison methods on average\nconsidering both base and new classes. The code is available at\nhttps://github.com/gaozhengqing/TTPT\n","authors":["Zhengqing Gao","Xiang Ao","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.16486v1.pdf","comment":"PRCV 2024"},{"id":"http://arxiv.org/abs/2403.06702v3","updated":"2024-08-29T12:27:12Z","published":"2024-03-11T13:17:55Z","title":"Fast Text-to-3D-Aware Face Generation and Manipulation via Direct\n  Cross-modal Mapping and Geometric Regularization","summary":"  Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging\nresearch hot spot in machine learning, which still suffers from low efficiency\nand poor quality. In this paper, we propose an End-to-End Efficient and\nEffective network for fast and accurate T3D face generation and manipulation,\ntermed $E^3$-FaceNet. Different from existing complex generation paradigms,\n$E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware\nvisual space. We introduce a novel Style Code Enhancer to enhance cross-modal\nsemantic alignment, alongside an innovative Geometric Regularization objective\nto maintain consistency across multi-view generations. Extensive experiments on\nthree benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve\npicture-like 3D face generation and manipulation, but also improve inference\nspeed by orders of magnitudes. For instance, compared with Latent3D,\n$E^3$-FaceNet speeds up the five-view generations by almost 470 times, while\nstill exceeding in generation quality. Our code is released at\nhttps://github.com/Aria-Zhangjl/E3-FaceNet.\n","authors":["Jinlu Zhang","Yiyi Zhou","Qiancheng Zheng","Xiaoxiong Du","Gen Luo","Jun Peng","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.06702v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16481v1","updated":"2024-08-29T12:16:55Z","published":"2024-08-29T12:16:55Z","title":"A Deep-Learning-Based Lable-free No-Reference Image Quality Assessment\n  Metric: Application in Sodium MRI Denoising","summary":"  New multinuclear MRI techniques, such as sodium MRI, generally suffer from\nlow image quality due to an inherently low signal. Postprocessing methods, such\nas image denoising, have been developed for image enhancement. However, the\nassessment of these enhanced images is challenging especially considering when\nthere is a lack of high resolution and high signal images as reference, such as\nin sodium MRI. No-reference Image Quality Assessment (NR-IQA) metrics are\napproaches to solve this problem. Existing learning-based NR-IQA metrics rely\non labels derived from subjective human opinions or metrics like\nSignal-to-Noise Ratio (SNR), which are either time-consuming or lack accurate\nground truths, resulting in unreliable assessment. We note that deep learning\n(DL) models have a unique characteristic in that they are specialized to a\ncharacteristic training set, meaning that deviations between the input testing\ndata from the training data will reduce prediction accuracy. Therefore, we\npropose a novel DL-based NR-IQA metric, the Model Specialization Metric (MSM),\nwhich does not depend on ground-truth images or labels. MSM measures the\ndifference between the input image and the model's prediction for evaluating\nthe quality of the input image. Experiments conducted on both simulated\ndistorted proton T1-weighted MR images and denoised sodium MR images\ndemonstrate that MSM exhibits a superior evaluation performance on various\nsimulated noises and distortions. MSM also has a substantial agreement with the\nexpert evaluations, achieving an averaged Cohen's Kappa coefficient of 0.6528,\noutperforming the existing NR-IQA metrics.\n","authors":["Shuaiyu Yuan","Tristan Whitmarsh","Dimitri A Kessler","Otso Arponen","Mary A McLean","Gabrielle Baxter","Frank Riemer","Aneurin J Kennerley","William J Brackenbury","Fiona J Gilbert","Joshua D Kaggie"],"pdf_url":"https://arxiv.org/pdf/2408.16481v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.16478v1","updated":"2024-08-29T12:15:10Z","published":"2024-08-29T12:15:10Z","title":"MICDrop: Masking Image and Depth Features via Complementary Dropout for\n  Domain-Adaptive Semantic Segmentation","summary":"  Unsupervised Domain Adaptation (UDA) is the task of bridging the domain gap\nbetween a labeled source domain, e.g., synthetic data, and an unlabeled target\ndomain. We observe that current UDA methods show inferior results on fine\nstructures and tend to oversegment objects with ambiguous appearance. To\naddress these shortcomings, we propose to leverage geometric information, i.e.,\ndepth predictions, as depth discontinuities often coincide with segmentation\nboundaries. We show that naively incorporating depth into current UDA methods\ndoes not fully exploit the potential of this complementary information. To this\nend, we present MICDrop, which learns a joint feature representation by masking\nimage encoder features while inversely masking depth encoder features. With\nthis simple yet effective complementary masking strategy, we enforce the use of\nboth modalities when learning the joint feature representation. To aid this\nprocess, we propose a feature fusion module to improve both global as well as\nlocal information sharing while being robust to errors in the depth\npredictions. We show that our method can be plugged into various recent UDA\nmethods and consistently improve results across standard UDA benchmarks,\nobtaining new state-of-the-art performances.\n","authors":["Linyan Yang","Lukas Hoyer","Mark Weber","Tobias Fischer","Dengxin Dai","Laura Leal-TaixÃ©","Marc Pollefeys","Daniel Cremers","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2408.16478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16472v1","updated":"2024-08-29T12:04:03Z","published":"2024-08-29T12:04:03Z","title":"Creating a Segmented Pointcloud of Grapevines by Combining Multiple\n  Viewpoints Through Visual Odometry","summary":"  Grapevine winter pruning is a labor-intensive and repetitive process that\nsignificantly influences the quality and quantity of the grape harvest and\nproduced wine of the following season. It requires a careful and expert\ndetection of the point to be cut. Because of its complexity, repetitive nature\nand time constraint, the task requires skilled labor that needs to be trained.\nThis extended abstract presents the computer vision pipeline employed in\nproject Vinum, using detectron2 as a segmentation network and keypoint visual\nodometry to merge different observation into a single pointcloud used to make\ninformed pruning decisions.\n","authors":["Michael Adlerstein","Angelo Bratta","JoÃ£o Carlos Virgolino Soares","Giovanni Dessy","Miguel Fernandes","Matteo Gatti","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2408.16472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16471v1","updated":"2024-08-29T12:01:23Z","published":"2024-08-29T12:01:23Z","title":"Improving 3D deep learning segmentation with biophysically motivated\n  cell synthesis","summary":"  Biomedical research increasingly relies on 3D cell culture models and\nAI-based analysis can potentially facilitate a detailed and accurate feature\nextraction on a single-cell level. However, this requires for a precise\nsegmentation of 3D cell datasets, which in turn demands high-quality ground\ntruth for training. Manual annotation, the gold standard for ground truth data,\nis too time-consuming and thus not feasible for the generation of large 3D\ntraining datasets. To address this, we present a novel framework for generating\n3D training data, which integrates biophysical modeling for realistic cell\nshape and alignment. Our approach allows the in silico generation of coherent\nmembrane and nuclei signals, that enable the training of segmentation models\nutilizing both channels for improved performance. Furthermore, we present a new\nGAN training scheme that generates not only image data but also matching\nlabels. Quantitative evaluation shows superior performance of biophysical\nmotivated synthetic training data, even outperforming manual annotation and\npretrained models. This underscores the potential of incorporating biophysical\nmodeling for enhancing synthetic training data quality.\n","authors":["Roman Bruch","Mario Vitacolonna","Elina NÃ¼rnberg","Simeon Sauer","RÃ¼diger Rudolf","Markus Reischl"],"pdf_url":"https://arxiv.org/pdf/2408.16471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16469v1","updated":"2024-08-29T12:00:11Z","published":"2024-08-29T12:00:11Z","title":"Multi-source Domain Adaptation for Panoramic Semantic Segmentation","summary":"  Panoramic semantic segmentation has received widespread attention recently\ndue to its comprehensive 360\\degree field of view. However, labeling such\nimages demands greater resources compared to pinhole images. As a result, many\nunsupervised domain adaptation methods for panoramic semantic segmentation have\nemerged, utilizing real pinhole images or low-cost synthetic panoramic images.\nBut, the segmentation model lacks understanding of the panoramic structure when\nonly utilizing real pinhole images, and it lacks perception of real-world\nscenes when only adopting synthetic panoramic images. Therefore, in this paper,\nwe propose a new task of multi-source domain adaptation for panoramic semantic\nsegmentation, aiming to utilize both real pinhole and synthetic panoramic\nimages in the source domains, enabling the segmentation model to perform well\non unlabeled real panoramic images in the target domain. Further, we propose\nDeformation Transform Aligner for Panoramic Semantic Segmentation (DTA4PASS),\nwhich converts all pinhole images in the source domains into panoramic-like\nimages, and then aligns the converted source domains with the target domain.\nSpecifically, DTA4PASS consists of two main components: Unpaired Semantic\nMorphing (USM) and Distortion Gating Alignment (DGA). Firstly, in USM, the\nSemantic Dual-view Discriminator (SDD) assists in training the diffeomorphic\ndeformation network, enabling the effective transformation of pinhole images\nwithout paired panoramic views. Secondly, DGA assigns pinhole-like and\npanoramic-like features to each image by gating, and aligns these two features\nthrough uncertainty estimation. DTA4PASS outperforms the previous\nstate-of-the-art methods by 1.92% and 2.19% on the outdoor and indoor\nmulti-source domain adaptation scenarios, respectively. The source code will be\nreleased.\n","authors":["Jing Jiang","Sicheng Zhao","Jiankun Zhu","Wenbo Tang","Zhaopan Xu","Jidong Yang","Pengfei Xu","Hongxun Yao"],"pdf_url":"https://arxiv.org/pdf/2408.16469v1.pdf","comment":"9 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.06831v2","updated":"2024-08-29T11:57:39Z","published":"2024-03-11T15:48:17Z","title":"HDRTransDC: High Dynamic Range Image Reconstruction with Transformer\n  Deformation Convolution","summary":"  High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image\nwith realistic details by fusing multi-exposure Low Dynamic Range (LDR) images.\nCaused by large motion and severe under-/over-exposure among input LDR images,\nHDR imaging suffers from ghosting artifacts and fusion distortions. To address\nthese critical issues, we propose an HDR Transformer Deformation Convolution\n(HDRTransDC) network to generate high-quality HDR images, which consists of the\nTransformer Deformable Convolution Alignment Module (TDCAM) and the Dynamic\nWeight Fusion Block (DWFB). To solve the ghosting artifacts, the proposed TDCAM\nextracts long-distance content similar to the reference feature in the entire\nnon-reference features, which can accurately remove misalignment and fill the\ncontent occluded by moving objects. For the purpose of eliminating fusion\ndistortions, we propose DWFB to spatially adaptively select useful information\nacross frames to effectively fuse multi-exposed features. Extensive experiments\nshow that our method quantitatively and qualitatively achieves state-of-the-art\nperformance.\n","authors":["Shuaikang Shang","Xuejing Kang","Anlong Ming"],"pdf_url":"https://arxiv.org/pdf/2403.06831v2.pdf","comment":"We request to withdraw our manuscript due to identified issues:\n  inaccuracies in the description of a submodule's composition, principles, and\n  functionality in Section 3.2, and potential problems in metric calculation in\n  Sections 4.2 and 4.3. To prevent the spread of misleading information, we\n  believe it is necessary to temporarily withdraw the manuscript for further\n  research and verification"},{"id":"http://arxiv.org/abs/2408.16467v1","updated":"2024-08-29T11:56:02Z","published":"2024-08-29T11:56:02Z","title":"Spiking Diffusion Models","summary":"  Recent years have witnessed Spiking Neural Networks (SNNs) gaining attention\nfor their ultra-low energy consumption and high biological plausibility\ncompared with traditional Artificial Neural Networks (ANNs). Despite their\ndistinguished properties, the application of SNNs in the computationally\nintensive field of image generation is still under exploration. In this paper,\nwe propose the Spiking Diffusion Models (SDMs), an innovative family of\nSNN-based generative models that excel in producing high-quality samples with\nsignificantly reduced energy consumption. In particular, we propose a\nTemporal-wise Spiking Mechanism (TSM) that allows SNNs to capture more temporal\nfeatures from a bio-plasticity perspective. In addition, we propose a\nthreshold-guided strategy that can further improve the performances by up to\n16.7% without any additional training. We also make the first attempt to use\nthe ANN-SNN approach for SNN-based generation tasks. Extensive experimental\nresults reveal that our approach not only exhibits comparable performance to\nits ANN counterpart with few spiking time steps, but also outperforms previous\nSNN-based generative models by a large margin. Moreover, we also demonstrate\nthe high-quality generation ability of SDM on large-scale datasets, e.g., LSUN\nbedroom. This development marks a pivotal advancement in the capabilities of\nSNN-based generation, paving the way for future research avenues to realize\nlow-energy and low-latency generative applications. Our code is available at\nhttps://github.com/AndyCao1125/SDM.\n","authors":["Jiahang Cao","Hanzhong Guo","Ziqing Wang","Deming Zhou","Hao Cheng","Qiang Zhang","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2408.16467v1.pdf","comment":"Accepted by IEEE Transactions on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2408.16451v1","updated":"2024-08-29T11:31:28Z","published":"2024-08-29T11:31:28Z","title":"Weakly Supervised Object Detection for Automatic Tooth-marked Tongue\n  Recognition","summary":"  Tongue diagnosis in Traditional Chinese Medicine (TCM) is a crucial\ndiagnostic method that can reflect an individual's health status. Traditional\nmethods for identifying tooth-marked tongues are subjective and inconsistent\nbecause they rely on practitioner experience. We propose a novel fully\nautomated Weakly Supervised method using Vision transformer and Multiple\ninstance learning WSVM for tongue extraction and tooth-marked tongue\nrecognition. Our approach first accurately detects and extracts the tongue\nregion from clinical images, removing any irrelevant background information.\nThen, we implement an end-to-end weakly supervised object detection method. We\nutilize Vision Transformer (ViT) to process tongue images in patches and employ\nmultiple instance loss to identify tooth-marked regions with only image-level\nannotations. WSVM achieves high accuracy in tooth-marked tongue classification,\nand visualization experiments demonstrate its effectiveness in pinpointing\nthese regions. This automated approach enhances the objectivity and accuracy of\ntooth-marked tongue diagnosis. It provides significant clinical value by\nassisting TCM practitioners in making precise diagnoses and treatment\nrecommendations. Code is available at https://github.com/yc-zh/WSVM.\n","authors":["Yongcun Zhang","Jiajun Xu","Yina He","Shaozi Li","Zhiming Luo","Huangwei Lei"],"pdf_url":"https://arxiv.org/pdf/2408.16451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16450v1","updated":"2024-08-29T11:30:21Z","published":"2024-08-29T11:30:21Z","title":"What to Preserve and What to Transfer: Faithful, Identity-Preserving\n  Diffusion-based Hairstyle Transfer","summary":"  Hairstyle transfer is a challenging task in the image editing field that\nmodifies the hairstyle of a given face image while preserving its other\nappearance and background features. The existing hairstyle transfer approaches\nheavily rely on StyleGAN, which is pre-trained on cropped and aligned face\nimages. Hence, they struggle to generalize under challenging conditions such as\nextreme variations of head poses or focal lengths. To address this issue, we\npropose a one-stage hairstyle transfer diffusion model, HairFusion, that\napplies to real-world scenarios. Specifically, we carefully design a\nhair-agnostic representation as the input of the model, where the original hair\ninformation is thoroughly eliminated. Next, we introduce a hair align\ncross-attention (Align-CA) to accurately align the reference hairstyle with the\nface image while considering the difference in their face shape. To enhance the\npreservation of the face image's original features, we leverage adaptive hair\nblending during the inference, where the output's hair regions are estimated by\nthe cross-attention map in Align-CA and blended with non-hair areas of the face\nimage. Our experimental results show that our method achieves state-of-the-art\nperformance compared to the existing methods in preserving the integrity of\nboth the transferred hairstyle and the surrounding features. The codes are\navailable at https://github.com/cychungg/HairFusion.\n","authors":["Chaeyeon Chung","Sunghyun Park","Jeongho Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2408.16450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16448v1","updated":"2024-08-29T11:24:51Z","published":"2024-08-29T11:24:51Z","title":"Enhancing Sound Source Localization via False Negative Elimination","summary":"  Sound source localization aims to localize objects emitting the sound in\nvisual scenes. Recent works obtaining impressive results typically rely on\ncontrastive learning. However, the common practice of randomly sampling\nnegatives in prior arts can lead to the false negative issue, where the sounds\nsemantically similar to visual instance are sampled as negatives and\nincorrectly pushed away from the visual anchor/query. As a result, this\nmisalignment of audio and visual features could yield inferior performance. To\naddress this issue, we propose a novel audio-visual learning framework which is\ninstantiated with two individual learning schemes: self-supervised predictive\nlearning (SSPL) and semantic-aware contrastive learning (SACL). SSPL explores\nimage-audio positive pairs alone to discover semantically coherent similarities\nbetween audio and visual features, while a predictive coding module for feature\nalignment is introduced to facilitate the positive-only learning. In this\nregard SSPL acts as a negative-free method to eliminate false negatives. By\ncontrast, SACL is designed to compact visual features and remove false\nnegatives, providing reliable visual anchor and audio negatives for contrast.\nDifferent from SSPL, SACL releases the potential of audio-visual contrastive\nlearning, offering an effective alternative to achieve the same goal.\nComprehensive experiments demonstrate the superiority of our approach over the\nstate-of-the-arts. Furthermore, we highlight the versatility of the learned\nrepresentation by extending the approach to audio-visual event classification\nand object detection tasks. Code and models are available at:\nhttps://github.com/zjsong/SACL.\n","authors":["Zengjie Song","Jiangshe Zhang","Yuxi Wang","Junsong Fan","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.16448v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2203.13412"},{"id":"http://arxiv.org/abs/2408.16445v1","updated":"2024-08-29T11:16:34Z","published":"2024-08-29T11:16:34Z","title":"Mismatched: Evaluating the Limits of Image Matching Approaches and\n  Benchmarks","summary":"  Three-dimensional (3D) reconstruction from two-dimensional images is an\nactive research field in computer vision, with applications ranging from\nnavigation and object tracking to segmentation and three-dimensional modeling.\nTraditionally, parametric techniques have been employed for this task. However,\nrecent advancements have seen a shift towards learning-based methods. Given the\nrapid pace of research and the frequent introduction of new image matching\nmethods, it is essential to evaluate them. In this paper, we present a\ncomprehensive evaluation of various image matching methods using a\nstructure-from-motion pipeline. We assess the performance of these methods on\nboth in-domain and out-of-domain datasets, identifying key limitations in both\nthe methods and benchmarks. We also investigate the impact of edge detection as\na pre-processing step. Our analysis reveals that image matching for 3D\nreconstruction remains an open challenge, necessitating careful selection and\ntuning of models for specific scenarios, while also highlighting mismatches in\nhow metrics currently represent method performance.\n","authors":["Sierra Bonilla","Chiara Di Vece","Rema Daher","Xinwei Ju","Danail Stoyanov","Francisco Vasconcelos","Sophia Bano"],"pdf_url":"https://arxiv.org/pdf/2408.16445v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.16442v1","updated":"2024-08-29T11:07:48Z","published":"2024-08-29T11:07:48Z","title":"Integrating Features for Recognizing Human Activities through Optimized\n  Parameters in Graph Convolutional Networks and Transformer Architectures","summary":"  Human activity recognition is a major field of study that employs computer\nvision, machine vision, and deep learning techniques to categorize human\nactions. The field of deep learning has made significant progress, with\narchitectures that are extremely effective at capturing human dynamics. This\nstudy emphasizes the influence of feature fusion on the accuracy of activity\nrecognition. This technique addresses the limitation of conventional models,\nwhich face difficulties in identifying activities because of their limited\ncapacity to understand spatial and temporal features. The technique employs\nsensory data obtained from four publicly available datasets: HuGaDB, PKU-MMD,\nLARa, and TUG. The accuracy and F1-score of two deep learning models,\nspecifically a Transformer model and a Parameter-Optimized Graph Convolutional\nNetwork (PO-GCN), were evaluated using these datasets. The feature fusion\ntechnique integrated the final layer features from both models and inputted\nthem into a classifier. Empirical evidence demonstrates that PO-GCN outperforms\nstandard models in activity recognition. HuGaDB demonstrated a 2.3% improvement\nin accuracy and a 2.2% increase in F1-score. TUG showed a 5% increase in\naccuracy and a 0.5% rise in F1-score. On the other hand, LARa and PKU-MMD\nachieved lower accuracies of 64% and 69% respectively. This indicates that the\nintegration of features enhanced the performance of both the Transformer model\nand PO-GCN.\n","authors":["Mohammad Belal","Taimur Hassan","Abdelfatah Hassan","Nael Alsheikh","Noureldin Elhendawi","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2408.16442v1.pdf","comment":"6 pages, 1 figure, conference"},{"id":"http://arxiv.org/abs/2408.16431v1","updated":"2024-08-29T10:47:17Z","published":"2024-08-29T10:47:17Z","title":"Discriminative Spatial-Semantic VOS Solution: 1st Place Solution for 6th\n  LSVOS","summary":"  Video object segmentation (VOS) is a crucial task in computer vision, but\ncurrent VOS methods struggle with complex scenes and prolonged object motions.\nTo address these challenges, the MOSE dataset aims to enhance object\nrecognition and differentiation in complex environments, while the LVOS dataset\nfocuses on segmenting objects exhibiting long-term, intricate movements. This\nreport introduces a discriminative spatial-temporal VOS model that utilizes\ndiscriminative object features as query representations. The semantic\nunderstanding of spatial-semantic modules enables it to recognize object parts,\nwhile salient features highlight more distinctive object characteristics. Our\nmodel, trained on extensive VOS datasets, achieved first place\n(\\textbf{80.90\\%} $\\mathcal{J \\& F}$) on the test set of the 6th LSVOS\nchallenge in the VOS Track, demonstrating its effectiveness in tackling the\naforementioned challenges. The code will be available at\n\\href{https://github.com/yahooo-m/VOS-Solution}{code}.\n","authors":["Deshui Miao","Yameng Gu","Xin Li","Zhenyu He","Yaowei Wang","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2408.16431v1.pdf","comment":"1st Place Solution for 6th LSVOS VOS Track. arXiv admin note:\n  substantial text overlap with arXiv:2406.04600"},{"id":"http://arxiv.org/abs/2408.16426v1","updated":"2024-08-29T10:36:29Z","published":"2024-08-29T10:36:29Z","title":"COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion\n  Estimation","summary":"  Estimating global human motion from moving cameras is challenging due to the\nentanglement of human and camera motions. To mitigate the ambiguity, existing\nmethods leverage learned human motion priors, which however often result in\noversmoothed motions with misaligned 2D projections. To tackle this problem, we\npropose COIN, a control-inpainting motion diffusion prior that enables\nfine-grained control to disentangle human and camera motions. Although\npre-trained motion diffusion models encode rich motion priors, we find it\nnon-trivial to leverage such knowledge to guide global motion estimation from\nRGB videos. COIN introduces a novel control-inpainting score distillation\nsampling method to ensure well-aligned, consistent, and high-quality motion\nfrom the diffusion prior within a joint optimization framework. Furthermore, we\nintroduce a new human-scene relation loss to alleviate the scale ambiguity by\nenforcing consistency among the humans, camera, and scene. Experiments on three\nchallenging benchmarks demonstrate the effectiveness of COIN, which outperforms\nthe state-of-the-art methods in terms of global human motion estimation and\ncamera motion estimation. As an illustrative example, COIN outperforms the\nstate-of-the-art method by 33% in world joint position error (W-MPJPE) on the\nRICH dataset.\n","authors":["Jiefeng Li","Ye Yuan","Davis Rempe","Haotian Zhang","Pavlo Molchanov","Cewu Lu","Jan Kautz","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2408.16426v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.16412v1","updated":"2024-08-29T10:20:05Z","published":"2024-08-29T10:20:05Z","title":"Text-Enhanced Zero-Shot Action Recognition: A training-free approach","summary":"  Vision-language models (VLMs) have demonstrated remarkable performance across\nvarious visual tasks, leveraging joint learning of visual and textual\nrepresentations. While these models excel in zero-shot image tasks, their\napplication to zero-shot video action recognition (ZSVAR) remains challenging\ndue to the dynamic and temporal nature of actions. Existing methods for ZS-VAR\ntypically require extensive training on specific datasets, which can be\nresource-intensive and may introduce domain biases. In this work, we propose\nText-Enhanced Action Recognition (TEAR), a simple approach to ZS-VAR that is\ntraining-free and does not require the availability of training data or\nextensive computational resources. Drawing inspiration from recent findings in\nvision and language literature, we utilize action descriptors for decomposition\nand contextual information to enhance zero-shot action recognition. Through\nexperiments on UCF101, HMDB51, and Kinetics-600 datasets, we showcase the\neffectiveness and applicability of our proposed approach in addressing the\nchallenges of ZS-VAR.\n","authors":["Massimo Bosetti","Shibingfeng Zhang","Bendetta Liberatori","Giacomo Zara","Elisa Ricci","Paolo Rota"],"pdf_url":"https://arxiv.org/pdf/2408.16412v1.pdf","comment":"accepted to ICPR 2024"},{"id":"http://arxiv.org/abs/2402.03973v2","updated":"2024-08-29T10:09:58Z","published":"2024-02-06T13:06:14Z","title":"A comparison between humans and AI at recognizing objects in unusual\n  poses","summary":"  Deep learning is closing the gap with human vision on several object\nrecognition benchmarks. Here we investigate this gap for challenging images\nwhere objects are seen in unusual poses. We find that humans excel at\nrecognizing objects in such poses. In contrast, state-of-the-art deep networks\nfor vision (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) and state-of-the-art\nlarge vision-language models (Claude 3.5, Gemini 1.5, GPT-4) are systematically\nbrittle on unusual poses, with the exception of Gemini showing excellent\nrobustness in that condition. As we limit image exposure time, human\nperformance degrades to the level of deep networks, suggesting that additional\nmental processes (requiring additional time) are necessary to identify objects\nin unusual poses. An analysis of error patterns of humans vs. networks reveals\nthat even time-limited humans are dissimilar to feed-forward deep networks. In\nconclusion, our comparison reveals that humans and deep networks rely on\ndifferent mechanisms for recognizing objects in unusual poses. Understanding\nthe nature of the mental processes taking place during extra viewing time may\nbe key to reproduce the robustness of human vision in silico.\n","authors":["Netta Ollikka","Amro Abbas","Andrea Perin","Markku KilpelÃ¤inen","StÃ©phane Deny"],"pdf_url":"https://arxiv.org/pdf/2402.03973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16395v1","updated":"2024-08-29T09:57:55Z","published":"2024-08-29T09:57:55Z","title":"IBO: Inpainting-Based Occlusion to Enhance Explainable Artificial\n  Intelligence Evaluation in Histopathology","summary":"  Histopathological image analysis is crucial for accurate cancer diagnosis and\ntreatment planning. While deep learning models, especially convolutional neural\nnetworks, have advanced this field, their \"black-box\" nature raises concerns\nabout interpretability and trustworthiness. Explainable Artificial Intelligence\n(XAI) techniques aim to address these concerns, but evaluating their\neffectiveness remains challenging. A significant issue with current\nocclusion-based XAI methods is that they often generate Out-of-Distribution\n(OoD) samples, leading to inaccurate evaluations. In this paper, we introduce\nInpainting-Based Occlusion (IBO), a novel occlusion strategy that utilizes a\nDenoising Diffusion Probabilistic Model to inpaint occluded regions in\nhistopathological images. By replacing cancerous areas with realistic,\nnon-cancerous tissue, IBO minimizes OoD artifacts and preserves data integrity.\nWe evaluate our method on the CAMELYON16 dataset through two phases: first, by\nassessing perceptual similarity using the Learned Perceptual Image Patch\nSimilarity (LPIPS) metric, and second, by quantifying the impact on model\npredictions through Area Under the Curve (AUC) analysis. Our results\ndemonstrate that IBO significantly improves perceptual fidelity, achieving\nnearly twice the improvement in LPIPS scores compared to the best existing\nocclusion strategy. Additionally, IBO increased the precision of XAI\nperformance prediction from 42% to 71% compared to traditional methods. These\nresults demonstrate IBO's potential to provide more reliable evaluations of XAI\ntechniques, benefiting histopathology and other applications. The source code\nfor this study is available at https://github.com/a-fsh-r/IBO.\n","authors":["Pardis Afshar","Sajjad Hashembeiki","Pouya Khani","Emad Fatemizadeh","Mohammad Hossein Rohban"],"pdf_url":"https://arxiv.org/pdf/2408.16395v1.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.14700v2","updated":"2024-08-29T09:44:53Z","published":"2024-05-23T15:34:53Z","title":"Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning\n  and Inference","summary":"  Parameter-efficient fine-tuning (PEFT) has emerged as a popular solution for\nadapting pre-trained Vision Transformer (ViT) models to downstream\napplications. While current PEFT methods have achieved parameter efficiency,\nthey overlook the efficiency of computation and GPU memory during both\nfine-tuning and inference, falling short of practical requirements. In this\npaper, we propose \\textbf{Sparse-Tuning}, a novel PEFT method that accounts for\nthe information redundancy in images and videos to boost the above efficiency.\nBy sparsely preserving the semantic-relevant tokens and merging irrelevant\nones, Sparse-Tuning minimizes the quantity of tokens processed at each layer,\nleading to a quadratic reduction in computational and memory overhead. To align\nour token sparsification strategy suitably with fine-tuning purposes, we\nfurther design Dense Adapters that establish dense connections from shallow\nlayers to deeper layers. These Dense Adapters integrate multi-level local\nfeatures to enrich the current tokens, improving both token preservation and\nmodel adaptation. Empirical results on VTAB-1K, three image datasets, and two\nvideo datasets show that our Sparse-Tuning reduces GFLOPs to \\textbf{62\\%-70\\%}\nof the original ViT-B while achieving state-of-the-art performance. Source code\nis available at \\url{https://github.com/liuting20/Sparse-Tuning}.\n","authors":["Ting Liu","Xuyang Liu","Siteng Huang","Liangtao Shi","Zunnan Xu","Yi Xin","Quanjun Yin","Xiaohong Liu"],"pdf_url":"https://arxiv.org/pdf/2405.14700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16380v1","updated":"2024-08-29T09:41:36Z","published":"2024-08-29T09:41:36Z","title":"Exploiting temporal information to detect conversational groups in\n  videos and predict the next speaker","summary":"  Studies in human human interaction have introduced the concept of F formation\nto describe the spatial arrangement of participants during social interactions.\nThis paper has two objectives. It aims at detecting F formations in video\nsequences and predicting the next speaker in a group conversation. The proposed\napproach exploits time information and human multimodal signals in video\nsequences. In particular, we rely on measuring the engagement level of people\nas a feature of group belonging. Our approach makes use of a recursive neural\nnetwork, the Long Short Term Memory (LSTM), to predict who will take the\nspeaker's turn in a conversation group. Experiments on the MatchNMingle dataset\nled to 85% true positives in group detection and 98% accuracy in predicting the\nnext speaker.\n","authors":["Lucrezia Tosato","Victor Fortier","Isabelle Bloch","Catherine Pelachaud"],"pdf_url":"https://arxiv.org/pdf/2408.16380v1.pdf","comment":"Accepted to Pattern Recognition Letter, 8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.15678v2","updated":"2024-08-29T09:37:38Z","published":"2024-08-28T10:07:17Z","title":"Deep Learning Based Speckle Filtering for Polarimetric SAR Images.\n  Application to Sentinel-1","summary":"  Speckle suppression in synthetic aperture radar (SAR) images is a key\nprocessing step which continues to be a research topic. A wide variety of\nmethods, using either spatially-based approaches or transform-based strategies,\nhave been developed and have shown to provide outstanding results. However,\nrecent advances in deep learning techniques and their application to SAR image\ndespeckling have been demonstrated to offer state-of-the-art results.\nUnfortunately, they have been mostly applied to single-polarimetric images. The\nextension of a deep learning-based approach for speckle removal to polarimetric\nSAR (PolSAR) images is complicated because of the complex nature of the\nmeasured covariance matrices for every image pixel, the properties of which\nmust be preserved during filtering. In this work, we propose a complete\nframework to remove speckle in polarimetric SAR images using a convolutional\nneural network. The methodology includes a reversible transformation of the\noriginal complex covariance matrix to obtain a set of real-valued intensity\nbands which are fed to the neural network. In addition, the proposed method\nincludes a change detection strategy to avoid the neural network to learn\nerroneous features in areas strongly affected by temporal changes, so that the\nnetwork only learns the underlying speckle component present in the data. The\nmethod is implemented and tested with dual-polarimetric images acquired by\nSentinel-1. Experiments show that the proposed approach offers exceptional\nresults in both speckle reduction and resolution preservation. More\nimportantly, it is also shown that the neural network is not generating\nartifacts or introducing bias in the filtered images, making them suitable for\nfurther polarimetric processing and exploitation.\n","authors":["Alejandro Mestre-Quereda","Juan M. Lopez-Sanchez"],"pdf_url":"https://arxiv.org/pdf/2408.15678v2.pdf","comment":"23 pages, 32 figures"},{"id":"http://arxiv.org/abs/2408.14342v2","updated":"2024-08-29T09:11:13Z","published":"2024-08-14T02:37:26Z","title":"Dual-Domain CLIP-Assisted Residual Optimization Perception Model for\n  Metal Artifact Reduction","summary":"  Metal artifacts in computed tomography (CT) imaging pose significant\nchallenges to accurate clinical diagnosis. The presence of high-density\nmetallic implants results in artifacts that deteriorate image quality,\nmanifesting in the forms of streaking, blurring, or beam hardening effects,\netc. Nowadays, various deep learning-based approaches, particularly generative\nmodels, have been proposed for metal artifact reduction (MAR). However, these\nmethods have limited perception ability in the diverse morphologies of\ndifferent metal implants with artifacts, which may generate spurious anatomical\nstructures and exhibit inferior generalization capability. To address the\nissues, we leverage visual-language model (VLM) to identify these morphological\nfeatures and introduce them into a dual-domain CLIP-assisted residual\noptimization perception model (DuDoCROP) for MAR. Specifically, a dual-domain\nCLIP (DuDoCLIP) is fine-tuned on the image domain and sinogram domain using\ncontrastive learning to extract semantic descriptions from anatomical\nstructures and metal artifacts. Subsequently, a diffusion model is guided by\nthe embeddings of DuDoCLIP, thereby enabling the dual-domain prior generation.\nAdditionally, we design prompt engineering for more precise image-text\ndescriptions that can enhance the model's perception capability. Then, a\ndownstream task is devised for the one-step residual optimization and\nintegration of dual-domain priors, while incorporating raw data fidelity.\nUltimately, a new perceptual indicator is proposed to validate the model's\nperception and generation performance. With the assistance of DuDoCLIP, our\nDuDoCROP exhibits at least 63.7% higher generalization capability compared to\nthe baseline model. Numerical experiments demonstrate that the proposed method\ncan generate more realistic image structures and outperform other SOTA\napproaches both qualitatively and quantitatively.\n","authors":["Xinrui Zhang","Ailong Cai","Shaoyu Wang","Linyuan Wang","Zhizhong Zheng","Lei Li","Bin Yan"],"pdf_url":"https://arxiv.org/pdf/2408.14342v2.pdf","comment":"14 pages, 18 figures"},{"id":"http://arxiv.org/abs/2408.13744v2","updated":"2024-08-29T09:08:54Z","published":"2024-08-25T07:08:58Z","title":"Enhancing Adaptive Deep Networks for Image Classification via\n  Uncertainty-aware Decision Fusion","summary":"  Handling varying computational resources is a critical issue in modern AI\napplications. Adaptive deep networks, featuring the dynamic employment of\nmultiple classifier heads among different layers, have been proposed to address\nclassification tasks under varying computing resources. Existing approaches\ntypically utilize the last classifier supported by the available resources for\ninference, as they believe that the last classifier always performs better\nacross all classes. However, our findings indicate that earlier classifier\nheads can outperform the last head for certain classes. Based on this\nobservation, we introduce the Collaborative Decision Making (CDM) module, which\nfuses the multiple classifier heads to enhance the inference performance of\nadaptive deep networks. CDM incorporates an uncertainty-aware fusion method\nbased on evidential deep learning (EDL), that utilizes the reliability\n(uncertainty values) from the first c-1 classifiers to improve the c-th\nclassifier' accuracy. We also design a balance term that reduces fusion\nsaturation and unfairness issues caused by EDL constraints to improve the\nfusion quality of CDM. Finally, a regularized training strategy that uses the\nlast classifier to guide the learning process of early classifiers is proposed\nto further enhance the CDM module's effect, called the Guided Collaborative\nDecision Making (GCDM) framework. The experimental evaluation demonstrates the\neffectiveness of our approaches. Results on ImageNet datasets show CDM and GCDM\nobtain 0.4% to 2.8% accuracy improvement (under varying computing resources) on\npopular adaptive networks. The code is available at the link\nhttps://github.com/Meteor-Stars/GCDM_AdaptiveNet.\n","authors":["Xu Zhang","Zhipeng Xie","Haiyang Yu","Qitong Wang","Peng Wang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.13744v2.pdf","comment":"13 pages, 27 figures. In ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.16357v1","updated":"2024-08-29T08:56:48Z","published":"2024-08-29T08:56:48Z","title":"Law of Vision Representation in MLLMs","summary":"  We present the \"Law of Vision Representation\" in multimodal large language\nmodels (MLLMs). It reveals a strong correlation between the combination of\ncross-modal alignment, correspondence in vision representation, and MLLM\nperformance. We quantify the two factors using the cross-modal Alignment and\nCorrespondence score (AC score). Through extensive experiments involving\nthirteen different vision representation settings and evaluations across eight\nbenchmarks, we find that the AC score is linearly correlated to model\nperformance. By leveraging this relationship, we are able to identify and train\nthe optimal vision representation only, which does not require finetuning the\nlanguage model every time, resulting in a 99.7% reduction in computational\ncost.\n","authors":["Shijia Yang","Bohan Zhai","Quanzeng You","Jianbo Yuan","Hongxia Yang","Chenfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2408.16357v1.pdf","comment":"The code is available at\n  https://github.com/bronyayang/Law_of_Vision_Representation_in_MLLMs"},{"id":"http://arxiv.org/abs/2401.03749v3","updated":"2024-08-29T08:52:40Z","published":"2024-01-08T09:20:46Z","title":"A Flying Bird Object Detection Method for Surveillance Video","summary":"  Aiming at the specific characteristics of flying bird objects in surveillance\nvideo, such as the typically non-obvious features in single-frame images, small\nsize in most instances, and asymmetric shapes, this paper proposes a Flying\nBird Object Detection method for Surveillance Video (FBOD-SV). Firstly, a new\nfeature aggregation module, the Correlation Attention Feature Aggregation\n(Co-Attention-FA) module, is designed to aggregate the features of the flying\nbird object according to the bird object's correlation on multiple consecutive\nframes of images. Secondly, a Flying Bird Object Detection Network (FBOD-Net)\nwith down-sampling followed by up-sampling is designed, which utilizes a large\nfeature layer that fuses fine spatial information and large receptive field\ninformation to detect special multi-scale (mostly small-scale) bird objects.\nFinally, the SimOTA dynamic label allocation method is applied to One-Category\nobject detection, and the SimOTA-OC dynamic label strategy is proposed to solve\nthe difficult problem of label allocation caused by irregular flying bird\nobjects. In this paper, the performance of the FBOD-SV is validated using\nexperimental datasets of flying bird objects in traction substation\nsurveillance videos. The experimental results show that the FBOD-SV effectively\nimproves the detection performance of flying bird objects in surveillance\nvideo.\n","authors":["Ziwei Sun","Zexi Hua","Hengchao Li","Yan Li"],"pdf_url":"https://arxiv.org/pdf/2401.03749v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16355v1","updated":"2024-08-29T08:51:25Z","published":"2024-08-29T08:51:25Z","title":"NeRF-CA: Dynamic Reconstruction of X-ray Coronary Angiography with\n  Extremely Sparse-views","summary":"  Dynamic three-dimensional (4D) reconstruction from two-dimensional X-ray\ncoronary angiography (CA) remains a significant clinical problem. Challenges\ninclude sparse-view settings, intra-scan motion, and complex vessel morphology\nsuch as structure sparsity and background occlusion. Existing CA reconstruction\nmethods often require extensive user interaction or large training datasets. On\nthe other hand, Neural Radiance Field (NeRF), a promising deep learning\ntechnique, has successfully reconstructed high-fidelity static scenes for\nnatural and medical scenes. Recent work, however, identified that sparse-views,\nbackground occlusion, and dynamics still pose a challenge when applying NeRF in\nthe X-ray angiography context. Meanwhile, many successful works for natural\nscenes propose regularization for sparse-view reconstruction or scene\ndecomposition to handle dynamics. However, these techniques do not directly\ntranslate to the CA context, where both challenges and background occlusion are\nsignificant. This paper introduces NeRF-CA, the first step toward a 4D CA\nreconstruction method that achieves reconstructions from sparse coronary\nangiograms with cardiac motion. We leverage the motion of the coronary artery\nto decouple the scene into a dynamic coronary artery component and static\nbackground. We combine this scene decomposition with tailored regularization\ntechniques. These techniques enforce the separation of the coronary artery from\nthe background by enforcing dynamic structure sparsity and scene smoothness. By\nuniquely combining these approaches, we achieve 4D reconstructions from as few\nas four angiogram sequences. This setting aligns with clinical workflows while\noutperforming state-of-the-art X-ray sparse-view NeRF reconstruction\ntechniques. We validate our approach quantitatively and qualitatively using 4D\nphantom datasets and ablation studies.\n","authors":["Kirsten W. H. Maas","Danny Ruijters","Anna Vilanova","Nicola Pezzotti"],"pdf_url":"https://arxiv.org/pdf/2408.16355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16343v1","updated":"2024-08-29T08:26:00Z","published":"2024-08-29T08:26:00Z","title":"Toward Robust Early Detection of Alzheimer's Disease via an Integrated\n  Multimodal Learning Approach","summary":"  Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by\nmemory loss, executive dysfunction, and personality changes. Early diagnosis is\nchallenging due to subtle symptoms and varied presentations, often leading to\nmisdiagnosis with traditional unimodal diagnostic methods due to their limited\nscope. This study introduces an advanced multimodal classification model that\nintegrates clinical, cognitive, neuroimaging, and EEG data to enhance\ndiagnostic accuracy. The model incorporates a feature tagger with a tabular\ndata coding architecture and utilizes the TimesBlock module to capture\nintricate temporal patterns in Electroencephalograms (EEG) data. By employing\nCross-modal Attention Aggregation module, the model effectively fuses Magnetic\nResonance Imaging (MRI) spatial information with EEG temporal data,\nsignificantly improving the distinction between AD, Mild Cognitive Impairment,\nand Normal Cognition. Simultaneously, we have constructed the first AD\nclassification dataset that includes three modalities: EEG, MRI, and tabular\ndata. Our innovative approach aims to facilitate early diagnosis and\nintervention, potentially slowing the progression of AD. The source code and\nour private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.\n","authors":["Yifei Chen","Shenghao Zhu","Zhaojie Fang","Chang Liu","Binfeng Zou","Yuhe Wang","Shuo Chang","Fan Jia","Feiwei Qin","Jin Fan","Yong Peng","Changmiao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16343v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.16340v1","updated":"2024-08-29T08:23:57Z","published":"2024-08-29T08:23:57Z","title":"Learned Image Transmission with Hierarchical Variational Autoencoder","summary":"  In this paper, we introduce an innovative hierarchical joint source-channel\ncoding (HJSCC) framework for image transmission, utilizing a hierarchical\nvariational autoencoder (VAE). Our approach leverages a combination of\nbottom-up and top-down paths at the transmitter to autoregressively generate\nmultiple hierarchical representations of the original image. These\nrepresentations are then directly mapped to channel symbols for transmission by\nthe JSCC encoder. We extend this framework to scenarios with a feedback link,\nmodeling transmission over a noisy channel as a probabilistic sampling process\nand deriving a novel generative formulation for JSCC with feedback. Compared\nwith existing approaches, our proposed HJSCC provides enhanced adaptability by\ndynamically adjusting transmission bandwidth, encoding these representations\ninto varying amounts of channel symbols. Additionally, we introduce a rate\nattention module to guide the JSCC encoder in optimizing its encoding strategy\nbased on prior information. Extensive experiments on images of varying\nresolutions demonstrate that our proposed model outperforms existing baselines\nin rate-distortion performance and maintains robustness against channel noise.\n","authors":["Guangyi Zhang","Hanlei Li","Yunlong Cai","Qiyu Hu","Guanding Yu","Runmin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.16340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16325v1","updated":"2024-08-29T08:00:07Z","published":"2024-08-29T08:00:07Z","title":"P2P-Bridge: Diffusion Bridges for 3D Point Cloud Denoising","summary":"  In this work, we tackle the task of point cloud denoising through a novel\nframework that adapts Diffusion Schr\\\"odinger bridges to points clouds. Unlike\nprevious approaches that predict point-wise displacements from point features\nor learned noise distributions, our method learns an optimal transport plan\nbetween paired point clouds. Experiments on object datasets like PU-Net and\nreal-world datasets such as ScanNet++ and ARKitScenes show that P2P-Bridge\nachieves significant improvements over existing methods. While our approach\ndemonstrates strong results using only point coordinates, we also show that\nincorporating additional features, such as color information or point-wise\nDINOv2 features, further enhances the performance. Code and pretrained models\nare available at https://p2p-bridge.github.io.\n","authors":["Mathias Vogel","Keisuke Tateno","Marc Pollefeys","Federico Tombari","Marie-Julie Rakotosaona","Francis Engelmann"],"pdf_url":"https://arxiv.org/pdf/2408.16325v1.pdf","comment":"ECCV 2024 Project page: https://p2p-bridge.github.io"},{"id":"http://arxiv.org/abs/2408.16322v1","updated":"2024-08-29T07:49:31Z","published":"2024-08-29T07:49:31Z","title":"BEVal: A Cross-dataset Evaluation Study of BEV Segmentation Models for\n  Autononomous Driving","summary":"  Current research in semantic bird's-eye view segmentation for autonomous\ndriving focuses solely on optimizing neural network models using a single\ndataset, typically nuScenes. This practice leads to the development of highly\nspecialized models that may fail when faced with different environments or\nsensor setups, a problem known as domain shift. In this paper, we conduct a\ncomprehensive cross-dataset evaluation of state-of-the-art BEV segmentation\nmodels to assess their performance across different training and testing\ndatasets and setups, as well as different semantic categories. We investigate\nthe influence of different sensors, such as cameras and LiDAR, on the models'\nability to generalize to diverse conditions and scenarios. Additionally, we\nconduct multi-dataset training experiments that improve models' BEV\nsegmentation performance compared to single-dataset training. Our work\naddresses the gap in evaluating BEV segmentation models under cross-dataset\nvalidation. And our findings underscore the importance of enhancing model\ngeneralizability and adaptability to ensure more robust and reliable BEV\nsegmentation approaches for autonomous driving applications.\n","authors":["Manuel Alejandro Diaz-Zapata","Wenqian Liu","Robin Baruffa","Christian Laugier"],"pdf_url":"https://arxiv.org/pdf/2408.16322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16314v1","updated":"2024-08-29T07:32:01Z","published":"2024-08-29T07:32:01Z","title":"ResVG: Enhancing Relation and Semantic Understanding in Multiple\n  Instances for Visual Grounding","summary":"  Visual grounding aims to localize the object referred to in an image based on\na natural language query. Although progress has been made recently, accurately\nlocalizing target objects within multiple-instance distractions (multiple\nobjects of the same category as the target) remains a significant challenge.\nExisting methods demonstrate a significant performance drop when there are\nmultiple distractions in an image, indicating an insufficient understanding of\nthe fine-grained semantics and spatial relationships between objects. In this\npaper, we propose a novel approach, the Relation and Semantic-sensitive Visual\nGrounding (ResVG) model, to address this issue. Firstly, we enhance the model's\nunderstanding of fine-grained semantics by injecting semantic prior information\nderived from text queries into the model. This is achieved by leveraging\ntext-to-image generation models to produce images representing the semantic\nattributes of target objects described in queries. Secondly, we tackle the lack\nof training samples with multiple distractions by introducing a\nrelation-sensitive data augmentation method. This method generates additional\ntraining data by synthesizing images containing multiple objects of the same\ncategory and pseudo queries based on their spatial relationships. The proposed\nReSVG model significantly improves the model's ability to comprehend both\nobject semantics and spatial relations, leading to enhanced performance in\nvisual grounding tasks, particularly in scenarios with multiple-instance\ndistractions. We conduct extensive experiments to validate the effectiveness of\nour methods on five datasets. Code is available at\nhttps://github.com/minghangz/ResVG.\n","authors":["Minghang Zheng","Jiahua Zhang","Qingchao Chen","Yuxin Peng","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.16314v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.16313v1","updated":"2024-08-29T07:22:16Z","published":"2024-08-29T07:22:16Z","title":"FA-YOLO: Research On Efficient Feature Selection YOLO Improved Algorithm\n  Based On FMDS and AGMF Modules","summary":"  Over the past few years, the YOLO series of models has emerged as one of the\ndominant methodologies in the realm of object detection. Many studies have\nadvanced these baseline models by modifying their architectures, enhancing data\nquality, and developing new loss functions. However, current models still\nexhibit deficiencies in processing feature maps, such as overlooking the fusion\nof cross-scale features and a static fusion approach that lacks the capability\nfor dynamic feature adjustment. To address these issues, this paper introduces\nan efficient Fine-grained Multi-scale Dynamic Selection Module (FMDS Module),\nwhich applies a more effective dynamic feature selection and fusion method on\nfine-grained multi-scale feature maps, significantly enhancing the detection\naccuracy of small, medium, and large-sized targets in complex environments.\nFurthermore, this paper proposes an Adaptive Gated Multi-branch Focus Fusion\nModule (AGMF Module), which utilizes multiple parallel branches to perform\ncomplementary fusion of various features captured by the gated unit branch,\nFMDS Module branch, and TripletAttention branch. This approach further enhances\nthe comprehensiveness, diversity, and integrity of feature fusion. This paper\nhas integrated the FMDS Module, AGMF Module, into Yolov9 to develop a novel\nobject detection model named FA-YOLO. Extensive experimental results show that\nunder identical experimental conditions, FA-YOLO achieves an outstanding 66.1%\nmean Average Precision (mAP) on the PASCAL VOC 2007 dataset, representing 1.0%\nimprovement over YOLOv9's 65.1%. Additionally, the detection accuracies of\nFA-YOLO for small, medium, and large targets are 44.1%, 54.6%, and 70.8%,\nrespectively, showing improvements of 2.0%, 3.1%, and 0.9% compared to YOLOv9's\n42.1%, 51.5%, and 69.9%.\n","authors":["Yukang Huo","Mingyuan Yao","Qingbin Tian","Tonghao Wang","Ruifeng Wang","Haihua Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16313v1.pdf","comment":"11 pages and 4 figures"},{"id":"http://arxiv.org/abs/2408.16310v1","updated":"2024-08-29T07:16:28Z","published":"2024-08-29T07:16:28Z","title":"Bootstrap Segmentation Foundation Model under Distribution Shift via\n  Object-Centric Learning","summary":"  Foundation models have made incredible strides in achieving zero-shot or\nfew-shot generalization, leveraging prompt engineering to mimic the\nproblem-solving approach of human intelligence. However, when it comes to some\nfoundation models like Segment Anything, there is still a challenge in\nperforming well on out-of-distribution data, including camouflaged and medical\nimages. Inconsistent prompting strategies during fine-tuning and testing\nfurther compound the issue, leading to decreased performance. Drawing\ninspiration from how human cognition processes new environments, we introduce\nSlotSAM, a method that reconstructs features from the encoder in a\nself-supervised manner to create object-centric representations. These\nrepresentations are then integrated into the foundation model, bolstering its\nobject-level perceptual capabilities while reducing the impact of\ndistribution-related variables. The beauty of SlotSAM lies in its simplicity\nand adaptability to various tasks, making it a versatile solution that\nsignificantly enhances the generalization abilities of foundation models.\nThrough limited parameter fine-tuning in a bootstrap manner, our approach paves\nthe way for improved generalization in novel environments. The code is\navailable at github.com/lytang63/SlotSAM.\n","authors":["Luyao Tang","Yuxuan Yuan","Chaoqi Chen","Kunze Huang","Xinghao Ding","Yue Huang"],"pdf_url":"https://arxiv.org/pdf/2408.16310v1.pdf","comment":"This work is accepted by ECCV 2024 EVAL-FoMo Workshop"},{"id":"http://arxiv.org/abs/2407.13307v2","updated":"2024-08-29T07:12:39Z","published":"2024-07-18T09:10:25Z","title":"Conformal Performance Range Prediction for Segmentation Output Quality\n  Control","summary":"  Recent works have introduced methods to estimate segmentation performance\nwithout ground truth, relying solely on neural network softmax outputs. These\ntechniques hold potential for intuitive output quality control. However, such\nperformance estimates rely on calibrated softmax outputs, which is often not\nthe case in modern neural networks. Moreover, the estimates do not take into\naccount inherent uncertainty in segmentation tasks. These limitations may\nrender precise performance predictions unattainable, restricting the practical\napplicability of performance estimation methods. To address these challenges,\nwe develop a novel approach for predicting performance ranges with statistical\nguarantees of containing the ground truth with a user specified probability.\nOur method leverages sampling-based segmentation uncertainty estimation to\nderive heuristic performance ranges, and applies split conformal prediction to\ntransform these estimates into rigorous prediction ranges that meet the desired\nguarantees. We demonstrate our approach on the FIVES retinal vessel\nsegmentation dataset and compare five commonly used sampling-based uncertainty\nestimation techniques. Our results show that it is possible to achieve the\ndesired coverage with small prediction ranges, highlighting the potential of\nperformance range prediction as a valuable tool for output quality control.\n","authors":["Anna M. Wundram","Paul Fischer","Michael Muehlebach","Lisa M. Koch","Christian F. Baumgartner"],"pdf_url":"https://arxiv.org/pdf/2407.13307v2.pdf","comment":"Accepted as an oral presentation at MICCAI UNSURE 2024"},{"id":"http://arxiv.org/abs/2408.16305v1","updated":"2024-08-29T07:11:50Z","published":"2024-08-29T07:11:50Z","title":"Semantics-Oriented Multitask Learning for DeepFake Detection: A Joint\n  Embedding Approach","summary":"  In recent years, the multimedia forensics and security community has seen\nremarkable progress in multitask learning for DeepFake (i.e., face forgery)\ndetection. The prevailing strategy has been to frame DeepFake detection as a\nbinary classification problem augmented by manipulation-oriented auxiliary\ntasks. This strategy focuses on learning features specific to face\nmanipulations, which exhibit limited generalizability. In this paper, we delve\ndeeper into semantics-oriented multitask learning for DeepFake detection,\nleveraging the relationships among face semantics via joint embedding. We first\npropose an automatic dataset expansion technique that broadens current face\nforgery datasets to support semantics-oriented DeepFake detection tasks at both\nthe global face attribute and local face region levels. Furthermore, we resort\nto joint embedding of face images and their corresponding labels (depicted by\ntextual descriptions) for prediction. This approach eliminates the need for\nmanually setting task-agnostic and task-specific parameters typically required\nwhen predicting labels directly from images. In addition, we employ a bi-level\noptimization strategy to dynamically balance the fidelity loss weightings of\nvarious tasks, making the training process fully automated. Extensive\nexperiments on six DeepFake datasets show that our method improves the\ngeneralizability of DeepFake detection and, meanwhile, renders some degree of\nmodel interpretation by providing human-understandable explanations.\n","authors":["Mian Zou","Baosheng Yu","Yibing Zhan","Siwei Lyu","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2408.16305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16303v1","updated":"2024-08-29T07:09:33Z","published":"2024-08-29T07:09:33Z","title":"Enhanced Control for Diffusion Bridge in Image Restoration","summary":"  Image restoration refers to the process of restoring a damaged low-quality\nimage back to its corresponding high-quality image. Typically, we use\nconvolutional neural networks to directly learn the mapping from low-quality\nimages to high-quality images achieving image restoration. Recently, a special\ntype of diffusion bridge model has achieved more advanced results in image\nrestoration. It can transform the direct mapping from low-quality to\nhigh-quality images into a diffusion process, restoring low-quality images\nthrough a reverse process. However, the current diffusion bridge restoration\nmodels do not emphasize the idea of conditional control, which may affect\nperformance. This paper introduces the ECDB model enhancing the control of the\ndiffusion bridge with low-quality images as conditions. Moreover, in response\nto the characteristic of diffusion models having low denoising level at larger\nvalues of \\(\\bm t \\), we also propose a Conditional Fusion Schedule, which more\neffectively handles the conditional feature information of various modules.\nExperimental results prove that the ECDB model has achieved state-of-the-art\nresults in many image restoration tasks, including deraining, inpainting and\nsuper-resolution. Code is avaliable at https://github.com/Hammour-steak/ECDB.\n","authors":["Conghan Yue","Zhengwei Peng","Junlong Ma","Dongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.16303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15844v2","updated":"2024-08-29T07:08:21Z","published":"2024-08-28T15:04:52Z","title":"Shot Segmentation Based on Von Neumann Entropy for Key Frame Extraction","summary":"  Video key frame extraction is important in various fields, such as video\nsummary, retrieval, and compression. Therefore, we suggest a video key frame\nextraction algorithm based on shot segmentation using Von Neumann entropy. The\nsegmentation of shots is achieved through the computation of Von Neumann\nentropy of the similarity matrix among frames within the video sequence. The\ninitial frame of each shot is selected as key frames, which combines the\ntemporal sequence information of frames. The experimental results show the\nextracted key frames can fully and accurately represent the original video\ncontent while minimizing the number of repeated frames.\n","authors":["Xueqing Zhang","Di Fu","Naihao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15844v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.15996v2","updated":"2024-08-29T06:54:11Z","published":"2024-08-28T17:59:05Z","title":"Spatio-Temporal Context Prompting for Zero-Shot Action Detection","summary":"  Spatio-temporal action detection encompasses the tasks of localizing and\nclassifying individual actions within a video. Recent works aim to enhance this\nprocess by incorporating interaction modeling, which captures the relationship\nbetween people and their surrounding context. However, these approaches have\nprimarily focused on fully-supervised learning, and the current limitation lies\nin the lack of generalization capability to recognize unseen action categories.\nIn this paper, we aim to adapt the pretrained image-language models to detect\nunseen actions. To this end, we propose a method which can effectively leverage\nthe rich knowledge of visual-language models to perform Person-Context\nInteraction. Meanwhile, our Context Prompting module will utilize contextual\ninformation to prompt labels, thereby enhancing the generation of more\nrepresentative text features. Moreover, to address the challenge of recognizing\ndistinct actions by multiple people at the same timestamp, we design the\nInterest Token Spotting mechanism which employs pretrained visual knowledge to\nfind each person's interest context tokens, and then these tokens will be used\nfor prompting to generate text features tailored to each individual. To\nevaluate the ability to detect unseen actions, we propose a comprehensive\nbenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our\nmethod achieves superior results compared to previous approaches and can be\nfurther extended to multi-action videos, bringing it closer to real-world\napplications. The code and data can be found in\nhttps://webber2933.github.io/ST-CLIP-project-page.\n","authors":["Wei-Jhe Huang","Min-Hung Chen","Shang-Hong Lai"],"pdf_url":"https://arxiv.org/pdf/2408.15996v2.pdf","comment":"Project page: https://webber2933.github.io/ST-CLIP-project-page"},{"id":"http://arxiv.org/abs/2408.16296v1","updated":"2024-08-29T06:54:03Z","published":"2024-08-29T06:54:03Z","title":"Rethinking Sparse Lexical Representations for Image Retrieval in the Age\n  of Rising Multi-Modal Large Language Models","summary":"  In this paper, we rethink sparse lexical representations for image retrieval.\nBy utilizing multi-modal large language models (M-LLMs) that support visual\nprompting, we can extract image features and convert them into textual data,\nenabling us to utilize efficient sparse retrieval algorithms employed in\nnatural language processing for image retrieval tasks. To assist the LLM in\nextracting image features, we apply data augmentation techniques for key\nexpansion and analyze the impact with a metric for relevance between images and\ntextual data. We empirically show the superior precision and recall performance\nof our image retrieval method compared to conventional vision-language\nmodel-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a\nkeyword-based image retrieval scenario, where keywords serve as search queries.\nWe also demonstrate that the retrieval performance can be improved by\niteratively incorporating keywords into search queries.\n","authors":["Kengo Nakata","Daisuke Miyashita","Youyang Ng","Yasuto Hoshi","Jun Deguchi"],"pdf_url":"https://arxiv.org/pdf/2408.16296v1.pdf","comment":"Accepted to ECCV 2024 Workshops: 2nd Workshop on Traditional Computer\n  Vision in the Age of Deep Learning (TradiCV)"},{"id":"http://arxiv.org/abs/2407.18520v3","updated":"2024-08-29T06:52:45Z","published":"2024-07-26T05:29:24Z","title":"Text-Region Matching for Multi-Label Image Recognition with Missing\n  Labels","summary":"  Recently, large-scale visual language pre-trained (VLP) models have\ndemonstrated impressive performance across various downstream tasks. Motivated\nby these advancements, pioneering efforts have emerged in multi-label image\nrecognition with missing labels, leveraging VLP prompt-tuning technology.\nHowever, they usually cannot match text and vision features well, due to\ncomplicated semantics gaps and missing labels in a multi-label image. To tackle\nthis challenge, we propose $\\textbf{T}$ext-$\\textbf{R}$egion\n$\\textbf{M}$atching for optimizing $\\textbf{M}$ulti-$\\textbf{L}$abel prompt\ntuning, namely TRM-ML, a novel method for enhancing meaningful cross-modal\nmatching. Compared to existing methods, we advocate exploring the information\nof category-aware regions rather than the entire image or pixels, which\ncontributes to bridging the semantic gap between textual and visual\nrepresentations in a one-to-one matching manner. Concurrently, we further\nintroduce multimodal contrastive learning to narrow the semantic gap between\ntextual and visual modalities and establish intra-class and inter-class\nrelationships. Additionally, to deal with missing labels, we propose a\nmultimodal category prototype that leverages intra- and inter-category semantic\nrelationships to estimate unknown labels, facilitating pseudo-label generation.\nExtensive experiments on the MS-COCO, PASCAL VOC, Visual Genome, NUS-WIDE, and\nCUB-200-211 benchmark datasets demonstrate that our proposed framework\noutperforms the state-of-the-art methods by a significant margin. Our code is\navailable here: https://github.com/yu-gi-oh-leilei/TRM-ML.\n","authors":["Leilei Ma","Hongxing Xie","Lei Wang","Yanping Fu","Dengdi Sun","Haifeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.18520v3.pdf","comment":"Accepted to ACM International Conference on Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2408.16289v1","updated":"2024-08-29T06:40:34Z","published":"2024-08-29T06:40:34Z","title":"Convolutional Neural Network Compression Based on Low-Rank Decomposition","summary":"  Deep neural networks typically impose significant computational loads and\nmemory consumption. Moreover, the large parameters pose constraints on\ndeploying the model on edge devices such as embedded systems. Tensor\ndecomposition offers a clear advantage in compressing large-scale weight\ntensors. Nevertheless, direct utilization of low-rank decomposition typically\nleads to significant accuracy loss. This paper proposes a model compression\nmethod that integrates Variational Bayesian Matrix Factorization (VBMF) with\northogonal regularization. Initially, the model undergoes over-parameterization\nand training, with orthogonal regularization applied to enhance its likelihood\nof achieving the accuracy of the original model. Secondly, VBMF is employed to\nestimate the rank of the weight tensor at each layer. Our framework is\nsufficiently general to apply to other convolutional neural networks and easily\nadaptable to incorporate other tensor decomposition methods. Experimental\nresults show that for both high and low compression ratios, our compression\nmodel exhibits advanced performance.\n","authors":["Yaping He","Linhao Jiang","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16289v1.pdf","comment":"10 pages, 1 figures"},{"id":"http://arxiv.org/abs/2405.07288v2","updated":"2024-08-29T06:22:48Z","published":"2024-05-12T14:01:05Z","title":"Erasing Concepts from Text-to-Image Diffusion Models with Few-shot\n  Unlearning","summary":"  Generating images from text has become easier because of the scaling of\ndiffusion models and advancements in the field of vision and language. These\nmodels are trained using vast amounts of data from the Internet. Hence, they\noften contain undesirable content such as copyrighted material. As it is\nchallenging to remove such data and retrain the models, methods for erasing\nspecific concepts from pre-trained models have been investigated. We propose a\nnovel concept-erasure method that updates the text encoder using few-shot\nunlearning in which a few real images are used. The discussion regarding the\ngenerated images after erasing a concept has been lacking. While there are\nmethods for specifying the transition destination for concepts, the validity of\nthe specified concepts is unclear. Our method implicitly achieves this by\ntransitioning to the latent concepts inherent in the model or the images. Our\nmethod can erase a concept within 10 s, making concept erasure more accessible\nthan ever before. Implicitly transitioning to related concepts leads to more\nnatural concept erasure. We applied the proposed method to various concepts and\nconfirmed that concept erasure can be achieved tens to hundreds of times faster\nthan with current methods. By varying the parameters to be updated, we obtained\nresults suggesting that, like previous research, knowledge is primarily\naccumulated in the feed-forward networks of the text encoder. Our code is\navailable at \\url{https://github.com/fmp453/few-shot-erasing}\n","authors":["Masane Fuchi","Tomohiro Takagi"],"pdf_url":"https://arxiv.org/pdf/2405.07288v2.pdf","comment":"25 pages, 28 figures, accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2407.07046v2","updated":"2024-08-29T06:15:55Z","published":"2024-07-09T17:07:29Z","title":"CorMulT: A Semi-supervised Modality Correlation-aware Multimodal\n  Transformer for Sentiment Analysis","summary":"  Multimodal sentiment analysis is an active research area that combines\nmultiple data modalities, e.g., text, image and audio, to analyze human\nemotions and benefits a variety of applications. Existing multimodal sentiment\nanalysis methods can be classified as modality interaction-based methods,\nmodality transformation-based methods and modality similarity-based methods.\nHowever, most of these methods highly rely on the strong correlations between\nmodalities, and cannot fully uncover and utilize the correlations between\nmodalities to enhance sentiment analysis. Therefore, these methods usually\nachieve bad performance for identifying the sentiment of multimodal data with\nweak correlations. To address this issue, we proposed a two-stage\nsemi-supervised model termed Correlation-aware Multimodal Transformer (CorMulT)\nwhich consists pre-training stage and prediction stage. At the pre-training\nstage, a modality correlation contrastive learning module is designed to\nefficiently learn modality correlation coefficients between different\nmodalities. At the prediction stage, the learned correlation coefficients are\nfused with modality representations to make the sentiment prediction. According\nto the experiments on the popular multimodal dataset CMU-MOSEI, CorMulT\nobviously surpasses state-of-the-art multimodal sentiment analysis methods.\n","authors":["Yangmin Li","Ruiqi Zhu","Wengen Li"],"pdf_url":"https://arxiv.org/pdf/2407.07046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16102v3","updated":"2024-08-29T05:56:45Z","published":"2023-03-28T16:11:31Z","title":"KeyMatchNet: Zero-Shot Pose Estimation in 3D Point Clouds by Generalized\n  Keypoint Matching","summary":"  In this paper, we present KeyMatchNet, a novel network for zero-shot pose\nestimation in 3D point clouds. Our method uses only depth information, making\nit more applicable for many industrial use cases, as color information is\nseldom available. The network is composed of two parallel components for\ncomputing object and scene features. The features are then combined to create\nmatches used for pose estimation. The parallel structure allows for\npre-processing of the individual parts, which decreases the run-time. Using a\nzero-shot network allows for a very short set-up time, as it is not necessary\nto train models for new objects. However, as the network is not trained for the\nspecific object, zero-shot pose estimation methods generally have lower\naccuracy compared with conventional methods. To address this, we reduce the\ncomplexity of the task by including the scenario information during training.\nThis is typically not feasible as collecting real data for new tasks\ndrastically increases the cost. However, for zero-shot pose estimation,\ntraining for new objects is not necessary and the expensive data collection can\nthus be performed only once. Our method is trained on 1,500 objects and is only\ntested on unseen objects. We demonstrate that the trained network can not only\naccurately estimate poses for novel objects, but also demonstrate the ability\nof the network on objects outside of the trained class. Test results are also\nshown on real data. We believe that the presented method is valuable for many\nreal-world scenarios. Project page available at keymatchnet.github.io\n","authors":["Frederik HagelskjÃ¦r","Rasmus Laurvig Haugaard"],"pdf_url":"https://arxiv.org/pdf/2303.16102v3.pdf","comment":"8 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2408.16277v1","updated":"2024-08-29T05:56:34Z","published":"2024-08-29T05:56:34Z","title":"Fine-grained Classification of Port Wine Stains Using Optical Coherence\n  Tomography Angiography","summary":"  Accurate classification of port wine stains (PWS, vascular malformations\npresent at birth), is critical for subsequent treatment planning. However, the\ncurrent method of classifying PWS based on the external skin appearance rarely\nreflects the underlying angiopathological heterogeneity of PWS lesions,\nresulting in inconsistent outcomes with the common vascular-targeted\nphotodynamic therapy (V-PDT) treatments. Conversely, optical coherence\ntomography angiography (OCTA) is an ideal tool for visualizing the vascular\nmalformations of PWS. Previous studies have shown no significant correlation\nbetween OCTA quantitative metrics and the PWS subtypes determined by the\ncurrent classification approach. This study proposes a new classification\napproach for PWS using both OCT and OCTA. By examining the hypodermic\nhistopathology and vascular structure of PWS, we have devised a fine-grained\nclassification method that subdivides PWS into five distinct types. To assess\nthe angiopathological differences of various PWS subtypes, we have analyzed six\nmetrics related to vascular morphology and depth information of PWS lesions.\nThe five PWS types present significant differences across all metrics compared\nto the conventional subtypes. Our findings suggest that an angiopathology-based\nclassification accurately reflects the heterogeneity in PWS lesions. This\nresearch marks the first attempt to classify PWS based on angiopathology,\npotentially guiding more effective subtyping and treatment strategies for PWS.\n","authors":["Xiaofeng Deng","Defu Chen","Bowen Liu","Xiwan Zhang","Haixia Qiu","Wu Yuan","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2408.16277v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.14400v2","updated":"2024-08-29T05:37:38Z","published":"2024-08-26T16:34:13Z","title":"Satellite Sunroof: High-res Digital Surface Models and Roof Segmentation\n  for Global Solar Mapping","summary":"  The transition to renewable energy, particularly solar, is key to mitigating\nclimate change. Google's Solar API aids this transition by estimating solar\npotential from aerial imagery, but its impact is constrained by geographical\ncoverage. This paper proposes expanding the API's reach using satellite\nimagery, enabling global solar potential assessment. We tackle challenges\ninvolved in building a Digital Surface Model (DSM) and roof instance\nsegmentation from lower resolution and single oblique views using deep learning\nmodels. Our models, trained on aligned satellite and aerial datasets, produce\n25cm DSMs and roof segments. With ~1m DSM MAE on buildings, ~5deg roof pitch\nerror and ~56% IOU on roof segmentation, they significantly enhance the Solar\nAPI's potential to promote solar adoption.\n","authors":["Vishal Batchu","Alex Wilson","Betty Peng","Carl Elkin","Umangi Jain","Christopher Van Arsdale","Ross Goroshin","Varun Gulshan"],"pdf_url":"https://arxiv.org/pdf/2408.14400v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.16273v1","updated":"2024-08-29T05:33:59Z","published":"2024-08-29T05:33:59Z","title":"SAU: A Dual-Branch Network to Enhance Long-Tailed Recognition via\n  Generative Models","summary":"  Long-tailed distributions in image recognition pose a considerable challenge\ndue to the severe imbalance between a few dominant classes with numerous\nexamples and many minority classes with few samples. Recently, the use of large\ngenerative models to create synthetic data for image classification has been\nrealized, but utilizing synthetic data to address the challenge of long-tailed\nrecognition remains relatively unexplored. In this work, we proposed the use of\nsynthetic data as a complement to long-tailed datasets to eliminate the impact\nof data imbalance. To tackle this real-synthetic mixed dataset, we designed a\ntwo-branch model that contains Synthetic-Aware and Unaware branches (SAU). The\ncore ideas are (1) a synthetic-unaware branch for classification that mixes\nreal and synthetic data and treats all data equally without distinguishing\nbetween them. (2) A synthetic-aware branch for improving the robustness of the\nfeature extractor by distinguishing between real and synthetic data and\nlearning their discrepancies. Extensive experimental results demonstrate that\nour method can improve the accuracy of long-tailed image recognition. Notably,\nour approach achieves state-of-the-art Top-1 accuracy and significantly\nsurpasses other methods on CIFAR-10-LT and CIFAR-100-LT datasets across various\nimbalance factors. Our code is available at https://github.com/lgX1123/gm4lt.\n","authors":["Guangxi Li","Yinsheng Song","Mingkai Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.16273v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2408.16272v1","updated":"2024-08-29T05:32:03Z","published":"2024-08-29T05:32:03Z","title":"Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal\n  Grounding","summary":"  Existing Video Temporal Grounding (VTG) models excel in accuracy but often\noverlook open-world challenges posed by open-vocabulary queries and untrimmed\nvideos. This leads to unreliable predictions for noisy, corrupted, and\nout-of-distribution data. Adapting VTG models to dynamically estimate\nuncertainties based on user input can address this issue. To this end, we\nintroduce SRAM, a robust network module that benefits from a two-stage\ncross-modal alignment task. More importantly, it integrates Deep Evidential\nRegression (DER) to explicitly and thoroughly quantify uncertainty during\ntraining, thus allowing the model to say \"I do not know\" in scenarios beyond\nits handling capacity. However, the direct application of traditional DER\ntheory and its regularizer reveals structural flaws, leading to unintended\nconstraints in VTG tasks. In response, we develop a simple yet effective\nGeom-regularizer that enhances the uncertainty learning framework from the\nground up. To the best of our knowledge, this marks the first successful\nattempt of DER in VTG. Our extensive quantitative and qualitative results\naffirm the effectiveness, robustness, and interpretability of our modules and\nthe uncertainty learning paradigm in VTG tasks. The code will be made\navailable.\n","authors":["Kaijing Ma","Haojian Huang","Jin Chen","Haodong Chen","Pengliang Ji","Xianghao Zang","Han Fang","Chao Ban","Hao Sun","Mulin Chen","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.16272v1.pdf","comment":"Ongoing work: 28pages, 19 figures, 7 tables. Code is available at:\n  https://kaijing.space/SRAM/"},{"id":"http://arxiv.org/abs/2408.16268v1","updated":"2024-08-29T05:13:01Z","published":"2024-08-29T05:13:01Z","title":"UDD: Dataset Distillation via Mining Underutilized Regions","summary":"  Dataset distillation synthesizes a small dataset such that a model trained on\nthis set approximates the performance of the original dataset. Recent studies\non dataset distillation focused primarily on the design of the optimization\nprocess, with methods such as gradient matching, feature alignment, and\ntraining trajectory matching. However, little attention has been given to the\nissue of underutilized regions in synthetic images. In this paper, we propose\nUDD, a novel approach to identify and exploit the underutilized regions to make\nthem informative and discriminate, and thus improve the utilization of the\nsynthetic dataset. Technically, UDD involves two underutilized regions\nsearching policies for different conditions, i.e., response-based policy and\ndata jittering-based policy. Compared with previous works, such two policies\nare utilization-sensitive, equipping with the ability to dynamically adjust the\nunderutilized regions during the training process. Additionally, we analyze the\ncurrent model optimization problem and design a category-wise feature\ncontrastive loss, which can enhance the distinguishability of different\ncategories and alleviate the shortcomings of the existing multi-formation\nmethods. Experimentally, our method improves the utilization of the synthetic\ndataset and outperforms the state-of-the-art methods on various datasets, such\nas MNIST, FashionMNIST, SVHN, CIFAR-10, and CIFAR-100. For example, the\nimprovements on CIFAR-10 and CIFAR-100 are 4.0\\% and 3.7\\% over the next best\nmethod with IPC=1, by mining the underutilized regions.\n","authors":["Shiguang Wang","Zhongyu Zhang","Jian Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.16268v1.pdf","comment":"PRCV2024"},{"id":"http://arxiv.org/abs/2408.16266v1","updated":"2024-08-29T05:05:02Z","published":"2024-08-29T05:05:02Z","title":"Improving Diffusion-based Data Augmentation with Inversion Spherical\n  Interpolation","summary":"  Data Augmentation (DA), \\ie, synthesizing faithful and diverse samples to\nexpand the original training set, is a prevalent and effective strategy to\nimprove various visual recognition tasks. With the powerful image generation\nability, diffusion-based DA has shown strong performance gains on different\nbenchmarks. In this paper, we analyze today's diffusion-based DA methods, and\nargue that they cannot take account of both faithfulness and diversity, which\nare two critical keys for generating high-quality samples and boosting final\nclassification performance. To this end, we propose a novel Diffusion-based\nInversion Interpolation DA method: Diff-II. Specifically, Diff-II consists of\nthree main steps: 1) Category concepts learning: Learning concept embeddings\nfor each category. 2) Inversion interpolation: Calculating the inversion for\neach image, and conducting spherical interpolation for two randomly sampled\ninversions from the same category. 3) Two-stage denoising: Using different\nprompts to generate synthesized images in a coarse-to-fine manner. Extensive\nexperiments on multiple image classification tasks (\\eg, few-shot, long-tailed,\nand out-of-distribution classification) have demonstrated its effectiveness\nover state-of-the-art diffusion-based DA methods.\n","authors":["Yanghao Wang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2408.16266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16265v1","updated":"2024-08-29T05:04:25Z","published":"2024-08-29T05:04:25Z","title":"Low Saturation Confidence Distribution-based Test-Time Adaptation for\n  Cross-Domain Remote Sensing Image Classification","summary":"  Although the Unsupervised Domain Adaptation (UDA) method has improved the\neffect of remote sensing image classification tasks, most of them are still\nlimited by access to the source domain (SD) data. Designs such as Source-free\nDomain Adaptation (SFDA) solve the challenge of a lack of SD data, however,\nthey still rely on a large amount of target domain data and thus cannot achieve\nfast adaptations, which seriously hinders their further application in broader\nscenarios. The real-world applications of cross-domain remote sensing image\nclassification require a balance of speed and accuracy at the same time.\nTherefore, we propose a novel and comprehensive test time adaptation (TTA)\nmethod -- Low Saturation Confidence Distribution Test Time Adaptation\n(LSCD-TTA), which is the first attempt to solve such scenarios through the idea\nof TTA. LSCD-TTA specifically considers the distribution characteristics of\nremote sensing images, including three main parts that concentrate on different\noptimization directions: First, low saturation distribution (LSD) considers the\ndominance of low-confidence samples during the later TTA stage. Second,\nweak-category cross-entropy (WCCE) increases the weight of categories that are\nmore difficult to classify with less prior knowledge. Finally, diverse\ncategories confidence (DIV) comprehensively considers the category diversity to\nalleviate the deviation of the sample distribution. By weighting the\nabovementioned three modules, the model can widely, quickly and accurately\nadapt to the target domain without much prior target distributions, repeated\ndata access, and manual annotation. We evaluate LSCD-TTA on three\nremote-sensing image datasets. The experimental results show that LSCD-TTA\nachieves a significant gain of 4.96%-10.51% with Resnet-50 and 5.33%-12.49%\nwith Resnet-101 in average accuracy compared to other state-of-the-art DA and\nTTA methods.\n","authors":["Yu Liang","Xiucheng Zhang","Juepeng Zheng","Jianxi Huang","Haohuan Fu"],"pdf_url":"https://arxiv.org/pdf/2408.16265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16258v1","updated":"2024-08-29T04:40:31Z","published":"2024-08-29T04:40:31Z","title":"Advancing Architectural Floorplan Design with Geometry-enhanced Graph\n  Diffusion","summary":"  Automating architectural floorplan design is vital for housing and interior\ndesign, offering a faster, cost-effective alternative to manual sketches by\narchitects. However, existing methods, including rule-based and learning-based\napproaches, face challenges in design complexity and constrained generation\nwith extensive post-processing, and tend to obvious geometric inconsistencies\nsuch as misalignment, overlap, and gaps. In this work, we propose a novel\ngenerative framework for vector floorplan design via structural graph\ngeneration, called GSDiff, focusing on wall junction generation and wall\nsegment prediction to capture both geometric and semantic aspects of structural\ngraphs. To improve the geometric rationality of generated structural graphs, we\npropose two innovative geometry enhancement methods. In wall junction\ngeneration, we propose a novel alignment loss function to improve geometric\nconsistency. In wall segment prediction, we propose a random self-supervision\nmethod to enhance the model's perception of the overall geometric structure,\nthereby promoting the generation of reasonable geometric structures. Employing\nthe diffusion model and the Transformer model, as well as the geometry\nenhancement strategies, our framework can generate wall junctions, wall\nsegments and room polygons with structural and semantic information, resulting\nin structural graphs that accurately represent floorplans. Extensive\nexperiments show that the proposed method surpasses existing techniques,\nenabling free generation and constrained generation, marking a shift towards\nstructure generation in architectural design.\n","authors":["Sizhe Hu","Wenming Wu","Yuntao Wang","Benzhu Xu","Liping Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.16258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16254v1","updated":"2024-08-29T04:30:31Z","published":"2024-08-29T04:30:31Z","title":"EvLight++: Low-Light Video Enhancement with an Event Camera: A\n  Large-Scale Real-World Dataset, Novel Method, and More","summary":"  Event cameras offer significant advantages for low-light video enhancement,\nprimarily due to their high dynamic range. Current research, however, is\nseverely limited by the absence of large-scale, real-world, and\nspatio-temporally aligned event-video datasets. To address this, we introduce a\nlarge-scale dataset with over 30,000 pairs of frames and events captured under\nvarying illumination. This dataset was curated using a robotic arm that traces\na consistent non-linear trajectory, achieving spatial alignment precision under\n0.03mm and temporal alignment with errors under 0.01s for 90% of the dataset.\nBased on the dataset, we propose \\textbf{EvLight++}, a novel event-guided\nlow-light video enhancement approach designed for robust performance in\nreal-world scenarios. Firstly, we design a multi-scale holistic fusion branch\nto integrate structural and textural information from both images and events.\nTo counteract variations in regional illumination and noise, we introduce\nSignal-to-Noise Ratio (SNR)-guided regional feature selection, enhancing\nfeatures from high SNR regions and augmenting those from low SNR regions by\nextracting structural information from events. To incorporate temporal\ninformation and ensure temporal coherence, we further introduce a recurrent\nmodule and temporal loss in the whole pipeline. Extensive experiments on our\nand the synthetic SDSD dataset demonstrate that EvLight++ significantly\noutperforms both single image- and video-based methods by 1.37 dB and 3.71 dB,\nrespectively. To further explore its potential in downstream tasks like\nsemantic segmentation and monocular depth estimation, we extend our datasets by\nadding pseudo segmentation and depth labels via meticulous annotation efforts\nwith foundation models. Experiments under diverse low-light scenes show that\nthe enhanced results achieve a 15.97% improvement in mIoU for semantic\nsegmentation.\n","authors":["Kanghao Chen","Guoqiang Liang","Hangyu Li","Yunfan Lu","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16254v1.pdf","comment":"Journal extension based on EvLight (arXiv:2404.00834)"},{"id":"http://arxiv.org/abs/2408.16247v1","updated":"2024-08-29T03:58:21Z","published":"2024-08-29T03:58:21Z","title":"Anno-incomplete Multi-dataset Detection","summary":"  Object detectors have shown outstanding performance on various public\ndatasets. However, annotating a new dataset for a new task is usually\nunavoidable in real, since 1) a single existing dataset usually does not\ncontain all object categories needed; 2) using multiple datasets usually\nsuffers from annotation incompletion and heterogeneous features. We propose a\nnovel problem as \"Annotation-incomplete Multi-dataset Detection\", and develop\nan end-to-end multi-task learning architecture which can accurately detect all\nthe object categories with multiple partially annotated datasets. Specifically,\nwe propose an attention feature extractor which helps to mine the relations\namong different datasets. Besides, a knowledge amalgamation training strategy\nis incorporated to accommodate heterogeneous features from different sources.\nExtensive experiments on different object detection datasets demonstrate the\neffectiveness of our methods and an improvement of 2.17%, 2.10% in mAP can be\nachieved on COCO and VOC respectively.\n","authors":["Yiran Xu","Haoxiang Zhong","Kai Wu","Jialin Li","Yong Liu","Chengjie Wang","Shu-Tao Xia","Hongen Liao"],"pdf_url":"https://arxiv.org/pdf/2408.16247v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.15643v2","updated":"2024-08-29T03:47:04Z","published":"2024-08-28T08:53:33Z","title":"RIDE: Boosting 3D Object Detection for LiDAR Point Clouds via\n  Rotation-Invariant Analysis","summary":"  The rotation robustness property has drawn much attention to point cloud\nanalysis, whereas it still poses a critical challenge in 3D object detection.\nWhen subjected to arbitrary rotation, most existing detectors fail to produce\nexpected outputs due to the poor rotation robustness. In this paper, we present\nRIDE, a pioneering exploration of Rotation-Invariance for the 3D\nLiDAR-point-based object DEtector, with the key idea of designing\nrotation-invariant features from LiDAR scenes and then effectively\nincorporating them into existing 3D detectors. Specifically, we design a\nbi-feature extractor that extracts (i) object-aware features though sensitive\nto rotation but preserve geometry well, and (ii) rotation-invariant features,\nwhich lose geometric information to a certain extent but are robust to\nrotation. These two kinds of features complement each other to decode 3D\nproposals that are robust to arbitrary rotations. Particularly, our RIDE is\ncompatible and easy to plug into the existing one-stage and two-stage 3D\ndetectors, and boosts both detection performance and rotation robustness.\nExtensive experiments on the standard benchmarks showcase that the mean average\nprecision (mAP) and rotation robustness can be significantly boosted by\nintegrating with our RIDE, with +5.6% mAP and 53% rotation robustness\nimprovement on KITTI, +5.1% and 28% improvement correspondingly on nuScenes.\nThe code will be available soon.\n","authors":["Zhaoxuan Wang","Xu Han","Hongxin Liu","Xianzhi Li"],"pdf_url":"https://arxiv.org/pdf/2408.15643v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16236v1","updated":"2024-08-29T03:26:14Z","published":"2024-08-29T03:26:14Z","title":"Neural Spectral Decomposition for Dataset Distillation","summary":"  In this paper, we propose Neural Spectrum Decomposition, a generic\ndecomposition framework for dataset distillation. Unlike previous methods, we\nconsider the entire dataset as a high-dimensional observation that is low-rank\nacross all dimensions. We aim to discover the low-rank representation of the\nentire dataset and perform distillation efficiently. Toward this end, we learn\na set of spectrum tensors and transformation matrices, which, through simple\nmatrix multiplication, reconstruct the data distribution. Specifically, a\nspectrum tensor can be mapped back to the image space by a transformation\nmatrix, and efficient information sharing during the distillation learning\nprocess is achieved through pairwise combinations of different spectrum vectors\nand transformation matrices. Furthermore, we integrate a trajectory matching\noptimization method guided by a real distribution. Our experimental results\ndemonstrate that our approach achieves state-of-the-art performance on\nbenchmarks, including CIFAR10, CIFAR100, Tiny Imagenet, and ImageNet Subset.\nOur code are available at \\url{https://github.com/slyang2021/NSD}.\n","authors":["Shaolei Yang","Shen Cheng","Mingbo Hong","Haoqiang Fan","Xing Wei","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2408.16236v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2311.01673v2","updated":"2024-08-29T03:25:04Z","published":"2023-11-03T02:43:51Z","title":"Content Significance Distribution of Sub-Text Blocks in Articles and Its\n  Application to Article-Organization Assessment","summary":"  We explore how to capture the significance of a sub-text block in an article\nand how it may be used for text mining tasks. A sub-text block is a\nsub-sequence of sentences in the article. We formulate the notion of content\nsignificance distribution (CSD) of sub-text blocks, referred to as CSD of the\nfirst kind and denoted by CSD-1. In particular, we leverage Hugging Face's\nSentenceTransformer to generate contextual sentence embeddings, and use\nMoverScore over text embeddings to measure how similar a sub-text block is to\nthe entire text. To overcome the exponential blowup on the number of sub-text\nblocks, we present an approximation algorithm and show that the approximated\nCSD-1 is almost identical to the exact CSD-1. Under this approximation, we show\nthat the average and median CSD-1's for news, scholarly research, argument, and\nnarrative articles share the same pattern. We also show that under a certain\nlinear transformation, the complement of the cumulative distribution function\nof the beta distribution with certain values of $\\alpha$ and $\\beta$ resembles\na CSD-1 curve. We then use CSD-1's to extract linguistic features to train an\nSVC classifier for assessing how well an article is organized. Through\nexperiments, we show that this method achieves high accuracy for assessing\nstudent essays. Moreover, we study CSD of sentence locations, referred to as\nCSD of the second kind and denoted by CSD-2, and show that average CSD-2's for\ndifferent types of articles possess distinctive patterns, which either conform\ncommon perceptions of article structures or provide rectification with minor\ndeviation.\n","authors":["You Zhou","Jie Wang"],"pdf_url":"https://arxiv.org/pdf/2311.01673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16235v1","updated":"2024-08-29T03:23:51Z","published":"2024-08-29T03:23:51Z","title":"LMT-GP: Combined Latent Mean-Teacher and Gaussian Process for\n  Semi-supervised Low-light Image Enhancement","summary":"  While recent low-light image enhancement (LLIE) methods have made significant\nadvancements, they still face challenges in terms of low visual quality and\nweak generalization ability when applied to complex scenarios. To address these\nissues, we propose a semi-supervised method based on latent mean-teacher and\nGaussian process, named LMT-GP. We first design a latent mean-teacher framework\nthat integrates both labeled and unlabeled data, as well as their latent\nvectors, into model training. Meanwhile, we use a mean-teacher-assisted\nGaussian process learning strategy to establish a connection between the latent\nand pseudo-latent vectors obtained from the labeled and unlabeled data. To\nguide the learning process, we utilize an assisted Gaussian process regression\n(GPR) loss function. Furthermore, we design a pseudo-label adaptation module\n(PAM) to ensure the reliability of the network learning. To demonstrate our\nmethod's generalization ability and effectiveness, we apply it to multiple LLIE\ndatasets and high-level vision tasks. Experiment results demonstrate that our\nmethod achieves high generalization performance and image quality. The code is\navailable at https://github.com/HFUT-CV/LMT-GP.\n","authors":["Ye Yu","Fengxin Chen","Jun Yu","Zhen Kan"],"pdf_url":"https://arxiv.org/pdf/2408.16235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16233v1","updated":"2024-08-29T03:20:43Z","published":"2024-08-29T03:20:43Z","title":"PSE-Net: Channel Pruning for Convolutional Neural Networks with\n  Parallel-subnets Estimator","summary":"  Channel Pruning is one of the most widespread techniques used to compress\ndeep neural networks while maintaining their performances. Currently, a typical\npruning algorithm leverages neural architecture search to directly find\nnetworks with a configurable width, the key step of which is to identify\nrepresentative subnet for various pruning ratios by training a supernet.\nHowever, current methods mainly follow a serial training strategy to optimize\nsupernet, which is very time-consuming. In this work, we introduce PSE-Net, a\nnovel parallel-subnets estimator for efficient channel pruning. Specifically,\nwe propose a parallel-subnets training algorithm that simulate the\nforward-backward pass of multiple subnets by droping extraneous features on\nbatch dimension, thus various subnets could be trained in one round. Our\nproposed algorithm facilitates the efficiency of supernet training and equips\nthe network with the ability to interpolate the accuracy of unsampled subnets,\nenabling PSE-Net to effectively evaluate and rank the subnets. Over the trained\nsupernet, we develop a prior-distributed-based sampling algorithm to boost the\nperformance of classical evolutionary search. Such algorithm utilizes the prior\ninformation of supernet training phase to assist in the search of optimal\nsubnets while tackling the challenge of discovering samples that satisfy\nresource constraints due to the long-tail distribution of network\nconfiguration. Extensive experiments demonstrate PSE-Net outperforms previous\nstate-of-the-art channel pruning methods on the ImageNet dataset while\nretaining superior supernet training efficiency. For example, under 300M FLOPs\nconstraint, our pruned MobileNetV2 achieves 75.2% Top-1 accuracy on ImageNet\ndataset, exceeding the original MobileNetV2 by 2.6 units while only cost\n30%/16% times than BCNet/AutoAlim.\n","authors":["Shiguang Wang","Tao Xie","Haijun Liu","Xingcheng Zhang","Jian Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.16233v1.pdf","comment":"10pages, Neural Networks"},{"id":"http://arxiv.org/abs/2408.16232v1","updated":"2024-08-29T03:12:04Z","published":"2024-08-29T03:12:04Z","title":"Enhancing Conditional Image Generation with Explainable Latent Space\n  Manipulation","summary":"  In the realm of image synthesis, achieving fidelity to a reference image\nwhile adhering to conditional prompts remains a significant challenge. This\npaper proposes a novel approach that integrates a diffusion model with latent\nspace manipulation and gradient-based selective attention mechanisms to address\nthis issue. Leveraging Grad-SAM (Gradient-based Selective Attention\nManipulation), we analyze the cross attention maps of the cross attention\nlayers and gradients for the denoised latent vector, deriving importance scores\nof elements of denoised latent vector related to the subject of interest. Using\nthis information, we create masks at specific timesteps during denoising to\npreserve subjects while seamlessly integrating the reference image features.\nThis approach ensures the faithful formation of subjects based on conditional\nprompts, while concurrently refining the background for a more coherent\ncomposition. Our experiments on places365 dataset demonstrate promising\nresults, with our proposed model achieving the lowest mean and median Frechet\nInception Distance (FID) scores compared to baseline models, indicating\nsuperior fidelity preservation. Furthermore, our model exhibits competitive\nperformance in aligning the generated images with provided textual\ndescriptions, as evidenced by high CLIP scores. These results highlight the\neffectiveness of our approach in both fidelity preservation and textual context\npreservation, offering a significant advancement in text-to-image synthesis\ntasks.\n","authors":["Kshitij Pathania"],"pdf_url":"https://arxiv.org/pdf/2408.16232v1.pdf","comment":"7 pages , 5 figures"},{"id":"http://arxiv.org/abs/2311.13385v4","updated":"2024-08-29T03:11:14Z","published":"2023-11-22T13:27:36Z","title":"SegVol: Universal and Interactive Volumetric Medical Image Segmentation","summary":"  Precise image segmentation provides clinical study with instructive\ninformation. Despite the remarkable progress achieved in medical image\nsegmentation, there is still an absence of a 3D foundation segmentation model\nthat can segment a wide range of anatomical categories with easy user\ninteraction. In this paper, we propose a 3D foundation segmentation model,\nnamed SegVol, supporting universal and interactive volumetric medical image\nsegmentation. By scaling up training data to 90K unlabeled Computed Tomography\n(CT) volumes and 6K labeled CT volumes, this foundation model supports the\nsegmentation of over 200 anatomical categories using semantic and spatial\nprompts. To facilitate efficient and precise inference on volumetric images, we\ndesign a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical\nsegmentation tasks verify that SegVol outperforms the competitors in 19 tasks,\nwith improvements up to 37.24% compared to the runner-up methods. We\ndemonstrate the effectiveness and importance of specific designs by ablation\nstudy. We expect this foundation model can promote the development of\nvolumetric medical image analysis. The model and code are publicly available\nat: https://github.com/BAAI-DCAI/SegVol.\n","authors":["Yuxin Du","Fan Bai","Tiejun Huang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.13385v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02139v3","updated":"2024-08-29T03:09:40Z","published":"2023-12-04T18:57:01Z","title":"DiffiT: Diffusion Vision Transformers for Image Generation","summary":"  Diffusion models with their powerful expressivity and high sample quality\nhave achieved State-Of-The-Art (SOTA) performance in the generative domain. The\npioneering Vision Transformer (ViT) has also demonstrated strong modeling\ncapabilities and scalability, especially for recognition tasks. In this paper,\nwe study the effectiveness of ViTs in diffusion-based generative learning and\npropose a new model denoted as Diffusion Vision Transformers (DiffiT).\nSpecifically, we propose a methodology for finegrained control of the denoising\nprocess and introduce the Time-dependant Multihead Self Attention (TMSA)\nmechanism. DiffiT is surprisingly effective in generating high-fidelity images\nwith significantly better parameter efficiency. We also propose latent and\nimage space DiffiT models and show SOTA performance on a variety of\nclass-conditional and unconditional synthesis tasks at different resolutions.\nThe Latent DiffiT model achieves a new SOTA FID score of 1.73 on ImageNet256\ndataset while having 19.85%, 16.88% less parameters than other\nTransformer-based diffusion models such as MDT and DiT,respectively. Code:\nhttps://github.com/NVlabs/DiffiT\n","authors":["Ali Hatamizadeh","Jiaming Song","Guilin Liu","Jan Kautz","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2312.02139v3.pdf","comment":"Accepted to ECCV'24"},{"id":"http://arxiv.org/abs/2408.16227v1","updated":"2024-08-29T02:58:35Z","published":"2024-08-29T02:58:35Z","title":"Revisiting 360 Depth Estimation with PanoGabor: A New Fusion Perspective","summary":"  Depth estimation from a monocular 360 image is important to the perception of\nthe entire 3D environment. However, the inherent distortion and large field of\nview (FoV) in 360 images pose great challenges for this task. To this end,\nexisting mainstream solutions typically introduce additional perspective-based\n360 representations (\\textit{e.g.}, Cubemap) to achieve effective feature\nextraction. Nevertheless, regardless of the introduced representations, they\neventually need to be unified into the equirectangular projection (ERP) format\nfor the subsequent depth estimation, which inevitably reintroduces the\ntroublesome distortions. In this work, we propose an oriented distortion-aware\nGabor Fusion framework (PGFuse) to address the above challenges. First, we\nintroduce Gabor filters that analyze texture in the frequency domain, thereby\nextending the receptive fields and enhancing depth cues. To address the\nreintroduced distortions, we design a linear latitude-aware distortion\nrepresentation method to generate customized, distortion-aware Gabor filters\n(PanoGabor filters). Furthermore, we design a channel-wise and spatial-wise\nunidirectional fusion module (CS-UFM) that integrates the proposed PanoGabor\nfilters to unify other representations into the ERP format, delivering\neffective and distortion-free features. Considering the orientation sensitivity\nof the Gabor transform, we introduce a spherical gradient constraint to\nstabilize this sensitivity. Experimental results on three popular indoor 360\nbenchmarks demonstrate the superiority of the proposed PGFuse to existing\nstate-of-the-art solutions. Code can be available upon acceptance.\n","authors":["Zhijie Shen","Chunyu Lin","Lang Nie","Kang Liao"],"pdf_url":"https://arxiv.org/pdf/2408.16227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13520v2","updated":"2024-08-29T02:52:46Z","published":"2024-07-18T13:55:54Z","title":"EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian\n  Splatting","summary":"  3D deblurring reconstruction techniques have recently seen significant\nadvancements with the development of Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). Although these techniques can recover relatively\nclear 3D reconstructions from blurry image inputs, they still face limitations\nin handling severe blurring and complex camera motion. To address these issues,\nwe propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting\n(EaDeblur-GS), which integrates event camera data to enhance the robustness of\n3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)\nnetwork to estimate Gaussian center deviations and using novel loss functions,\nEaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating\nperformance comparable to state-of-the-art methods.\n","authors":["Yuchen Weng","Zhengwen Shen","Ruofan Chen","Qi Wang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16224v1","updated":"2024-08-29T02:43:20Z","published":"2024-08-29T02:43:20Z","title":"LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in\n  Vision-Language Models","summary":"  Recent advances in large vision-language models (VLMs) typically employ\nvision encoders based on the Vision Transformer (ViT) architecture. The\ndivision of the images into patches by ViT results in a fragmented perception,\nthereby hindering the visual understanding capabilities of VLMs. In this paper,\nwe propose an innovative enhancement to address this limitation by introducing\na Scene Graph Expression (SGE) module in VLMs. This module extracts and\nstructurally expresses the complex semantic information within images, thereby\nimproving the foundational perception and understanding abilities of VLMs.\nExtensive experiments demonstrate that integrating our SGE module significantly\nenhances the VLM's performance in vision-language tasks, indicating its\neffectiveness in preserving intricate semantic details and facilitating better\nvisual understanding. Code and data would be available.\n","authors":["Jingyi Wang","Jianzhong Ju","Jian Luan","Zhidong Deng"],"pdf_url":"https://arxiv.org/pdf/2408.16224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10121v3","updated":"2024-08-29T02:35:21Z","published":"2023-09-18T19:49:22Z","title":"Pre-training on Synthetic Driving Data for Trajectory Prediction","summary":"  Accumulating substantial volumes of real-world driving data proves pivotal in\nthe realm of trajectory forecasting for autonomous driving. Given the heavy\nreliance of current trajectory forecasting models on data-driven methodologies,\nwe aim to tackle the challenge of learning general trajectory forecasting\nrepresentations under limited data availability. We propose a pipeline-level\nsolution to mitigate the issue of data scarcity in trajectory forecasting. The\nsolution is composed of two parts: firstly, we adopt HD map augmentation and\ntrajectory synthesis for generating driving data, and then we learn\nrepresentations by pre-training on them. Specifically, we apply vector\ntransformations to reshape the maps, and then employ a rule-based model to\ngenerate trajectories on both original and augmented scenes; thus enlarging the\ndriving data without collecting additional real ones. To foster the learning of\ngeneral representations within this augmented dataset, we comprehensively\nexplore the different pre-training strategies, including extending the concept\nof a Masked AutoEncoder (MAE) for trajectory forecasting. Without bells and\nwhistles, our proposed pipeline-level solution is general, simple, yet\neffective: we conduct extensive experiments to demonstrate the effectiveness of\nour data expansion and pre-training strategies, which outperform the baseline\nprediction model by large margins, e.g. 5.04%, 3.84% and 8.30% in terms of\n$MR_6$, $minADE_6$ and $minFDE_6$. The pre-training dataset and the codes for\npre-training and fine-tuning are released at\nhttps://github.com/yhli123/Pretraining_on_Synthetic_Driving_Data_for_Trajectory_Prediction.\n","authors":["Yiheng Li","Seth Z. Zhao","Chenfeng Xu","Chen Tang","Chenran Li","Mingyu Ding","Masayoshi Tomizuka","Wei Zhan"],"pdf_url":"https://arxiv.org/pdf/2309.10121v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16219v1","updated":"2024-08-29T02:25:12Z","published":"2024-08-29T02:25:12Z","title":"Training-free Video Temporal Grounding using Large-scale Pre-trained\n  Models","summary":"  Video temporal grounding aims to identify video segments within untrimmed\nvideos that are most relevant to a given natural language query. Existing video\ntemporal localization models rely on specific datasets for training and have\nhigh data collection costs, but they exhibit poor generalization capability\nunder the across-dataset and out-of-distribution (OOD) settings. In this paper,\nwe propose a Training-Free Video Temporal Grounding (TFVTG) approach that\nleverages the ability of pre-trained large models. A naive baseline is to\nenumerate proposals in the video and use the pre-trained visual language models\n(VLMs) to select the best proposal according to the vision-language alignment.\nHowever, most existing VLMs are trained on image-text pairs or trimmed video\nclip-text pairs, making it struggle to (1) grasp the relationship and\ndistinguish the temporal boundaries of multiple events within the same video;\n(2) comprehend and be sensitive to the dynamic transition of events (the\ntransition from one event to another) in the video. To address these issues, we\npropose leveraging large language models (LLMs) to analyze multiple sub-events\ncontained in the query text and analyze the temporal order and relationships\nbetween these events. Secondly, we split a sub-event into dynamic transition\nand static status parts and propose the dynamic and static scoring functions\nusing VLMs to better evaluate the relevance between the event and the\ndescription. Finally, for each sub-event description, we use VLMs to locate the\ntop-k proposals and leverage the order and relationships between sub-events\nprovided by LLMs to filter and integrate these proposals. Our method achieves\nthe best performance on zero-shot video temporal grounding on Charades-STA and\nActivityNet Captions datasets without any training and demonstrates better\ngeneralization capabilities in cross-dataset and OOD settings.\n","authors":["Minghang Zheng","Xinhao Cai","Qingchao Chen","Yuxin Peng","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.16219v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2305.18680v2","updated":"2024-08-29T02:22:08Z","published":"2023-05-30T01:38:54Z","title":"Improving Deep Representation Learning via Auxiliary Learnable Target\n  Coding","summary":"  Deep representation learning is a subfield of machine learning that focuses\non learning meaningful and useful representations of data through deep neural\nnetworks. However, existing methods for semantic classification typically\nemploy pre-defined target codes such as the one-hot and the Hadamard codes,\nwhich can either fail or be less flexible to model inter-class correlation. In\nlight of this, this paper introduces a novel learnable target coding as an\nauxiliary regularization of deep representation learning, which can not only\nincorporate latent dependency across classes but also impose geometric\nproperties of target codes into representation space. Specifically, a\nmargin-based triplet loss and a correlation consistency loss on the proposed\ntarget codes are designed to encourage more discriminative representations\nowing to enlarging between-class margins in representation space and favoring\nequal semantic correlation of learnable target codes respectively. Experimental\nresults on several popular visual classification and retrieval benchmarks can\ndemonstrate the effectiveness of our method on improving representation\nlearning, especially for imbalanced data. Source codes are made publicly\navailable at\n\\href{https://github.com/AkonLau/LTC}{https://github.com/AkonLau/LTC}.\n","authors":["Kangjun Liu","Ke Chen","Kui Jia","Yaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2305.18680v2.pdf","comment":"Accepted by Pattern Recognition, 33 pages, 8 figures, 11 tables"},{"id":"http://arxiv.org/abs/2408.15829v2","updated":"2024-08-29T02:16:02Z","published":"2024-08-28T14:44:42Z","title":"SITransformer: Shared Information-Guided Transformer for Extreme\n  Multimodal Summarization","summary":"  Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes an\nattractive summarization approach by integrating various types of information\nto create extremely concise yet informative summaries for individual\nmodalities. Existing methods overlook the issue that multimodal data often\ncontains more topic irrelevant information, which can mislead the model into\nproducing inaccurate summaries especially for extremely short ones. In this\npaper, we propose SITransformer, a Shared Information-guided Transformer for\nextreme multimodal summarization. It has a shared information guided pipeline\nwhich involves a cross-modal shared information extractor and a cross-modal\ninteraction module. The extractor formulates semantically shared salient\ninformation from different modalities by devising a novel filtering process\nconsisting of a differentiable top-k selector and a shared-information guided\ngating unit. As a result, the common, salient, and relevant contents across\nmodalities are identified. Next, a transformer with cross-modal attentions is\ndeveloped for intra- and inter-modality learning with the shared information\nguidance to produce the extreme summary. Comprehensive experiments demonstrate\nthat SITransformer significantly enhances the summarization quality for both\nvideo and text summaries for XMSMO. Our code will be publicly available at\nhttps://github.com/SichengLeoLiu/MMAsia24-XMSMO.\n","authors":["Sicheng Liu","Lintao Wang","Xiaogan Zhu","Xuequan Lu","Zhiyong Wang","Kun Hu"],"pdf_url":"https://arxiv.org/pdf/2408.15829v2.pdf","comment":"8 pages, 5 figures, submitted to ACM Multimedia Asia 2024"},{"id":"http://arxiv.org/abs/2403.11541v2","updated":"2024-08-29T02:13:09Z","published":"2024-03-18T07:51:22Z","title":"Hierarchical Spatial Proximity Reasoning for Vision-and-Language\n  Navigation","summary":"  Most Vision-and-Language Navigation (VLN) algorithms are prone to making\ndecision due to a lack of visual common sense and insufficient reasoning\ncapabilities. To address this issue, we propose a Hierarchical Spatial\nProximity Reasoning (HSPR) method. First, we introduce a scene understanding\nauxiliary task to help the agent build a knowledge base of hierarchical spatial\nproximity. This task utilizes panoramic views and object features to identify\ntypes of nodes and uncover the adjacency relationships between nodes, objects,\nand between nodes and objects. Second, we propose a multi-step reasoning\nnavigation algorithm based on hierarchical spatial proximity knowledge base,\nwhich continuously plans feasible paths to enhance exploration efficiency.\nThird, we introduce a residual fusion method to improve navigation decision\naccuracy. Finally, we validate our approach with experiments on publicly\navailable datasets including REVERIE, SOON, R2R, and R4R. Our code is available\nat https://github.com/iCityLab/HSPR.\n","authors":["Ming Xu","Zilong Xie"],"pdf_url":"https://arxiv.org/pdf/2403.11541v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16213v1","updated":"2024-08-29T02:12:58Z","published":"2024-08-29T02:12:58Z","title":"M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language\n  Models for Chest X-ray Interpretation","summary":"  The rapid evolution of artificial intelligence, especially in large language\nmodels (LLMs), has significantly impacted various domains, including\nhealthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,\nbut with limitations: either underutilizing the multi-tasking capabilities of\nLLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM\ndesigned to enhance CXR interpretation. The model is trained on a visual\ninstruction-following dataset that integrates various task-specific datasets in\na conversational format. As a result, the model supports multiple tasks such as\nmedical report generation (MRG), visual grounding, and visual question\nanswering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by\nemploying a chain-of-thought prompting strategy, in which it identifies\nfindings in CXR images and subsequently generates corresponding reports. The\nmodel is adaptable to various MRG scenarios depending on the available inputs,\nsuch as single-image, multi-image, and multi-study contexts. In addition to\nMRG, M4CXR performs visual grounding at a level comparable to specialized\nmodels and also demonstrates outstanding performance in VQA. Both quantitative\nand qualitative assessments reveal M4CXR's versatility in MRG, visual\ngrounding, and VQA, while consistently maintaining clinical accuracy.\n","authors":["Jonggwon Park","Soobum Kim","Byungmu Yoon","Jihun Hyun","Kyoyun Choi"],"pdf_url":"https://arxiv.org/pdf/2408.16213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10577v2","updated":"2024-08-29T02:09:11Z","published":"2024-05-17T07:04:29Z","title":"DuoSpaceNet: Leveraging Both Bird's-Eye-View and Perspective View\n  Representations for 3D Object Detection","summary":"  Recent advances in multi-view camera-only 3D object detection either rely on\nan accurate reconstruction of bird's-eye-view (BEV) 3D features or on\ntraditional 2D perspective view (PV) image features. While both have their own\npros and cons, few have found a way to stitch them together in order to benefit\nfrom \"the best of both worlds\". To this end, we explore a duo space (i.e., BEV\nand PV) 3D perception framework, in conjunction with some useful duo space\nfusion strategies that allow effective aggregation of the two feature\nrepresentations. To the best of our knowledge, our proposed method,\nDuoSpaceNet, is the first to leverage two distinct feature spaces and achieves\nthe state-of-the-art 3D object detection and BEV map segmentation results on\nnuScenes dataset.\n","authors":["Zhe Huang","Yizhe Zhao","Hao Xiao","Chenyan Wu","Lingting Ge"],"pdf_url":"https://arxiv.org/pdf/2405.10577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18038v2","updated":"2024-08-29T02:05:04Z","published":"2024-07-25T13:31:55Z","title":"TiCoSS: Tightening the Coupling between Semantic Segmentation and Stereo\n  Matching within A Joint Learning Framework","summary":"  Semantic segmentation and stereo matching, respectively analogous to the\nventral and dorsal streams in our human brain, are two key components of\nautonomous driving perception systems. Addressing these two tasks with separate\nnetworks is no longer the mainstream direction in developing computer vision\nalgorithms, particularly with the recent advances in large vision models and\nembodied artificial intelligence. The trend is shifting towards combining them\nwithin a joint learning framework, especially emphasizing feature sharing\nbetween the two tasks. The major contributions of this study lie in\ncomprehensively tightening the coupling between semantic segmentation and\nstereo matching. Specifically, this study introduces three novelties: (1) a\ntightly coupled, gated feature fusion strategy, (2) a hierarchical deep\nsupervision strategy, and (3) a coupling tightening loss function. The combined\nuse of these technical contributions results in TiCoSS, a state-of-the-art\njoint learning framework that simultaneously tackles semantic segmentation and\nstereo matching. Through extensive experiments on the KITTI and vKITTI2\ndatasets, along with qualitative and quantitative analyses, we validate the\neffectiveness of our developed strategies and loss function, and demonstrate\nits superior performance compared to prior arts, with a notable increase in\nmIoU by over 9%. Our source code will be publicly available at\nmias.group/TiCoSS upon publication.\n","authors":["Guanfeng Tang","Zhiyuan Wu","Jiahang Li","Ping Zhong","Xieyuanli Chen","Huiming Liu","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2407.18038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16201v1","updated":"2024-08-29T01:46:37Z","published":"2024-08-29T01:46:37Z","title":"Uni-3DAD: GAN-Inversion Aided Universal 3D Anomaly Detection on\n  Model-free Products","summary":"  Anomaly detection is a long-standing challenge in manufacturing systems.\nTraditionally, anomaly detection has relied on human inspectors. However, 3D\npoint clouds have gained attention due to their robustness to environmental\nfactors and their ability to represent geometric data. Existing 3D anomaly\ndetection methods generally fall into two categories. One compares scanned 3D\npoint clouds with design files, assuming these files are always available.\nHowever, such assumptions are often violated in many real-world applications\nwhere model-free products exist, such as fresh produce (i.e., ``Cookie\",\n``Potato\", etc.), dentures, bone, etc. The other category compares patches of\nscanned 3D point clouds with a library of normal patches named memory bank.\nHowever, those methods usually fail to detect incomplete shapes, which is a\nfairly common defect type (i.e., missing pieces of different products). The\nmain challenge is that missing areas in 3D point clouds represent the absence\nof scanned points. This makes it infeasible to compare the missing region with\nexisting point cloud patches in the memory bank. To address these two\nchallenges, we proposed a unified, unsupervised 3D anomaly detection framework\ncapable of identifying all types of defects on model-free products. Our method\nintegrates two detection modules: a feature-based detection module and a\nreconstruction-based detection module. Feature-based detection covers geometric\ndefects, such as dents, holes, and cracks, while the reconstruction-based\nmethod detects missing regions. Additionally, we employ a One-class Support\nVector Machine (OCSVM) to fuse the detection results from both modules. The\nresults demonstrate that (1) our proposed method outperforms the\nstate-of-the-art methods in identifying incomplete shapes and (2) it still\nmaintains comparable performance with the SOTA methods in detecting all other\ntypes of anomalies.\n","authors":["Jiayu Liu","Shancong Mou","Nathan Gaw","Yinan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16268v2","updated":"2024-08-29T01:44:27Z","published":"2023-12-26T12:16:03Z","title":"360 Layout Estimation via Orthogonal Planes Disentanglement and\n  Multi-view Geometric Consistency Perception","summary":"  Existing panoramic layout estimation solutions tend to recover room\nboundaries from a vertically compressed sequence, yielding imprecise results as\nthe compression process often muddles the semantics between various planes.\nBesides, these data-driven approaches impose an urgent demand for massive data\nannotations, which are laborious and time-consuming. For the first problem, we\npropose an orthogonal plane disentanglement network (termed DOPNet) to\ndistinguish ambiguous semantics. DOPNet consists of three modules that are\nintegrated to deliver distortion-free, semantics-clean, and detail-sharp\ndisentangled representations, which benefit the subsequent layout recovery. For\nthe second problem, we present an unsupervised adaptation technique tailored\nfor horizon-depth and ratio representations. Concretely, we introduce an\noptimization strategy for decision-level layout analysis and a 1D cost volume\nconstruction method for feature-level multi-view aggregation, both of which are\ndesigned to fully exploit the geometric consistency across multiple\nperspectives. The optimizer provides a reliable set of pseudo-labels for\nnetwork training, while the 1D cost volume enriches each view with\ncomprehensive scene information derived from other perspectives. Extensive\nexperiments demonstrate that our solution outperforms other SoTA models on both\nmonocular layout estimation and multi-view layout estimation tasks. Cobe can be\navailable at https://github.com/zhijieshen-bjtu/MV-DOPNet.\n","authors":["Zhijie Shen","Chunyu Lin","Junsong Zhang","Lang Nie","Kang Liao","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.16268v2.pdf","comment":"Accept to TPAMI2024. arXiv admin note: substantial text overlap with\n  arXiv:2303.00971"},{"id":"http://arxiv.org/abs/2408.16200v1","updated":"2024-08-29T01:42:38Z","published":"2024-08-29T01:42:38Z","title":"PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object\n  Detection in Bird's-Eye-View","summary":"  Recently, LSS-based multi-view 3D object detection provides an economical and\ndeployment-friendly solution for autonomous driving. However, all the existing\nLSS-based methods transform multi-view image features into a Cartesian\nBird's-Eye-View(BEV) representation, which does not take into account the\nnon-uniform image information distribution and hardly exploits the view\nsymmetry. In this paper, in order to adapt the image information distribution\nand preserve the view symmetry by regular convolution, we propose to employ the\npolar BEV representation to substitute the Cartesian BEV representation. To\nachieve this, we elaborately tailor three modules: a polar view transformer to\ngenerate the polar BEV representation, a polar temporal fusion module for\nfusing historical polar BEV features and a polar detection head to predict the\npolar-parameterized representation of the object. In addition, we design a 2D\nauxiliary detection head and a spatial attention enhancement module to improve\nthe quality of feature extraction in perspective view and BEV, respectively.\nFinally, we integrate the above improvements into a novel multi-view 3D object\ndetector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves\nthe superior performance. The code is available at\nhttps://github.com/Yzichen/PolarBEVDet.git.\n","authors":["Zichen Yu","Quanli Liu","Wei Wang","Liyong Zhang","Xiaoguang Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.16200v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.01565v2","updated":"2024-08-29T01:32:17Z","published":"2024-08-02T20:40:19Z","title":"Embodiment: Self-Supervised Depth Estimation Based on Camera Models","summary":"  Depth estimation is a critical topic for robotics and vision-related tasks.\nIn monocular depth estimation, in comparison with supervised learning that\nrequires expensive ground truth labeling, self-supervised methods possess great\npotential due to no labeling cost. However, self-supervised learning still has\na large gap with supervised learning in 3D reconstruction and depth estimation\nperformance. Meanwhile, scaling is also a major issue for monocular\nunsupervised depth estimation, which commonly still needs ground truth scale\nfrom GPS, LiDAR, or existing maps to correct. In the era of deep learning,\nexisting methods primarily rely on exploring image relationships to train\nunsupervised neural networks, while the physical properties of the camera\nitself such as intrinsics and extrinsics are often overlooked. These physical\nproperties are not just mathematical parameters; they are embodiments of the\ncamera's interaction with the physical world. By embedding these physical\nproperties into the deep learning model, we can calculate depth priors for\nground regions and regions connected to the ground based on physical\nprinciples, providing free supervision signals without the need for additional\nsensors. This approach is not only easy to implement but also enhances the\neffects of all unsupervised methods by embedding the camera's physical\nproperties into the model, thereby achieving an embodied understanding of the\nreal world.\n","authors":["Jinchang Zhang","Praveen Kumar Reddy","Xue-Iuan Wong","Yiannis Aloimonos","Guoyu Lu"],"pdf_url":"https://arxiv.org/pdf/2408.01565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16195v1","updated":"2024-08-29T01:25:36Z","published":"2024-08-29T01:25:36Z","title":"DLM-VMTL:A Double Layer Mapper for heterogeneous data video Multi-task\n  prompt learning","summary":"  In recent years, the parameters of backbones of Video Understanding tasks\ncontinue to increase and even reach billion-level. Whether fine-tuning a\nspecific task on the Video Foundation Model or pre-training the model designed\nfor the specific task, incurs a lot of overhead. How to make these models play\nother values than their own tasks becomes a worthy question. Multi-Task\nLearning(MTL) makes the visual task acquire the rich shareable knowledge from\nother tasks while joint training. It is fully explored in Image Recognition\ntasks especially dense predict tasks. Nevertheless, it is rarely used in video\ndomain due to the lack of multi-labels video data. In this paper, a\nheterogenous data video multi-task prompt learning (VMTL) method is proposed to\naddress above problem. It's different from it in image domain, a Double-Layers\nMapper(DLM) is proposed to extract the shareable knowledge into visual promptS\nand align it with representation of primary task. Extensive experiments prove\nthat our DLM-VMTL performs better than baselines on 6 different video\nunderstanding tasks and 11 datasets.\n","authors":["Zeyi Bo","Wuxi Sun","Ye Jin"],"pdf_url":"https://arxiv.org/pdf/2408.16195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16190v1","updated":"2024-08-29T01:06:51Z","published":"2024-08-29T01:06:51Z","title":"Estimating Dynamic Flow Features in Groups of Tracked Objects","summary":"  Interpreting motion captured in image sequences is crucial for a wide range\nof computer vision applications. Typical estimation approaches include optical\nflow (OF), which approximates the apparent motion instantaneously in a scene,\nand multiple object tracking (MOT), which tracks the motion of subjects over\ntime. Often, the motion of objects in a scene is governed by some underlying\ndynamical system which could be inferred by analyzing the motion of groups of\nobjects. Standard motion analyses, however, are not designed to intuit flow\ndynamics from trajectory data, making such measurements difficult in practice.\nThe goal of this work is to extend gradient-based dynamical systems analyses to\nreal-world applications characterized by complex, feature-rich image sequences\nwith imperfect tracers. The tracer trajectories are tracked using deep vision\nnetworks and gradients are approximated using Lagrangian gradient regression\n(LGR), a tool designed to estimate spatial gradients from sparse data. From\ngradients, dynamical features such as regions of coherent rotation and\ntransport barriers are identified. The proposed approach is affordably\nimplemented and enables advanced studies including the motion analysis of two\ndistinct object classes in a single image sequence. Two examples of the method\nare presented on data sets for which standard gradient-based analyses do not\napply.\n","authors":["Tanner D. Harms","Steven L. Brunton","Beverley J. McKeon"],"pdf_url":"https://arxiv.org/pdf/2408.16190v1.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.16930v1","updated":"2024-08-29T22:13:29Z","published":"2024-08-29T22:13:29Z","title":"VLM-KD: Knowledge Distillation from VLM for Long-Tail Visual Recognition","summary":"  For visual recognition, knowledge distillation typically involves\ntransferring knowledge from a large, well-trained teacher model to a smaller\nstudent model. In this paper, we introduce an effective method to distill\nknowledge from an off-the-shelf vision-language model (VLM), demonstrating that\nit provides novel supervision in addition to those from a conventional\nvision-only teacher model. Our key technical contribution is the development of\na framework that generates novel text supervision and distills free-form text\ninto a vision encoder. We showcase the effectiveness of our approach, termed\nVLM-KD, across various benchmark datasets, showing that it surpasses several\nstate-of-the-art long-tail visual classifiers. To our knowledge, this work is\nthe first to utilize knowledge distillation with text supervision generated by\nan off-the-shelf VLM and apply it to vanilla randomly initialized vision\nencoders.\n","authors":["Zaiwei Zhang","Gregory P. Meyer","Zhichao Lu","Ashish Shrivastava","Avinash Ravichandran","Eric M. Wolff"],"pdf_url":"https://arxiv.org/pdf/2408.16930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16924v1","updated":"2024-08-29T21:53:01Z","published":"2024-08-29T21:53:01Z","title":"Enhancing Autism Spectrum Disorder Early Detection with the Parent-Child\n  Dyads Block-Play Protocol and an Attention-enhanced GCN-xLSTM Hybrid Deep\n  Learning Framework","summary":"  Autism Spectrum Disorder (ASD) is a rapidly growing neurodevelopmental\ndisorder. Performing a timely intervention is crucial for the growth of young\nchildren with ASD, but traditional clinical screening methods lack objectivity.\nThis study introduces an innovative approach to early detection of ASD. The\ncontributions are threefold. First, this work proposes a novel Parent-Child\nDyads Block-Play (PCB) protocol, grounded in kinesiological and neuroscientific\nresearch, to identify behavioral patterns distinguishing ASD from typically\ndeveloping (TD) toddlers. Second, we have compiled a substantial video dataset,\nfeaturing 40 ASD and 89 TD toddlers engaged in block play with parents. This\ndataset exceeds previous efforts on both the scale of participants and the\nlength of individual sessions. Third, our approach to action analysis in videos\nemploys a hybrid deep learning framework, integrating a two-stream graph\nconvolution network with attention-enhanced xLSTM (2sGCN-AxLSTM). This\nframework is adept at capturing dynamic interactions between toddlers and\nparents by extracting spatial features correlated with upper body and head\nmovements and focusing on global contextual information of action sequences\nover time. By learning these global features with spatio-temporal correlations,\nour 2sGCN-AxLSTM effectively analyzes dynamic human behavior patterns and\ndemonstrates an unprecedented accuracy of 89.6\\% in early detection of ASD. Our\napproach shows strong potential for enhancing early ASD diagnosis by accurately\nanalyzing parent-child interactions, providing a critical tool to support\ntimely and informed clinical decision-making.\n","authors":["Xiang Li","Lizhou Fan","Hanbo Wu","Kunping Chen","Xiaoxiao Yu","Chao Che","Zhifeng Cai","Xiuhong Niu","Aihua Cao","Xin Ma"],"pdf_url":"https://arxiv.org/pdf/2408.16924v1.pdf","comment":"18 pages, 8 figures, and 4 tables"},{"id":"http://arxiv.org/abs/2408.16907v1","updated":"2024-08-29T21:08:07Z","published":"2024-08-29T21:08:07Z","title":"Ig3D: Integrating 3D Face Representations in Facial Expression Inference","summary":"  Reconstructing 3D faces with facial geometry from single images has allowed\nfor major advances in animation, generative models, and virtual reality.\nHowever, this ability to represent faces with their 3D features is not as fully\nexplored by the facial expression inference (FEI) community. This study\ntherefore aims to investigate the impacts of integrating such 3D\nrepresentations into the FEI task, specifically for facial expression\nclassification and face-based valence-arousal (VA) estimation. To accomplish\nthis, we first assess the performance of two 3D face representations (both\nbased on the 3D morphable model, FLAME) for the FEI tasks. We further explore\ntwo fusion architectures, intermediate fusion and late fusion, for integrating\nthe 3D face representations with existing 2D inference frameworks. To evaluate\nour proposed architecture, we extract the corresponding 3D representations and\nperform extensive tests on the AffectNet and RAF-DB datasets. Our experimental\nresults demonstrate that our proposed method outperforms the state-of-the-art\nAffectNet VA estimation and RAF-DB classification tasks. Moreover, our method\ncan act as a complement to other existing methods to boost performance in many\nemotion inference tasks.\n","authors":["Lu Dong","Xiao Wang","Srirangaraj Setlur","Venu Govindaraju","Ifeoma Nwogu"],"pdf_url":"https://arxiv.org/pdf/2408.16907v1.pdf","comment":"Accepted by ECCVW 2024"},{"id":"http://arxiv.org/abs/2408.16892v1","updated":"2024-08-29T20:26:27Z","published":"2024-08-29T20:26:27Z","title":"Tex-ViT: A Generalizable, Robust, Texture-based dual-branch\n  cross-attention deepfake detector","summary":"  Deepfakes, which employ GAN to produce highly realistic facial modification,\nare widely regarded as the prevailing method. Traditional CNN have been able to\nidentify bogus media, but they struggle to perform well on different datasets\nand are vulnerable to adversarial attacks due to their lack of robustness.\nVision transformers have demonstrated potential in the realm of image\nclassification problems, but they require enough training data. Motivated by\nthese limitations, this publication introduces Tex-ViT (Texture-Vision\nTransformer), which enhances CNN features by combining ResNet with a vision\ntransformer. The model combines traditional ResNet features with a texture\nmodule that operates in parallel on sections of ResNet before each\ndown-sampling operation. The texture module then serves as an input to the dual\nbranch of the cross-attention vision transformer. It specifically focuses on\nimproving the global texture module, which extracts feature map correlation.\nEmpirical analysis reveals that fake images exhibit smooth textures that do not\nremain consistent over long distances in manipulations. Experiments were\nperformed on different categories of FF++, such as DF, f2f, FS, and NT,\ntogether with other types of GAN datasets in cross-domain scenarios.\nFurthermore, experiments also conducted on FF++, DFDCPreview, and Celeb-DF\ndataset underwent several post-processing situations, such as blurring,\ncompression, and noise. The model surpassed the most advanced models in terms\nof generalization, achieving a 98% accuracy in cross-domain scenarios. This\ndemonstrates its ability to learn the shared distinguishing textural\ncharacteristics in the manipulated samples. These experiments provide evidence\nthat the proposed model is capable of being applied to various situations and\nis resistant to many post-processing procedures.\n","authors":["Deepak Dagar","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2408.16892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16886v1","updated":"2024-08-29T20:19:10Z","published":"2024-08-29T20:19:10Z","title":"LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation","summary":"  Although the progress made by large models in computer vision, optimization\nchallenges, the complexity of transformer models, computational limitations,\nand the requirements of practical applications call for simpler designs in\nmodel architecture for medical image segmentation, especially in mobile medical\ndevices that require lightweight and deployable models with real-time\nperformance. However, some of the current lightweight models exhibit poor\nrobustness across different datasets, which hinders their broader adoption.\nThis paper proposes a lightweight and vanilla model called LV-UNet, which\neffectively utilizes pre-trained MobileNetv3-Large models and introduces\nfusible modules. It can be trained using an improved deep training strategy and\nswitched to deployment mode during inference, reducing both parameter count and\ncomputational load. Experiments are conducted on ISIC 2016, BUSI, CVC-\nClinicDB, CVC-ColonDB, and Kvair-SEG datasets, achieving better performance\ncompared to the state-of-the-art and classic models.\n","authors":["Juntao Jiang","Mengmeng Wang","Huizhong Tian","Lingbo Cheng","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.16886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16883v1","updated":"2024-08-29T20:12:01Z","published":"2024-08-29T20:12:01Z","title":"Revising Multimodal VAEs with Diffusion Decoders","summary":"  Multimodal VAEs often struggle with generating high-quality outputs, a\nchallenge that extends beyond the inherent limitations of the VAE framework.\nThe core issue lies in the restricted joint representation of the latent space,\nparticularly when complex modalities like images are involved. Feedforward\ndecoders, commonly used for these intricate modalities, inadvertently constrain\nthe joint latent space, leading to a degradation in the quality of the other\nmodalities as well. Although recent studies have shown improvement by\nintroducing modality-specific representations, the issue remains significant.\nIn this work, we demonstrate that incorporating a flexible diffusion decoder\nspecifically for the image modality not only enhances the generation quality of\nthe images but also positively impacts the performance of the other modalities\nthat rely on feedforward decoders. This approach addresses the limitations\nimposed by conventional joint representations and opens up new possibilities\nfor improving multimodal generation tasks using the multimodal VAE framework.\nOur model provides state-of-the-art results compared to other multimodal VAEs\nin different datasets with higher coherence and superior quality in the\ngenerated modalities\n","authors":["Daniel Wesego","Amirmohammad Rooshenas"],"pdf_url":"https://arxiv.org/pdf/2408.16883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16881v1","updated":"2024-08-29T20:08:22Z","published":"2024-08-29T20:08:22Z","title":"FineFACE: Fair Facial Attribute Classification Leveraging Fine-grained\n  Features","summary":"  Published research highlights the presence of demographic bias in automated\nfacial attribute classification algorithms, particularly impacting women and\nindividuals with darker skin tones. Existing bias mitigation techniques\ntypically require demographic annotations and often obtain a trade-off between\nfairness and accuracy, i.e., Pareto inefficiency. Facial attributes, whether\ncommon ones like gender or others such as \"chubby\" or \"high cheekbones\",\nexhibit high interclass similarity and intraclass variation across demographics\nleading to unequal accuracy. This requires the use of local and subtle cues\nusing fine-grained analysis for differentiation. This paper proposes a novel\napproach to fair facial attribute classification by framing it as a\nfine-grained classification problem. Our approach effectively integrates both\nlow-level local features (like edges and color) and high-level semantic\nfeatures (like shapes and structures) through cross-layer mutual attention\nlearning. Here, shallow to deep CNN layers function as experts, offering\ncategory predictions and attention regions. An exhaustive evaluation on facial\nattribute annotated datasets demonstrates that our FineFACE model improves\naccuracy by 1.32% to 1.74% and fairness by 67% to 83.6%, over the SOTA bias\nmitigation techniques. Importantly, our approach obtains a Pareto-efficient\nbalance between accuracy and fairness between demographic groups. In addition,\nour approach does not require demographic annotations and is applicable to\ndiverse downstream classification tasks. To facilitate reproducibility, the\ncode and dataset information is available at\nhttps://github.com/VCBSL-Fairness/FineFACE.\n","authors":["Ayesha Manzoor","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2408.16881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16879v1","updated":"2024-08-29T20:05:02Z","published":"2024-08-29T20:05:02Z","title":"MSLIQA: Enhancing Learning Representations for Image Quality Assessment\n  through Multi-Scale Learning","summary":"  No-Reference Image Quality Assessment (NR-IQA) remains a challenging task due\nto the diversity of distortions and the lack of large annotated datasets. Many\nstudies have attempted to tackle these challenges by developing more accurate\nNR-IQA models, often employing complex and computationally expensive networks,\nor by bridging the domain gap between various distortions to enhance\nperformance on test datasets. In our work, we improve the performance of a\ngeneric lightweight NR-IQA model by introducing a novel augmentation strategy\nthat boosts its performance by almost 28\\%. This augmentation strategy enables\nthe network to better discriminate between different distortions in various\nparts of the image by zooming in and out. Additionally, the inclusion of\ntest-time augmentation further enhances performance, making our lightweight\nnetwork's results comparable to the current state-of-the-art models, simply\nthrough the use of augmentations.\n","authors":["Nasim Jamshidi Avanaki","Abhijay Ghildiyal","Nabajeet Barman","Saman Zadtootaghaj"],"pdf_url":"https://arxiv.org/pdf/2408.16879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00264v2","updated":"2024-08-29T19:38:54Z","published":"2024-05-01T00:48:55Z","title":"Using Texture to Classify Forests Separately from Vegetation","summary":"  Identifying terrain within satellite image data is a key issue in\ngeographical information sciences, with numerous environmental and safety\nimplications. Many techniques exist to derive classifications from spectral\ndata captured by satellites. However, the ability to reliably classify\nvegetation remains a challenge. In particular, no precise methods exist for\nclassifying forest vs. non-forest vegetation in high-level satellite images.\nThis paper provides an initial proposal for a static, algorithmic process to\nidentify forest regions in satellite image data through texture features\ncreated from detected edges and the NDVI ratio captured by Sentinel-2 satellite\nimages. With strong initial results, this paper also identifies the next steps\nto improve the accuracy of the classification and verification processes.\n","authors":["David R. Treadwell IV","Derek Jacoby","Will Parkinson","Bruce Maxwell","Yvonne Coady"],"pdf_url":"https://arxiv.org/pdf/2405.00264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16866v1","updated":"2024-08-29T19:11:46Z","published":"2024-08-29T19:11:46Z","title":"GameIR: A Large-Scale Synthesized Ground-Truth Dataset for Image\n  Restoration over Gaming Content","summary":"  Image restoration methods like super-resolution and image synthesis have been\nsuccessfully used in commercial cloud gaming products like NVIDIA's DLSS.\nHowever, restoration over gaming content is not well studied by the general\npublic. The discrepancy is mainly caused by the lack of ground-truth gaming\ntraining data that match the test cases. Due to the unique characteristics of\ngaming content, the common approach of generating pseudo training data by\ndegrading the original HR images results in inferior restoration performance.\nIn this work, we develop GameIR, a large-scale high-quality\ncomputer-synthesized ground-truth dataset to fill in the blanks, targeting at\ntwo different applications. The first is super-resolution with deferred\nrendering, to support the gaming solution of rendering and transferring LR\nimages only and restoring HR images on the client side. We provide 19200 LR-HR\npaired ground-truth frames coming from 640 videos rendered at 720p and 1440p\nfor this task. The second is novel view synthesis (NVS), to support the\nmultiview gaming solution of rendering and transferring part of the multiview\nframes and generating the remaining frames on the client side. This task has\n57,600 HR frames from 960 videos of 160 scenes with 6 camera views. In addition\nto the RGB frames, the GBuffers during the deferred rendering stage are also\nprovided, which can be used to help restoration. Furthermore, we evaluate\nseveral SOTA super-resolution algorithms and NeRF-based NVS algorithms over our\ndataset, which demonstrates the effectiveness of our ground-truth GameIR data\nin improving restoration performance for gaming content. Also, we test the\nmethod of incorporating the GBuffers as additional input information for\nhelping super-resolution and NVS. We release our dataset and models to the\ngeneral public to facilitate research on restoration methods over gaming\ncontent.\n","authors":["Lebin Zhou","Kun Han","Nam Ling","Wei Wang","Wei Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.16866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16859v1","updated":"2024-08-29T18:49:32Z","published":"2024-08-29T18:49:32Z","title":"Comparative Analysis of Transfer Learning Models for Breast Cancer\n  Classification","summary":"  The classification of histopathological images is crucial for the early and\nprecise detection of breast cancer. This study investigates the efficiency of\ndeep learning models in distinguishing between Invasive Ductal Carcinoma (IDC)\nand non-IDC in histopathology slides. We conducted a thorough comparison\nexamination of eight sophisticated models: ResNet-50, DenseNet-121, ResNeXt-50,\nVision Transformer (ViT), GoogLeNet (Inception v3), EfficientNet, MobileNet,\nand SqueezeNet. This analysis was carried out using a large dataset of 277,524\nimage patches. Our research makes a substantial contribution to the field by\noffering a comprehensive assessment of the performance of each model. We\nparticularly highlight the exceptional efficacy of attention-based mechanisms\nin the ViT model, which achieved a remarkable validation accuracy of 93\\%,\nsurpassing conventional convolutional networks. This study highlights the\npromise of advanced machine learning approaches in clinical settings, offering\nimproved precision as well as efficiency in breast cancer diagnosis.\n","authors":["Sania Eskandari","Ali Eslamian","Qiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.16859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16845v1","updated":"2024-08-29T18:21:50Z","published":"2024-08-29T18:21:50Z","title":"Enabling Local Editing in Diffusion Models by Joint and Individual\n  Component Analysis","summary":"  Recent advances in Diffusion Models (DMs) have led to significant progress in\nvisual synthesis and editing tasks, establishing them as a strong competitor to\nGenerative Adversarial Networks (GANs). However, the latent space of DMs is not\nas well understood as that of GANs. Recent research has focused on unsupervised\nsemantic discovery in the latent space of DMs by leveraging the bottleneck\nlayer of the denoising network, which has been shown to exhibit properties of a\nsemantic latent space. However, these approaches are limited to discovering\nglobal attributes. In this paper we address, the challenge of local image\nmanipulation in DMs and introduce an unsupervised method to factorize the\nlatent semantics learned by the denoising network of pre-trained DMs. Given an\narbitrary image and defined regions of interest, we utilize the Jacobian of the\ndenoising network to establish a relation between the regions of interest and\ntheir corresponding subspaces in the latent space. Furthermore, we disentangle\nthe joint and individual components of these subspaces to identify latent\ndirections that enable local image manipulation. Once discovered, these\ndirections can be applied to different images to produce semantically\nconsistent edits, making our method suitable for practical applications.\nExperimental results on various datasets demonstrate that our method can\nproduce semantic edits that are more localized and have better fidelity\ncompared to the state-of-the-art.\n","authors":["Theodoros Kouzelis","Manos Plitsis","Mihalis A. Nikolaou","Yannis Panagakis"],"pdf_url":"https://arxiv.org/pdf/2408.16845v1.pdf","comment":"Code available here: https://zelaki.github.io/localdiff/"},{"id":"http://arxiv.org/abs/2408.16827v1","updated":"2024-08-29T18:00:03Z","published":"2024-08-29T18:00:03Z","title":"Fluent and Accurate Image Captioning with a Self-Trained Reward Model","summary":"  Fine-tuning image captioning models with hand-crafted rewards like the CIDEr\nmetric has been a classical strategy for promoting caption quality at the\nsequence level. This approach, however, is known to limit descriptiveness and\nsemantic richness and tends to drive the model towards the style of\nground-truth sentences, thus losing detail and specificity. On the contrary,\nrecent attempts to employ image-text models like CLIP as reward have led to\ngrammatically incorrect and repetitive captions. In this paper, we propose\nSelf-Cap, a captioning approach that relies on a learnable reward model based\non self-generated negatives that can discriminate captions based on their\nconsistency with the image. Specifically, our discriminator is a fine-tuned\ncontrastive image-text model trained to promote caption correctness while\navoiding the aberrations that typically happen when training with a CLIP-based\nreward. To this end, our discriminator directly incorporates negative samples\nfrom a frozen captioner, which significantly improves the quality and richness\nof the generated captions but also reduces the fine-tuning time in comparison\nto using the CIDEr score as the sole metric for optimization. Experimental\nresults demonstrate the effectiveness of our training strategy on both standard\nand zero-shot image captioning datasets.\n","authors":["Nicholas Moratelli","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2408.16827v1.pdf","comment":"ICPR 2024"},{"id":"http://arxiv.org/abs/2408.16809v1","updated":"2024-08-29T17:59:57Z","published":"2024-08-29T17:59:57Z","title":"See or Guess: Counterfactually Regularized Image Captioning","summary":"  Image captioning, which generates natural language descriptions of the visual\ninformation in an image, is a crucial task in vision-language research.\nPrevious models have typically addressed this task by aligning the generative\ncapabilities of machines with human intelligence through statistical fitting of\nexisting datasets. While effective for normal images, they may struggle to\naccurately describe those where certain parts of the image are obscured or\nedited, unlike humans who excel in such cases. These weaknesses they exhibit,\nincluding hallucinations and limited interpretability, often hinder performance\nin scenarios with shifted association patterns. In this paper, we present a\ngeneric image captioning framework that employs causal inference to make\nexisting models more capable of interventional tasks, and counterfactually\nexplainable. Our approach includes two variants leveraging either total effect\nor natural direct effect. Integrating them into the training process enables\nmodels to handle counterfactual scenarios, increasing their generalizability.\nExtensive experiments on various datasets show that our method effectively\nreduces hallucinations and improves the model's faithfulness to images,\ndemonstrating high portability across both small-scale and large-scale\nimage-to-text models. The code is available at\nhttps://github.com/Aman-4-Real/See-or-Guess.\n","authors":["Qian Cao","Xu Chen","Ruihua Song","Xiting Wang","Xinting Huang","Yuchen Ren"],"pdf_url":"https://arxiv.org/pdf/2408.16809v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.16807v1","updated":"2024-08-29T17:29:26Z","published":"2024-08-29T17:29:26Z","title":"STEREO: Towards Adversarially Robust Concept Erasing from Text-to-Image\n  Generation Models","summary":"  The rapid proliferation of large-scale text-to-image generation (T2IG) models\nhas led to concerns about their potential misuse in generating harmful content.\nThough many methods have been proposed for erasing undesired concepts from T2IG\nmodels, they only provide a false sense of security, as recent works\ndemonstrate that concept-erased models (CEMs) can be easily deceived to\ngenerate the erased concept through adversarial attacks. The problem of\nadversarially robust concept erasing without significant degradation to model\nutility (ability to generate benign concepts) remains an unresolved challenge,\nespecially in the white-box setting where the adversary has access to the CEM.\nTo address this gap, we propose an approach called STEREO that involves two\ndistinct stages. The first stage searches thoroughly enough for strong and\ndiverse adversarial prompts that can regenerate an erased concept from a CEM,\nby leveraging robust optimization principles from adversarial training. In the\nsecond robustly erase once stage, we introduce an anchor-concept-based\ncompositional objective to robustly erase the target concept at one go, while\nattempting to minimize the degradation on model utility. By benchmarking the\nproposed STEREO approach against four state-of-the-art concept erasure methods\nunder three adversarial attacks, we demonstrate its ability to achieve a better\nrobustness vs. utility trade-off. Our code and models are available at\nhttps://github.com/koushiksrivats/robust-concept-erasing.\n","authors":["Koushik Srivatsan","Fahad Shamshad","Muzammal Naseer","Karthik Nandakumar"],"pdf_url":"https://arxiv.org/pdf/2408.16807v1.pdf","comment":"Project Page:\n  https://koushiksrivats.github.io/robust-concept-erasing/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.16672v1","updated":"2024-08-29T16:21:00Z","published":"2024-08-29T16:21:00Z","title":"Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever","summary":"  Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis paper, we introduce several improvements to the ColBERT model architecture\nand training pipeline, leveraging techniques successful in the more established\nsingle-vector embedding model paradigm, particularly those suited for\nheterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates\nstrong performance across a range of English and multilingual retrieval tasks,\nwhile also cutting storage requirements by up to 50% compared to previous\nmodels.\n","authors":["Rohan Jha","Bo Wang","Michael GÃ¼nther","Saba Sturua","Mohammad Kalim Akram","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.16672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14698v2","updated":"2024-08-29T15:14:48Z","published":"2024-08-26T23:52:27Z","title":"Smart Multi-Modal Search: Contextual Sparse and Dense Embedding\n  Integration in Adobe Express","summary":"  As user content and queries become increasingly multi-modal, the need for\neffective multi-modal search systems has grown. Traditional search systems\noften rely on textual and metadata annotations for indexed images, while\nmulti-modal embeddings like CLIP enable direct search using text and image\nembeddings. However, embedding-based approaches face challenges in integrating\ncontextual features such as user locale and recency. Building a scalable\nmulti-modal search system requires fine-tuning several components. This paper\npresents a multi-modal search architecture and a series of AB tests that\noptimize embeddings and multi-modal technologies in Adobe Express template\nsearch. We address considerations such as embedding model selection, the roles\nof embeddings in matching and ranking, and the balance between dense and sparse\nembeddings. Our iterative approach demonstrates how utilizing sparse, dense,\nand contextual features enhances short and long query search, significantly\nreduces null rates (over 70\\%), and increases click-through rates (CTR). Our\nfindings provide insights into developing robust multi-modal search systems,\nthereby enhancing relevance for complex queries.\n","authors":["Cherag Aroraa","Tracy Holloway King","Jayant Kumar","Yi Lu","Sanat Sharma","Arvind Srikantan","David Uvalle","Josep Valls-Vargas","Harsha Vardhan"],"pdf_url":"https://arxiv.org/pdf/2408.14698v2.pdf","comment":"CIKM 2024 (International Conference on Information and Knowledge\n  Management), Multimodal Search and Recommendations Workshop"},{"id":"http://arxiv.org/abs/2408.16578v1","updated":"2024-08-29T14:44:12Z","published":"2024-08-29T14:44:12Z","title":"Transformers Meet ACT-R: Repeat-Aware and Sequential Listening Session\n  Recommendation","summary":"  Music streaming services often leverage sequential recommender systems to\npredict the best music to showcase to users based on past sequences of\nlistening sessions. Nonetheless, most sequential recommendation methods ignore\nor insufficiently account for repetitive behaviors. This is a crucial\nlimitation for music recommendation, as repeatedly listening to the same song\nover time is a common phenomenon that can even change the way users perceive\nthis song. In this paper, we introduce PISA (Psychology-Informed Session\nembedding using ACT-R), a session-level sequential recommender system that\novercomes this limitation. PISA employs a Transformer architecture learning\nembedding representations of listening sessions and users using attention\nmechanisms inspired by Anderson's ACT-R (Adaptive Control of Thought-Rational),\na cognitive architecture modeling human information access and memory dynamics.\nThis approach enables us to capture dynamic and repetitive patterns from user\nbehaviors, allowing us to effectively predict the songs they will listen to in\nsubsequent sessions, whether they are repeated or new ones. We demonstrate the\nempirical relevance of PISA using both publicly available listening data from\nLast.fm and proprietary data from Deezer, a global music streaming service,\nconfirming the critical importance of repetition modeling for sequential\nlistening session recommendation. Along with this paper, we publicly release\nour proprietary dataset to foster future research in this field, as well as the\nsource code of PISA to facilitate its future use.\n","authors":["Viet-Anh Tran","Guillaume Salha-Galvan","Bruno Sguerra","Romain Hennequin"],"pdf_url":"https://arxiv.org/pdf/2408.16578v1.pdf","comment":"11 pages. Accepted by RecSys'2024, full paper"},{"id":"http://arxiv.org/abs/2408.16446v1","updated":"2024-08-29T11:19:57Z","published":"2024-08-29T11:19:57Z","title":"Is text normalization relevant for classifying medieval charters?","summary":"  This study examines the impact of historical text normalization on the\nclassification of medieval charters, specifically focusing on document dating\nand locating. Using a data set of Middle High German charters from a digital\narchive, we evaluate various classifiers, including traditional and\ntransformer-based models, with and without normalization. Our results indicate\nthat the given normalization minimally improves locating tasks but reduces\naccuracy for dating, implying that original texts contain crucial features that\nnormalization may obscure. We find that support vector machines and gradient\nboosting outperform other models, questioning the efficiency of transformers\nfor this use case. Results suggest a selective approach to historical text\nnormalization, emphasizing the significance of preserving some textual\ncharacteristics that are critical for classification tasks in document\nanalysis.\n","authors":["Florian Atzenhofer-Baumgartner","TamÃ¡s KovÃ¡cs"],"pdf_url":"https://arxiv.org/pdf/2408.16446v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections"},{"id":"http://arxiv.org/abs/2408.16430v1","updated":"2024-08-29T10:44:59Z","published":"2024-08-29T10:44:59Z","title":"Do Recommender Systems Promote Local Music? A Reproducibility Study\n  Using Music Streaming Data","summary":"  This paper examines the influence of recommender systems on local music\nrepresentation, discussing prior findings from an empirical study on the LFM-2b\npublic dataset. This prior study argued that different recommender systems\nexhibit algorithmic biases shifting music consumption either towards or against\nlocal content. However, LFM-2b users do not reflect the diverse audience of\nmusic streaming services. To assess the robustness of this study's conclusions,\nwe conduct a comparative analysis using proprietary listening data from a\nglobal music streaming service, which we publicly release alongside this paper.\nWe observe significant differences in local music consumption patterns between\nour dataset and LFM-2b, suggesting that caution should be exercised when\ndrawing conclusions on local music based solely on LFM-2b. Moreover, we show\nthat the algorithmic biases exhibited in the original work vary in our dataset,\nand that several unexplored model parameters can significantly influence these\nbiases and affect the study's conclusion on both datasets. Finally, we discuss\nthe complexity of accurately labeling local music, emphasizing the risk of\nmisleading conclusions due to unreliable, biased, or incomplete labels. To\nencourage further research and ensure reproducibility, we have publicly shared\nour dataset and code.\n","authors":["Kristina Matrosova","Lilian Marey","Guillaume Salha-Galvan","Thomas Louail","Olivier Bodini","Manuel Moussallam"],"pdf_url":"https://arxiv.org/pdf/2408.16430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16312v1","updated":"2024-08-29T07:20:56Z","published":"2024-08-29T07:20:56Z","title":"SynDL: A Large-Scale Synthetic Test Collection","summary":"  Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings.\n","authors":["Hossein A. Rahmani","Xi Wang","Emine Yilmaz","Nick Craswell","Bhaskar Mitra","Paul Thomas"],"pdf_url":"https://arxiv.org/pdf/2408.16312v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2408.16296v1","updated":"2024-08-29T06:54:03Z","published":"2024-08-29T06:54:03Z","title":"Rethinking Sparse Lexical Representations for Image Retrieval in the Age\n  of Rising Multi-Modal Large Language Models","summary":"  In this paper, we rethink sparse lexical representations for image retrieval.\nBy utilizing multi-modal large language models (M-LLMs) that support visual\nprompting, we can extract image features and convert them into textual data,\nenabling us to utilize efficient sparse retrieval algorithms employed in\nnatural language processing for image retrieval tasks. To assist the LLM in\nextracting image features, we apply data augmentation techniques for key\nexpansion and analyze the impact with a metric for relevance between images and\ntextual data. We empirically show the superior precision and recall performance\nof our image retrieval method compared to conventional vision-language\nmodel-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a\nkeyword-based image retrieval scenario, where keywords serve as search queries.\nWe also demonstrate that the retrieval performance can be improved by\niteratively incorporating keywords into search queries.\n","authors":["Kengo Nakata","Daisuke Miyashita","Youyang Ng","Yasuto Hoshi","Jun Deguchi"],"pdf_url":"https://arxiv.org/pdf/2408.16296v1.pdf","comment":"Accepted to ECCV 2024 Workshops: 2nd Workshop on Traditional Computer\n  Vision in the Age of Deep Learning (TradiCV)"},{"id":"http://arxiv.org/abs/2408.16238v1","updated":"2024-08-29T03:34:39Z","published":"2024-08-29T03:34:39Z","title":"Efficient Transfer Learning Framework for Cross-Domain Click-Through\n  Rate Prediction","summary":"  Natural content and advertisement coexist in industrial recommendation\nsystems but differ in data distribution. Concretely, traffic related to the\nadvertisement is considerably sparser compared to that of natural content,\nwhich motivates the development of transferring knowledge from the richer\nsource natural content domain to the sparser advertising domain. The challenges\ninclude the inefficiencies arising from the management of extensive source data\nand the problem of 'catastrophic forgetting' that results from the CTR model's\ndaily updating. To this end, we propose a novel tri-level asynchronous\nframework, i.e., Efficient Transfer Learning Framework for Cross-Domain\nClick-Through Rate Prediction (E-CDCTR), to transfer comprehensive knowledge of\nnatural content to advertisement CTR models. This framework consists of three\nkey components: Tiny Pre-training Model ((TPM), which trains a tiny CTR model\nwith several basic features on long-term natural data; Complete Pre-training\nModel (CPM), which trains a CTR model holding network structure and input\nfeatures the same as target advertisement on short-term natural data;\nAdvertisement CTR model (A-CTR), which derives its parameter initialization\nfrom CPM together with multiple historical embeddings from TPM as extra feature\nand then fine-tunes on advertisement data. TPM provides richer representations\nof user and item for both the CPM and A-CTR, effectively alleviating the\nforgetting problem inherent in the daily updates. CPM further enhances the\nadvertisement model by providing knowledgeable initialization, thereby\nalleviating the data sparsity challenges typically encountered by advertising\nCTR models. Such a tri-level cross-domain transfer learning framework offers an\nefficient solution to address both data sparsity and `catastrophic forgetting',\nyielding remarkable improvements.\n","authors":["Qi Liu","Xingyuan Tang","Jianqiang Huang","Xiangqian Yu","Haoran Jin","Jin Chen","Yuanhao Pu","Defu Lian","Tan Qu","Zhe Wang","Jia Cheng","Jun Lei"],"pdf_url":"https://arxiv.org/pdf/2408.16238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21191v2","updated":"2024-08-29T02:27:19Z","published":"2024-07-30T20:58:36Z","title":"GenRec: Generative Sequential Recommendation with Large Language Models","summary":"  Sequential recommendation is a task to capture hidden user preferences from\nhistorical user item interaction data and recommend next items for the user.\nSignificant progress has been made in this domain by leveraging classification\nbased learning methods. Inspired by the recent paradigm of 'pretrain, prompt\nand predict' in NLP, we consider sequential recommendation as a sequence to\nsequence generation task and propose a novel model named Generative\nRecommendation (GenRec). Unlike classification based models that learn explicit\nuser and item representations, GenRec utilizes the sequence modeling capability\nof Transformer and adopts the masked item prediction objective to effectively\nlearn the hidden bidirectional sequential patterns. Different from existing\ngenerative sequential recommendation models, GenRec does not rely on manually\ndesigned hard prompts. The input to GenRec is textual user item sequence and\nthe output is top ranked next items. Moreover, GenRec is lightweight and\nrequires only a few hours to train effectively in low-resource settings, making\nit highly applicable to real-world scenarios and helping to democratize large\nlanguage models in the sequential recommendation domain. Our extensive\nexperiments have demonstrated that GenRec generalizes on various public\nreal-world datasets and achieves state-of-the-art results. Our experiments also\nvalidate the effectiveness of the the proposed masked item prediction objective\nthat improves the model performance by a large margin.\n","authors":["Panfeng Cao","Pietro Lio"],"pdf_url":"https://arxiv.org/pdf/2407.21191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15793v2","updated":"2024-08-29T00:32:15Z","published":"2023-07-28T20:25:11Z","title":"Summaries, Highlights, and Action items: Design, implementation and\n  evaluation of an LLM-powered meeting recap system","summary":"  Meetings play a critical infrastructural role in the coordination of work. In\nrecent years, due to shift to hybrid and remote work, more meetings are moving\nto online Computer Mediated Spaces. This has led to new problems (e.g. more\ntime spent in less engaging meetings) and new opportunities (e.g. automated\ntranscription/captioning and recap support). Recent advances in large language\nmodels (LLMs) for dialog summarization have the potential to improve the\nexperience of meetings by reducing individuals' meeting load and increasing the\nclarity and alignment of meeting outputs. Despite this potential, they face\ntechnological limitation due to long transcripts and inability to capture\ndiverse recap needs based on user's context. To address these gaps, we design,\nimplement and evaluate in-context a meeting recap system. We first\nconceptualize two salient recap representations -- important highlights, and a\nstructured, hierarchical minutes view. We develop a system to operationalize\nthe representations with dialogue summarization as its building blocks.\nFinally, we evaluate the effectiveness of the system with seven users in the\ncontext of their work meetings. Our findings show promise in using LLM-based\ndialogue summarization for meeting recap and the need for both representations\nin different contexts. However, we find that LLM-based recap still lacks an\nunderstanding of whats personally relevant to participants, can miss important\ndetails, and mis-attributions can be detrimental to group dynamics. We identify\ncollaboration opportunities such as a shared recap document that a high quality\nrecap enables. We report on implications for designing AI systems to partner\nwith users to learn and improve from natural interactions to overcome the\nlimitations related to personal relevance and summarization quality.\n","authors":["Sumit Asthana","Sagih Hilleli","Pengcheng He","Aaron Halfaker"],"pdf_url":"https://arxiv.org/pdf/2307.15793v2.pdf","comment":"in review for CSCW 24"},{"id":"http://arxiv.org/abs/2404.05893v4","updated":"2024-08-29T21:34:22Z","published":"2024-04-08T22:29:53Z","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models","summary":"  Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base\n","authors":["Sowmya S. Sundaram","Benjamin Solomon","Avani Khatri","Anisha Laumas","Purvesh Khatri","Mark A. Musen"],"pdf_url":"https://arxiv.org/pdf/2404.05893v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16885v1","updated":"2024-08-29T20:18:00Z","published":"2024-08-29T20:18:00Z","title":"A Prototype Model of Zero-Trust Architecture Blockchain with\n  EigenTrust-Based Practical Byzantine Fault Tolerance Protocol to Manage\n  Decentralized Clinical Trials","summary":"  The COVID-19 pandemic necessitated the emergence of decentralized Clinical\nTrials (DCTs) due to patient retention, accelerate trials, improve data\naccessibility, enable virtual care, and facilitate seamless communication\nthrough integrated systems. However, integrating systems in DCTs exposes\nclinical data to potential security threats, making them susceptible to theft\nat any stage, a high risk of protocol deviations, and monitoring issues. To\nmitigate these challenges, blockchain technology serves as a secure framework,\nacting as a decentralized ledger, creating an immutable environment by\nestablishing a zero-trust architecture, where data are deemed untrusted until\nverified. In combination with Internet of Things (IoT)-enabled wearable\ndevices, blockchain secures the transfer of clinical trial data on private\nblockchains during DCT automation and operations. This paper proposes a\nprototype model of the Zero-Trust Architecture Blockchain (z-TAB) to integrate\npatient-generated clinical trial data during DCT operation management. The\nEigenTrust-based Practical Byzantine Fault Tolerance (T-PBFT) algorithm has\nbeen incorporated as a consensus protocol, leveraging Hyperledger Fabric.\nFurthermore, the Internet of Things (IoT) has been integrated to streamline\ndata processing among stakeholders within the blockchain platforms. Rigorous\nevaluation has been done to evaluate the quality of the system.\n","authors":["Ashok Kumar Peepliwall","Hari Mohan Pandey","Surya Prakash","Anand A Mahajan","Sudhinder Singh Chowhan","Vinesh Kumar","Rahul Sharma"],"pdf_url":"https://arxiv.org/pdf/2408.16885v1.pdf","comment":"NA"},{"id":"http://arxiv.org/abs/2408.16877v1","updated":"2024-08-29T19:58:46Z","published":"2024-08-29T19:58:46Z","title":"Longitudinal Modularity, a Modularity for Link Streams","summary":"  Temporal networks are commonly used to model real-life phenomena. When these\nphenomena represent interactions and are captured at a fine-grained temporal\nresolution, they are modeled as link streams. Community detection is an\nessential network analysis task. Although many methods exist for static\nnetworks, and some methods have been developed for temporal networks\nrepresented as sequences of snapshots, few works can handle link streams. This\narticle introduces the first adaptation of the well-known Modularity quality\nfunction to link streams. Unlike existing methods, it is independent of the\ntime scale of analysis. After introducing the quality function, and its\nrelation to existing static and dynamic definitions of Modularity, we show\nexperimentally its relevance for dynamic community evaluation.\n","authors":["Victor Brabant","Yasaman Asgari","Pierre Borgnat","Angela Bonifati","Remy Cazabet"],"pdf_url":"https://arxiv.org/pdf/2408.16877v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.16765v1","updated":"2024-08-29T17:59:07Z","published":"2024-08-29T17:59:07Z","title":"A Score-Based Density Formula, with Applications in Diffusion Generative\n  Models","summary":"  Score-based generative models (SGMs) have revolutionized the field of\ngenerative modeling, achieving unprecedented success in generating realistic\nand diverse content. Despite empirical advances, the theoretical basis for why\noptimizing the evidence lower bound (ELBO) on the log-likelihood is effective\nfor training diffusion generative models, such as DDPMs, remains largely\nunexplored. In this paper, we address this question by establishing a density\nformula for a continuous-time diffusion process, which can be viewed as the\ncontinuous-time limit of the forward process in an SGM. This formula reveals\nthe connection between the target density and the score function associated\nwith each step of the forward process. Building on this, we demonstrate that\nthe minimizer of the optimization objective for training DDPMs nearly coincides\nwith that of the true objective, providing a theoretical foundation for\noptimizing DDPMs using the ELBO. Furthermore, we offer new insights into the\nrole of score-matching regularization in training GANs, the use of ELBO in\ndiffusion classifiers, and the recently proposed diffusion loss.\n","authors":["Gen Li","Yuling Yan"],"pdf_url":"https://arxiv.org/pdf/2408.16765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05733v2","updated":"2024-08-29T17:58:35Z","published":"2024-05-09T12:50:16Z","title":"Batched Stochastic Bandit for Nondegenerate Functions","summary":"  This paper studies batched bandit learning problems for nondegenerate\nfunctions. We introduce an algorithm that solves the batched bandit problem for\nnondegenerate functions near-optimally. More specifically, we introduce an\nalgorithm, called Geometric Narrowing (GN), whose regret bound is of order\n$\\widetilde{{\\mathcal{O}}} ( A_{+}^d \\sqrt{T} )$. In addition, GN only needs\n$\\mathcal{O} (\\log \\log T)$ batches to achieve this regret. We also provide\nlower bound analysis for this problem. More specifically, we prove that over\nsome (compact) doubling metric space of doubling dimension $d$: 1. For any\npolicy $\\pi$, there exists a problem instance on which $\\pi$ admits a regret of\norder ${\\Omega} ( A_-^d \\sqrt{T})$; 2. No policy can achieve a regret of order\n$ A_-^d \\sqrt{T} $ over all problem instances, using less than $ \\Omega ( \\log\n\\log T ) $ rounds of communications. Our lower bound analysis shows that the GN\nalgorithm achieves near optimal regret with minimal number of batches.\n","authors":["Yu Liu","Yunlu Shu","Tianyu Wang"],"pdf_url":"https://arxiv.org/pdf/2405.05733v2.pdf","comment":"34 pages, 14 colored figures"},{"id":"http://arxiv.org/abs/2408.16762v1","updated":"2024-08-29T17:57:05Z","published":"2024-08-29T17:57:05Z","title":"UV-free Texture Generation with Denoising and Geodesic Heat Diffusions","summary":"  Seams, distortions, wasted UV space, vertex-duplication, and varying\nresolution over the surface are the most prominent issues of the standard\nUV-based texturing of meshes. These issues are particularly acute when\nautomatic UV-unwrapping techniques are used. For this reason, instead of\ngenerating textures in automatically generated UV-planes like most\nstate-of-the-art methods, we propose to represent textures as coloured\npoint-clouds whose colours are generated by a denoising diffusion probabilistic\nmodel constrained to operate on the surface of 3D objects. Our sampling and\nresolution agnostic generative model heavily relies on heat diffusion over the\nsurface of the meshes for spatial communication between points. To enable\nprocessing of arbitrarily sampled point-cloud textures and ensure long-distance\ntexture consistency we introduce a fast re-sampling of the mesh spectral\nproperties used during the heat diffusion and introduce a novel\nheat-diffusion-based self-attention mechanism. Our code and pre-trained models\nare available at github.com/simofoti/UV3-TeD.\n","authors":["Simone Foti","Stefanos Zafeiriou","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2408.16762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10972v2","updated":"2024-08-29T17:55:52Z","published":"2024-07-15T17:59:55Z","title":"VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation","summary":"  In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io.\n","authors":["Bocheng Zou","Mu Cai","Jianrui Zhang","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2407.10972v2.pdf","comment":"Project Page: https://vgbench.github.io"},{"id":"http://arxiv.org/abs/2408.16753v1","updated":"2024-08-29T17:49:18Z","published":"2024-08-29T17:49:18Z","title":"Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning\n  of Large Language Models","summary":"  Reinforcement learning is used to align language models with human preference\nsignals after first pre-training the model to predict the next token of text\nwithin a large corpus using likelihood maximization. Before being deployed in a\nspecific domain, models are often further fine-tuned on task specific data.\nSince human preferences are often unavailable for the last step, it is\nperformed using likelihood maximization as that is the typical default method.\nHowever, reinforcement learning has other advantages besides facilitating\nalignment to a human derived reward function. For one, whereas likelihood\nmaximization is a form of imitation learning in which the model is trained on\nwhat to do under ideal conditions, reinforcement learning is not limited to\ndemonstrating actions just for optimally reached states and trains a model what\nto do under a range of scenarios as it explores the policy space. In addition,\nit also trains a model what not to do, suppressing competitive but poor\nactions. This work develops a framework for last-mile fine-tuning using\nreinforcement learning and tests whether it garners performance gains. The\nexperiments center on abstractive summarization, but the framework is general\nand broadly applicable. Use of the procedure produced significantly better\nresults than likelihood maximization when comparing raw predictions. For the\nspecific data tested, the gap could be bridged by employing post-processing of\nthe maximum likelihood outputs. Nonetheless, the framework offers a new avenue\nfor model optimization in situations where post-processing may be less\nstraightforward or effective, and it can be extended to include more complex\nclasses of undesirable outputs to penalize and train against, such as\nhallucinations.\n","authors":["Alec Solway"],"pdf_url":"https://arxiv.org/pdf/2408.16753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13154v3","updated":"2024-08-29T17:47:18Z","published":"2024-06-19T02:09:15Z","title":"Conditional score-based diffusion models for solving inverse problems in\n  mechanics","summary":"  We propose a framework to perform Bayesian inference using conditional\nscore-based diffusion models to solve a class of inverse problems in mechanics\ninvolving the inference of a specimen's spatially varying material properties\nfrom noisy measurements of its mechanical response to loading. Conditional\nscore-based diffusion models are generative models that learn to approximate\nthe score function of a conditional distribution using samples from the joint\ndistribution. More specifically, the score functions corresponding to multiple\nrealizations of the measurement are approximated using a single neural network,\nthe so-called score network, which is subsequently used to sample the posterior\ndistribution using an appropriate Markov chain Monte Carlo scheme based on\nLangevin dynamics. Training the score network only requires simulating the\nforward model. Hence, the proposed approach can accommodate black-box forward\nmodels and complex measurement noise. Moreover, once the score network has been\ntrained, it can be re-used to solve the inverse problem for different\nrealizations of the measurements. We demonstrate the efficacy of the proposed\napproach on a suite of high-dimensional inverse problems in mechanics that\ninvolve inferring heterogeneous material properties from noisy measurements.\nSome examples we consider involve synthetic data, while others include data\ncollected from actual elastography experiments. Further, our applications\ndemonstrate that the proposed approach can handle different measurement\nmodalities, complex patterns in the inferred quantities, non-Gaussian and\nnon-additive noise models, and nonlinear black-box forward models. The results\nshow that the proposed framework can solve large-scale physics-based inverse\nproblems efficiently.\n","authors":["Agnimitra Dasgupta","Harisankar Ramaswamy","Javier Murgoitio-Esandi","Ken Foo","Runze Li","Qifa Zhou","Brendan Kennedy","Assad Oberai"],"pdf_url":"https://arxiv.org/pdf/2406.13154v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16751v1","updated":"2024-08-29T17:46:18Z","published":"2024-08-29T17:46:18Z","title":"A Gradient Analysis Framework for Rewarding Good and Penalizing Bad\n  Examples in Language Models","summary":"  Beyond maximum likelihood estimation (MLE), the standard objective of a\nlanguage model (LM) that optimizes good examples probabilities, many studies\nhave explored ways that also penalize bad examples for enhancing the quality of\noutput distribution, including unlikelihood training, exponential maximizing\naverage treatment effect (ExMATE), and direct preference optimization (DPO). To\nsystematically compare these methods and further provide a unified recipe for\nLM optimization, in this paper, we present a unique angle of gradient analysis\nof loss functions that simultaneously reward good examples and penalize bad\nones in LMs. Through both mathematical results and experiments on\nCausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functional\ncharacteristics among these methods. We find that ExMATE serves as a superior\nsurrogate for MLE, and that combining DPO with ExMATE instead of MLE further\nenhances both the statistical (5-7%) and generative (+18% win rate)\nperformance.\n","authors":["Yi-Lin Tuan","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06599v3","updated":"2024-08-29T17:31:26Z","published":"2023-02-13T18:55:31Z","title":"FilFL: Client Filtering for Optimized Client Participation in Federated\n  Learning","summary":"  Federated learning, an emerging machine learning paradigm, enables clients to\ncollaboratively train a model without exchanging local data. Clients\nparticipating in the training process significantly impact the convergence\nrate, learning efficiency, and model generalization. We propose a novel\napproach, client filtering, to improve model generalization and optimize client\nparticipation and training. The proposed method periodically filters available\nclients to identify a subset that maximizes a combinatorial objective function\nwith an efficient greedy filtering algorithm. Thus, the clients are assessed as\na combination rather than individually. We theoretically analyze the\nconvergence of federated learning with client filtering in heterogeneous\nsettings and evaluate its performance across diverse vision and language tasks,\nincluding realistic scenarios with time-varying client availability. Our\nempirical results demonstrate several benefits of our approach, including\nimproved learning efficiency, faster convergence, and up to 10% higher test\naccuracy than training without client filtering.\n","authors":["Fares Fourati","Salma Kharrat","Vaneet Aggarwal","Mohamed-Slim Alouini","Marco Canini"],"pdf_url":"https://arxiv.org/pdf/2302.06599v3.pdf","comment":"Accepted at ECAI'24"},{"id":"http://arxiv.org/abs/2310.03103v5","updated":"2024-08-29T17:24:20Z","published":"2023-10-04T18:47:34Z","title":"Learning to Prompt Your Domain for Vision-Language Models","summary":"  Prompt learning has recently become a very efficient transfer learning\nparadigm for Contrastive Language Image Pretraining (CLIP) models. Compared\nwith fine-tuning the entire encoder, prompt learning can obtain highly\ncompetitive results by optimizing only a small number of parameters, which\npresents considerably exciting benefits for federated learning applications\nthat prioritizes communication efficiency. However, in this work, we identify\nthat directly transferring prompt learning approaches into federated learning\ndoes not yield favorable results since the model often suffers from\nconsiderable domain gaps across different clients. To address this issue, we\npropose ADAPT, a novel domain-aware prompt learning approach that facilitates\nboth intra- and inter-domain prompts across federated participants. The basic\nidea of ADAPT is that the prompted CLIP should detect the input image's domain\ncorrespondence and before making the prediction of its category. Extensive\nexperiments of ADAPT demonstrate its significant efficiency and effectiveness\nin federated learning. For example, by learning and sharing only 0.08M\nparameters, our ADAPT attains a 68.4% average accuracy over six domains in the\nDomainNet dataset, which improves the original CLIP by a large margin of 14.8%.\n","authors":["Guoyizhe Wei","Feng Wang","Anshul Shah","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2310.03103v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09359v4","updated":"2024-08-29T17:21:27Z","published":"2024-04-14T21:14:47Z","title":"Evaluation Framework for Feedback Generation Methods in Skeletal\n  Movement Assessment","summary":"  The application of machine-learning solutions to movement assessment from\nskeleton videos has attracted significant research attention in recent years.\nThis advancement has made rehabilitation at home more accessible, utilizing\nmovement assessment algorithms that can operate on affordable equipment for\nhuman pose detection and analysis from 2D or 3D videos. While the primary\nobjective of automatic assessment tasks is to score movements, the automatic\ngeneration of feedback highlighting key movement issues has the potential to\nsignificantly enhance and accelerate the rehabilitation process. While numerous\nresearch works exist in the field of automatic movement assessment, only a\nhandful address feedback generation. In this study, we propose terminology and\ncriteria for the classification, evaluation, and comparison of feedback\ngeneration solutions. We discuss the challenges associated with each feedback\ngeneration approach and use our proposed criteria to classify existing\nsolutions. To our knowledge, this is the first work that formulates feedback\ngeneration in skeletal movement assessment.\n","authors":["Tal Hakim"],"pdf_url":"https://arxiv.org/pdf/2404.09359v4.pdf","comment":"Accepted to xAI4Biometrics 2024 at ECCV 2024"},{"id":"http://arxiv.org/abs/2303.15477v5","updated":"2024-08-29T17:20:14Z","published":"2023-03-26T18:31:52Z","title":"Adaptive Log-Euclidean Metrics for SPD Matrix Learning","summary":"  Symmetric Positive Definite (SPD) matrices have received wide attention in\nmachine learning due to their intrinsic capacity to encode underlying\nstructural correlation in data. Many successful Riemannian metrics have been\nproposed to reflect the non-Euclidean geometry of SPD manifolds. However, most\nexisting metric tensors are fixed, which might lead to sub-optimal performance\nfor SPD matrix learning, especially for deep SPD neural networks. To remedy\nthis limitation, we leverage the commonly encountered pullback techniques and\npropose Adaptive Log-Euclidean Metrics (ALEMs), which extend the widely used\nLog-Euclidean Metric (LEM). Compared with the previous Riemannian metrics, our\nmetrics contain learnable parameters, which can better adapt to the complex\ndynamics of Riemannian neural networks with minor extra computations. We also\npresent a complete theoretical analysis to support our ALEMs, including\nalgebraic and Riemannian properties. The experimental and theoretical results\ndemonstrate the merit of the proposed metrics in improving the performance of\nSPD neural networks. The efficacy of our metrics is further showcased on a set\nof recently developed Riemannian building blocks, including Riemannian batch\nnormalization, Riemannian Residual blocks, and Riemannian classifiers.\n","authors":["Ziheng Chen","Yue Song","Tianyang Xu","Zhiwu Huang","Xiao-Jun Wu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2303.15477v5.pdf","comment":"Accepted by TIP 2024"},{"id":"http://arxiv.org/abs/2408.16725v1","updated":"2024-08-29T17:18:53Z","published":"2024-08-29T17:18:53Z","title":"Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming","summary":"  Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.\n","authors":["Zhifei Xie","Changqiao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16725v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2408.16717v1","updated":"2024-08-29T17:07:43Z","published":"2024-08-29T17:07:43Z","title":"A GREAT Architecture for Edge-Based Graph Problems Like TSP","summary":"  In the last years, many neural network-based approaches have been proposed to\ntackle combinatorial optimization problems such as routing problems. Many of\nthese approaches are based on graph neural networks (GNNs) or related\ntransformers, operating on the Euclidean coordinates representing the routing\nproblems. However, GNNs are inherently not well suited to operate on dense\ngraphs, such as in routing problems. Furthermore, models operating on Euclidean\ncoordinates cannot be applied to non-Euclidean versions of routing problems\nthat are often found in real-world settings. To overcome these limitations, we\npropose a novel GNN-related edge-based neural model called Graph Edge Attention\nNetwork (GREAT). We evaluate the performance of GREAT in the\nedge-classification task to predict optimal edges in the Traveling Salesman\nProblem (TSP). We can use such a trained GREAT model to produce sparse TSP\ngraph instances, keeping only the edges GREAT finds promising. Compared to\nother, non-learning-based methods to sparsify TSP graphs, GREAT can produce\nvery sparse graphs while keeping most of the optimal edges. Furthermore, we\nbuild a reinforcement learning-based GREAT framework which we apply to\nEuclidean and non-Euclidean asymmetric TSP. This framework achieves\nstate-of-the-art results.\n","authors":["Attila Lischka","Jiaming Wu","Morteza Haghir Chehreghani","BalÃ¡zs KulcsÃ¡r"],"pdf_url":"https://arxiv.org/pdf/2408.16717v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.09536v2","updated":"2024-08-29T17:06:52Z","published":"2024-05-15T17:45:59Z","title":"Wasserstein Gradient Boosting: A Framework for Distribution-Valued\n  Supervised Learning","summary":"  Gradient boosting is a sequential ensemble method that fits a new weaker\nlearner to pseudo residuals at each iteration. We propose Wasserstein gradient\nboosting, a novel extension of gradient boosting that fits a new weak learner\nto alternative pseudo residuals that are Wasserstein gradients of loss\nfunctionals of probability distributions assigned at each input. It solves\ndistribution-valued supervised learning, where the output values of the\ntraining dataset are probability distributions for each input. In\nclassification and regression, a model typically returns, for each input, a\npoint estimate of a parameter of a noise distribution specified for a response\nvariable, such as the class probability parameter of a categorical distribution\nspecified for a response label. A main application of Wasserstein gradient\nboosting in this paper is tree-based evidential learning, which returns a\ndistributional estimate of the response parameter for each input. We\nempirically demonstrate the superior performance of the probabilistic\nprediction by Wasserstein gradient boosting in comparison with existing\nuncertainty quantification methods.\n","authors":["Takuo Matsubara"],"pdf_url":"https://arxiv.org/pdf/2405.09536v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16707v1","updated":"2024-08-29T17:00:47Z","published":"2024-08-29T17:00:47Z","title":"Enhanced forecasting of stock prices based on variational mode\n  decomposition, PatchTST, and adaptive scale-weighted layer","summary":"  The significant fluctuations in stock index prices in recent years highlight\nthe critical need for accurate forecasting to guide investment and financial\nstrategies. This study introduces a novel composite forecasting framework that\nintegrates variational mode decomposition (VMD), PatchTST, and adaptive\nscale-weighted layer (ASWL) to address these challenges. Utilizing datasets of\nfour major stock indices--SP500, DJI, SSEC, and FTSE--from 2000 to 2024, the\nproposed method first decomposes the raw price series into intrinsic mode\nfunctions (IMFs) using VMD. Each IMF is then modeled with PatchTST to capture\ntemporal patterns effectively. The ASWL module is applied to incorporate scale\ninformation, enhancing prediction accuracy. The final forecast is derived by\naggregating predictions from all IMFs. The VMD-PatchTST-ASWL framework\ndemonstrates significant improvements in forecasting accuracy compared to\ntraditional models, showing robust performance across different indices. This\ninnovative approach provides a powerful tool for stock index price forecasting,\nwith potential applications in various financial analysis and investment\ndecision-making contexts.\n","authors":["Xiaorui Xue","Shaofang Li","Xiaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05527v3","updated":"2024-08-29T16:48:58Z","published":"2024-03-08T18:48:30Z","title":"GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM","summary":"  Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.\n","authors":["Hao Kang","Qingru Zhang","Souvik Kundu","Geonhwa Jeong","Zaoxing Liu","Tushar Krishna","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.05527v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16698v1","updated":"2024-08-29T16:47:58Z","published":"2024-08-29T16:47:58Z","title":"SympGNNs: Symplectic Graph Neural Networks for identifiying\n  high-dimensional Hamiltonian systems and node classification","summary":"  Existing neural network models to learn Hamiltonian systems, such as\nSympNets, although accurate in low-dimensions, struggle to learn the correct\ndynamics for high-dimensional many-body systems. Herein, we introduce\nSymplectic Graph Neural Networks (SympGNNs) that can effectively handle system\nidentification in high-dimensional Hamiltonian systems, as well as node\nclassification. SympGNNs combines symplectic maps with permutation\nequivariance, a property of graph neural networks. Specifically, we propose two\nvariants of SympGNNs: i) G-SympGNN and ii) LA-SympGNN, arising from different\nparameterizations of the kinetic and potential energy. We demonstrate the\ncapabilities of SympGNN on two physical examples: a 40-particle coupled\nHarmonic oscillator, and a 2000-particle molecular dynamics simulation in a\ntwo-dimensional Lennard-Jones potential. Furthermore, we demonstrate the\nperformance of SympGNN in the node classification task, achieving accuracy\ncomparable to the state-of-the-art. We also empirically show that SympGNN can\novercome the oversmoothing and heterophily problems, two key challenges in the\nfield of graph neural networks.\n","authors":["Alan John Varghese","Zhen Zhang","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2408.16698v1.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.10166v2","updated":"2024-08-29T16:44:17Z","published":"2024-06-14T16:36:35Z","title":"Misam: Using ML in Dataflow Selection of Sparse-Sparse Matrix\n  Multiplication","summary":"  Sparse matrix-matrix multiplication (SpGEMM) is a critical operation in\nnumerous fields, including scientific computing, graph analytics, and deep\nlearning. These applications exploit the sparsity of matrices to reduce storage\nand computational demands. However, the irregular structure of sparse matrices\nposes significant challenges for performance optimization. Traditional hardware\naccelerators are tailored for specific sparsity patterns with fixed dataflow\nschemes - inner, outer, and row-wise but often perform suboptimally when the\nactual sparsity deviates from these predetermined patterns. As the use of\nSpGEMM expands across various domains, each with distinct sparsity\ncharacteristics, the demand for hardware accelerators that can efficiently\nhandle a range of sparsity patterns is increasing. This paper presents a\nmachine learning based approach for adaptively selecting the most appropriate\ndataflow scheme for SpGEMM tasks with diverse sparsity patterns. By employing\ndecision trees and deep reinforcement learning, we explore the potential of\nthese techniques to surpass heuristic-based methods in identifying optimal\ndataflow schemes. We evaluate our models by comparing their performance with\nthat of a heuristic, highlighting the strengths and weaknesses of each\napproach. Our findings suggest that using machine learning for dynamic dataflow\nselection in hardware accelerators can provide upto 28 times gains.\n","authors":["Sanjali Yadav","Bahar Asgari"],"pdf_url":"https://arxiv.org/pdf/2406.10166v2.pdf","comment":"Accepted to ISCA 2024 MLArchSys workshop\n  https://openreview.net/forum?id=A1V9FaZRbV"},{"id":"http://arxiv.org/abs/2310.12000v2","updated":"2024-08-29T16:40:44Z","published":"2023-10-18T14:31:16Z","title":"Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian\n  Process Models","summary":"  Latent Gaussian process (GP) models are flexible probabilistic non-parametric\nfunction models. Vecchia approximations are accurate approximations for GPs to\novercome computational bottlenecks for large data, and the Laplace\napproximation is a fast method with asymptotic convergence guarantees to\napproximate marginal likelihoods and posterior predictive distributions for\nnon-Gaussian likelihoods. Unfortunately, the computational complexity of\ncombined Vecchia-Laplace approximations grows faster than linearly in the\nsample size when used in combination with direct solver methods such as the\nCholesky decomposition. Computations with Vecchia-Laplace approximations can\nthus become prohibitively slow precisely when the approximations are usually\nthe most accurate, i.e., on large data sets. In this article, we present\niterative methods to overcome this drawback. Among other things, we introduce\nand analyze several preconditioners, derive new convergence results, and\npropose novel methods for accurately approximating predictive variances. We\nanalyze our proposed methods theoretically and in experiments with simulated\nand real-world data. In particular, we obtain a speed-up of an order of\nmagnitude compared to Cholesky-based calculations and a threefold increase in\nprediction accuracy in terms of the continuous ranked probability score\ncompared to a state-of-the-art method on a large satellite data set. All\nmethods are implemented in a free C++ software library with high-level Python\nand R packages.\n","authors":["Pascal KÃ¼ndig","Fabio Sigrist"],"pdf_url":"https://arxiv.org/pdf/2310.12000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.06829v3","updated":"2024-08-29T16:38:24Z","published":"2022-11-13T06:11:38Z","title":"Methods for Recovering Conditional Independence Graphs: A Survey","summary":"  Conditional Independence (CI) graphs are a type of probabilistic graphical\nmodels that are primarily used to gain insights about feature relationships.\nEach edge represents the partial correlation between the connected features\nwhich gives information about their direct dependence. In this survey, we list\nout different methods and study the advances in techniques developed to recover\nCI graphs. We cover traditional optimization methods as well as recently\ndeveloped deep learning architectures along with their recommended\nimplementations. To facilitate wider adoption, we include preliminaries that\nconsolidate associated operations, for example techniques to obtain covariance\nmatrix for mixed datatypes.\n","authors":["Harsh Shrivastava","Urszula Chajewska"],"pdf_url":"https://arxiv.org/pdf/2211.06829v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16686v1","updated":"2024-08-29T16:32:24Z","published":"2024-08-29T16:32:24Z","title":"CW-CNN & CW-AN: Convolutional Networks and Attention Networks for\n  CW-Complexes","summary":"  We present a novel framework for learning on CW-complex structured data\npoints. Recent advances have discussed CW-complexes as ideal learning\nrepresentations for problems in cheminformatics. However, there is a lack of\navailable machine learning methods suitable for learning on CW-complexes. In\nthis paper we develop notions of convolution and attention that are well\ndefined for CW-complexes. These notions enable us to create the first neural\nnetwork that can receive a CW-complex as input. We illustrate and interpret\nthis framework in the context of supervised prediction.\n","authors":["Rahul Khorana"],"pdf_url":"https://arxiv.org/pdf/2408.16686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16683v1","updated":"2024-08-29T16:28:43Z","published":"2024-08-29T16:28:43Z","title":"A Catalog of Fairness-Aware Practices in Machine Learning Engineering","summary":"  Machine learning's widespread adoption in decision-making processes raises\nconcerns about fairness, particularly regarding the treatment of sensitive\nfeatures and potential discrimination against minorities. The software\nengineering community has responded by developing fairness-oriented metrics,\nempirical studies, and approaches. However, there remains a gap in\nunderstanding and categorizing practices for engineering fairness throughout\nthe machine learning lifecycle. This paper presents a novel catalog of\npractices for addressing fairness in machine learning derived from a systematic\nmapping study. The study identifies and categorizes 28 practices from existing\nliterature, mapping them onto different stages of the machine learning\nlifecycle. From this catalog, the authors extract actionable items and\nimplications for both researchers and practitioners in software engineering.\nThis work aims to provide a comprehensive resource for integrating fairness\nconsiderations into the development and deployment of machine learning systems,\nenhancing their reliability, accountability, and credibility.\n","authors":["Gianmario Voria","Giulia Sellitto","Carmine Ferrara","Francesco Abate","Andrea De Lucia","Filomena Ferrucci","Gemma Catolino","Fabio Palomba"],"pdf_url":"https://arxiv.org/pdf/2408.16683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16673v1","updated":"2024-08-29T16:21:00Z","published":"2024-08-29T16:21:00Z","title":"Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less\n  Overfitting and Better Diversity","summary":"  Large language models rely on Supervised Fine-Tuning (SFT) to specialize in\ndownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it\noften leads to overfitting and limited output diversity due to its aggressive\nupdates to the data distribution. This paper aim to address these issues by\nintroducing the maximum entropy principle, which favors models with flatter\ndistributions that still effectively capture the data. Specifically, we develop\na new distribution matching method called GEM, which solves reverse\nKullback-Leibler divergence minimization with an entropy regularizer.\n  For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.\nFirst, when applied to the UltraFeedback dataset to develop general\ninstruction-following abilities, GEM exhibits reduced overfitting, evidenced by\nlower perplexity and better performance on the IFEval benchmark. Furthermore,\nGEM enhances output diversity, leading to performance gains of up to 7 points\non math reasoning and code generation tasks using best-of-n sampling, even\nwithout domain-specific data. Second, when fine-tuning with domain-specific\ndatasets for math reasoning and code generation, GEM also shows less\noverfitting and improvements of up to 10 points compared with CE.\n","authors":["Ziniu Li","Congliang Chen","Tian Xu","Zeyu Qin","Jiancong Xiao","Ruoyu Sun","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2408.16673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16667v1","updated":"2024-08-29T16:15:01Z","published":"2024-08-29T16:15:01Z","title":"Iterative Graph Alignment","summary":"  By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment.\n","authors":["Fangyuan Yu","Hardeep Singh Arora","Matt Johnson"],"pdf_url":"https://arxiv.org/pdf/2408.16667v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.15096v2","updated":"2024-08-29T15:59:13Z","published":"2024-08-27T14:26:56Z","title":"Post-processing fairness with minimal changes","summary":"  In this paper, we introduce a novel post-processing algorithm that is both\nmodel-agnostic and does not require the sensitive attribute at test time. In\naddition, our algorithm is explicitly designed to enforce minimal changes\nbetween biased and debiased predictions; a property that, while highly\ndesirable, is rarely prioritized as an explicit objective in fairness\nliterature. Our approach leverages a multiplicative factor applied to the logit\nvalue of probability scores produced by a black-box classifier. We demonstrate\nthe efficacy of our method through empirical evaluations, comparing its\nperformance against other four debiasing algorithms on two widely used datasets\nin fairness research.\n","authors":["Federico Di Gennaro","Thibault Laugel","Vincent Grari","Xavier Renard","Marcin Detyniecki"],"pdf_url":"https://arxiv.org/pdf/2408.15096v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04559v2","updated":"2024-08-29T15:58:09Z","published":"2024-07-05T14:48:15Z","title":"Not (yet) the whole story: Evaluating Visual Storytelling Requires More\n  than Measuring Coherence, Grounding, and Repetition","summary":"  Visual storytelling consists in generating a natural language story given a\ntemporally ordered sequence of images. This task is not only challenging for\nmodels, but also very difficult to evaluate with automatic metrics since there\nis no consensus about what makes a story 'good'. In this paper, we introduce a\nnovel method that measures story quality in terms of human likeness regarding\nthree key aspects highlighted in previous work: visual grounding, coherence,\nand repetitiveness. We then use this method to evaluate the stories generated\nby several models, showing that the foundation model LLaVA obtains the best\nresult, but only slightly so compared to TAPM, a 50-times smaller visual\nstorytelling model. Upgrading the visual and language components of TAPM\nresults in a model that yields competitive performance with a relatively low\nnumber of parameters. Finally, we carry out a human evaluation study, whose\nresults suggest that a 'good' story may require more than a human-like level of\nvisual grounding, coherence, and repetition.\n","authors":["Aditya K Surikuchi","Raquel FernÃ¡ndez","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2407.04559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16653v1","updated":"2024-08-29T15:56:22Z","published":"2024-08-29T15:56:22Z","title":"Optimal Parallelization of Boosting","summary":"  Recent works on the parallel complexity of Boosting have established strong\nlower bounds on the tradeoff between the number of training rounds $p$ and the\ntotal parallel work per round $t$. These works have also presented highly\nnon-trivial parallel algorithms that shed light on different regions of this\ntradeoff. Despite these advancements, a significant gap persists between the\ntheoretical lower bounds and the performance of these algorithms across much of\nthe tradeoff space. In this work, we essentially close this gap by providing\nboth improved lower bounds on the parallel complexity of weak-to-strong\nlearners, and a parallel Boosting algorithm whose performance matches these\nbounds across the entire $p$ vs.~$t$ compromise spectrum, up to logarithmic\nfactors. Ultimately, this work settles the true parallel complexity of Boosting\nalgorithms that are nearly sample-optimal.\n","authors":["Arthur da Cunha","Mikael MÃ¸ller HÃ¸gsgaard","Kasper Green Larsen"],"pdf_url":"https://arxiv.org/pdf/2408.16653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16650v1","updated":"2024-08-29T15:55:27Z","published":"2024-08-29T15:55:27Z","title":"Towards Efficient Modelling of String Dynamics: A Comparison of State\n  Space and Koopman based Deep Learning Methods","summary":"  This paper presents an examination of State Space Models (SSM) and\nKoopman-based deep learning methods for modelling the dynamics of both linear\nand non-linear stiff strings. Through experiments with datasets generated under\ndifferent initial conditions and sample rates, we assess the capacity of these\nmodels to accurately model the complex behaviours observed in string dynamics.\nOur findings indicate that our proposed Koopman-based model performs as well as\nor better than other existing approaches in non-linear cases for long-sequence\nmodelling.\n  We inform the design of these architectures with the structure of the\nproblems at hand. Although challenges remain in extending model predictions\nbeyond the training horizon (i.e., extrapolation), the focus of our\ninvestigation lies in the models' ability to generalise across different\ninitial conditions within the training time interval. This research contributes\ninsights into the physical modelling of dynamical systems (in particular those\naddressing musical acoustics) by offering a comparative overview of these and\nprevious methods and introducing innovative strategies for model improvement.\nOur results highlight the efficacy of these models in simulating non-linear\ndynamics and emphasise their wide-ranging applicability in accurately modelling\ndynamical systems over extended sequences.\n","authors":["Rodrigo Diaz","Carlos De La Vega Martin","Mark Sandler"],"pdf_url":"https://arxiv.org/pdf/2408.16650v1.pdf","comment":"Accepted to DAFx2024"},{"id":"http://arxiv.org/abs/2405.00846v3","updated":"2024-08-29T15:53:29Z","published":"2024-05-01T20:21:44Z","title":"Gameplay Filters: Robust Zero-Shot Safety through Adversarial\n  Imagination","summary":"  Despite the impressive recent advances in learning-based robot control,\nensuring robustness to out-of-distribution conditions remains an open\nchallenge. Safety filters can, in principle, keep arbitrary control policies\nfrom incurring catastrophic failures by overriding unsafe actions, but existing\nsolutions for complex (e.g., legged) robot dynamics do not span the full motion\nenvelope and instead rely on local, reduced-order models. These filters tend to\noverly restrict agility and can still fail when perturbed away from nominal\nconditions. This paper presents the gameplay filter, a new class of predictive\nsafety filter that continually plays out hypothetical matches between its\nsimulation-trained safety strategy and a virtual adversary co-trained to invoke\nworst-case events and sim-to-real error, and precludes actions that would cause\nit to fail down the line. We demonstrate the scalability and robustness of the\napproach with a first-of-its-kind full-order safety filter for (36-D)\nquadrupedal dynamics. Physical experiments on two different quadruped platforms\ndemonstrate the superior zero-shot effectiveness of the gameplay filter under\nlarge perturbations such as tugging and unmodeled terrain.\n","authors":["Duy P. Nguyen","Kai-Chieh Hsu","Wenhao Yu","Jie Tan","Jaime F. Fisac"],"pdf_url":"https://arxiv.org/pdf/2405.00846v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16638v1","updated":"2024-08-29T15:42:06Z","published":"2024-08-29T15:42:06Z","title":"3D Pose-Based Temporal Action Segmentation for Figure Skating: A\n  Fine-Grained and Jump Procedure-Aware Annotation Approach","summary":"  Understanding human actions from videos is essential in many domains,\nincluding sports. In figure skating, technical judgments are performed by\nwatching skaters' 3D movements, and its part of the judging procedure can be\nregarded as a Temporal Action Segmentation (TAS) task. TAS tasks in figure\nskating that automatically assign temporal semantics to video are actively\nresearched. However, there is a lack of datasets and effective methods for TAS\ntasks requiring 3D pose data. In this study, we first created the FS-Jump3D\ndataset of complex and dynamic figure skating jumps using optical markerless\nmotion capture. We also propose a new fine-grained figure skating jump TAS\ndataset annotation method with which TAS models can learn jump procedures. In\nthe experimental results, we validated the usefulness of 3D pose features as\ninput and the fine-grained dataset for the TAS model in figure skating.\nFS-Jump3D Dataset is available at https://github.com/ryota-skating/FS-Jump3D.\n","authors":["Ryota Tanaka","Tomohiro Suzuki","Keisuke Fujii"],"pdf_url":"https://arxiv.org/pdf/2408.16638v1.pdf","comment":"10 pages, 7th ACM International Workshop on Multimedia Content\n  Analysis in Sports"},{"id":"http://arxiv.org/abs/2405.14469v2","updated":"2024-08-29T15:41:52Z","published":"2024-05-23T11:56:05Z","title":"Generalization of Hamiltonian algorithms","summary":"  The paper proves generalization results for a class of stochastic learning\nalgorithms. The method applies whenever the algorithm generates an absolutely\ncontinuous distribution relative to some a-priori measure and the Radon Nikodym\nderivative has subgaussian concentration. Applications are bounds for the Gibbs\nalgorithm and randomizations of stable deterministic algorithms as well as\nPAC-Bayesian bounds with data-dependent priors.\n","authors":["Andreas Maurer"],"pdf_url":"https://arxiv.org/pdf/2405.14469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20743v2","updated":"2024-08-29T15:31:58Z","published":"2024-05-31T10:13:17Z","title":"Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent\n  Codes","summary":"  Trajectory forecasting is crucial for video surveillance analytics, as it\nenables the anticipation of future movements for a set of agents, e.g.\nbasketball players engaged in intricate interactions with long-term intentions.\nDeep generative models offer a natural learning approach for trajectory\nforecasting, yet they encounter difficulties in achieving an optimal balance\nbetween sampling fidelity and diversity. We address this challenge by\nleveraging Vector Quantized Variational Autoencoders (VQ-VAEs), which utilize a\ndiscrete latent space to tackle the issue of posterior collapse. Specifically,\nwe introduce an instance-based codebook that allows tailored latent\nrepresentations for each example. In a nutshell, the rows of the codebook are\ndynamically adjusted to reflect contextual information (i.e., past motion\npatterns extracted from the observed trajectories). In this way, the\ndiscretization process gains flexibility, leading to improved reconstructions.\nNotably, instance-level dynamics are injected into the codebook through\nlow-rank updates, which restrict the customization of the codebook to a lower\ndimension space. The resulting discrete space serves as the basis of the\nsubsequent step, which regards the training of a diffusion-based predictive\nmodel. We show that such a two-fold framework, augmented with instance-level\ndiscretization, leads to accurate and diverse forecasts, yielding\nstate-of-the-art performance on three established benchmarks.\n","authors":["Riccardo Benaglia","Angelo Porrello","Pietro Buzzega","Simone Calderara","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2405.20743v2.pdf","comment":"15 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2408.16623v1","updated":"2024-08-29T15:31:51Z","published":"2024-08-29T15:31:51Z","title":"Turbulence Strength $C_n^2$ Estimation from Video using Physics-based\n  Deep Learning","summary":"  Images captured from a long distance suffer from dynamic image distortion due\nto turbulent flow of air cells with random temperatures, and thus refractive\nindices. This phenomenon, known as image dancing, is commonly characterized by\nits refractive-index structure constant $C_n^2$ as a measure of the turbulence\nstrength. For many applications such as atmospheric forecast model,\nlong-range/astronomy imaging, and aviation safety, optical communication\ntechnology, $C_n^2$ estimation is critical for accurately sensing the turbulent\nenvironment. Previous methods for $C_n^2$ estimation include estimation from\nmeteorological data (temperature, relative humidity, wind shear, etc.) for\nsingle-point measurements, two-ended pathlength measurements from optical\nscintillometer for path-averaged $C_n^2$, and more recently estimating $C_n^2$\nfrom passive video cameras for low cost and hardware complexity. In this paper,\nwe present a comparative analysis of classical image gradient methods for\n$C_n^2$ estimation and modern deep learning-based methods leveraging\nconvolutional neural networks. To enable this, we collect a dataset of video\ncapture along with reference scintillometer measurements for ground truth, and\nwe release this unique dataset to the scientific community. We observe that\ndeep learning methods can achieve higher accuracy when trained on similar data,\nbut suffer from generalization errors to other, unseen imagery as compared to\nclassical methods. To overcome this trade-off, we present a novel physics-based\nnetwork architecture that combines learned convolutional layers with a\ndifferentiable image gradient method that maintains high accuracy while being\ngeneralizable across image datasets.\n","authors":["Ripon Kumar Saha","Esen Salcin","Jihoo Kim","Joseph Smith","Suren Jayasuriya"],"pdf_url":"https://arxiv.org/pdf/2408.16623v1.pdf","comment":"Code Available: https://github.com/Riponcs/Cn2Estimation"},{"id":"http://arxiv.org/abs/2408.13140v2","updated":"2024-08-29T15:31:35Z","published":"2024-08-23T15:02:09Z","title":"Verification of Geometric Robustness of Neural Networks via Piecewise\n  Linear Approximation and Lipschitz Optimisation","summary":"  We address the problem of verifying neural networks against geometric\ntransformations of the input image, including rotation, scaling, shearing, and\ntranslation. The proposed method computes provably sound piecewise linear\nconstraints for the pixel values by using sampling and linear approximations in\ncombination with branch-and-bound Lipschitz optimisation. The method obtains\nprovably tighter over-approximations of the perturbation region than the\npresent state-of-the-art. We report results from experiments on a comprehensive\nset of verification benchmarks on MNIST and CIFAR10. We show that our proposed\nimplementation resolves up to 32% more verification cases than present\napproaches.\n","authors":["Ben Batten","Yang Zheng","Alessandro De Palma","Panagiotis Kouvaros","Alessio Lomuscio"],"pdf_url":"https://arxiv.org/pdf/2408.13140v2.pdf","comment":"ECAI 2024"},{"id":"http://arxiv.org/abs/2408.16621v1","updated":"2024-08-29T15:28:42Z","published":"2024-08-29T15:28:42Z","title":"Towards Infusing Auxiliary Knowledge for Distracted Driver Detection","summary":"  Distracted driving is a leading cause of road accidents globally.\nIdentification of distracted driving involves reliably detecting and\nclassifying various forms of driver distraction (e.g., texting, eating, or\nusing in-car devices) from in-vehicle camera feeds to enhance road safety. This\ntask is challenging due to the need for robust models that can generalize to a\ndiverse set of driver behaviors without requiring extensive annotated datasets.\nIn this paper, we propose KiD3, a novel method for distracted driver detection\n(DDD) by infusing auxiliary knowledge about semantic relations between entities\nin a scene and the structural configuration of the driver's pose. Specifically,\nwe construct a unified framework that integrates the scene graphs, and driver\npose information with the visual cues in video frames to create a holistic\nrepresentation of the driver's actions.Our results indicate that KiD3 achieves\na 13.64% accuracy improvement over the vision-only baseline by incorporating\nsuch auxiliary knowledge with visual information.\n","authors":["Ishwar B Balappanawar","Ashmit Chamoli","Ruwan Wickramarachchi","Aditya Mishra","Ponnurangam Kumaraguru","Amit P. Sheth"],"pdf_url":"https://arxiv.org/pdf/2408.16621v1.pdf","comment":"Accepted at KiL 2024: Workshop on Knowledge-infused Learning\n  co-located with 30th ACM KDD Conference"},{"id":"http://arxiv.org/abs/2408.16620v1","updated":"2024-08-29T15:28:01Z","published":"2024-08-29T15:28:01Z","title":"Hyperdimensional Vector Tsetlin Machines with Applications to Sequence\n  Learning and Generation","summary":"  We construct a two-layered model for learning and generating sequential data\nthat is both computationally fast and competitive with vanilla Tsetlin\nmachines, adding numerous advantages. Through the use of hyperdimensional\nvector computing (HVC) algebras and Tsetlin machine clause structures, we\ndemonstrate that the combination of both inherits the generality of data\nencoding and decoding of HVC with the fast interpretable nature of Tsetlin\nmachines to yield a powerful machine learning model. We apply the approach in\ntwo areas, namely in forecasting, generating new sequences, and classification.\nFor the latter, we derive results for the entire UCR Time Series Archive and\ncompare with the standard benchmarks to see how well the method competes in\ntime series classification.\n","authors":["Christian D. Blakely"],"pdf_url":"https://arxiv.org/pdf/2408.16620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16613v1","updated":"2024-08-29T15:20:17Z","published":"2024-08-29T15:20:17Z","title":"Blending Low and High-Level Semantics of Time Series for Better Masked\n  Time Series Generation","summary":"  State-of-the-art approaches in time series generation (TSG), such as\nTimeVQVAE, utilize vector quantization-based tokenization to effectively model\ncomplex distributions of time series. These approaches first learn to transform\ntime series into a sequence of discrete latent vectors, and then a prior model\nis learned to model the sequence. The discrete latent vectors, however, only\ncapture low-level semantics (\\textit{e.g.,} shapes). We hypothesize that\nhigher-fidelity time series can be generated by training a prior model on more\ninformative discrete latent vectors that contain both low and high-level\nsemantics (\\textit{e.g.,} characteristic dynamics). In this paper, we introduce\na novel framework, termed NC-VQVAE, to integrate self-supervised learning into\nthose TSG methods to derive a discrete latent space where low and high-level\nsemantics are captured. Our experimental results demonstrate that NC-VQVAE\nresults in a considerable improvement in the quality of synthetic samples.\n","authors":["Johan Vik Mathisen","Erlend Lokna","Daesoo Lee","Erlend Aune"],"pdf_url":"https://arxiv.org/pdf/2408.16613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16612v1","updated":"2024-08-29T15:19:06Z","published":"2024-08-29T15:19:06Z","title":"Data Quality Monitoring through Transfer Learning on Anomaly Detection\n  for the Hadron Calorimeters","summary":"  The proliferation of sensors brings an immense volume of spatio-temporal (ST)\ndata in many domains for various purposes, including monitoring, diagnostics,\nand prognostics applications. Data curation is a time-consuming process for a\nlarge volume of data, making it challenging and expensive to deploy data\nanalytics platforms in new environments. Transfer learning (TL) mechanisms\npromise to mitigate data sparsity and model complexity by utilizing pre-trained\nmodels for a new task. Despite the triumph of TL in fields like computer vision\nand natural language processing, efforts on complex ST models for anomaly\ndetection (AD) applications are limited. In this study, we present the\npotential of TL within the context of AD for the Hadron Calorimeter of the\nCompact Muon Solenoid experiment at CERN. We have transferred the ST AD models\ntrained on data collected from one part of a calorimeter to another. We have\ninvestigated different configurations of TL on semi-supervised autoencoders of\nthe ST AD models -- transferring convolutional, graph, and recurrent neural\nnetworks of both the encoder and decoder networks. The experiment results\ndemonstrate that TL effectively enhances the model learning accuracy on a\ntarget subdetector. The TL achieves promising data reconstruction and AD\nperformance while substantially reducing the trainable parameters of the AD\nmodels. It also improves robustness against anomaly contamination in the\ntraining data sets of the semi-supervised AD models.\n","authors":["Mulugeta Weldezgina Asres","Christian Walter Omlin","Long Wang","Pavel Parygin","David Yu","Jay Dittmann","The CMS-HCAL Collaboration"],"pdf_url":"https://arxiv.org/pdf/2408.16612v1.pdf","comment":"28 pages, 15 figures, and 9 tables"},{"id":"http://arxiv.org/abs/2408.16605v1","updated":"2024-08-29T15:14:52Z","published":"2024-08-29T15:14:52Z","title":"Subspace Representation Learning for Sparse Linear Arrays to Localize\n  More Sources than Sensors: A Deep Learning Methodology","summary":"  Localizing more sources than sensors with a sparse linear array (SLA) has\nlong relied on minimizing a distance between two covariance matrices and recent\nalgorithms often utilize semidefinite programming (SDP). Although deep neural\nnetwork (DNN)-based methods offer new alternatives, they still depend on\ncovariance matrix fitting. In this paper, we develop a novel methodology that\nestimates the co-array subspaces from a sample covariance for SLAs. Our\nmethodology trains a DNN to learn signal and noise subspace representations\nthat are invariant to the selection of bases. To learn such representations, we\npropose loss functions that gauge the separation between the desired and the\nestimated subspace. In particular, we propose losses that measure the length of\nthe shortest path between subspaces viewed on a union of Grassmannians, and\nprove that it is possible for a DNN to approximate signal subspaces. The\ncomputation of learning subspaces of different dimensions is accelerated by a\nnew batch sampling strategy called consistent rank sampling. The methodology is\nrobust to array imperfections due to its geometry-agnostic and data-driven\nnature. In addition, we propose a fully end-to-end gridless approach that\ndirectly learns angles to study the possibility of bypassing subspace methods.\nNumerical results show that learning such subspace representations is more\nbeneficial than learning covariances or angles. It outperforms conventional\nSDP-based methods such as the sparse and parametric approach (SPA) and existing\nDNN-based covariance reconstruction methods for a wide range of signal-to-noise\nratios (SNRs), snapshots, and source numbers for both perfect and imperfect\narrays.\n","authors":["Kuan-Lin Chen","Bhaskar D. Rao"],"pdf_url":"https://arxiv.org/pdf/2408.16605v1.pdf","comment":"13 pages. Submitted to the IEEE Transactions on Signal Processing"},{"id":"http://arxiv.org/abs/2401.12972v3","updated":"2024-08-29T15:11:29Z","published":"2024-01-23T18:58:35Z","title":"On the Efficacy of Text-Based Input Modalities for Action Anticipation","summary":"  Anticipating future actions is a highly challenging task due to the diversity\nand scale of potential future actions; yet, information from different\nmodalities help narrow down plausible action choices. Each modality can provide\ndiverse and often complementary context for the model to learn from. While\nprevious multi-modal methods leverage information from modalities such as video\nand audio, we primarily explore how text descriptions of actions and objects\ncan also lead to more accurate action anticipation by providing additional\ncontextual cues, e.g., about the environment and its contents. We propose a\nMulti-modal Contrastive Anticipative Transformer (M-CAT), a video transformer\narchitecture that jointly learns from multi-modal features and text\ndescriptions of actions and objects. We train our model in two stages, where\nthe model first learns to align video clips with descriptions of future\nactions, and is subsequently fine-tuned to predict future actions. Compared to\nexisting methods, M-CAT has the advantage of learning additional context from\ntwo types of text inputs: rich descriptions of future actions during\npre-training, and, text descriptions for detected objects and actions during\nmodality feature fusion. Through extensive experimental evaluation, we\ndemonstrate that our model outperforms previous methods on the EpicKitchens\ndatasets, and show that using simple text descriptions of actions and objects\naid in more effective action anticipation. In addition, we examine the impact\nof object and action information obtained via text, and perform extensive\nablations.\n","authors":["Apoorva Beedu","Harish Haresamudram","Karan Samel","Irfan Essa"],"pdf_url":"https://arxiv.org/pdf/2401.12972v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16599v1","updated":"2024-08-29T15:09:04Z","published":"2024-08-29T15:09:04Z","title":"sEMG-Driven Physics-Informed Gated Recurrent Networks for Modeling Upper\n  Limb Multi-Joint Movement Dynamics","summary":"  Exoskeletons and rehabilitation systems offer great potential for enhancing\nhuman strength and recovery through advanced human-machine interfaces (HMIs)\nthat adapt to movement dynamics. However, the real-time application of\nphysics-informed neural networks (PINNs) is limited by their reliance on fixed\ninput lengths and surrogate models. This study introduces a novel\nphysics-informed Gated Recurrent Network (PiGRN) designed to predict\nmulti-joint torques using surface electromyography (sEMG) data. The PiGRN model\nemploys a Gated Recurrent Unit (GRU) to convert time-series sEMG inputs into\nmulti-joint kinematics and external loads, which are then integrated into an\nequation of motion to ensure consistency with physical laws. Experimental\nvalidation with sEMG data from five participants performing elbow\nflexion-extension tasks showed that the PiGRN model accurately predicted joint\ntorques for 10 unfamiliar movements, with RMSE values between 4.02\\% and\n11.40\\% and correlation coefficients ranging from 0.87 to 0.98. These findings\nhighlight the PiGRN's potential for real-time exoskeleton and rehabilitation\napplications. Future research will explore more diverse datasets, improve\nmusculoskeletal models, and investigate unsupervised learning methods.\n","authors":["Rajnish Kumar","Anand Gupta","Suriya Prakash Muthukrishnan","Lalan Kumar","Sitikantha Roy"],"pdf_url":"https://arxiv.org/pdf/2408.16599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16592v1","updated":"2024-08-29T14:55:33Z","published":"2024-08-29T14:55:33Z","title":"High-Dimensional Sparse Data Low-rank Representation via Accelerated\n  Asynchronous Parallel Stochastic Gradient Descent","summary":"  Data characterized by high dimensionality and sparsity are commonly used to\ndescribe real-world node interactions. Low-rank representation (LR) can map\nhigh-dimensional sparse (HDS) data to low-dimensional feature spaces and infer\nnode interactions via modeling data latent associations. Unfortunately,\nexisting optimization algorithms for LR models are computationally inefficient\nand slowly convergent on large-scale datasets. To address this issue, this\npaper proposes an Accelerated Asynchronous Parallel Stochastic Gradient Descent\nA2PSGD for High-Dimensional Sparse Data Low-rank Representation with three\nfold-ideas: a) establishing a lock-free scheduler to simultaneously respond to\nscheduling requests from multiple threads; b) introducing a greedy\nalgorithm-based load balancing strategy for balancing the computational load\namong threads; c) incorporating Nesterov's accelerated gradient into the\nlearning scheme to accelerate model convergence. Empirical studies show that\nA2PSGD outperforms existing optimization algorithms for HDS data LR in both\naccuracy and training time.\n","authors":["Qicong Hu","Hao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16589v1","updated":"2024-08-29T14:52:42Z","published":"2024-08-29T14:52:42Z","title":"CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions","summary":"  We demonstrate that carefully adjusting the tokenizer of the Whisper speech\nrecognition model significantly improves the precision of word-level timestamps\nwhen applying dynamic time warping to the decoder's cross-attention scores. We\nfine-tune the model to produce more verbatim speech transcriptions and employ\nseveral techniques to increase robustness against multiple speakers and\nbackground noise. These adjustments achieve state-of-the-art performance on\nbenchmarks for verbatim speech transcription, word segmentation, and the timed\ndetection of filler events, and can further mitigate transcription\nhallucinations. The code is available open\nhttps://github.com/nyrahealth/CrisperWhisper.\n","authors":["Laurin Wagner","Bernhard Thallinger","Mario Zusag"],"pdf_url":"https://arxiv.org/pdf/2408.16589v1.pdf","comment":"Published at INTERSPEECH2024"},{"id":"http://arxiv.org/abs/2308.11375v2","updated":"2024-08-29T14:45:26Z","published":"2023-08-22T12:01:49Z","title":"Standardized Interpretable Fairness Measures for Continuous Risk Scores","summary":"  We propose a standardized version of fairness measures for continuous scores\nwith a reasonable interpretation based on the Wasserstein distance. Our\nmeasures are easily computable and well suited for quantifying and interpreting\nthe strength of group disparities as well as for comparing biases across\ndifferent models, datasets, or time points. We derive a link between the\ndifferent families of existing fairness measures for scores and show that the\nproposed standardized fairness measures outperform ROC-based fairness measures\nbecause they are more explicit and can quantify significant biases that\nROC-based fairness measures miss.\n","authors":["Ann-Kristin Becker","Oana Dumitrasc","Klaus Broelemann"],"pdf_url":"https://arxiv.org/pdf/2308.11375v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16578v1","updated":"2024-08-29T14:44:12Z","published":"2024-08-29T14:44:12Z","title":"Transformers Meet ACT-R: Repeat-Aware and Sequential Listening Session\n  Recommendation","summary":"  Music streaming services often leverage sequential recommender systems to\npredict the best music to showcase to users based on past sequences of\nlistening sessions. Nonetheless, most sequential recommendation methods ignore\nor insufficiently account for repetitive behaviors. This is a crucial\nlimitation for music recommendation, as repeatedly listening to the same song\nover time is a common phenomenon that can even change the way users perceive\nthis song. In this paper, we introduce PISA (Psychology-Informed Session\nembedding using ACT-R), a session-level sequential recommender system that\novercomes this limitation. PISA employs a Transformer architecture learning\nembedding representations of listening sessions and users using attention\nmechanisms inspired by Anderson's ACT-R (Adaptive Control of Thought-Rational),\na cognitive architecture modeling human information access and memory dynamics.\nThis approach enables us to capture dynamic and repetitive patterns from user\nbehaviors, allowing us to effectively predict the songs they will listen to in\nsubsequent sessions, whether they are repeated or new ones. We demonstrate the\nempirical relevance of PISA using both publicly available listening data from\nLast.fm and proprietary data from Deezer, a global music streaming service,\nconfirming the critical importance of repetition modeling for sequential\nlistening session recommendation. Along with this paper, we publicly release\nour proprietary dataset to foster future research in this field, as well as the\nsource code of PISA to facilitate its future use.\n","authors":["Viet-Anh Tran","Guillaume Salha-Galvan","Bruno Sguerra","Romain Hennequin"],"pdf_url":"https://arxiv.org/pdf/2408.16578v1.pdf","comment":"11 pages. Accepted by RecSys'2024, full paper"},{"id":"http://arxiv.org/abs/2408.16577v1","updated":"2024-08-29T14:43:42Z","published":"2024-08-29T14:43:42Z","title":"Seeking the Sufficiency and Necessity Causal Features in Multimodal\n  Representation Learning","summary":"  Learning representations with a high Probability of Necessary and Sufficient\nCauses (PNS) has been shown to enhance deep learning models' ability. This task\ninvolves identifying causal features that are both sufficient (guaranteeing the\noutcome) and necessary (without which the outcome cannot occur). However,\ncurrent research predominantly focuses on unimodal data, and extending PNS\nlearning to multimodal settings presents significant challenges. The challenges\narise as the conditions for PNS identifiability, Exogeneity and Monotonicity,\nneed to be reconsidered in a multimodal context, where sufficient and necessary\ncausal features are distributed across different modalities. To address this,\nwe first propose conceptualizing multimodal representations as comprising\nmodality-invariant and modality-specific components. We then analyze PNS\nidentifiability for each component, while ensuring non-trivial PNS estimation.\nFinally, we formulate tractable optimization objectives that enable multimodal\nmodels to learn high-PNS representations, thereby enhancing their predictive\nperformance. Experiments demonstrate the effectiveness of our method on both\nsynthetic and real-world data.\n","authors":["Boyu Chen","Junjie Liu","Zhu Li","Mengyue yang"],"pdf_url":"https://arxiv.org/pdf/2408.16577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16573v1","updated":"2024-08-29T14:40:32Z","published":"2024-08-29T14:40:32Z","title":"An Adaptive Latent Factorization of Tensors Model for Embedding Dynamic\n  Communication Network","summary":"  The Dynamic Communication Network (DCN) describes the interactions over time\namong various communication nodes, and it is widely used in Big-data\napplications as a data source. As the number of communication nodes increases\nand temporal slots accumulate, each node interacts in with only a few nodes in\na given temporal slot, the DCN can be represented by an High-Dimensional Sparse\n(HDS) tensor. In order to extract rich behavioral patterns from an HDS tensor\nin DCN, this paper proposes an Adaptive Temporal-dependent Tensor low-rank\nrepresentation (ATT) model. It adopts a three-fold approach: a) designing a\ntemporal-dependent method to reconstruct temporal feature matrix, thereby\nprecisely represent the data by capturing the temporal patterns; b) achieving\nhyper-parameters adaptation of the model via the Differential Evolutionary\nAlgorithms (DEA) to avoid tedious hyper-parameters tuning; c) employing\nnonnegative learning schemes for the model parameters to effectively handle an\nthe nonnegativity inherent in HDS data. The experimental results on four\nreal-world DCNs demonstrate that the proposed ATT model significantly\noutperforms several state-of-the-art models in both prediction errors and\nconvergence rounds.\n","authors":["Xin Liao","Qicong Hu","Peng Tang"],"pdf_url":"https://arxiv.org/pdf/2408.16573v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2210.05506v2","updated":"2024-08-29T14:36:20Z","published":"2022-10-11T14:58:58Z","title":"Follow-up Attention: An Empirical Study of Developer and Neural Model\n  Code Exploration","summary":"  Recent neural models of code, such as OpenAI Codex and AlphaCode, have\ndemonstrated remarkable proficiency at code generation due to the underlying\nattention mechanism. However, it often remains unclear how the models actually\nprocess code, and to what extent their reasoning and the way their attention\nmechanism scans the code matches the patterns of developers. A poor\nunderstanding of the model reasoning process limits the way in which current\nneural models are leveraged today, so far mostly for their raw prediction. To\nfill this gap, this work studies how the processed attention signal of three\nopen large language models - CodeGen, InCoder and GPT-J - agrees with how\ndevelopers look at and explore code when each answers the same sensemaking\nquestions about code. Furthermore, we contribute an open-source eye-tracking\ndataset comprising 92 manually-labeled sessions from 25 developers engaged in\nsensemaking tasks. We empirically evaluate five heuristics that do not use the\nattention and ten attention-based post-processing approaches of the attention\nsignal of CodeGen against our ground truth of developers exploring code,\nincluding the novel concept of follow-up attention which exhibits the highest\nagreement between model and human attention. Our follow-up attention method can\npredict the next line a developer will look at with 47% accuracy. This\noutperforms the baseline prediction accuracy of 42.3%, which uses the session\nhistory of other developers to recommend the next line. These results\ndemonstrate the potential of leveraging the attention signal of pre-trained\nmodels for effective code exploration.\n","authors":["Matteo Paltenghi","Rahul Pandita","Austin Z. Henley","Albert Ziegler"],"pdf_url":"https://arxiv.org/pdf/2210.05506v2.pdf","comment":"Published at IEEE Transactions on Software Engineering"},{"id":"http://arxiv.org/abs/2408.16567v1","updated":"2024-08-29T14:35:14Z","published":"2024-08-29T14:35:14Z","title":"Identifying Terrain Physical Parameters from Vision -- Towards\n  Physical-Parameter-Aware Locomotion and Navigation","summary":"  Identifying the physical properties of the surrounding environment is\nessential for robotic locomotion and navigation to deal with non-geometric\nhazards, such as slippery and deformable terrains. It would be of great benefit\nfor robots to anticipate these extreme physical properties before contact;\nhowever, estimating environmental physical parameters from vision is still an\nopen challenge. Animals can achieve this by using their prior experience and\nknowledge of what they have seen and how it felt. In this work, we propose a\ncross-modal self-supervised learning framework for vision-based environmental\nphysical parameter estimation, which paves the way for future\nphysical-property-aware locomotion and navigation. We bridge the gap between\nexisting policies trained in simulation and identification of physical terrain\nparameters from vision. We propose to train a physical decoder in simulation to\npredict friction and stiffness from multi-modal input. The trained network\nallows the labeling of real-world images with physical parameters in a\nself-supervised manner to further train a visual network during deployment,\nwhich can densely predict the friction and stiffness from image data. We\nvalidate our physical decoder in simulation and the real world using a\nquadruped ANYmal robot, outperforming an existing baseline method. We show that\nour visual network can predict the physical properties in indoor and outdoor\nexperiments while allowing fast adaptation to new environments.\n","authors":["Jiaqi Chen","Jonas Frey","Ruyi Zhou","Takahiro Miki","Georg Martius","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2408.16567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11962v3","updated":"2024-08-29T14:31:58Z","published":"2023-02-23T12:18:28Z","title":"Unified Convergence Theory of Stochastic and Variance-Reduced Cubic\n  Newton Methods","summary":"  We study stochastic Cubic Newton methods for solving general possibly\nnon-convex minimization problems. We propose a new framework, which we call the\nhelper framework, that provides a unified view of the stochastic and\nvariance-reduced second-order algorithms equipped with global complexity\nguarantees. It can also be applied to learning with auxiliary information. Our\nhelper framework offers the algorithm designer high flexibility for\nconstructing and analyzing the stochastic Cubic Newton methods, allowing\narbitrary size batches, and the use of noisy and possibly biased estimates of\nthe gradients and Hessians, incorporating both the variance reduction and the\nlazy Hessian updates. We recover the best-known complexities for the stochastic\nand variance-reduced Cubic Newton, under weak assumptions on the noise. A\ndirect consequence of our theory is the new lazy stochastic second-order\nmethod, which significantly improves the arithmetic complexity for large\ndimension problems. We also establish complexity bounds for the classes of\ngradient-dominated objectives, that include convex and strongly convex\nproblems. For Auxiliary Learning, we show that using a helper (auxiliary\nfunction) can outperform training alone if a given similarity measure is small.\n","authors":["El Mahdi Chayti","Nikita Doikov","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2302.11962v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15099v2","updated":"2024-08-29T14:20:44Z","published":"2024-08-27T14:31:54Z","title":"No Regrets: Investigating and Improving Regret Approximations for\n  Curriculum Discovery","summary":"  What data or environments to use for training to improve downstream\nperformance is a longstanding and very topical question in reinforcement\nlearning. In particular, Unsupervised Environment Design (UED) methods have\ngained recent attention as their adaptive curricula enable agents to be robust\nto in- and out-of-distribution tasks. We ask to what extent these methods are\nthemselves robust when applied to a novel setting, closely inspired by a\nreal-world robotics problem. Surprisingly, we find that the state-of-the-art\nUED methods either do not improve upon the na\\\"{i}ve baseline of Domain\nRandomisation (DR), or require substantial hyperparameter tuning to do so. Our\nanalysis shows that this is due to their underlying scoring functions failing\nto predict intuitive measures of ``learnability'', i.e., in finding the\nsettings that the agent sometimes solves, but not always. Based on this, we\ninstead directly train on levels with high learnability and find that this\nsimple and intuitive approach outperforms UED methods and DR in several\nbinary-outcome environments, including on our domain and the standard UED\ndomain of Minigrid. We further introduce a new adversarial evaluation procedure\nfor directly measuring robustness, closely mirroring the conditional value at\nrisk (CVaR). We open-source all our code and present visualisations of final\npolicies here: https://github.com/amacrutherford/sampling-for-learnability.\n","authors":["Alexander Rutherford","Michael Beukman","Timon Willi","Bruno Lacerda","Nick Hawes","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2408.15099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16555v1","updated":"2024-08-29T14:18:54Z","published":"2024-08-29T14:18:54Z","title":"Android Malware Detection Based on RGB Images and Multi-feature Fusion","summary":"  With the widespread adoption of smartphones, Android malware has become a\nsignificant challenge in the field of mobile device security. Current Android\nmalware detection methods often rely on feature engineering to construct\ndynamic or static features, which are then used for learning. However, static\nfeature-based methods struggle to counter code obfuscation, packing, and\nsigning techniques, while dynamic feature-based methods involve time-consuming\nfeature extraction. Image-based methods for Android malware detection offer\nbetter resilience against malware variants and polymorphic malware. This paper\nproposes an end-to-end Android malware detection technique based on RGB images\nand multi-feature fusion. The approach involves extracting Dalvik Executable\n(DEX) files, AndroidManifest.xml files, and API calls from APK files,\nconverting them into grayscale images, and enhancing their texture features\nusing Canny edge detection, histogram equalization, and adaptive thresholding\ntechniques. These grayscale images are then combined into an RGB image\ncontaining multi-feature fusion information, which is analyzed using mainstream\nimage classification models for Android malware detection. Extensive\nexperiments demonstrate that the proposed method effectively captures Android\nmalware characteristics, achieving an accuracy of up to 97.25%, outperforming\nexisting detection methods that rely solely on DEX files as classification\nfeatures. Additionally, ablation experiments confirm the effectiveness of using\nthe three key files for feature representation in the proposed approach.\n","authors":["Zhiqiang Wang","Qiulong Yu","Sicheng Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.16555v1.pdf","comment":"9 pages,10 figures"},{"id":"http://arxiv.org/abs/2408.16553v1","updated":"2024-08-29T14:16:13Z","published":"2024-08-29T14:16:13Z","title":"Super-Resolution works for coastal simulations","summary":"  Learning fine-scale details of a coastal ocean simulation from a coarse\nrepresentation is a challenging task. For real-world applications,\nhigh-resolution simulations are necessary to advance understanding of many\ncoastal processes, specifically, to predict flooding resulting from tsunamis\nand storm surges. We propose a Deep Network for Coastal Super-Resolution\n(DNCSR) for spatiotemporal enhancement to efficiently learn the high-resolution\nnumerical solution. Given images of coastal simulations produced on\nlow-resolution computational meshes using low polynomial order discontinuous\nGalerkin discretizations and a coarse temporal resolution, the proposed DNCSR\nlearns to produce high-resolution free surface elevation and velocity\nvisualizations in both time and space. To efficiently model the dynamic changes\nover time and space, we propose grid-aware spatiotemporal attention to project\nthe temporal features to the spatial domain for non-local feature matching. The\ncoordinate information is also utilized via positional encoding. For the final\nreconstruction, we use the spatiotemporal bilinear operation to interpolate the\nmissing frames and then expand the feature maps to the frequency domain for\nresidual mapping. Besides data-driven losses, the proposed physics-informed\nloss guarantees gradient consistency and momentum changes. Their combination\ncontributes to the overall 24% improvements in RMSE. To train the proposed\nmodel, we propose a large-scale coastal simulation dataset and use it for model\noptimization and evaluation. Our method shows superior super-resolution quality\nand fast computation compared to the state-of-the-art methods.\n","authors":["Zhi-Song Liu","Markus Buttner","Vadym Aizinger","Andreas Rupp"],"pdf_url":"https://arxiv.org/pdf/2408.16553v1.pdf","comment":"13 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.17844v2","updated":"2024-08-29T14:06:57Z","published":"2024-07-25T07:58:19Z","title":"Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease\n  Classification: A Systematic Review","summary":"  Parkinson's disease (PD), the second most prevalent neurodegenerative\ndisorder worldwide, frequently presents with early-stage speech impairments.\nRecent advancements in Artificial Intelligence (AI), particularly deep learning\n(DL), have significantly enhanced PD diagnosis through the analysis of speech\ndata. Nevertheless, the progress of research is restricted by the limited\navailability of publicly accessible speech-based PD datasets, primarily due to\nprivacy concerns. The goal of this systematic review is to explore the current\nlandscape of speech-based DL approaches for PD classification, based on 33\nscientific works published between 2020 and March 2024. We discuss their\navailable resources, capabilities, potential limitations, and issues related to\nbias, explainability, and privacy. Furthermore, this review provides an\noverview of publicly accessible speech-based datasets and open-source material\nfor PD. The DL approaches are categorized into end-to-end (E2E) learning,\ntransfer learning (TL) and deep acoustic features extraction (DAFE) approaches.\nAmong E2E approaches, Convolutional Neural Networks (CNNs) are prevalent,\nthough Transformers are increasingly popular. E2E approaches face challenges\nsuch as limited data and computational resources, especially with Transformers.\nTL addresses these issues by providing more robust PD diagnosis and better\ngeneralizability across languages. DAFE aims to improve the explainability and\ninterpretability of results by examining the specific effects of deep features\non both other DL approaches and more traditional machine learning (ML) methods.\nHowever, it often underperforms compared to E2E and TL approaches.\n","authors":["Lisanne van Gelderen","Cristian Tejedor-GarcÃ­a"],"pdf_url":"https://arxiv.org/pdf/2407.17844v2.pdf","comment":"Submitted in Applied Sciences - peer reviewed Open Access journal.\n  This research was funded by the NWO research programme AiNed Fellowship\n  Grants under the project Responsible AI for Voice Diagnostics (RAIVD) - grant\n  number NGF.1607.22.013"},{"id":"http://arxiv.org/abs/2408.16543v1","updated":"2024-08-29T14:01:30Z","published":"2024-08-29T14:01:30Z","title":"Statistical and Geometrical properties of regularized Kernel\n  Kullback-Leibler divergence","summary":"  In this paper, we study the statistical and geometrical properties of the\nKullback-Leibler divergence with kernel covariance operators (KKL) introduced\nby Bach [2022]. Unlike the classical Kullback-Leibler (KL) divergence that\ninvolves density ratios, the KKL compares probability distributions through\ncovariance operators (embeddings) in a reproducible kernel Hilbert space\n(RKHS), and compute the Kullback-Leibler quantum divergence. This novel\ndivergence hence shares parallel but different aspects with both the standard\nKullback-Leibler between probability distributions and kernel embeddings\nmetrics such as the maximum mean discrepancy. A limitation faced with the\noriginal KKL divergence is its inability to be defined for distributions with\ndisjoint supports. To solve this problem, we propose in this paper a\nregularised variant that guarantees that the divergence is well defined for all\ndistributions. We derive bounds that quantify the deviation of the regularised\nKKL to the original one, as well as finite-sample bounds. In addition, we\nprovide a closed-form expression for the regularised KKL, specifically\napplicable when the distributions consist of finite sets of points, which makes\nit implementable. Furthermore, we derive a Wasserstein gradient descent scheme\nof the KKL divergence in the case of discrete distributions, and study\nempirically its properties to transport a set of points to a target\ndistribution.\n","authors":["ClÃ©mentine Chazal","Anna Korba","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2408.16543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16542v1","updated":"2024-08-29T14:00:57Z","published":"2024-08-29T14:00:57Z","title":"SALSA: Speedy ASR-LLM Synchronous Aggregation","summary":"  Harnessing pre-trained LLMs to improve ASR systems, particularly for\nlow-resource languages, is now an emerging area of research. Existing methods\nrange from using LLMs for ASR error correction to tightly coupled systems that\nreplace the ASR decoder with the LLM. These approaches either increase decoding\ntime or require expensive training of the cross-attention layers. We propose\nSALSA, which couples the decoder layers of the ASR to the LLM decoder, while\nsynchronously advancing both decoders. Such coupling is performed with a simple\nprojection of the last decoder state, and is thus significantly more training\nefficient than earlier approaches. A challenge of our proposed coupling is\nhandling the mismatch between the tokenizers of the LLM and ASR systems. We\nhandle this mismatch using cascading tokenization with respect to the LLM and\nASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS\nbenchmark, yielding substantial WER reductions of up to 38%.\n","authors":["Ashish Mittal","Darshan Prabhu","Sunita Sarawagi","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2408.16542v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2408.16537v1","updated":"2024-08-29T13:52:28Z","published":"2024-08-29T13:52:28Z","title":"SFR-GNN: Simple and Fast Robust GNNs against Structural Attacks","summary":"  Graph Neural Networks (GNNs) have demonstrated commendable performance for\ngraph-structured data. Yet, GNNs are often vulnerable to adversarial structural\nattacks as embedding generation relies on graph topology. Existing efforts are\ndedicated to purifying the maliciously modified structure or applying adaptive\naggregation, thereby enhancing the robustness against adversarial structural\nattacks. It is inevitable for a defender to consume heavy computational costs\ndue to lacking prior knowledge about modified structures. To this end, we\npropose an efficient defense method, called Simple and Fast Robust Graph Neural\nNetwork (SFR-GNN), supported by mutual information theory. The SFR-GNN first\npre-trains a GNN model using node attributes and then fine-tunes it over the\nmodified graph in the manner of contrastive learning, which is free of\npurifying modified structures and adaptive aggregation, thus achieving great\nefficiency gains. Consequently, SFR-GNN exhibits a 24%--162% speedup compared\nto advanced robust models, demonstrating superior robustness for node\nclassification tasks.\n","authors":["Xing Ai","Guanyu Zhu","Yulin Zhu","Yu Zheng","Gaolei Li","Jianhua Li","Kai Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.16537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16535v1","updated":"2024-08-29T13:50:08Z","published":"2024-08-29T13:50:08Z","title":"TinyTNAS: GPU-Free, Time-Bound, Hardware-Aware Neural Architecture\n  Search for TinyML Time Series Classification","summary":"  In this work, we present TinyTNAS, a novel hardware-aware multi-objective\nNeural Architecture Search (NAS) tool specifically designed for TinyML time\nseries classification. Unlike traditional NAS methods that rely on GPU\ncapabilities, TinyTNAS operates efficiently on CPUs, making it accessible for a\nbroader range of applications. Users can define constraints on RAM, FLASH, and\nMAC operations to discover optimal neural network architectures within these\nparameters. Additionally, the tool allows for time-bound searches, ensuring the\nbest possible model is found within a user-specified duration. By experimenting\nwith benchmark dataset UCI HAR, PAMAP2, WISDM, MIT BIH, and PTB Diagnostic ECG\nDatabas TinyTNAS demonstrates state-of-the-art accuracy with significant\nreductions in RAM, FLASH, MAC usage, and latency. For example, on the UCI HAR\ndataset, TinyTNAS achieves a 12x reduction in RAM usage, a 144x reduction in\nMAC operations, and a 78x reduction in FLASH memory while maintaining superior\naccuracy and reducing latency by 149x. Similarly, on the PAMAP2 and WISDM\ndatasets, it achieves a 6x reduction in RAM usage, a 40x reduction in MAC\noperations, an 83x reduction in FLASH, and a 67x reduction in latency, all\nwhile maintaining superior accuracy. Notably, the search process completes\nwithin 10 minutes in a CPU environment. These results highlight TinyTNAS's\ncapability to optimize neural network architectures effectively for\nresource-constrained TinyML applications, ensuring both efficiency and high\nperformance. The code for TinyTNAS is available at the GitHub repository and\ncan be accessed at https://github.com/BidyutSaha/TinyTNAS.git.\n","authors":["Bidyut Saha","Riya Samanta","Soumya K. Ghosh","Ram Babu Roy"],"pdf_url":"https://arxiv.org/pdf/2408.16535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15640v2","updated":"2024-08-29T13:47:38Z","published":"2024-08-28T08:52:14Z","title":"GANs Conditioning Methods: A Survey","summary":"  In recent years, Generative Adversarial Networks (GANs) have seen significant\nadvancements, leading to their widespread adoption across various fields. The\noriginal GAN architecture enables the generation of images without any specific\ncontrol over the content, making it an unconditional generation process.\nHowever, many practical applications require precise control over the generated\noutput, which has led to the development of conditional GANs (cGANs) that\nincorporate explicit conditioning to guide the generation process. cGANs extend\nthe original framework by incorporating additional information (conditions),\nenabling the generation of samples that adhere to that specific criteria.\nVarious conditioning methods have been proposed, each differing in how they\nintegrate the conditioning information into both the generator and the\ndiscriminator networks. In this work, we review the conditioning methods\nproposed for GANs, exploring the characteristics of each method and\nhighlighting their unique mechanisms and theoretical foundations. Furthermore,\nwe conduct a comparative analysis of these methods, evaluating their\nperformance on various image datasets. Through these analyses, we aim to\nprovide insights into the strengths and limitations of various conditioning\ntechniques, guiding future research and application in generative modeling.\n","authors":["Anis Bourou","Auguste Genovesio","ValÃ©rie Mezger"],"pdf_url":"https://arxiv.org/pdf/2408.15640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16532v1","updated":"2024-08-29T13:43:36Z","published":"2024-08-29T13:43:36Z","title":"WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio\n  Language Modeling","summary":"  Language models have been effectively applied to modeling natural signals,\nsuch as images, video, speech, and audio. A crucial component of these models\nis the codec tokenizer, which compresses high-dimensional natural signals into\nlower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,\nwhich offers several advantages over previous SOTA acoustic codec models in the\naudio domain: 1)extreme compression. By compressing the layers of quantizers\nand the temporal dimension of the discrete codec, one-second audio of 24kHz\nsampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved\nsubjective quality. Despite the reduced number of tokens, WavTokenizer achieves\nstate-of-the-art reconstruction quality with outstanding UTMOS scores and\ninherently contains richer semantic information. Specifically, we achieve these\nresults by designing a broader VQ space, extended contextual windows, and\nimproved attention networks, as well as introducing a powerful multi-scale\ndiscriminator and an inverse Fourier transform structure. We conducted\nextensive reconstruction experiments in the domains of speech, audio, and\nmusic. WavTokenizer exhibited strong performance across various objective and\nsubjective metrics compared to state-of-the-art models. We also tested semantic\ninformation, VQ utilization, and adaptability to generative models.\nComprehensive ablation studies confirm the necessity of each module in\nWavTokenizer. The related code, demos, and pre-trained models are available at\nhttps://github.com/jishengpeng/WavTokenizer.\n","authors":["Shengpeng Ji","Ziyue Jiang","Xize Cheng","Yifu Chen","Minghui Fang","Jialong Zuo","Qian Yang","Ruiqi Li","Ziang Zhang","Xiaoda Yang","Rongjie Huang","Yidi Jiang","Qian Chen","Siqi Zheng","Wen Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.16532v1.pdf","comment":"Working in progress. arXiv admin note: text overlap with\n  arXiv:2402.12208"},{"id":"http://arxiv.org/abs/2408.16527v1","updated":"2024-08-29T13:39:01Z","published":"2024-08-29T13:39:01Z","title":"Multitask learning for improved scour detection: A dynamic wave tank\n  study","summary":"  Population-based structural health monitoring (PBSHM), aims to share\ninformation between members of a population. An offshore wind (OW) farm could\nbe considered as a population of nominally-identical wind-turbine structures.\nHowever, benign variations exist among members, such as geometry, sea-bed\nconditions and temperature differences. These factors could influence\nstructural properties and therefore the dynamic response, making it more\ndifficult to detect structural problems via traditional SHM techniques.\n  This paper explores the use of a Bayesian hierarchical model as a means of\nmultitask learning, to infer foundation stiffness distribution parameters at\nboth population and local levels. To do this, observations of natural frequency\nfrom populations of structures were first generated from both numerical and\nexperimental models. These observations were then used in a partially-pooled\nBayesian hierarchical model in tandem with surrogate FE models of the\nstructures to infer foundation stiffness parameters. Finally, it is\ndemonstrated how the learned parameters may be used as a basis to perform more\nrobust anomaly detection (as compared to a no-pooling approach) e.g. as a\nresult of scour.\n","authors":["Simon M. Brealy","Aidan J. Hughes","Tina A. Dardeno","Lawrence A. Bull","Robin S. Mills","Nikolaos Dervilis","Keith Worden"],"pdf_url":"https://arxiv.org/pdf/2408.16527v1.pdf","comment":"25 pages, 12 figures, early work features in ISWHM 2023 conference\n  proceedings and available here: arXiv:2402.19295. Submitted to the Renewable\n  Energy journal"},{"id":"http://arxiv.org/abs/2408.16517v1","updated":"2024-08-29T13:28:11Z","published":"2024-08-29T13:28:11Z","title":"Adaptive Variational Continual Learning via Task-Heuristic Modelling","summary":"  Variational continual learning (VCL) is a turn-key learning algorithm that\nhas state-of-the-art performance among the best continual learning models. In\nour work, we explore an extension of the generalized variational continual\nlearning (GVCL) model, named AutoVCL, which combines task heuristics for\ninformed learning and model optimization. We demonstrate that our model\noutperforms the standard GVCL with fixed hyperparameters, benefiting from the\nautomatic adjustment of the hyperparameter based on the difficulty and\nsimilarity of the incoming task compared to the previous tasks.\n","authors":["Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2408.16517v1.pdf","comment":"4 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.15126v2","updated":"2024-08-29T13:21:26Z","published":"2024-08-27T15:07:27Z","title":"Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of\n  Peptides","summary":"  Molecular Dynamics (MD) simulations are irreplaceable and ubiquitous in\nfields of materials science, chemistry, pharmacology just to name a few.\nConventional MD simulations are plagued by numerical stability as well as long\nequilibration time issues, which limits broader applications of MD simulations.\nRecently, a surge of deep learning approaches have been devised for\ntime-coarsened dynamics, which learns the state transition mechanism over much\nlarger time scales to overcome these limitations. However, only a few methods\ntarget the underlying Boltzmann distribution by resampling techniques, where\nproposals are rarely accepted as new states with low efficiency. In this work,\nwe propose a force-guided bridge matching model, FBM, a novel framework that\nfirst incorporates physical priors into bridge matching for full-atom\ntime-coarsened dynamics. With the guidance of our well-designed intermediate\nforce field, FBM is feasible to target the Boltzmann-like distribution by\ndirect inference without extra steps. Experiments on small peptides verify our\nsuperiority in terms of comprehensive metrics and demonstrate transferability\nto unseen peptide systems.\n","authors":["Ziyang Yu","Wenbing Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15126v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16495v1","updated":"2024-08-29T12:49:22Z","published":"2024-08-29T12:49:22Z","title":"On-device AI: Quantization-aware Training of Transformers in Time-Series","summary":"  Artificial Intelligence (AI) models for time-series in pervasive computing\nkeep getting larger and more complicated. The Transformer model is by far the\nmost compelling of these AI models. However, it is difficult to obtain the\ndesired performance when deploying such a massive model on a sensor device with\nlimited resources. My research focuses on optimizing the Transformer model for\ntime-series forecasting tasks. The optimized model will be deployed as hardware\naccelerators on embedded Field Programmable Gate Arrays (FPGAs). I will\ninvestigate the impact of applying Quantization-aware Training to the\nTransformer model to reduce its size and runtime memory footprint while\nmaximizing the advantages of FPGAs.\n","authors":["Tianheng Ling","Gregor Schiele"],"pdf_url":"https://arxiv.org/pdf/2408.16495v1.pdf","comment":"This paper is accepted by 2023 IEEE International Conference on\n  Pervasive Computing and Communications(PhD Forum)"},{"id":"http://arxiv.org/abs/2408.16463v1","updated":"2024-08-29T11:51:41Z","published":"2024-08-29T11:51:41Z","title":"An Exploratory Deep Learning Approach for Predicting Subsequent Suicidal\n  Acts in Chinese Psychological Support Hotlines","summary":"  Psychological support hotlines are an effective suicide prevention measure\nthat typically relies on professionals using suicide risk assessment scales to\npredict individual risk scores. However, the accuracy of scale-based predictive\nmethods for suicide risk assessment can vary widely depending on the expertise\nof the operator. This limitation underscores the need for more reliable\nmethods, prompting this research's innovative exploration of the use of\nartificial intelligence to improve the accuracy and efficiency of suicide risk\nprediction within the context of psychological support hotlines. The study\nincluded data from 1,549 subjects from 2015-2017 in China who contacted a\npsychological support hotline. Each participant was followed for 12 months to\nidentify instances of suicidal behavior. We proposed a novel multi-task\nlearning method that uses the large-scale pre-trained model Whisper for feature\nextraction and fits psychological scales while predicting the risk of suicide.\nThe proposed method yields a 2.4\\% points improvement in F1-score compared to\nthe traditional manual approach based on the psychological scales. Our model\ndemonstrated superior performance compared to the other eight popular models.\nTo our knowledge, this study is the first to apply deep learning to long-term\nspeech data to predict suicide risk in China, indicating grate potential for\nclinical applications. The source code is publicly available at:\n\\url{https://github.com/songchangwei/Suicide-Risk-Prediction}.\n","authors":["Changwei Song","Qing Zhao","Jianqiang Li","Yining Chen","Yongsheng Tong","Guanghui Fu"],"pdf_url":"https://arxiv.org/pdf/2408.16463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16457v1","updated":"2024-08-29T11:45:01Z","published":"2024-08-29T11:45:01Z","title":"HYGENE: A Diffusion-based Hypergraph Generation Method","summary":"  Hypergraphs are powerful mathematical structures that can model complex,\nhigh-order relationships in various domains, including social networks,\nbioinformatics, and recommender systems. However, generating realistic and\ndiverse hypergraphs remains challenging due to their inherent complexity and\nlack of effective generative models. In this paper, we introduce a\ndiffusion-based Hypergraph Generation (HYGENE) method that addresses these\nchallenges through a progressive local expansion approach. HYGENE works on the\nbipartite representation of hypergraphs, starting with a single pair of\nconnected nodes and iteratively expanding it to form the target hypergraph. At\neach step, nodes and hyperedges are added in a localized manner using a\ndenoising diffusion process, which allows for the construction of the global\nstructure before refining local details. Our experiments demonstrated the\neffectiveness of HYGENE, proving its ability to closely mimic a variety of\nproperties in hypergraphs. To the best of our knowledge, this is the first\nattempt to employ deep learning models for hypergraph generation, and our work\naims to lay the groundwork for future research in this area.\n","authors":["Dorian Gailhard","Enzo Tartaglione","Lirida Naviner De Barros","Jhony H. Giraldo"],"pdf_url":"https://arxiv.org/pdf/2408.16457v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.11529 by other authors"},{"id":"http://arxiv.org/abs/2403.04447v2","updated":"2024-08-29T11:28:10Z","published":"2024-03-07T12:34:03Z","title":"FRRI: a novel algorithm for fuzzy-rough rule induction","summary":"  Interpretability is the next frontier in machine learning research. In the\nsearch for white box models - as opposed to black box models, like random\nforests or neural networks - rule induction algorithms are a logical and\npromising option, since the rules can easily be understood by humans. Fuzzy and\nrough set theory have been successfully applied to this archetype, almost\nalways separately. As both approaches to rule induction involve granular\ncomputing based on the concept of equivalence classes, it is natural to combine\nthem. The QuickRules\\cite{JensenCornelis2009} algorithm was a first attempt at\nusing fuzzy rough set theory for rule induction. It is based on QuickReduct, a\ngreedy algorithm for building decision reducts. QuickRules already showed an\nimprovement over other rule induction methods. However, to evaluate the full\npotential of a fuzzy rough rule induction algorithm, one needs to start from\nthe foundations. In this paper, we introduce a novel rule induction algorithm\ncalled Fuzzy Rough Rule Induction (FRRI). We provide background and explain the\nworkings of our algorithm. Furthermore, we perform a computational experiment\nto evaluate the performance of our algorithm and compare it to other\nstate-of-the-art rule induction approaches. We find that our algorithm is more\naccurate while creating small rulesets consisting of relatively short rules. We\nend the paper by outlining some directions for future work.\n","authors":["Henri Bollaert","Marko PalangetiÄ","Chris Cornelis","Salvatore Greco","Roman SÅowiÅski"],"pdf_url":"https://arxiv.org/pdf/2403.04447v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12862v2","updated":"2024-08-29T11:18:16Z","published":"2024-04-19T13:01:59Z","title":"A Guide to Feature Importance Methods for Scientific Inference","summary":"  While machine learning (ML) models are increasingly used due to their high\npredictive power, their use in understanding the data-generating process (DGP)\nis limited. Understanding the DGP requires insights into feature-target\nassociations, which many ML models cannot directly provide due to their opaque\ninternal mechanisms. Feature importance (FI) methods provide useful insights\ninto the DGP under certain conditions. Since the results of different FI\nmethods have different interpretations, selecting the correct FI method for a\nconcrete use case is crucial and still requires expert knowledge. This paper\nserves as a comprehensive guide to help understand the different\ninterpretations of global FI methods. Through an extensive review of FI methods\nand providing new proofs regarding their interpretation, we facilitate a\nthorough understanding of these methods and formulate concrete recommendations\nfor scientific inference. We conclude by discussing options for FI uncertainty\nestimation and point to directions for future research aiming at full\nstatistical inference from black-box ML models.\n","authors":["Fiona Katharina Ewald","Ludwig Bothmann","Marvin N. Wright","Bernd Bischl","Giuseppe Casalicchio","Gunnar KÃ¶nig"],"pdf_url":"https://arxiv.org/pdf/2404.12862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15381v3","updated":"2024-08-29T11:03:57Z","published":"2024-04-23T09:44:58Z","title":"Advances and Open Challenges in Federated Foundation Models","summary":"  The integration of Foundation Models (FMs) with Federated Learning (FL)\npresents a transformative paradigm in Artificial Intelligence (AI). This\nintegration offers enhanced capabilities while addressing concerns of privacy,\ndata decentralization, and computational efficiency. This paper provides a\ncomprehensive survey of the emerging field of Federated Foundation Models\n(FedFM), elucidating their synergistic relationship and exploring novel\nmethodologies, challenges, and future directions that the FL research field\nneeds to focus on in order to thrive in the age of FMs. A systematic\nmulti-tiered taxonomy is proposed, categorizing existing FedFM approaches for\nmodel training, aggregation, trustworthiness, and incentivization. Key\nchallenges, including how to enable FL to deal with high complexity of\ncomputational demands, privacy considerations, contribution evaluation, and\ncommunication efficiency, are thoroughly discussed. Moreover, the paper\nexplores the intricate challenges of communication, scalability, and security\ninherent in training/fine-tuning FMs via FL. It highlights the potential of\nquantum computing to revolutionize the processes of training, inference,\noptimization, and data encryption. This survey also introduces the\nimplementation requirement of FedFM and some practical FedFM applications.\nThen, this survey provides the lessons with a clear understanding of our\nfindings for FedFM. Finally, this survey not only provides insights into the\ncurrent state and challenges of FedFM but also paves the way for future\nresearch directions, emphasizing the need for developing trustworthy solutions.\nIt serves as a foundational guide for researchers and practitioners interested\nin contributing to this interdisciplinary and rapidly advancing field.\n","authors":["Chao Ren","Han Yu","Hongyi Peng","Xiaoli Tang","Bo Zhao","Liping Yi","Alysa Ziying Tan","Yulan Gao","Anran Li","Xiaoxiao Li","Zengxiang Li","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2404.15381v3.pdf","comment":"Survey of Federated Foundation Models (FedFM)"},{"id":"http://arxiv.org/abs/2408.16430v1","updated":"2024-08-29T10:44:59Z","published":"2024-08-29T10:44:59Z","title":"Do Recommender Systems Promote Local Music? A Reproducibility Study\n  Using Music Streaming Data","summary":"  This paper examines the influence of recommender systems on local music\nrepresentation, discussing prior findings from an empirical study on the LFM-2b\npublic dataset. This prior study argued that different recommender systems\nexhibit algorithmic biases shifting music consumption either towards or against\nlocal content. However, LFM-2b users do not reflect the diverse audience of\nmusic streaming services. To assess the robustness of this study's conclusions,\nwe conduct a comparative analysis using proprietary listening data from a\nglobal music streaming service, which we publicly release alongside this paper.\nWe observe significant differences in local music consumption patterns between\nour dataset and LFM-2b, suggesting that caution should be exercised when\ndrawing conclusions on local music based solely on LFM-2b. Moreover, we show\nthat the algorithmic biases exhibited in the original work vary in our dataset,\nand that several unexplored model parameters can significantly influence these\nbiases and affect the study's conclusion on both datasets. Finally, we discuss\nthe complexity of accurately labeling local music, emphasizing the risk of\nmisleading conclusions due to unreliable, biased, or incomplete labels. To\nencourage further research and ensure reproducibility, we have publicly shared\nour dataset and code.\n","authors":["Kristina Matrosova","Lilian Marey","Guillaume Salha-Galvan","Thomas Louail","Olivier Bodini","Manuel Moussallam"],"pdf_url":"https://arxiv.org/pdf/2408.16430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16429v1","updated":"2024-08-29T10:43:55Z","published":"2024-08-29T10:43:55Z","title":"Gradient-free variational learning with conditional mixture networks","summary":"  Balancing computational efficiency with robust predictive performance is\ncrucial in supervised learning, especially for critical applications. Standard\ndeep learning models, while accurate and scalable, often lack probabilistic\nfeatures like calibrated predictions and uncertainty quantification. Bayesian\nmethods address these issues but can be computationally expensive as model and\ndata complexity increase. Previous work shows that fast variational methods can\nreduce the compute requirements of Bayesian methods by eliminating the need for\ngradient computation or sampling, but are often limited to simple models. We\ndemonstrate that conditional mixture networks (CMNs), a probabilistic variant\nof the mixture-of-experts (MoE) model, are suitable for fast, gradient-free\ninference and can solve complex classification tasks. CMNs employ linear\nexperts and a softmax gating network. By exploiting conditional conjugacy and\nP\\'olya-Gamma augmentation, we furnish Gaussian likelihoods for the weights of\nboth the linear experts and the gating network. This enables efficient\nvariational updates using coordinate ascent variational inference (CAVI),\navoiding traditional gradient-based optimization. We validate this approach by\ntraining two-layer CMNs on standard benchmarks from the UCI repository. Our\nmethod, CAVI-CMN, achieves competitive and often superior predictive accuracy\ncompared to maximum likelihood estimation (MLE) with backpropagation, while\nmaintaining competitive runtime and full posterior distributions over all model\nparameters. Moreover, as input size or the number of experts increases,\ncomputation time scales competitively with MLE and other gradient-based\nsolutions like black-box variational inference (BBVI), making CAVI-CMN a\npromising tool for deep, fast, and gradient-free Bayesian networks.\n","authors":["Conor Heins","Hao Wu","Dimitrije Markovic","Alexander Tschantz","Jeff Beck","Christopher Buckley"],"pdf_url":"https://arxiv.org/pdf/2408.16429v1.pdf","comment":"16 pages main text (3 figures), including references. 9 pages\n  supplementary material (5 figures)"},{"id":"http://arxiv.org/abs/2408.16425v1","updated":"2024-08-29T10:35:07Z","published":"2024-08-29T10:35:07Z","title":"A Comparative Study of Hyperparameter Tuning Methods","summary":"  The study emphasizes the challenge of finding the optimal trade-off between\nbias and variance, especially as hyperparameter optimization increases in\ncomplexity. Through empirical analysis, three hyperparameter tuning algorithms\nTree-structured Parzen Estimator (TPE), Genetic Search, and Random Search are\nevaluated across regression and classification tasks. The results show that\nnonlinear models, with properly tuned hyperparameters, significantly outperform\nlinear models. Interestingly, Random Search excelled in regression tasks, while\nTPE was more effective for classification tasks. This suggests that there is no\none-size-fits-all solution, as different algorithms perform better depending on\nthe task and model type. The findings underscore the importance of selecting\nthe appropriate tuning method and highlight the computational challenges\ninvolved in optimizing machine learning models, particularly as search spaces\nexpand.\n","authors":["Subhasis Dasgupta","Jaydip Sen"],"pdf_url":"https://arxiv.org/pdf/2408.16425v1.pdf","comment":"This chapter has been accepted in the edited volume titles \"Data\n  Science in Theory and Practice\", editor J Sen & S Roy Choudhury. The volume\n  is expected to be published in October 2024 by Cambridge Scholars Publishing,\n  New Castle upon Tyne, UK. This chapter is 34 pages long and it contains 11\n  tables and 8 images"},{"id":"http://arxiv.org/abs/2406.15852v2","updated":"2024-08-29T10:28:42Z","published":"2024-06-22T13:57:09Z","title":"Next Level Message-Passing with Hierarchical Support Graphs","summary":"  Message-Passing Neural Networks (MPNNs) are extensively employed in graph\nlearning tasks but suffer from limitations such as the restricted scope of\ninformation exchange, by being confined to neighboring nodes during each round\nof message passing. Various strategies have been proposed to address these\nlimitations, including incorporating virtual nodes to facilitate global\ninformation exchange. In this study, we introduce the Hierarchical Support\nGraph (HSG), an extension of the virtual node concept created through recursive\ncoarsening of the original graph. This approach provides a flexible framework\nfor enhancing information flow in graphs, independent of the specific MPNN\nlayers utilized. We present a theoretical analysis of HSGs, investigate their\nempirical performance, and demonstrate that HSGs can surpass other methods\naugmented with virtual nodes, achieving state-of-the-art results across\nmultiple datasets.\n","authors":["Carlos Vonessen","Florian GrÃ¶tschla","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2406.15852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16414v1","updated":"2024-08-29T10:21:00Z","published":"2024-08-29T10:21:00Z","title":"Fourier Spectral Physics Informed Neural Network: An Efficient and\n  Low-Memory PINN","summary":"  With growing investigations into solving partial differential equations by\nphysics-informed neural networks (PINNs), more accurate and efficient PINNs are\nrequired to meet the practical demands of scientific computing. One bottleneck\nof current PINNs is computing the high-order derivatives via automatic\ndifferentiation which often necessitates substantial computing resources. In\nthis paper, we focus on removing the automatic differentiation of the spatial\nderivatives and propose a spectral-based neural network that substitutes the\ndifferential operator with a multiplication. Compared to the PINNs, our\napproach requires lower memory and shorter training time. Thanks to the\nexponential convergence of the spectral basis, our approach is more accurate.\nMoreover, to handle the different situations between physics domain and\nspectral domain, we provide two strategies to train networks by their spectral\ninformation. Through a series of comprehensive experiments, We validate the\naforementioned merits of our proposed network.\n","authors":["Tianchi Yu","Yiming Qi","Ivan Oseledets","Shiyi Chen"],"pdf_url":"https://arxiv.org/pdf/2408.16414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03973v2","updated":"2024-08-29T10:09:58Z","published":"2024-02-06T13:06:14Z","title":"A comparison between humans and AI at recognizing objects in unusual\n  poses","summary":"  Deep learning is closing the gap with human vision on several object\nrecognition benchmarks. Here we investigate this gap for challenging images\nwhere objects are seen in unusual poses. We find that humans excel at\nrecognizing objects in such poses. In contrast, state-of-the-art deep networks\nfor vision (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) and state-of-the-art\nlarge vision-language models (Claude 3.5, Gemini 1.5, GPT-4) are systematically\nbrittle on unusual poses, with the exception of Gemini showing excellent\nrobustness in that condition. As we limit image exposure time, human\nperformance degrades to the level of deep networks, suggesting that additional\nmental processes (requiring additional time) are necessary to identify objects\nin unusual poses. An analysis of error patterns of humans vs. networks reveals\nthat even time-limited humans are dissimilar to feed-forward deep networks. In\nconclusion, our comparison reveals that humans and deep networks rely on\ndifferent mechanisms for recognizing objects in unusual poses. Understanding\nthe nature of the mental processes taking place during extra viewing time may\nbe key to reproduce the robustness of human vision in silico.\n","authors":["Netta Ollikka","Amro Abbas","Andrea Perin","Markku KilpelÃ¤inen","StÃ©phane Deny"],"pdf_url":"https://arxiv.org/pdf/2402.03973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16403v1","updated":"2024-08-29T10:02:29Z","published":"2024-08-29T10:02:29Z","title":"DeepSPoC: A Deep Learning-Based PDE Solver Governed by Sequential\n  Propagation of Chaos","summary":"  Sequential propagation of chaos (SPoC) is a recently developed tool to solve\nmean-field stochastic differential equations and their related nonlinear\nFokker-Planck equations. Based on the theory of SPoC, we present a new method\n(deepSPoC) that combines the interacting particle system of SPoC and deep\nlearning. Under the framework of deepSPoC, two classes of frequently used deep\nmodels include fully connected neural networks and normalizing flows are\nconsidered. For high-dimensional problems, spatial adaptive method are designed\nto further improve the accuracy and efficiency of deepSPoC. We analysis the\nconvergence of the framework of deepSPoC under some simplified conditions and\nalso provide a posterior error estimation for the algorithm. Finally, we test\nour methods on a wide range of different types of mean-field equations.\n","authors":["Kai Du","Yongle Xie","Tao Zhou","Yuancheng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.16403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12660v2","updated":"2024-08-29T09:58:47Z","published":"2023-10-19T11:33:33Z","title":"Gradient Descent Fails to Learn High-frequency Functions and Modular\n  Arithmetic","summary":"  Classes of target functions containing a large number of approximately\northogonal elements are known to be hard to learn by the Statistical Query\nalgorithms. Recently this classical fact re-emerged in a theory of\ngradient-based optimization of neural networks. In the novel framework, the\nhardness of a class is usually quantified by the variance of the gradient with\nrespect to a random choice of a target function.\n  A set of functions of the form $x\\to ax \\bmod p$, where $a$ is taken from\n${\\mathbb Z}_p$, has attracted some attention from deep learning theorists and\ncryptographers recently. This class can be understood as a subset of\n$p$-periodic functions on ${\\mathbb Z}$ and is tightly connected with a class\nof high-frequency periodic functions on the real line.\n  We present a mathematical analysis of limitations and challenges associated\nwith using gradient-based learning techniques to train a high-frequency\nperiodic function or modular multiplication from examples. We highlight that\nthe variance of the gradient is negligibly small in both cases when either a\nfrequency or the prime base $p$ is large. This in turn prevents such a learning\nalgorithm from being successful.\n","authors":["Rustem Takhanov","Maxat Tezekbayev","Artur Pak","Arman Bolatov","Zhenisbek Assylbekov"],"pdf_url":"https://arxiv.org/pdf/2310.12660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16393v1","updated":"2024-08-29T09:55:55Z","published":"2024-08-29T09:55:55Z","title":"Illuminating the Diversity-Fitness Trade-Off in Black-Box Optimization","summary":"  In real-world applications, users often favor structurally diverse design\nchoices over one high-quality solution. It is hence important to consider more\nsolutions that decision-makers can compare and further explore based on\nadditional criteria. Alongside the existing approaches of evolutionary\ndiversity optimization, quality diversity, and multimodal optimization, this\npaper presents a fresh perspective on this challenge by considering the problem\nof identifying a fixed number of solutions with a pairwise distance above a\nspecified threshold while maximizing their average quality.\n  We obtain first insight into these objectives by performing a subset\nselection on the search trajectories of different well-established search\nheuristics, whether specifically designed with diversity in mind or not. We\nemphasize that the main goal of our work is not to present a new algorithm but\nto look at the problem in a more fundamental and theoretically tractable way by\nasking the question: What trade-off exists between the minimum distance within\nbatches of solutions and the average quality of their fitness? These insights\nalso provide us with a way of making general claims concerning the properties\nof optimization problems that shall be useful in turn for benchmarking\nalgorithms of the approaches enumerated above.\n  A possibly surprising outcome of our empirical study is the observation that\nnaive uniform random sampling establishes a very strong baseline for our\nproblem, hardly ever outperformed by the search trajectories of the considered\nheuristics. We interpret these results as a motivation to develop algorithms\ntailored to produce diverse solutions of high average quality.\n","authors":["Maria Laura Santoni","Elena Raponi","Aneta Neumann","Frank Neumann","Mike Preuss","Carola Doerr"],"pdf_url":"https://arxiv.org/pdf/2408.16393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16391v1","updated":"2024-08-29T09:54:46Z","published":"2024-08-29T09:54:46Z","title":"TempoKGAT: A Novel Graph Attention Network Approach for Temporal Graph\n  Analysis","summary":"  Graph neural networks (GNN) have shown significant capabilities in handling\nstructured data, yet their application to dynamic, temporal data remains\nlimited. This paper presents a new type of graph attention network, called\nTempoKGAT, which combines time-decaying weight and a selective neighbor\naggregation mechanism on the spatial domain, which helps uncover latent\npatterns in the graph data. In this approach, a top-k neighbor selection based\non the edge weights is introduced to represent the evolving features of the\ngraph data. We evaluated the performance of our TempoKGAT on multiple datasets\nfrom the traffic, energy, and health sectors involving spatio-temporal data. We\ncompared the performance of our approach to several state-of-the-art methods\nfound in the literature on several open-source datasets. Our method shows\nsuperior accuracy on all datasets. These results indicate that TempoKGAT builds\non existing methodologies to optimize prediction accuracy and provide new\ninsights into model interpretation in temporal contexts.\n","authors":["Lena Sasal","Daniel Busby","Abdenour Hadid"],"pdf_url":"https://arxiv.org/pdf/2408.16391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09441v2","updated":"2024-08-29T09:52:58Z","published":"2024-07-12T17:27:43Z","title":"The $Î¼\\mathcal{G}$ Language for Programming Graph Neural Networks","summary":"  Graph neural networks form a class of deep learning architectures\nspecifically designed to work with graph-structured data. As such, they share\nthe inherent limitations and problems of deep learning, especially regarding\nthe issues of explainability and trustworthiness. We propose $\\mu\\mathcal{G}$,\nan original domain-specific language for the specification of graph neural\nnetworks that aims to overcome these issues. The language's syntax is\nintroduced, and its meaning is rigorously defined by a denotational semantics.\nAn equivalent characterization in the form of an operational semantics is also\nprovided and, together with a type system, is used to prove the type soundness\nof $\\mu\\mathcal{G}$. We show how $\\mu\\mathcal{G}$ programs can be represented\nin a more user-friendly graphical visualization, and provide examples of its\ngenerality by showing how it can be used to define some of the most popular\ngraph neural network models, or to develop any custom graph processing\napplication.\n","authors":["Matteo Belenchia","Flavio Corradini","Michela Quadrini","Michele Loreti"],"pdf_url":"https://arxiv.org/pdf/2407.09441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16389v1","updated":"2024-08-29T09:50:31Z","published":"2024-08-29T09:50:31Z","title":"Addressing Common Misinterpretations of KART and UAT in Neural Network\n  Literature","summary":"  This note addresses the Kolmogorov-Arnold Representation Theorem (KART) and\nthe Universal Approximation Theorem (UAT), focusing on their common\nmisinterpretations in some papers related to neural network approximation. Our\nremarks aim to support a more accurate understanding of KART and UAT among\nneural network specialists.\n","authors":["Vugar Ismailov"],"pdf_url":"https://arxiv.org/pdf/2408.16389v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2408.15294v2","updated":"2024-08-29T09:43:04Z","published":"2024-08-27T09:48:25Z","title":"Evaluating the Predictive Features of Person-Centric Knowledge Graph\n  Embeddings: Unfolding Ablation Studies","summary":"  Developing novel predictive models with complex biomedical information is\nchallenging due to various idiosyncrasies related to heterogeneity,\nstandardization or sparseness of the data. We previously introduced a\nperson-centric ontology to organize information about individual patients, and\na representation learning framework to extract person-centric knowledge graphs\n(PKGs) and to train Graph Neural Networks (GNNs). In this paper, we propose a\nsystematic approach to examine the results of GNN models trained with both\nstructured and unstructured information from the MIMIC-III dataset. Through\nablation studies on different clinical, demographic, and social data, we show\nthe robustness of this approach in identifying predictive features in PKGs for\nthe task of readmission prediction.\n","authors":["Christos Theodoropoulos","Natasha Mulligan","Joao Bettencourt-Silva"],"pdf_url":"https://arxiv.org/pdf/2408.15294v2.pdf","comment":"Published in the 34th Medical Informatics Europe Conference"},{"id":"http://arxiv.org/abs/2408.16379v1","updated":"2024-08-29T09:41:17Z","published":"2024-08-29T09:41:17Z","title":"TG-PhyNN: An Enhanced Physically-Aware Graph Neural Network framework\n  for forecasting Spatio-Temporal Data","summary":"  Accurately forecasting dynamic processes on graphs, such as traffic flow or\ndisease spread, remains a challenge. While Graph Neural Networks (GNNs) excel\nat modeling and forecasting spatio-temporal data, they often lack the ability\nto directly incorporate underlying physical laws. This work presents TG-PhyNN,\na novel Temporal Graph Physics-Informed Neural Network framework. TG-PhyNN\nleverages the power of GNNs for graph-based modeling while simultaneously\nincorporating physical constraints as a guiding principle during training. This\nis achieved through a two-step prediction strategy that enables the calculation\nof physical equation derivatives within the GNN architecture. Our findings\ndemonstrate that TG-PhyNN significantly outperforms traditional forecasting\nmodels (e.g., GRU, LSTM, GAT) on real-world spatio-temporal datasets like\nPedalMe (traffic flow), COVID-19 spread, and Chickenpox outbreaks. These\ndatasets are all governed by well-defined physical principles, which TG-PhyNN\neffectively exploits to offer more reliable and accurate forecasts in various\ndomains where physical processes govern the dynamics of data. This paves the\nway for improved forecasting in areas like traffic flow prediction, disease\noutbreak prediction, and potentially other fields where physics plays a crucial\nrole.\n","authors":["Zakaria Elabid","Lena Sasal","Daniel Busby","Abdenour Hadid"],"pdf_url":"https://arxiv.org/pdf/2408.16379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08800v2","updated":"2024-08-29T09:27:45Z","published":"2024-06-13T04:33:05Z","title":"Can Synthetic Audio From Generative Foundation Models Assist Audio\n  Recognition and Speech Modeling?","summary":"  Recent advances in foundation models have enabled audio-generative models\nthat produce high-fidelity sounds associated with music, events, and human\nactions. Despite the success achieved in modern audio-generative models, the\nconventional approach to assessing the quality of the audio generation relies\nheavily on distance metrics like Frechet Audio Distance. In contrast, we aim to\nevaluate the quality of audio generation by examining the effectiveness of\nusing them as training data. Specifically, we conduct studies to explore the\nuse of synthetic audio for audio recognition. Moreover, we investigate whether\nsynthetic audio can serve as a resource for data augmentation in speech-related\nmodeling. Our comprehensive experiments demonstrate the potential of using\nsynthetic audio for audio recognition and speech-related modeling. Our code is\navailable at https://github.com/usc-sail/SynthAudio.\n","authors":["Tiantian Feng","Dimitrios Dimitriadis","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2406.08800v2.pdf","comment":"Accepted to 2024 INTERSPEECH; corrections to ActivityNet labels"},{"id":"http://arxiv.org/abs/2406.05482v4","updated":"2024-08-29T08:46:46Z","published":"2024-06-08T14:14:19Z","title":"Efficient Topology-aware Data Augmentation for High-Degree Graph Neural\n  Networks","summary":"  In recent years, graph neural networks (GNNs) have emerged as a potent tool\nfor learning on graph-structured data and won fruitful successes in varied\nfields. The majority of GNNs follow the message-passing paradigm, where\nrepresentations of each node are learned by recursively aggregating features of\nits neighbors. However, this mechanism brings severe over-smoothing and\nefficiency issues over high-degree graphs (HDGs), wherein most nodes have\ndozens (or even hundreds) of neighbors, such as social networks, transaction\ngraphs, power grids, etc. Additionally, such graphs usually encompass rich and\ncomplex structure semantics, which are hard to capture merely by feature\naggregations in GNNs. Motivated by the above limitations, we propose TADA, an\nefficient and effective front-mounted data augmentation framework for GNNs on\nHDGs. Under the hood, TADA includes two key modules: (i) feature expansion with\nstructure embeddings, and (ii) topology- and attribute-aware graph\nsparsification. The former obtains augmented node features and enhanced model\ncapacity by encoding the graph structure into high-quality structure embeddings\nwith our highly-efficient sketching method. Further, by exploiting\ntask-relevant features extracted from graph structures and attributes, the\nsecond module enables the accurate identification and reduction of numerous\nredundant/noisy edges from the input graph, thereby alleviating over-smoothing\nand facilitating faster feature aggregations over HDGs. Empirically, TADA\nconsiderably improves the predictive performance of mainstream GNN models on 8\nreal homophilic/heterophilic HDGs in terms of node classification, while\nachieving efficient training and inference processes.\n","authors":["Yurui Lai","Xiaoyang Lin","Renchi Yang","Hongtao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05482v4.pdf","comment":"This is the technical report for the paper accepted to KDD 2024. 16\n  pages"},{"id":"http://arxiv.org/abs/2408.16349v1","updated":"2024-08-29T08:36:22Z","published":"2024-08-29T08:36:22Z","title":"Machine learning models for daily rainfall forecasting in Northern\n  Tropical Africa using tropical wave predictors","summary":"  Numerical weather prediction (NWP) models often underperform compared to\nsimpler climatology-based precipitation forecasts in northern tropical Africa,\neven after statistical postprocessing. AI-based forecasting models show promise\nbut have avoided precipitation due to its complexity. Synoptic-scale forcings\nlike African easterly waves and other tropical waves (TWs) are important for\npredictability in tropical Africa, yet their value for predicting daily\nrainfall remains unexplored. This study uses two machine-learning models--gamma\nregression and a convolutional neural network (CNN)--trained on TW predictors\nfrom satellite-based GPM IMERG data to predict daily rainfall during the\nJuly-September monsoon season. Predictor variables are derived from the local\namplitude and phase information of seven TW from the target and\nup-and-downstream neighboring grids at 1-degree spatial resolution. The ML\nmodels are combined with Easy Uncertainty Quantification (EasyUQ) to generate\ncalibrated probabilistic forecasts and are compared with three benchmarks:\nExtended Probabilistic Climatology (EPC15), ECMWF operational ensemble forecast\n(ENS), and a probabilistic forecast from the ENS control member using EasyUQ\n(CTRL EasyUQ). The study finds that downstream predictor variables offer the\nhighest predictability, with downstream tropical depression (TD)-type\nwave-based predictors being most important. Other waves like mixed-Rossby\ngravity (MRG), Kelvin, and inertio-gravity waves also contribute significantly\nbut show regional preferences. ENS forecasts exhibit poor skill due to\nmiscalibration. CTRL EasyUQ shows improvement over ENS and marginal enhancement\nover EPC15. Both gamma regression and CNN forecasts significantly outperform\nbenchmarks in tropical Africa. This study highlights the potential of ML models\ntrained on TW-based predictors to improve daily precipitation forecasts in\ntropical Africa.\n","authors":["Athul Rasheeda Satheesh","Peter Knippertz","Andreas H. Fink"],"pdf_url":"https://arxiv.org/pdf/2408.16349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12326v2","updated":"2024-08-29T08:27:27Z","published":"2024-02-19T18:00:30Z","title":"PsychoGAT: A Novel Psychological Measurement Paradigm through\n  Interactive Fiction Games with LLM Agents","summary":"  Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction.\n","authors":["Qisen Yang","Zekun Wang","Honghui Chen","Shenzhi Wang","Yifan Pu","Xin Gao","Wenhao Huang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.12326v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2408.14144v2","updated":"2024-08-29T08:27:26Z","published":"2024-08-26T09:42:18Z","title":"Neighborhood and Global Perturbations Supported SAM in Federated\n  Learning: From Local Tweaks To Global Awareness","summary":"  Federated Learning (FL) can be coordinated under the orchestration of a\ncentral server to collaboratively build a privacy-preserving model without the\nneed for data exchange. However, participant data heterogeneity leads to local\noptima divergence, subsequently affecting convergence outcomes. Recent research\nhas focused on global sharpness-aware minimization (SAM) and dynamic\nregularization techniques to enhance consistency between global and local\ngeneralization and optimization objectives. Nonetheless, the estimation of\nglobal SAM introduces additional computational and memory overhead, while\ndynamic regularization suffers from bias in the local and global dual variables\ndue to training isolation. In this paper, we propose a novel FL algorithm,\nFedTOGA, designed to consider optimization and generalization objectives while\nmaintaining minimal uplink communication overhead. By linking local\nperturbations to global updates, global generalization consistency is improved.\nAdditionally, global updates are used to correct local dynamic regularizers,\nreducing dual variables bias and enhancing optimization consistency. Global\nupdates are passively received by clients, reducing overhead. We also propose\nneighborhood perturbation to approximate local perturbation, analyzing its\nstrengths and limitations. Theoretical analysis shows FedTOGA achieves faster\nconvergence $O(1/T)$ under non-convex functions. Empirical studies demonstrate\nthat FedTOGA outperforms state-of-the-art algorithms, with a 1\\% accuracy\nincrease and 30\\% faster convergence, achieving state-of-the-art.\n","authors":["Boyuan Li","Zihao Peng","Yafei Li","Mingliang Xu","Shengbo Chen","Baofeng Ji","Cong Shen"],"pdf_url":"https://arxiv.org/pdf/2408.14144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16337v1","updated":"2024-08-29T08:20:02Z","published":"2024-08-29T08:20:02Z","title":"Do Graph Neural Networks Work for High Entropy Alloys?","summary":"  Graph neural networks (GNNs) have excelled in predictive modeling for both\ncrystals and molecules, owing to the expressiveness of graph representations.\nHigh-entropy alloys (HEAs), however, lack chemical long-range order, limiting\nthe applicability of current graph representations. To overcome this challenge,\nwe propose a representation of HEAs as a collection of local environment (LE)\ngraphs. Based on this representation, we introduce the LESets machine learning\nmodel, an accurate, interpretable GNN for HEA property prediction. We\ndemonstrate the accuracy of LESets in modeling the mechanical properties of\nquaternary HEAs. Through analyses and interpretation, we further extract\ninsights into the modeling and design of HEAs. In a broader sense, LESets\nextends the potential applicability of GNNs to disordered materials with\ncombinatorial complexity formed by diverse constituents and their flexible\nconfigurations.\n","authors":["Hengrui Zhang","Ruishu Huang","Jie Chen","James M. Rondinelli","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2408.16337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11299v2","updated":"2024-08-29T08:14:31Z","published":"2023-12-18T15:49:03Z","title":"Uncertainty-based Fairness Measures","summary":"  Unfair predictions of machine learning (ML) models impede their broad\nacceptance in real-world settings. Tackling this arduous challenge first\nnecessitates defining what it means for an ML model to be fair. This has been\naddressed by the ML community with various measures of fairness that depend on\nthe prediction outcomes of the ML models, either at the group level or the\nindividual level. These fairness measures are limited in that they utilize\npoint predictions, neglecting their variances, or uncertainties, making them\nsusceptible to noise, missingness and shifts in data. In this paper, we first\nshow that an ML model may appear to be fair with existing point-based fairness\nmeasures but biased against a demographic group in terms of prediction\nuncertainties. Then, we introduce new fairness measures based on different\ntypes of uncertainties, namely, aleatoric uncertainty and epistemic\nuncertainty. We demonstrate on many datasets that (i) our uncertainty-based\nmeasures are complementary to existing measures of fairness, and (ii) they\nprovide more insights about the underlying issues leading to bias.\n","authors":["Selim Kuzucu","Jiaee Cheong","Hatice Gunes","Sinan Kalkan"],"pdf_url":"https://arxiv.org/pdf/2312.11299v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16336v1","updated":"2024-08-29T08:14:20Z","published":"2024-08-29T08:14:20Z","title":"GL-TSVM: A robust and smooth twin support vector machine with guardian\n  loss function","summary":"  Twin support vector machine (TSVM), a variant of support vector machine\n(SVM), has garnered significant attention due to its $3/4$ times lower\ncomputational complexity compared to SVM. However, due to the utilization of\nthe hinge loss function, TSVM is sensitive to outliers or noise. To remedy it,\nwe introduce the guardian loss (G-loss), a novel loss function distinguished by\nits asymmetric, bounded, and smooth characteristics. We then fuse the proposed\nG-loss function into the TSVM and yield a robust and smooth classifier termed\nGL-TSVM. Further, to adhere to the structural risk minimization (SRM) principle\nand reduce overfitting, we incorporate a regularization term into the objective\nfunction of GL-TSVM. To address the optimization challenges of GL-TSVM, we\ndevise an efficient iterative algorithm. The experimental analysis on UCI and\nKEEL datasets substantiates the effectiveness of the proposed GL-TSVM in\ncomparison to the baseline models. Moreover, to showcase the efficacy of the\nproposed GL-TSVM in the biomedical domain, we evaluated it on the breast cancer\n(BreaKHis) and schizophrenia datasets. The outcomes strongly demonstrate the\ncompetitiveness of the proposed GL-TSVM against the baseline models.\n","authors":["Mushir Akhtar","M. Tanveer","Mohd. Arshad"],"pdf_url":"https://arxiv.org/pdf/2408.16336v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2404.18101"},{"id":"http://arxiv.org/abs/2408.16333v1","updated":"2024-08-29T08:12:18Z","published":"2024-08-29T08:12:18Z","title":"Self-Improving Diffusion Models with Synthetic Data","summary":"  The artificial intelligence (AI) world is running out of real data for\ntraining increasingly large generative models, resulting in accelerating\npressure to train on synthetic data. Unfortunately, training new generative\nmodels with synthetic data from current or past generation models creates an\nautophagous (self-consuming) loop that degrades the quality and/or diversity of\nthe synthetic data in what has been termed model autophagy disorder (MAD) and\nmodel collapse. Current thinking around model autophagy recommends that\nsynthetic data is to be avoided for model training lest the system deteriorate\ninto MADness. In this paper, we take a different tack that treats synthetic\ndata differently from real data. Self-IMproving diffusion models with Synthetic\ndata (SIMS) is a new training concept for diffusion models that uses\nself-synthesized data to provide negative guidance during the generation\nprocess to steer a model's generative process away from the non-ideal synthetic\ndata manifold and towards the real data distribution. We demonstrate that SIMS\nis capable of self-improvement; it establishes new records based on the\nFr\\'echet inception distance (FID) metric for CIFAR-10 and ImageNet-64\ngeneration and achieves competitive results on FFHQ-64 and ImageNet-512.\nMoreover, SIMS is, to the best of our knowledge, the first prophylactic\ngenerative AI algorithm that can be iteratively trained on self-generated\nsynthetic data without going MAD. As a bonus, SIMS can adjust a diffusion\nmodel's synthetic data distribution to match any desired in-domain target\ndistribution to help mitigate biases and ensure fairness.\n","authors":["Sina Alemohammad","Ahmed Imtiaz Humayun","Shruti Agarwal","John Collomosse","Richard Baraniuk"],"pdf_url":"https://arxiv.org/pdf/2408.16333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18269v2","updated":"2024-08-29T08:07:43Z","published":"2024-07-19T22:51:41Z","title":"LaMAGIC: Language-Model-based Topology Generation for Analog Integrated\n  Circuits","summary":"  In the realm of electronic and electrical engineering, automation of analog\ncircuit is increasingly vital given the complexity and customized requirements\nof modern applications. However, existing methods only develop search-based\nalgorithms that require many simulation iterations to design a custom circuit\ntopology, which is usually a time-consuming process. To this end, we introduce\nLaMAGIC, a pioneering language model-based topology generation model that\nleverages supervised finetuning for automated analog circuit design. LaMAGIC\ncan efficiently generate an optimized circuit design from the custom\nspecification in a single pass. Our approach involves a meticulous development\nand analysis of various input and output formulations for circuit. These\nformulations can ensure canonical representations of circuits and align with\nthe autoregressive nature of LMs to effectively addressing the challenges of\nrepresenting analog circuits as graphs. The experimental results show that\nLaMAGIC achieves a success rate of up to 96\\% under a strict tolerance of 0.01.\nWe also examine the scalability and adaptability of LaMAGIC, specifically\ntesting its performance on more complex circuits. Our findings reveal the\nenhanced effectiveness of our adjacency matrix-based circuit formulation with\nfloating-point input, suggesting its suitability for handling intricate circuit\ndesigns. This research not only demonstrates the potential of language models\nin graph generation, but also builds a foundational framework for future\nexplorations in automated analog circuit design.\n","authors":["Chen-Chia Chang","Yikang Shen","Shaoze Fan","Jing Li","Shun Zhang","Ningyuan Cao","Yiran Chen","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.18269v2.pdf","comment":"Proceedings of the 41st International Conference on Machine Learning,\n  PMLR 235:6253-6262 https://proceedings.mlr.press/v235/chang24c.html"},{"id":"http://arxiv.org/abs/2408.16321v1","updated":"2024-08-29T07:48:55Z","published":"2024-08-29T07:48:55Z","title":"Minimising changes to audit when updating decision trees","summary":"  Interpretable models are important, but what happens when the model is\nupdated on new training data? We propose an algorithm for updating a decision\ntree while minimising the number of changes to the tree that a human would need\nto audit. We achieve this via a greedy approach that incorporates the number of\nchanges to the tree as part of the objective function. We compare our algorithm\nto existing methods and show that it sits in a sweet spot between final\naccuracy and number of changes to audit.\n","authors":["Anj Simmons","Scott Barnett","Anupam Chaudhuri","Sankhya Singh","Shangeetha Sivasothy"],"pdf_url":"https://arxiv.org/pdf/2408.16321v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2310.15274v2","updated":"2024-08-29T07:32:45Z","published":"2023-10-23T18:20:54Z","title":"1 From the Pursuit of Universal AGI Architecture to Systematic Approach\n  to Heterogenous AGI: Addressing Alignment, Energy, & AGI Grand Challenges","summary":"  AI faces a trifecta of grand challenges: the Energy Wall, the Alignment\nProblem and the Leap from Narrow AI to AGI. Contemporary AI solutions consume\nunsustainable amounts of energy during model training and daily operations.\nMaking things worse, the amount of computation required to train each new AI\nmodel has been doubling every 2 months since 2020, directly translating to\nunprecedented increases in energy consumption.\n  The leap from AI to AGI requires multiple functional subsystems operating in\na balanced manner, which requires a system architecture. However, the current\napproach to artificial intelligence lacks system design; even though system\ncharacteristics play a key role in the human brain; from the way it processes\ninformation to how it makes decisions. System design is the key to alignment,\none of the most challenging goals in AI. This difficulty stems from the fact\nthat the complexity of human moral system requires a similarly sophisticated\nsystem for alignment. Without accurately reflecting the complexity of these\ncore moral subsystems and systems, aligning AI with human values becomes\nsignificantly more challenging.\n  In this paper, we posit that system design is the missing piece in overcoming\nthe grand challenges. We present a Systematic Approach to AGI that utilizes\nsystem design principles to AGI, while providing ways to overcome the energy\nwall and the alignment challenges. This paper asserts that artificial\nintelligence can be realized through a multiplicity of design-specific\npathways, rather than a singular, overarching AGI architecture. AGI systems may\nexhibit diverse architectural configurations and capabilities, contingent upon\ntheir intended use cases. It advocates for a focus on employing system design\nprinciples as a guiding framework, rather than solely concentrating on a\nuniversal AGI architecture.\n","authors":["Eren Kurshan"],"pdf_url":"https://arxiv.org/pdf/2310.15274v2.pdf","comment":"International Journal on Semantic Computing (2024) Categories:\n  Artificial Intelligence; AI; Artificial General Intelligence; AGI; System\n  Design; System Architecture"},{"id":"http://arxiv.org/abs/2408.16315v1","updated":"2024-08-29T07:32:30Z","published":"2024-08-29T07:32:30Z","title":"Passenger hazard perception based on EEG signals for highly automated\n  driving vehicles","summary":"  Enhancing the safety of autonomous vehicles is crucial, especially given\nrecent accidents involving automated systems. As passengers in these vehicles,\nhumans' sensory perception and decision-making can be integrated with\nautonomous systems to improve safety. This study explores neural mechanisms in\npassenger-vehicle interactions, leading to the development of a Passenger\nCognitive Model (PCM) and the Passenger EEG Decoding Strategy (PEDS). Central\nto PEDS is a novel Convolutional Recurrent Neural Network (CRNN) that captures\nspatial and temporal EEG data patterns. The CRNN, combined with stacking\nalgorithms, achieves an accuracy of $85.0\\% \\pm 3.18\\%$. Our findings highlight\nthe predictive power of pre-event EEG data, enhancing the detection of\nhazardous scenarios and offering a network-driven framework for safer\nautonomous vehicles.\n","authors":["Ashton Yu Xuan Tan","Yingkai Yang","Xiaofei Zhang","Bowen Li","Xiaorong Gao","Sifa Zheng","Jianqiang Wang","Xinyu Gu","Jun Li","Yang Zhao","Yuxin Zhang","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2408.16315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16293v1","updated":"2024-08-29T06:49:20Z","published":"2024-08-29T06:49:20Z","title":"Physics of Language Models: Part 2.2, How to Learn From Mistakes on\n  Grade-School Math Problems","summary":"  Language models have demonstrated remarkable performance in solving reasoning\ntasks; however, even the strongest models still occasionally make reasoning\nmistakes. Recently, there has been active research aimed at improving reasoning\naccuracy, particularly by using pretrained language models to \"self-correct\"\ntheir mistakes via multi-round prompting. In this paper, we follow this line of\nwork but focus on understanding the usefulness of incorporating\n\"error-correction\" data directly into the pretraining stage. This data consists\nof erroneous solution steps immediately followed by their corrections. Using a\nsynthetic math dataset, we show promising results: this type of pretrain data\ncan help language models achieve higher reasoning accuracy directly (i.e.,\nthrough simple auto-regression, without multi-round prompting) compared to\npretraining on the same amount of error-free data. We also delve into many\ndetails, such as (1) how this approach differs from beam search, (2) how such\ndata can be prepared, (3) whether masking is needed on the erroneous tokens,\n(4) the amount of error required, (5) whether such data can be deferred to the\nfine-tuning stage, and many others.\n","authors":["Tian Ye","Zicheng Xu","Yuanzhi Li","Zeyuan Allen-Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.16293v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.20311"},{"id":"http://arxiv.org/abs/2408.16291v1","updated":"2024-08-29T06:48:07Z","published":"2024-08-29T06:48:07Z","title":"Flexible framework for generating synthetic electrocardiograms and\n  photoplethysmograms","summary":"  By generating synthetic biosignals, the quantity and variety of health data\ncan be increased. This is especially useful when training machine learning\nmodels by enabling data augmentation and introduction of more physiologically\nplausible variation to the data. For these purposes, we have developed a\nsynthetic biosignal model for two signal modalities, electrocardiography (ECG)\nand photoplethysmography (PPG). The model produces realistic signals that\naccount for physiological effects such as breathing modulation and changes in\nheart rate due to physical stress. Arrhythmic signals can be generated with\nbeat intervals extracted from real measurements. The model also includes a\nflexible approach to adding different kinds of noise and signal artifacts. The\nnoise is generated from power spectral densities extracted from both measured\nnoisy signals and modeled power spectra. Importantly, the model also\nautomatically produces labels for noise, segmentation (e.g. P and T waves, QRS\ncomplex, for electrocardiograms), and artifacts. We assessed how this\ncomprehensive model can be used in practice to improve the performance of\nmodels trained on ECG or PPG data. For example, we trained an LSTM to detect\nECG R-peaks using both real ECG signals from the MIT-BIH arrythmia set and our\nnew generator. The F1 score of the model was 0.83 using real data, in\ncomparison to 0.98 using our generator. In addition, the model can be used for\nexample in signal segmentation, quality detection and bench-marking detection\nalgorithms. The model code has been released in\n\\url{https://github.com/UTU-Health-Research/framework_for_synthetic_biosignals}\n","authors":["Katri Karhinoja","Antti Vasankari","Jukka-Pekka SirkiÃ¤","Antti Airola","David Wong","Matti Kaisti"],"pdf_url":"https://arxiv.org/pdf/2408.16291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16288v1","updated":"2024-08-29T06:40:01Z","published":"2024-08-29T06:40:01Z","title":"OpenFGL: A Comprehensive Benchmarks for Federated Graph Learning","summary":"  Federated graph learning (FGL) has emerged as a promising distributed\ntraining paradigm for graph neural networks across multiple local systems\nwithout direct data sharing. This approach is particularly beneficial in\nprivacy-sensitive scenarios and offers a new perspective on addressing\nscalability challenges in large-scale graph learning. Despite the proliferation\nof FGL, the diverse motivations from practical applications, spanning various\nresearch backgrounds and experimental settings, pose a significant challenge to\nfair evaluation. To fill this gap, we propose OpenFGL, a unified benchmark\ndesigned for the primary FGL scenarios: Graph-FL and Subgraph-FL. Specifically,\nOpenFGL includes 38 graph datasets from 16 application domains, 8 federated\ndata simulation strategies that emphasize graph properties, and 5 graph-based\ndownstream tasks. Additionally, it offers 18 recently proposed SOTA FGL\nalgorithms through a user-friendly API, enabling a thorough comparison and\ncomprehensive evaluation of their effectiveness, robustness, and efficiency.\nEmpirical results demonstrate the ability of FGL while also revealing its\npotential limitations, offering valuable insights for future exploration in\nthis thriving field.\n","authors":["Xunkai Li","Yinlin Zhu","Boyang Pang","Guochen Yan","Yeyu Yan","Zening Li","Zhengyu Wu","Wentao Zhang","Rong-Hua Li","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16288v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2408.16286v1","updated":"2024-08-29T06:37:16Z","published":"2024-08-29T06:37:16Z","title":"Near-Optimal Policy Identification in Robust Constrained Markov Decision\n  Processes via Epigraph Form","summary":"  Designing a safe policy for uncertain environments is crucial in real-world\ncontrol applications. However, this challenge remains inadequately addressed\nwithin the Markov decision process (MDP) framework. This paper presents the\nfirst algorithm capable of identifying a near-optimal policy in a robust\nconstrained MDP (RCMDP), where an optimal policy minimizes cumulative cost\nwhile satisfying constraints in the worst-case scenario across a set of\nenvironments. We first prove that the conventional Lagrangian max-min\nformulation with policy gradient methods can become trapped in suboptimal\nsolutions by encountering a sum of conflicting gradients from the objective and\nconstraint functions during its inner minimization problem. To address this, we\nleverage the epigraph form of the RCMDP problem, which resolves the conflict by\nselecting a single gradient from either the objective or the constraints.\nBuilding on the epigraph form, we propose a binary search algorithm with a\npolicy gradient subroutine and prove that it identifies an\n$\\varepsilon$-optimal policy in an RCMDP with\n$\\tilde{\\mathcal{O}}(\\varepsilon^{-4})$ policy evaluations.\n","authors":["Toshinori Kitamura","Tadashi Kozuno","Wataru Kumagai","Kenta Hoshino","Yohei Hosoe","Kazumi Kasaura","Masashi Hamaya","Paavo Parmas","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2408.16286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16285v1","updated":"2024-08-29T06:30:23Z","published":"2024-08-29T06:30:23Z","title":"ART: Actually Robust Training","summary":"  Current interest in deep learning captures the attention of many programmers\nand researchers. Unfortunately, the lack of a unified schema for developing\ndeep learning models results in methodological inconsistencies, unclear\ndocumentation, and problems with reproducibility. Some guidelines have been\nproposed, yet currently, they lack practical implementations. Furthermore,\nneural network training often takes on the form of trial and error, lacking a\nstructured and thoughtful process. To alleviate these issues, in this paper, we\nintroduce Art, a Python library designed to help automatically impose rules and\nstandards while developing deep learning pipelines. Art divides model\ndevelopment into a series of smaller steps of increasing complexity, each\nconcluded with a validation check improving the interpretability and robustness\nof the process. The current version of Art comes equipped with nine predefined\nsteps inspired by Andrej Karpathy's Recipe for Training Neural Networks, a\nvisualization dashboard, and integration with loggers such as Neptune. The code\nrelated to this paper is available at:\nhttps://github.com/SebChw/Actually-Robust-Training.\n","authors":["Sebastian ChwilczyÅski","Kacper TrÄbacz","Karol Cyganik","Mateusz MaÅecki","Dariusz Brzezinski"],"pdf_url":"https://arxiv.org/pdf/2408.16285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16284v1","updated":"2024-08-29T06:27:42Z","published":"2024-08-29T06:27:42Z","title":"Enhancing Customer Churn Prediction in Telecommunications: An Adaptive\n  Ensemble Learning Approach","summary":"  Customer churn, the discontinuation of services by existing customers, poses\na significant challenge to the telecommunications industry. This paper proposes\na novel adaptive ensemble learning framework for highly accurate customer churn\nprediction. The framework integrates multiple base models, including XGBoost,\nLightGBM, LSTM, a Multi-Layer Perceptron (MLP) neural network, and Support\nVector Machine (SVM). These models are strategically combined using a stacking\nensemble method, further enhanced by meta-feature generation from base model\npredictions. A rigorous data preprocessing pipeline, coupled with a\nmulti-faceted feature engineering approach, optimizes model performance. The\nframework is evaluated on three publicly available telecom churn datasets,\ndemonstrating substantial accuracy improvements over state-of-the-art\ntechniques. The research achieves a remarkable 99.28% accuracy, signifying a\nmajor advancement in churn prediction.The implications of this research for\ndeveloping proactive customer retention strategies withinthe telecommunications\nindustry are discussed.\n","authors":["Mohammed Affan Shaikhsurab","Pramod Magadum"],"pdf_url":"https://arxiv.org/pdf/2408.16284v1.pdf","comment":"12 pages,2 figures"},{"id":"http://arxiv.org/abs/2405.07288v2","updated":"2024-08-29T06:22:48Z","published":"2024-05-12T14:01:05Z","title":"Erasing Concepts from Text-to-Image Diffusion Models with Few-shot\n  Unlearning","summary":"  Generating images from text has become easier because of the scaling of\ndiffusion models and advancements in the field of vision and language. These\nmodels are trained using vast amounts of data from the Internet. Hence, they\noften contain undesirable content such as copyrighted material. As it is\nchallenging to remove such data and retrain the models, methods for erasing\nspecific concepts from pre-trained models have been investigated. We propose a\nnovel concept-erasure method that updates the text encoder using few-shot\nunlearning in which a few real images are used. The discussion regarding the\ngenerated images after erasing a concept has been lacking. While there are\nmethods for specifying the transition destination for concepts, the validity of\nthe specified concepts is unclear. Our method implicitly achieves this by\ntransitioning to the latent concepts inherent in the model or the images. Our\nmethod can erase a concept within 10 s, making concept erasure more accessible\nthan ever before. Implicitly transitioning to related concepts leads to more\nnatural concept erasure. We applied the proposed method to various concepts and\nconfirmed that concept erasure can be achieved tens to hundreds of times faster\nthan with current methods. By varying the parameters to be updated, we obtained\nresults suggesting that, like previous research, knowledge is primarily\naccumulated in the feed-forward networks of the text encoder. Our code is\navailable at \\url{https://github.com/fmp453/few-shot-erasing}\n","authors":["Masane Fuchi","Tomohiro Takagi"],"pdf_url":"https://arxiv.org/pdf/2405.07288v2.pdf","comment":"25 pages, 28 figures, accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2304.14326v2","updated":"2024-08-29T06:17:11Z","published":"2023-04-27T16:58:29Z","title":"A Best-of-Both-Worlds Algorithm for Constrained MDPs with Long-Term\n  Constraints","summary":"  We study online learning in episodic constrained Markov decision processes\n(CMDPs), where the learner aims at collecting as much reward as possible over\nthe episodes, while satisfying some long-term constraints during the learning\nprocess. Rewards and constraints can be selected either stochastically or\nadversarially, and the transition function is not known to the learner. While\nonline learning in classical (unconstrained) MDPs has received considerable\nattention over the last years, the setting of CMDPs is still largely\nunexplored. This is surprising, since in real-world applications, such as,\ne.g., autonomous driving, automated bidding, and recommender systems, there are\nusually additional constraints and specifications that an agent has to obey\nduring the learning process. In this paper, we provide the first\nbest-of-both-worlds algorithm for CMDPs with long-term constraints, in the\nflavor of Balseiro et al. (2023). Our algorithm is capable of handling settings\nin which rewards and constraints are selected either stochastically or\nadversarially, without requiring any knowledge of the underling process.\nMoreover, our algorithm matches state-of-the-art regret and constraint\nviolation bounds for settings in which constraints are selected stochastically,\nwhile it is the first to provide guarantees in the case in which they are\nchosen adversarially.\n","authors":["Jacopo Germano","Francesco Emanuele Stradi","Gianmarco Genalti","Matteo Castiglioni","Alberto Marchesi","Nicola Gatti"],"pdf_url":"https://arxiv.org/pdf/2304.14326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14014v2","updated":"2024-08-29T06:04:57Z","published":"2024-08-26T04:39:33Z","title":"Category-Theoretical and Topos-Theoretical Frameworks in Machine\n  Learning: A Survey","summary":"  In this survey, we provide an overview of category theory-derived machine\nlearning from four mainstream perspectives: gradient-based learning,\nprobability-based learning, invariance and equivalence-based learning, and\ntopos-based learning. For the first three topics, we primarily review research\nin the past five years, updating and expanding on the previous survey by\nShiebler et al.. The fourth topic, which delves into higher category theory,\nparticularly topos theory, is surveyed for the first time in this paper. In\ncertain machine learning methods, the compositionality of functors plays a\nvital role, prompting the development of specific categorical frameworks.\nHowever, when considering how the global properties of a network reflect in\nlocal structures and how geometric properties are expressed with logic, the\ntopos structure becomes particularly significant and profound.\n","authors":["Yiyang Jia","Guohong Peng","Zheng Yang","Tianhao Chen"],"pdf_url":"https://arxiv.org/pdf/2408.14014v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06380v3","updated":"2024-08-29T05:59:24Z","published":"2023-10-10T07:46:54Z","title":"CAST: Cluster-Aware Self-Training for Tabular Data via Reliable\n  Confidence","summary":"  Tabular data is one of the most widely used data modalities, encompassing\nnumerous datasets with substantial amounts of unlabeled data. Despite this\nprevalence, there is a notable lack of simple and versatile methods for\nutilizing unlabeled data in the tabular domain, where both gradient-boosting\ndecision trees and neural networks are employed. In this context, self-training\nhas gained attraction due to its simplicity and versatility, yet it is\nvulnerable to noisy pseudo-labels caused by erroneous confidence. Several\nsolutions have been proposed to handle this problem, but they often compromise\nthe inherent advantages of self-training, resulting in limited applicability in\nthe tabular domain. To address this issue, we explore a novel direction of\nreliable confidence in self-training contexts and conclude that self-training\ncan be improved by making that the confidence, which represents the value of\nthe pseudo-label, aligns with the cluster assumption. In this regard, we\npropose Cluster-Aware Self-Training (CAST) for tabular data, which enhances\nexisting self-training algorithms at a negligible cost while maintaining\nsimplicity and versatility. Concretely, CAST calibrates confidence by\nregularizing the classifier's confidence based on local density for each class\nin the labeled training data, resulting in lower confidence for pseudo-labels\nin low-density regions. Extensive empirical evaluations on up to 21 real-world\ndatasets confirm not only the superior performance of CAST but also its\nrobustness in various setups in self-training contexts.\n","authors":["Minwook Kim","Juseong Kim","Ki Beom Kim","Giltae Song"],"pdf_url":"https://arxiv.org/pdf/2310.06380v3.pdf","comment":"11 pages for main body, and 10 additional pages for appendix"},{"id":"http://arxiv.org/abs/2408.16278v1","updated":"2024-08-29T05:56:35Z","published":"2024-08-29T05:56:35Z","title":"Web Service QoS Prediction via Extended Canonical Polyadic-based Tensor\n  Network","summary":"  Today, numerous web services with similar functionalities are available on\nthe Internet. Users often evaluate the Quality of Service (QoS) to choose the\nbest option among them. Predicting the QoS values of these web services is a\nsignificant challenge in the field of web services. A Canonical Polyadic\n(CP)-based tensor network model has proven to be efficient for predicting\ndynamic QoS data. However, current CP-based tensor network models do not\nconsider the correlation of users and services in the low-dimensional latent\nfeature space, thereby limiting model's prediction capability. To tackle this\nissue, this paper proposes an Extended Canonical polyadic-based Tensor Network\n(ECTN) model. It models the correlation of users and services via building a\nrelation dimension between user feature and service feature in low-dimensional\nspace, and then designs an extended CP decomposition structure to improve\nprediction accuracy. Experiments are conducted on two public dynamic QoS data,\nand the results show that compared with state-of-the-art QoS prediction models,\nthe ECTN obtains higher prediction accuracy.\n","authors":["Qu Wang","Hao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16278v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.01942v4","updated":"2024-08-29T05:48:42Z","published":"2024-03-04T11:24:51Z","title":"Mitigating Label Noise on Graph via Topological Sample Selection","summary":"  Despite the success of the carefully-annotated benchmarks, the effectiveness\nof existing graph neural networks (GNNs) can be considerably impaired in\npractice when the real-world graph data is noisily labeled. Previous\nexplorations in sample selection have been demonstrated as an effective way for\nrobust learning with noisy labels, however, the conventional studies focus on\ni.i.d data, and when moving to non-iid graph data and GNNs, two notable\nchallenges remain: (1) nodes located near topological class boundaries are very\ninformative for classification but cannot be successfully distinguished by the\nheuristic sample selection. (2) there is no available measure that considers\nthe graph topological information to promote sample selection in a graph. To\naddress this dilemma, we propose a $\\textit{Topological Sample Selection}$\n(TSS) method that boosts the informative sample selection process in a graph by\nutilising topological information. We theoretically prove that our procedure\nminimizes an upper bound of the expected risk under target clean distribution,\nand experimentally show the superiority of our method compared with\nstate-of-the-art baselines.\n","authors":["Yuhao Wu","Jiangchao Yao","Xiaobo Xia","Jun Yu","Ruxin Wang","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.01942v4.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.14400v2","updated":"2024-08-29T05:37:38Z","published":"2024-08-26T16:34:13Z","title":"Satellite Sunroof: High-res Digital Surface Models and Roof Segmentation\n  for Global Solar Mapping","summary":"  The transition to renewable energy, particularly solar, is key to mitigating\nclimate change. Google's Solar API aids this transition by estimating solar\npotential from aerial imagery, but its impact is constrained by geographical\ncoverage. This paper proposes expanding the API's reach using satellite\nimagery, enabling global solar potential assessment. We tackle challenges\ninvolved in building a Digital Surface Model (DSM) and roof instance\nsegmentation from lower resolution and single oblique views using deep learning\nmodels. Our models, trained on aligned satellite and aerial datasets, produce\n25cm DSMs and roof segments. With ~1m DSM MAE on buildings, ~5deg roof pitch\nerror and ~56% IOU on roof segmentation, they significantly enhance the Solar\nAPI's potential to promote solar adoption.\n","authors":["Vishal Batchu","Alex Wilson","Betty Peng","Carl Elkin","Umangi Jain","Christopher Van Arsdale","Ross Goroshin","Varun Gulshan"],"pdf_url":"https://arxiv.org/pdf/2408.14400v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.16262v1","updated":"2024-08-29T04:57:44Z","published":"2024-08-29T04:57:44Z","title":"On Convergence of Average-Reward Q-Learning in Weakly Communicating\n  Markov Decision Processes","summary":"  This paper analyzes reinforcement learning (RL) algorithms for Markov\ndecision processes (MDPs) under the average-reward criterion. We focus on\nQ-learning algorithms based on relative value iteration (RVI), which are\nmodel-free stochastic analogues of the classical RVI method for average-reward\nMDPs. These algorithms have low per-iteration complexity, making them\nwell-suited for large state space problems. We extend the almost-sure\nconvergence analysis of RVI Q-learning algorithms developed by Abounadi,\nBertsekas, and Borkar (2001) from unichain to weakly communicating MDPs. This\nextension is important both practically and theoretically: weakly communicating\nMDPs cover a much broader range of applications compared to unichain MDPs, and\ntheir optimality equations have a richer solution structure (with multiple\ndegrees of freedom), introducing additional complexity in proving algorithmic\nconvergence. We also characterize the sets to which RVI Q-learning algorithms\nconverge, showing that they are compact, connected, potentially nonconvex, and\ncomprised of solutions to the average-reward optimality equation, with exactly\none less degree of freedom than the general solution set of this equation.\nFurthermore, we extend our analysis to two RVI-based hierarchical\naverage-reward RL algorithms using the options framework, proving their\nalmost-sure convergence and characterizing their sets of convergence under the\nassumption that the underlying semi-Markov decision process is weakly\ncommunicating.\n","authors":["Yi Wan","Huizhen Yu","Richard S. Sutton"],"pdf_url":"https://arxiv.org/pdf/2408.16262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16261v1","updated":"2024-08-29T04:46:49Z","published":"2024-08-29T04:46:49Z","title":"Evaluating Time-Series Training Dataset through Lens of Spectrum in Deep\n  State Space Models","summary":"  This study investigates a method to evaluate time-series datasets in terms of\nthe performance of deep neural networks (DNNs) with state space models (deep\nSSMs) trained on the dataset. SSMs have attracted attention as components\ninside DNNs to address time-series data. Since deep SSMs have powerful\nrepresentation capacities, training datasets play a crucial role in solving a\nnew task. However, the effectiveness of training datasets cannot be known until\ndeep SSMs are actually trained on them. This can increase the cost of data\ncollection for new tasks, as a trial-and-error process of data collection and\ntime-consuming training are needed to achieve the necessary performance. To\nadvance the practical use of deep SSMs, the metric of datasets to estimate the\nperformance early in the training can be one key element. To this end, we\nintroduce the concept of data evaluation methods used in system identification.\nIn system identification of linear dynamical systems, the effectiveness of\ndatasets is evaluated by using the spectrum of input signals. We introduce this\nconcept to deep SSMs, which are nonlinear dynamical systems. We propose the\nK-spectral metric, which is the sum of the top-K spectra of signals inside deep\nSSMs, by focusing on the fact that each layer of a deep SSM can be regarded as\na linear dynamical system. Our experiments show that the K-spectral metric has\na large absolute value of the correlation coefficient with the performance and\ncan be used to evaluate the quality of training datasets.\n","authors":["Sekitoshi Kanai","Yasutoshi Ida","Kazuki Adachi","Mihiro Uchida","Tsukasa Yoshida","Shin'ya Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2408.16261v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.16256v1","updated":"2024-08-29T04:35:36Z","published":"2024-08-29T04:35:36Z","title":"Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer\n  Metastasis Using Real-World Clinical Data with AUC up to 0.9","summary":"  Breast cancer is one of the two cancers responsible for the most deaths in\nwomen, with about 42,000 deaths each year in the US. That there are over\n300,000 breast cancers newly diagnosed each year suggests that only a fraction\nof the cancers result in mortality. Thus, most of the women undergo seemingly\ncurative treatment for localized cancers, but a significant later succumb to\nmetastatic disease for which current treatments are only temporizing for the\nvast majority. The current prognostic metrics are of little actionable value\nfor 4 of the 5 women seemingly cured after local treatment, and many women are\nexposed to morbid and even mortal adjuvant therapies unnecessarily, with these\nadjuvant therapies reducing metastatic recurrence by only a third. Thus, there\nis a need for better prognostics to target aggressive treatment at those who\nare likely to relapse and spare those who were actually cured. While there is a\nplethora of molecular and tumor-marker assays in use and under-development to\ndetect recurrence early, these are time consuming, expensive and still often\nun-validated as to actionable prognostic utility. A different approach would\nuse large data techniques to determine clinical and histopathological\nparameters that would provide accurate prognostics using existing data. Herein,\nwe report on machine learning, together with grid search and Bayesian Networks\nto develop algorithms that present a AUC of up to 0.9 in ROC analyses, using\nonly extant data. Such algorithms could be rapidly translated to clinical\nmanagement as they do not require testing beyond routine tumor evaluations.\n","authors":["Xia Jiang","Yijun Zhou","Alan Wells","Adam Brufsky"],"pdf_url":"https://arxiv.org/pdf/2408.16256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16249v1","updated":"2024-08-29T04:06:34Z","published":"2024-08-29T04:06:34Z","title":"Iterated Energy-based Flow Matching for Sampling from Boltzmann\n  Densities","summary":"  In this work, we consider the problem of training a generator from\nevaluations of energy functions or unnormalized densities. This is a\nfundamental problem in probabilistic inference, which is crucial for scientific\napplications such as learning the 3D coordinate distribution of a molecule. To\nsolve this problem, we propose iterated energy-based flow matching (iEFM), the\nfirst off-policy approach to train continuous normalizing flow (CNF) models\nfrom unnormalized densities. We introduce the simulation-free energy-based flow\nmatching objective, which trains the model to predict the Monte Carlo\nestimation of the marginal vector field constructed from known energy\nfunctions. Our framework is general and can be extended to variance-exploding\n(VE) and optimal transport (OT) conditional probability paths. We evaluate iEFM\non a two-dimensional Gaussian mixture model (GMM) and an eight-dimensional\nfour-particle double-well potential (DW-4) energy function. Our results\ndemonstrate that iEFM outperforms existing methods, showcasing its potential\nfor efficient and scalable probabilistic modeling in complex high-dimensional\nsystems.\n","authors":["Dongyeop Woo","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2408.16249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16246v1","updated":"2024-08-29T03:58:19Z","published":"2024-08-29T03:58:19Z","title":"PACiM: A Sparsity-Centric Hybrid Compute-in-Memory Architecture via\n  Probabilistic Approximation","summary":"  Approximate computing emerges as a promising approach to enhance the\nefficiency of compute-in-memory (CiM) systems in deep neural network\nprocessing. However, traditional approximate techniques often significantly\ntrade off accuracy for power efficiency, and fail to reduce data transfer\nbetween main memory and CiM banks, which dominates power consumption. This\npaper introduces a novel probabilistic approximate computation (PAC) method\nthat leverages statistical techniques to approximate multiply-and-accumulation\n(MAC) operations, reducing approximation error by 4X compared to existing\napproaches. PAC enables efficient sparsity-based computation in CiM systems by\nsimplifying complex MAC vector computations into scalar calculations. Moreover,\nPAC enables sparsity encoding and eliminates the LSB activations transmission,\nsignificantly reducing data reads and writes. This sets PAC apart from\ntraditional approximate computing techniques, minimizing not only computation\npower but also memory accesses by 50%, thereby boosting system-level\nefficiency. We developed PACiM, a sparsity-centric architecture that fully\nexploits sparsity to reduce bit-serial cycles by 81% and achieves a peak 8b/8b\nefficiency of 14.63 TOPS/W in 65 nm CMOS while maintaining high accuracy of\n93.85/72.36/66.02% on CIFAR-10/CIFAR-100/ImageNet benchmarks using a ResNet-18\nmodel, demonstrating the effectiveness of our PAC methodology.\n","authors":["Wenlun Zhang","Shimpei Ando","Yung-Chin Chen","Satomi Miyagi","Shinya Takamaeda-Yamazaki","Kentaro Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2408.16246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16245v1","updated":"2024-08-29T03:56:40Z","published":"2024-08-29T03:56:40Z","title":"Large-Scale Multi-omic Biosequence Transformers for Modeling\n  Peptide-Nucleotide Interactions","summary":"  The transformer architecture has revolutionized bioinformatics and driven\nprogress in the understanding and prediction of the properties of biomolecules.\nAlmost all research on large-scale biosequence transformers has focused on one\ndomain at a time (single-omic), usually nucleotides or peptides. These models\nhave seen incredible success in downstream tasks in each domain and have\nachieved particularly noteworthy breakthroughs in sequences of peptides and\nstructural modeling. However, these single-omic models are naturally incapable\nof modeling multi-omic tasks, one of the most biologically critical being\nnucleotide-peptide interactions.\n  We present our work training the first multi-omic nucleotide-peptide\nfoundation models. We show that these multi-omic models (MOMs) can learn joint\nrepresentations between various single-omic distributions that are emergently\nconsistent with the Central Dogma of molecular biology, despite only being\ntrained on unlabeled biosequences. We further demonstrate that MOMs can be\nfine-tuned to achieve state-of-the-art results on peptide-nucleotide\ninteraction tasks, namely predicting the change in Gibbs free energy\n({\\Delta}G) of the binding interaction between a given oligonucleotide and\npeptide, as well as the effect on this binding interaction due to mutations in\nthe oligonucleotide sequence ({\\Delta}{\\Delta}G).\n  Remarkably, we show that multi-omic biosequence transformers emergently learn\nuseful structural information without any prior structural training, allowing\nus to predict which peptide residues are most involved in the\npeptide-nucleotide binding interaction. Lastly, we provide evidence that\nmulti-omic biosequence models are non-inferior to foundation models trained on\nsingle-omics distributions, suggesting a more generalized or foundational\napproach to building these models.\n","authors":["Sully F. Chen","Robert J. Steele","Beakal Lemeneh","Shivanand P. Lad","Eric Oermann"],"pdf_url":"https://arxiv.org/pdf/2408.16245v1.pdf","comment":"27 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.16232v1","updated":"2024-08-29T03:12:04Z","published":"2024-08-29T03:12:04Z","title":"Enhancing Conditional Image Generation with Explainable Latent Space\n  Manipulation","summary":"  In the realm of image synthesis, achieving fidelity to a reference image\nwhile adhering to conditional prompts remains a significant challenge. This\npaper proposes a novel approach that integrates a diffusion model with latent\nspace manipulation and gradient-based selective attention mechanisms to address\nthis issue. Leveraging Grad-SAM (Gradient-based Selective Attention\nManipulation), we analyze the cross attention maps of the cross attention\nlayers and gradients for the denoised latent vector, deriving importance scores\nof elements of denoised latent vector related to the subject of interest. Using\nthis information, we create masks at specific timesteps during denoising to\npreserve subjects while seamlessly integrating the reference image features.\nThis approach ensures the faithful formation of subjects based on conditional\nprompts, while concurrently refining the background for a more coherent\ncomposition. Our experiments on places365 dataset demonstrate promising\nresults, with our proposed model achieving the lowest mean and median Frechet\nInception Distance (FID) scores compared to baseline models, indicating\nsuperior fidelity preservation. Furthermore, our model exhibits competitive\nperformance in aligning the generated images with provided textual\ndescriptions, as evidenced by high CLIP scores. These results highlight the\neffectiveness of our approach in both fidelity preservation and textual context\npreservation, offering a significant advancement in text-to-image synthesis\ntasks.\n","authors":["Kshitij Pathania"],"pdf_url":"https://arxiv.org/pdf/2408.16232v1.pdf","comment":"7 pages , 5 figures"},{"id":"http://arxiv.org/abs/2312.02139v3","updated":"2024-08-29T03:09:40Z","published":"2023-12-04T18:57:01Z","title":"DiffiT: Diffusion Vision Transformers for Image Generation","summary":"  Diffusion models with their powerful expressivity and high sample quality\nhave achieved State-Of-The-Art (SOTA) performance in the generative domain. The\npioneering Vision Transformer (ViT) has also demonstrated strong modeling\ncapabilities and scalability, especially for recognition tasks. In this paper,\nwe study the effectiveness of ViTs in diffusion-based generative learning and\npropose a new model denoted as Diffusion Vision Transformers (DiffiT).\nSpecifically, we propose a methodology for finegrained control of the denoising\nprocess and introduce the Time-dependant Multihead Self Attention (TMSA)\nmechanism. DiffiT is surprisingly effective in generating high-fidelity images\nwith significantly better parameter efficiency. We also propose latent and\nimage space DiffiT models and show SOTA performance on a variety of\nclass-conditional and unconditional synthesis tasks at different resolutions.\nThe Latent DiffiT model achieves a new SOTA FID score of 1.73 on ImageNet256\ndataset while having 19.85%, 16.88% less parameters than other\nTransformer-based diffusion models such as MDT and DiT,respectively. Code:\nhttps://github.com/NVlabs/DiffiT\n","authors":["Ali Hatamizadeh","Jiaming Song","Guilin Liu","Jan Kautz","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2312.02139v3.pdf","comment":"Accepted to ECCV'24"},{"id":"http://arxiv.org/abs/2408.16228v1","updated":"2024-08-29T03:03:35Z","published":"2024-08-29T03:03:35Z","title":"Policy Adaptation via Language Optimization: Decomposing Tasks for\n  Few-Shot Imitation","summary":"  Learned language-conditioned robot policies often struggle to effectively\nadapt to new real-world tasks even when pre-trained across a diverse set of\ninstructions. We propose a novel approach for few-shot adaptation to unseen\ntasks that exploits the semantic understanding of task decomposition provided\nby vision-language models (VLMs). Our method, Policy Adaptation via Language\nOptimization (PALO), combines a handful of demonstrations of a task with\nproposed language decompositions sampled from a VLM to quickly enable rapid\nnonparametric adaptation, avoiding the need for a larger fine-tuning dataset.\nWe evaluate PALO on extensive real-world experiments consisting of challenging\nunseen, long-horizon robot manipulation tasks. We find that PALO is able of\nconsistently complete long-horizon, multi-tier tasks in the real world,\noutperforming state of the art pre-trained generalist policies, and methods\nthat have access to the same demonstrations.\n","authors":["Vivek Myers","Bill Chunyuan Zheng","Oier Mees","Sergey Levine","Kuan Fang"],"pdf_url":"https://arxiv.org/pdf/2408.16228v1.pdf","comment":"27 pages, 14 figures"},{"id":"http://arxiv.org/abs/2307.03411v2","updated":"2024-08-29T02:45:14Z","published":"2023-07-07T06:26:44Z","title":"Learning from Heterogeneity: A Dynamic Learning Framework for\n  Hypergraphs","summary":"  Graph neural network (GNN) has gained increasing popularity in recent years\nowing to its capability and flexibility in modeling complex graph structure\ndata. Among all graph learning methods, hypergraph learning is a technique for\nexploring the implicit higher-order correlations when training the embedding\nspace of the graph. In this paper, we propose a hypergraph learning framework\nnamed LFH that is capable of dynamic hyperedge construction and attentive\nembedding update utilizing the heterogeneity attributes of the graph.\nSpecifically, in our framework, the high-quality features are first generated\nby the pairwise fusion strategy that utilizes explicit graph structure\ninformation when generating initial node embedding. Afterwards, a hypergraph is\nconstructed through the dynamic grouping of implicit hyperedges, followed by\nthe type-specific hypergraph learning process. To evaluate the effectiveness of\nour proposed framework, we conduct comprehensive experiments on several popular\ndatasets with eleven state-of-the-art models on both node classification and\nlink prediction tasks, which fall into categories of homogeneous pairwise graph\nlearning, heterogeneous pairwise graph learning, and hypergraph learning. The\nexperiment results demonstrate a significant performance gain (average 12.5% in\nnode classification and 13.3% in link prediction) compared with recent\nstate-of-the-art methods.\n","authors":["Tiehua Zhang","Yuze Liu","Zhishu Shen","Xingjun Ma","Peng Qi","Zhijun Ding","Jiong Jin"],"pdf_url":"https://arxiv.org/pdf/2307.03411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07585v2","updated":"2024-08-29T02:37:03Z","published":"2024-03-12T12:15:57Z","title":"Communication Optimization for Distributed Training: Architecture,\n  Advances, and Opportunities","summary":"  The past few years have witnessed the flourishing of large-scale deep neural\nnetwork models with ever-growing parameter numbers. Training such large-scale\nmodels typically requires massive memory and computing resources, necessitating\ndistributed training. As GPU performance has rapidly evolved in recent years,\ncomputation time has shrunk, making communication a larger portion of the\noverall training time. Consequently, optimizing communication for distributed\ntraining has become crucial. In this article, we briefly introduce the general\narchitecture of distributed deep neural network training and analyze\nrelationships among Parallelization Strategy, Collective Communication Library,\nand Network from the perspective of communication optimization, which forms a\nthree-layer paradigm. We then review current representative research advances\nwithin this three-layer paradigm. We find that layers in the current\nthree-layer paradigm are relatively independent and there is a rich design\nspace for cross-layer collaborative optimization in distributed training\nscenarios. Therefore, we advocate \"Vertical\" and \"Horizontal\" co-designs which\nextend the three-layer paradigm to a five-layer paradigm. We also advocate\n\"Intra-Inter\" and \"Host-Net\" co-designs to further utilize the potential of\nheterogeneous resources. We hope this article can shed some light on future\nresearch on communication optimization for distributed training.\n","authors":["Yunze Wei","Tianshuo Hu","Cong Liang","Yong Cui"],"pdf_url":"https://arxiv.org/pdf/2403.07585v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.21191v2","updated":"2024-08-29T02:27:19Z","published":"2024-07-30T20:58:36Z","title":"GenRec: Generative Sequential Recommendation with Large Language Models","summary":"  Sequential recommendation is a task to capture hidden user preferences from\nhistorical user item interaction data and recommend next items for the user.\nSignificant progress has been made in this domain by leveraging classification\nbased learning methods. Inspired by the recent paradigm of 'pretrain, prompt\nand predict' in NLP, we consider sequential recommendation as a sequence to\nsequence generation task and propose a novel model named Generative\nRecommendation (GenRec). Unlike classification based models that learn explicit\nuser and item representations, GenRec utilizes the sequence modeling capability\nof Transformer and adopts the masked item prediction objective to effectively\nlearn the hidden bidirectional sequential patterns. Different from existing\ngenerative sequential recommendation models, GenRec does not rely on manually\ndesigned hard prompts. The input to GenRec is textual user item sequence and\nthe output is top ranked next items. Moreover, GenRec is lightweight and\nrequires only a few hours to train effectively in low-resource settings, making\nit highly applicable to real-world scenarios and helping to democratize large\nlanguage models in the sequential recommendation domain. Our extensive\nexperiments have demonstrated that GenRec generalizes on various public\nreal-world datasets and achieves state-of-the-art results. Our experiments also\nvalidate the effectiveness of the the proposed masked item prediction objective\nthat improves the model performance by a large margin.\n","authors":["Panfeng Cao","Pietro Lio"],"pdf_url":"https://arxiv.org/pdf/2407.21191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16218v1","updated":"2024-08-29T02:21:11Z","published":"2024-08-29T02:21:11Z","title":"Targeted Cause Discovery with Data-Driven Learning","summary":"  We propose a novel machine learning approach for inferring causal variables\nof a target variable from observations. Our goal is to identify both direct and\nindirect causes within a system, thereby efficiently regulating the target\nvariable when the difficulty and cost of intervening on each causal variable\nvary. Our method employs a neural network trained to identify causality through\nsupervised learning on simulated data. By implementing a local-inference\nstrategy, we achieve linear complexity with respect to the number of variables,\nefficiently scaling up to thousands of variables. Empirical results demonstrate\nthe effectiveness of our method in identifying causal relationships within\nlarge-scale gene regulatory networks, outperforming existing causal discovery\nmethods that primarily focus on direct causality. We validate our model's\ngeneralization capability across novel graph structures and generating\nmechanisms, including gene regulatory networks of E. coli and the human K562\ncell line. Implementation codes are available at\nhttps://github.com/snu-mllab/Targeted-Cause-Discovery.\n","authors":["Jang-Hyun Kim","Claudia Skok Gibbs","Sangdoo Yun","Hyun Oh Song","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2408.16218v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2408.16215v1","updated":"2024-08-29T02:18:28Z","published":"2024-08-29T02:18:28Z","title":"Adversarial Network Optimization under Bandit Feedback: Maximizing\n  Utility in Non-Stationary Multi-Hop Networks","summary":"  Stochastic Network Optimization (SNO) concerns scheduling in stochastic\nqueueing systems. It has been widely studied in network theory. Classical SNO\nalgorithms require network conditions to be stationary with time, which fails\nto capture the non-stationary components in many real-world scenarios. Many\nexisting algorithms also assume knowledge of network conditions before\ndecision, which rules out applications where unpredictability presents.\n  Motivated by these issues, we consider Adversarial Network Optimization (ANO)\nunder bandit feedback. Specifically, we consider the task of *i)* maximizing\nsome unknown and time-varying utility function associated to scheduler's\nactions, where *ii)* the underlying network is a non-stationary multi-hop one\nwhose conditions change arbitrarily with time, and *iii)* only bandit feedback\n(effect of actually deployed actions) is revealed after decisions. Our proposed\n`UMO2` algorithm ensures network stability and also matches the utility\nmaximization performance of any \"mildly varying\" reference policy up to a\npolynomially decaying gap. To our knowledge, no previous ANO algorithm handled\nmulti-hop networks or achieved utility guarantees under bandit feedback,\nwhereas ours can do both.\n  Technically, our method builds upon a novel integration of online learning\ninto Lyapunov analyses: To handle complex inter-dependencies among queues in\nmulti-hop networks, we propose meticulous techniques to balance online learning\nand Lyapunov arguments. To tackle the learning obstacles due to potentially\nunbounded queue sizes, we design a new online linear optimization algorithm\nthat automatically adapts to loss magnitudes. To maximize utility, we propose a\nbandit convex optimization algorithm with novel queue-dependent learning rate\nscheduling that suites drastically varying queue lengths. Our new insights in\nonline learning can be of independent interest.\n","authors":["Yan Dai","Longbo Huang"],"pdf_url":"https://arxiv.org/pdf/2408.16215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13989v2","updated":"2024-08-29T02:17:51Z","published":"2024-07-19T02:34:10Z","title":"Enhancing Data-Limited Graph Neural Networks by Actively Distilling\n  Knowledge from Large Language Models","summary":"  Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins.\n","authors":["Quan Li","Tianxiang Zhao","Lingwei Chen","Junjie Xu","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13989v2.pdf","comment":"10 pages, 3 Figures"},{"id":"http://arxiv.org/abs/2408.16212v1","updated":"2024-08-29T02:09:19Z","published":"2024-08-29T02:09:19Z","title":"The Application of Machine Learning in Tidal Evolution Simulation of\n  Star-Planet Systems","summary":"  With the release of a large amount of astronomical data, an increasing number\nof close-in hot Jupiters have been discovered. Calculating their evolutionary\ncurves using star-planet interaction models presents a challenge. To expedite\nthe generation of evolutionary curves for these close-in hot Jupiter systems,\nwe utilized tidal interaction models established on MESA to create 15,745\nsamples of star-planet systems and 7,500 samples of stars. Additionally, we\nemployed a neural network (Multi-Layer Perceptron - MLP) to predict the\nevolutionary curves of the systems, including stellar effective temperature,\nradius, stellar rotation period, and planetary orbital period. The median\nrelative errors of the predicted evolutionary curves were found to be 0.15%,\n0.43%, 2.61%, and 0.57%, respectively. Furthermore, the speed at which we\ngenerate evolutionary curves exceeds that of model-generated curves by more\nthan four orders of magnitude. We also extracted features of planetary\nmigration states and utilized lightGBM to classify the samples into 6\ncategories for prediction. We found that by combining three types that undergo\nlong-term double synchronization into one label, the classifier effectively\nrecognized these features. Apart from systems experiencing long-term double\nsynchronization, the median relative errors of the predicted evolutionary\ncurves were all below 4%. Our work provides an efficient method to save\nsignificant computational resources and time with minimal loss in accuracy.\nThis research also lays the foundation for analyzing the evolutionary\ncharacteristics of systems under different migration states, aiding in the\nunderstanding of the underlying physical mechanisms of such systems. Finally,\nto a large extent, our approach could replace the calculations of theoretical\nmodels.\n","authors":["Shuaishuai Guo","Jianheng Guo","KaiFan Ji","Hui Liu","Lei Xing"],"pdf_url":"https://arxiv.org/pdf/2408.16212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16208v1","updated":"2024-08-29T02:03:05Z","published":"2024-08-29T02:03:05Z","title":"ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology\n  Report Generation Metrics","summary":"  Given the rapidly expanding capabilities of generative AI models for\nradiology, there is a need for robust metrics that can accurately measure the\nquality of AI-generated radiology reports across diverse hospitals. We develop\nReXamine-Global, a LLM-powered, multi-site framework that tests metrics across\ndifferent writing styles and patient populations, exposing gaps in their\ngeneralization. First, our method tests whether a metric is undesirably\nsensitive to reporting style, providing different scores depending on whether\nAI-generated reports are stylistically similar to ground-truth reports or not.\nSecond, our method measures whether a metric reliably agrees with experts, or\nwhether metric and expert scores of AI-generated report quality diverge for\nsome sites. Using 240 reports from 6 hospitals around the world, we apply\nReXamine-Global to 7 established report evaluation metrics and uncover serious\ngaps in their generalizability. Developers can apply ReXamine-Global when\ndesigning new report evaluation metrics, ensuring their robustness across\nsites. Additionally, our analysis of existing metrics can guide users of those\nmetrics towards evaluation procedures that work reliably at their sites of\ninterest.\n","authors":["Oishi Banerjee","Agustina Saenz","Kay Wu","Warren Clements","Adil Zia","Dominic Buensalido","Helen Kavnoudias","Alain S. Abi-Ghanem","Nour El Ghawi","Cibele Luna","Patricia Castillo","Khaled Al-Surimi","Rayyan A. Daghistani","Yuh-Min Chen","Heng-sheng Chao","Lars Heiliger","Moon Kim","Johannes Haubold","Frederic Jonske","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2408.16208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15591v2","updated":"2024-08-29T02:01:56Z","published":"2024-08-28T07:31:32Z","title":"VFLIP: A Backdoor Defense for Vertical Federated Learning via\n  Identification and Purification","summary":"  Vertical Federated Learning (VFL) focuses on handling vertically partitioned\ndata over FL participants. Recent studies have discovered a significant\nvulnerability in VFL to backdoor attacks which specifically target the distinct\ncharacteristics of VFL. Therefore, these attacks may neutralize existing\ndefense mechanisms designed primarily for Horizontal Federated Learning (HFL)\nand deep neural networks. In this paper, we present the first backdoor defense,\ncalled VFLIP, specialized for VFL. VFLIP employs the identification and\npurification techniques that operate at the inference stage, consequently\nimproving the robustness against backdoor attacks to a great extent. VFLIP\nfirst identifies backdoor-triggered embeddings by adopting a participant-wise\nanomaly detection approach. Subsequently, VFLIP conducts purification which\nremoves the embeddings identified as malicious and reconstructs all the\nembeddings based on the remaining embeddings. We conduct extensive experiments\non CIFAR10, CINIC10, Imagenette, NUS-WIDE, and BankMarketing to demonstrate\nthat VFLIP can effectively mitigate backdoor attacks in VFL.\nhttps://github.com/blingcho/VFLIP-esorics24\n","authors":["Yungi Cho","Woorim Han","Miseon Yu","Younghan Lee","Ho Bae","Yunheung Paek"],"pdf_url":"https://arxiv.org/pdf/2408.15591v2.pdf","comment":"Accepted by 29th European Symposium on Research in Computer Security\n  (ESORICS 2024)"},{"id":"http://arxiv.org/abs/2406.02126v3","updated":"2024-08-29T02:00:25Z","published":"2024-06-04T09:10:14Z","title":"CityLight: A Universal Model for Coordinated Traffic Signal Control in\n  City-scale Heterogeneous Intersections","summary":"  The increasingly severe congestion problem in modern cities strengthens the\nsignificance of developing city-scale traffic signal control (TSC) methods for\ntraffic efficiency enhancement. While reinforcement learning has been widely\nexplored in TSC, most of them still target small-scale optimization and cannot\ndirectly scale to the city level due to unbearable resource demand. Only a few\nof them manage to tackle city-level optimization, namely a thousand-scale\noptimization, by incorporating parameter-sharing mechanisms, but hardly have\nthey fully tackled the heterogeneity of intersections and intricate\nbetween-intersection interactions inherent in real-world city road networks. To\nfill in the gap, we target at the two important challenges in adopting\nparameter-sharing paradigms to solve TSC: inconsistency of inner state\nrepresentations for intersections heterogeneous in configuration, scale, and\norders of available traffic phases; intricacy of impacts from neighborhood\nintersections that have various relative traffic relationships due to\ninconsistent phase orders and diverse relative positioning. Our method,\nCityLight, features a universal representation module that not only aligns the\nstate representations of intersections by reindexing their phases based on\ntheir semantics and designing heterogeneity-preserving observations, but also\nencodes the narrowed relative traffic relation types to project the\nneighborhood intersections onto a uniform relative traffic impact space. We\nfurther attentively fuse neighborhood representations based on their competing\nrelations and incorporate neighborhood-integrated rewards to boost\ncoordination. Extensive experiments with hundreds to tens of thousands of\nintersections validate the surprising effectiveness and generalizability of\nCityLight, with an overall performance gain of 11.68% and a 22.59% improvement\nin transfer scenarios in throughput.\n","authors":["Jinwei Zeng","Chao Yu","Xinyi Yang","Wenxuan Ao","Qianyue Hao","Jian Yuan","Yong Li","Yu Wang","Huazhong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.02126v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16204v1","updated":"2024-08-29T01:50:13Z","published":"2024-08-29T01:50:13Z","title":"Revisit Micro-batch Clipping: Adaptive Data Pruning via Gradient\n  Manipulation","summary":"  Micro-batch clipping, a gradient clipping method, has recently shown\npotential in enhancing auto-speech recognition (ASR) model performance.\nHowever, the underlying mechanism behind this improvement remains mysterious,\nparticularly the observation that only certain micro-batch sizes are\nbeneficial. In this paper, we make the first attempt to explain this\nphenomenon. Inspired by recent data pruning research, we assume that specific\ntraining samples may impede model convergence during certain training phases.\nUnder this assumption, the convergence analysis shows that micro-batch clipping\ncan improve the convergence rate asymptotically at the cost of an additional\nconstant bias that does not diminish with more training iterations. The bias is\ndependent on a few factors and can be minimized at specific micro-batch size,\nthereby elucidating the existence of the sweet-spot micro-batch size observed\npreviously. We also verify the effectiveness of micro-batch clipping beyond\nspeech models on vision and language models, and show promising performance\ngains in these domains. An exploration of potential limitations shows that\nmicro-batch clipping is less effective when training data originates from\nmultiple distinct domains.\n","authors":["Lun Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16202v1","updated":"2024-08-29T01:47:09Z","published":"2024-08-29T01:47:09Z","title":"Short-Term Electricity-Load Forecasting by Deep Learning: A\n  Comprehensive Survey","summary":"  Short-Term Electricity-Load Forecasting (STELF) refers to the prediction of\nthe immediate demand (in the next few hours to several days) for the power\nsystem. Various external factors, such as weather changes and the emergence of\nnew electricity consumption scenarios, can impact electricity demand, causing\nload data to fluctuate and become non-linear, which increases the complexity\nand difficulty of STELF. In the past decade, deep learning has been applied to\nSTELF, modeling and predicting electricity demand with high accuracy, and\ncontributing significantly to the development of STELF. This paper provides a\ncomprehensive survey on deep-learning-based STELF over the past ten years. It\nexamines the entire forecasting process, including data pre-processing, feature\nextraction, deep-learning modeling and optimization, and results evaluation.\nThis paper also identifies some research challenges and potential research\ndirections to be further investigated in future work.\n","authors":["Qi Dong","Rubing Huang","Chenhui Cui","Dave Towey","Ling Zhou","Jinyu Tian","Jianzhou Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16201v1","updated":"2024-08-29T01:46:37Z","published":"2024-08-29T01:46:37Z","title":"Uni-3DAD: GAN-Inversion Aided Universal 3D Anomaly Detection on\n  Model-free Products","summary":"  Anomaly detection is a long-standing challenge in manufacturing systems.\nTraditionally, anomaly detection has relied on human inspectors. However, 3D\npoint clouds have gained attention due to their robustness to environmental\nfactors and their ability to represent geometric data. Existing 3D anomaly\ndetection methods generally fall into two categories. One compares scanned 3D\npoint clouds with design files, assuming these files are always available.\nHowever, such assumptions are often violated in many real-world applications\nwhere model-free products exist, such as fresh produce (i.e., ``Cookie\",\n``Potato\", etc.), dentures, bone, etc. The other category compares patches of\nscanned 3D point clouds with a library of normal patches named memory bank.\nHowever, those methods usually fail to detect incomplete shapes, which is a\nfairly common defect type (i.e., missing pieces of different products). The\nmain challenge is that missing areas in 3D point clouds represent the absence\nof scanned points. This makes it infeasible to compare the missing region with\nexisting point cloud patches in the memory bank. To address these two\nchallenges, we proposed a unified, unsupervised 3D anomaly detection framework\ncapable of identifying all types of defects on model-free products. Our method\nintegrates two detection modules: a feature-based detection module and a\nreconstruction-based detection module. Feature-based detection covers geometric\ndefects, such as dents, holes, and cracks, while the reconstruction-based\nmethod detects missing regions. Additionally, we employ a One-class Support\nVector Machine (OCSVM) to fuse the detection results from both modules. The\nresults demonstrate that (1) our proposed method outperforms the\nstate-of-the-art methods in identifying incomplete shapes and (2) it still\nmaintains comparable performance with the SOTA methods in detecting all other\ntypes of anomalies.\n","authors":["Jiayu Liu","Shancong Mou","Nathan Gaw","Yinan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.01970v7","updated":"2024-08-29T01:38:57Z","published":"2022-05-04T09:37:16Z","title":"Non-Stationary Bandit Learning via Predictive Sampling","summary":"  Thompson sampling has proven effective across a wide range of stationary\nbandit environments. However, as we demonstrate in this paper, it can perform\npoorly when applied to non-stationary environments. We attribute such failures\nto the fact that, when exploring, the algorithm does not differentiate actions\nbased on how quickly the information acquired loses its usefulness due to\nnon-stationarity. Building upon this insight, we propose predictive sampling,\nan algorithm that deprioritizes acquiring information that quickly loses\nusefulness. A theoretical guarantee on the performance of predictive sampling\nis established through a Bayesian regret bound. We provide versions of\npredictive sampling for which computations tractably scale to complex bandit\nenvironments of practical interest. Through numerical simulations, we\ndemonstrate that predictive sampling outperforms Thompson sampling in all\nnon-stationary environments examined.\n","authors":["Yueyang Liu","Xu Kuang","Benjamin Van Roy"],"pdf_url":"https://arxiv.org/pdf/2205.01970v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06727v2","updated":"2024-08-29T01:26:31Z","published":"2023-06-11T17:17:48Z","title":"A Normalized Bottleneck Distance on Persistence Diagrams and Homology\n  Preservation under Dimension Reduction","summary":"  Persistence diagrams (PDs) are used as signatures of point cloud data. Two\nclouds of points can be compared using the bottleneck distance d_B between\ntheir PDs. A potential drawback of this pipeline is that point clouds sampled\nfrom topologically similar manifolds can have arbitrarily large d_B when there\nis a large scaling between them. This situation is typical in dimension\nreduction frameworks.\n  We define, and study properties of, a new scale-invariant distance between\nPDs termed normalized bottleneck distance, d_N. In defining d_N, we develop a\nbroader framework called metric decomposition for comparing finite metric\nspaces of equal cardinality with a bijection. We utilize metric decomposition\nto prove a stability result for d_N by deriving an explicit bound on the\ndistortion of the bijective map. We then study two popular dimension reduction\ntechniques, Johnson-Lindenstrauss (JL) projections and metric multidimensional\nscaling (mMDS), and a third class of general biLipschitz mappings. We provide\nnew bounds on how well these dimension reduction techniques preserve homology\nwith respect to d_N. For a JL map f that transforms input X to f(X), we show\nthat d_N(dgm(X),dgm(f(X))) < e, where dgm(X) is the Vietoris-Rips PD of X, and\npairwise distances are preserved by f up to the tolerance 0 < \\epsilon < 1. For\nmMDS, we present new bounds for d_B and d_N between PDs of X and its projection\nin terms of the eigenvalues of the covariance matrix. And for k-biLipschitz\nmaps, we show that d_N is bounded by the product of (k^2-1)/k and the ratio of\ndiameters of X and f(X). Finally, we use computational experiments to\ndemonstrate the increased effectiveness of using the normalized bottleneck\ndistance for clustering sets of point clouds sampled from different shapes.\n","authors":["Nathan H. May","Bala Krishnamoorthy","Patrick Gambill"],"pdf_url":"https://arxiv.org/pdf/2306.06727v2.pdf","comment":"Added computational experiments; published in La Matematica"},{"id":"http://arxiv.org/abs/2408.16191v1","updated":"2024-08-29T01:09:30Z","published":"2024-08-29T01:09:30Z","title":"Variational Mode-Driven Graph Convolutional Network for Spatiotemporal\n  Traffic Forecasting","summary":"  This paper focuses on spatio-temporal (ST) traffic prediction traffic using\ngraph neural networks. Given that ST data consists of non-stationary and\ncomplex time events, interpreting and predicting such trends is comparatively\ncomplicated. Representation of ST data in modes helps us infer behavior and\nassess the impact of noise on prediction applications. We propose a framework\nthat decomposes ST data into modes using the variational mode decomposition\n(VMD) method, which is then fed into the neural network for forecasting future\nstates. This hybrid approach is known as a variational mode graph convolutional\nnetwork (VMGCN). Instead of exhaustively searching for the number of modes,\nthey are determined using the reconstruction loss from the real-time\napplication data. We also study the significance of each mode and the impact of\nbandwidth constraints on different horizon predictions in traffic flow data. We\nevaluate the performance of our proposed network on the LargeST dataset for\nboth short and long-term predictions. Our framework yields better results\ncompared to state-of-the-art methods.\n","authors":["Osama Ahmad","Zubair Khalid"],"pdf_url":"https://arxiv.org/pdf/2408.16191v1.pdf","comment":"IEEE Transactions on Intelligent Transportation Systems Submission,\n  2024"},{"id":"http://arxiv.org/abs/2408.16189v1","updated":"2024-08-29T01:02:40Z","published":"2024-08-29T01:02:40Z","title":"A More Unified Theory of Transfer Learning","summary":"  We show that some basic moduli of continuity $\\delta$ -- which measure how\nfast target risk decreases as source risk decreases -- appear to be at the root\nof many of the classical relatedness measures in transfer learning and related\nliterature. Namely, bounds in terms of $\\delta$ recover many of the existing\nbounds in terms of other measures of relatedness -- both in regression and\nclassification -- and can at times be tighter.\n  We are particularly interested in general situations where the learner has\naccess to both source data and some or no target data. The unified perspective\nallowed by the moduli $\\delta$ allow us to extend many existing notions of\nrelatedness at once to these scenarios involving target data: interestingly,\nwhile $\\delta$ itself might not be efficiently estimated, adaptive procedures\nexist -- based on reductions to confidence sets -- which can get nearly tight\nrates in terms of $\\delta$ with no prior distributional knowledge. Such\nadaptivity to unknown $\\delta$ immediately implies adaptivity to many classical\nrelatedness notions, in terms of combined source and target samples' sizes.\n","authors":["Steve Hanneke","Samory Kpotufe"],"pdf_url":"https://arxiv.org/pdf/2408.16189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13467v2","updated":"2024-08-29T00:54:27Z","published":"2024-08-24T05:03:08Z","title":"LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to\n  Small-Scale Local LLMs","summary":"  The widespread adoption of cloud-based proprietary large language models\n(LLMs) has introduced significant challenges, including operational\ndependencies, privacy concerns, and the necessity of continuous internet\nconnectivity. In this work, we introduce an LLMOps pipeline, \"LlamaDuo\", for\nthe seamless migration of knowledge and abilities from service-oriented LLMs to\nsmaller, locally manageable models. This pipeline is crucial for ensuring\nservice continuity in the presence of operational failures, strict privacy\npolicies, or offline requirements. Our LlamaDuo involves fine-tuning a small\nlanguage model against the service LLM using a synthetic dataset generated by\nthe latter. If the performance of the fine-tuned model falls short of\nexpectations, it is enhanced by further fine-tuning with additional similar\ndata created by the service LLM. This iterative process guarantees that the\nsmaller model can eventually match or even surpass the service LLM's\ncapabilities in specific downstream tasks, offering a practical and scalable\nsolution for managing AI deployments in constrained environments. Extensive\nexperiments with leading edge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of LlamaDuo across various\ndownstream tasks. Our pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo.\n","authors":["Chansung Park","Juyong Jiang","Fan Wang","Sayak Paul","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2408.13467v2.pdf","comment":"28 pages, 18 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.16187v1","updated":"2024-08-29T00:53:21Z","published":"2024-08-29T00:53:21Z","title":"Real-Time Energy Pricing in New Zealand: An Evolving Stream Analysis","summary":"  This paper introduces a group of novel datasets representing real-time\ntime-series and streaming data of energy prices in New Zealand, sourced from\nthe Electricity Market Information (EMI) website maintained by the New Zealand\ngovernment. The datasets are intended to address the scarcity of proper\ndatasets for streaming regression learning tasks. We conduct extensive analyses\nand experiments on these datasets, covering preprocessing techniques,\nregression tasks, prediction intervals, concept drift detection, and anomaly\ndetection. Our experiments demonstrate the datasets' utility and highlight the\nchallenges and opportunities for future research in energy price forecasting.\n","authors":["Yibin Sun","Heitor Murilo Gomes","Bernhard Pfahringer","Albert Bifet"],"pdf_url":"https://arxiv.org/pdf/2408.16187v1.pdf","comment":"12 Pages, 8 figures, short version accepted by PRICAI"},{"id":"http://arxiv.org/abs/2408.16186v1","updated":"2024-08-29T00:50:35Z","published":"2024-08-29T00:50:35Z","title":"Single-Loop Deterministic and Stochastic Interior-Point Algorithms for\n  Nonlinearly Constrained Optimization","summary":"  An interior-point algorithm framework is proposed, analyzed, and tested for\nsolving nonlinearly constrained continuous optimization problems. The main\nsetting of interest is when the objective and constraint functions may be\nnonlinear and/or nonconvex, and when constraint values and derivatives are\ntractable to compute, but objective function values and derivatives can only be\nestimated. The algorithm is intended primarily for a setting that is similar\nfor stochastic-gradient methods for unconstrained optimization, namely, the\nsetting when stochastic-gradient estimates are available and employed in place\nof gradients of the objective, and when no objective function values (nor\nestimates of them) are employed. This is achieved by the interior-point\nframework having a single-loop structure rather than the nested-loop structure\nthat is typical of contemporary interior-point methods. For completeness,\nconvergence guarantees for the framework are provided both for deterministic\nand stochastic settings. Numerical experiments show that the algorithm yields\ngood performance on a large set of test problems.\n","authors":["Frank E. Curtis","Xin Jiang","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04992v2","updated":"2024-08-29T00:40:05Z","published":"2024-07-06T07:56:23Z","title":"Scalable Variational Causal Discovery Unconstrained by Acyclicity","summary":"  Bayesian causal discovery offers the power to quantify epistemic\nuncertainties among a broad range of structurally diverse causal theories\npotentially explaining the data, represented in forms of directed acyclic\ngraphs (DAGs). However, existing methods struggle with efficient DAG sampling\ndue to the complex acyclicity constraint. In this study, we propose a scalable\nBayesian approach to effectively learn the posterior distribution over causal\ngraphs given observational data thanks to the ability to generate DAGs without\nexplicitly enforcing acyclicity. Specifically, we introduce a novel\ndifferentiable DAG sampling method that can generate a valid acyclic causal\ngraph by mapping an unconstrained distribution of implicit topological orders\nto a distribution over DAGs. Given this efficient DAG sampling scheme, we are\nable to model the posterior distribution over causal graphs using a simple\nvariational distribution over a continuous domain, which can be learned via the\nvariational inference framework. Extensive empirical experiments on both\nsimulated and real datasets demonstrate the superior performance of the\nproposed model compared to several state-of-the-art baselines.\n","authors":["Nu Hoang","Bao Duong","Thin Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.04992v2.pdf","comment":"Accepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2408.16181v1","updated":"2024-08-29T00:36:34Z","published":"2024-08-29T00:36:34Z","title":"A Minibatch-SGD-Based Learning Meta-Policy for Inventory Systems with\n  Myopic Optimal Policy","summary":"  Stochastic gradient descent (SGD) has proven effective in solving many\ninventory control problems with demand learning. However, it often faces the\npitfall of an infeasible target inventory level that is lower than the current\ninventory level. Several recent works (e.g., Huh and Rusmevichientong (2009),\nShi et al.(2016)) are successful to resolve this issue in various inventory\nsystems. However, their techniques are rather sophisticated and difficult to be\napplied to more complicated scenarios such as multi-product and\nmulti-constraint inventory systems.\n  In this paper, we address the infeasible-target-inventory-level issue from a\nnew technical perspective -- we propose a novel minibatch-SGD-based\nmeta-policy. Our meta-policy is flexible enough to be applied to a general\ninventory systems framework covering a wide range of inventory management\nproblems with myopic clairvoyant optimal policy. By devising the optimal\nminibatch scheme, our meta-policy achieves a regret bound of\n$\\mathcal{O}(\\sqrt{T})$ for the general convex case and $\\mathcal{O}(\\log T)$\nfor the strongly convex case. To demonstrate the power and flexibility of our\nmeta-policy, we apply it to three important inventory control problems:\nmulti-product and multi-constraint systems, multi-echelon serial systems, and\none-warehouse and multi-store systems by carefully designing\napplication-specific subroutines.We also conduct extensive numerical\nexperiments to demonstrate that our meta-policy enjoys competitive regret\nperformance, high computational efficiency, and low variances among a wide\nrange of applications.\n","authors":["Jiameng Lyu","Jinxing Xie","Shilin Yuan","Yuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.16181v1.pdf","comment":"Forthcoming in Management Science"},{"id":"http://arxiv.org/abs/2407.04980v2","updated":"2024-08-29T00:34:59Z","published":"2024-07-06T07:19:21Z","title":"Enabling Causal Discovery in Post-Nonlinear Models with Normalizing\n  Flows","summary":"  Post-nonlinear (PNL) causal models stand out as a versatile and adaptable\nframework for modeling intricate causal relationships. However, accurately\ncapturing the invertibility constraint required in PNL models remains\nchallenging in existing studies. To address this problem, we introduce CAF-PoNo\n(Causal discovery via Normalizing Flows for Post-Nonlinear models), harnessing\nthe power of the normalizing flows architecture to enforce the crucial\ninvertibility constraint in PNL models. Through normalizing flows, our method\nprecisely reconstructs the hidden noise, which plays a vital role in\ncause-effect identification through statistical independence testing.\nFurthermore, the proposed approach exhibits remarkable extensibility, as it can\nbe seamlessly expanded to facilitate multivariate causal discovery via causal\norder identification, empowering us to efficiently unravel complex causal\nrelationships. Extensive experimental evaluations on both simulated and real\ndatasets consistently demonstrate that the proposed method outperforms several\nstate-of-the-art approaches in both bivariate and multivariate causal discovery\ntasks.\n","authors":["Nu Hoang","Bao Duong","Thin Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.04980v2.pdf","comment":"Acepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2108.02497v5","updated":"2024-08-29T10:12:35Z","published":"2021-08-05T10:15:17Z","title":"How to avoid machine learning pitfalls: a guide for academic researchers","summary":"  Mistakes in machine learning practice are commonplace, and can result in a\nloss of confidence in the findings and products of machine learning. This guide\noutlines common mistakes that occur when using machine learning, and what can\nbe done to avoid them. Whilst it should be accessible to anyone with a basic\nunderstanding of machine learning techniques, it focuses on issues that are of\nparticular concern within academic research, such as the need to do rigorous\ncomparisons and reach valid conclusions. It covers five stages of the machine\nlearning process: what to do before model building, how to reliably build\nmodels, how to robustly evaluate models, how to compare models fairly, and how\nto report results.\n","authors":["Michael A. Lones"],"pdf_url":"https://arxiv.org/pdf/2108.02497v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16945v1","updated":"2024-08-29T23:51:51Z","published":"2024-08-29T23:51:51Z","title":"Different Victims, Same Layout: Email Visual Similarity Detection for\n  Enhanced Email Protection","summary":"  In the pursuit of an effective spam detection system, the focus has often\nbeen on identifying known spam patterns either through rule-based detection\nsystems or machine learning (ML) solutions. However, both systems are\nsusceptible to evasion techniques and zero-day attacks that can be achieved at\nlow cost. Therefore, an email that bypassed the defense system once can do it\nagain in the following days, even though rules are updated or the ML models are\nretrained. The recurrence of failures to detect emails that exhibit layout\nsimilarities to previously undetected spam is concerning for customers and can\nerode their trust in a company. Our observations show that threat actors reuse\nemail kits extensively and can bypass detection with little effort, for\nexample, by making changes to the content of emails. In this work, we propose\nan email visual similarity detection approach, named Pisco, to improve the\ndetection capabilities of an email threat defense system. We apply our proof of\nconcept to some real-world samples received from different sources. Our results\nshow that email kits are being reused extensively and visually similar emails\nare sent to our customers at various time intervals. Therefore, this method\ncould be very helpful in situations where detection features that rely on\ncontextual information and keywords are bypassed, an occurrence our\nobservations show happens frequently.\n","authors":["Sachin Shukla","Omid Mirzaei"],"pdf_url":"https://arxiv.org/pdf/2408.16945v1.pdf","comment":"To be published in the proceedings of the ACM Conference on Computer\n  and Communications Security (ACM CCS 2024)"},{"id":"http://arxiv.org/abs/2408.16944v1","updated":"2024-08-29T23:48:08Z","published":"2024-08-29T23:48:08Z","title":"FlowRetrieval: Flow-Guided Data Retrieval for Few-Shot Imitation\n  Learning","summary":"  Few-shot imitation learning relies on only a small amount of task-specific\ndemonstrations to efficiently adapt a policy for a given downstream tasks.\nRetrieval-based methods come with a promise of retrieving relevant past\nexperiences to augment this target data when learning policies. However,\nexisting data retrieval methods fall under two extremes: they either rely on\nthe existence of exact behaviors with visually similar scenes in the prior\ndata, which is impractical to assume; or they retrieve based on semantic\nsimilarity of high-level language descriptions of the task, which might not be\nthat informative about the shared low-level behaviors or motions across tasks\nthat is often a more important factor for retrieving relevant data for policy\nlearning. In this work, we investigate how we can leverage motion similarity in\nthe vast amount of cross-task data to improve few-shot imitation learning of\nthe target task. Our key insight is that motion-similar data carries rich\ninformation about the effects of actions and object interactions that can be\nleveraged during few-shot adaptation. We propose FlowRetrieval, an approach\nthat leverages optical flow representations for both extracting similar motions\nto target tasks from prior data, and for guiding learning of a policy that can\nmaximally benefit from such data. Our results show FlowRetrieval significantly\noutperforms prior methods across simulated and real-world domains, achieving on\naverage 27% higher success rate than the best retrieval-based prior method. In\nthe Pen-in-Cup task with a real Franka Emika robot, FlowRetrieval achieves 3.7x\nthe performance of the baseline imitation learning technique that learns from\nall prior and target data. Website: https://flow-retrieval.github.io\n","authors":["Li-Heng Lin","Yuchen Cui","Amber Xie","Tianyu Hua","Dorsa Sadigh"],"pdf_url":"https://arxiv.org/pdf/2408.16944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16941v1","updated":"2024-08-29T23:36:10Z","published":"2024-08-29T23:36:10Z","title":"Efficient Transonic Aeroelastic Model Reduction Using Optimized Sparse\n  Multi-Input Polynomial Functionals","summary":"  Nonlinear aeroelastic reduced-order models (ROMs) based on machine learning\nor artificial intelligence algorithms can be complex and computationally\ndemanding to train, meaning that for practical aeroelastic applications, the\nconservative nature of linearization is often favored. Therefore, there is a\nrequirement for novel nonlinear aeroelastic model reduction approaches that are\naccurate, simple and, most importantly, efficient to generate. This paper\nproposes a novel formulation for the identification of a compact multi-input\nVolterra series, where Orthogonal Matching Pursuit is used to obtain a set of\noptimally sparse nonlinear multi-input ROM coefficients from unsteady\naerodynamic training data. The framework is exemplified using the Benchmark\nSupercritical Wing, considering; forced response, flutter and limit cycle\noscillation. The simple and efficient Optimal Sparsity Multi-Input ROM\n(OSM-ROM) framework performs with high accuracy compared to the full-order\naeroelastic model, requiring only a fraction of the tens-of-thousands of\npossible multi-input terms to be identified and allowing a 96% reduction in the\nnumber of training samples.\n","authors":["Michael Candon","Maciej Balajewicz","Arturo Delgado-Gutierrez","Pier Marzocca","Earl H. Dowell"],"pdf_url":"https://arxiv.org/pdf/2408.16941v1.pdf","comment":"24 pages, preprint, under review"},{"id":"http://arxiv.org/abs/2408.16939v1","updated":"2024-08-29T23:22:40Z","published":"2024-08-29T23:22:40Z","title":"Theoretical Insights into Overparameterized Models in Multi-Task and\n  Replay-Based Continual Learning","summary":"  Multi-task learning (MTL) is a machine learning paradigm that aims to improve\nthe generalization performance of a model on multiple related tasks by training\nit simultaneously on those tasks. Unlike MTL, where the model has instant\naccess to the training data of all tasks, continual learning (CL) involves\nadapting to new sequentially arriving tasks over time without forgetting the\npreviously acquired knowledge. Despite the wide practical adoption of CL and\nMTL and extensive literature on both areas, there remains a gap in the\ntheoretical understanding of these methods when used with overparameterized\nmodels such as deep neural networks. This paper studies the overparameterized\nlinear models as a proxy for more complex models. We develop theoretical\nresults describing the effect of various system parameters on the model's\nperformance in an MTL setup. Specifically, we study the impact of model size,\ndataset size, and task similarity on the generalization error and knowledge\ntransfer. Additionally, we present theoretical results to characterize the\nperformance of replay-based CL models. Our results reveal the impact of buffer\nsize and model capacity on the forgetting rate in a CL setup and help shed\nlight on some of the state-of-the-art CL methods. Finally, through extensive\nempirical evaluations, we demonstrate that our theoretical findings are also\napplicable to deep neural networks, offering valuable guidance for designing\nMTL and CL models in practice.\n","authors":["Mohammadamin Banayeeanzade","Mahdi Soltanolkotabi","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2408.16939v1.pdf","comment":"41 pages, 21 figures"},{"id":"http://arxiv.org/abs/2405.08174v2","updated":"2024-08-29T23:21:03Z","published":"2024-05-13T20:39:27Z","title":"Estimating Direct and Indirect Causal Effects of Spatiotemporal\n  Interventions in Presence of Spatial Interference","summary":"  Spatial interference (SI) occurs when the treatment at one location affects\nthe outcomes at other locations. Accounting for spatial interference in\nspatiotemporal settings poses further challenges as interference violates the\nstable unit treatment value assumption, making it infeasible for standard\ncausal inference methods to quantify the effects of time-varying treatment at\nspatially varying outcomes. In this paper, we first formalize the concept of\nspatial interference in case of time-varying treatment assignments by extending\nthe potential outcome framework under the assumption of no unmeasured\nconfounding. We then propose our deep learning based potential outcome model\nfor spatiotemporal causal inference. We utilize latent factor modeling to\nreduce the bias due to time-varying confounding while leveraging the power of\nU-Net architecture to capture global and local spatial interference in data\nover time. Our causal estimators are an extension of average treatment effect\n(ATE) for estimating direct (DATE) and indirect effects (IATE) of spatial\ninterference on treated and untreated data. Being the first of its kind deep\nlearning based spatiotemporal causal inference technique, our approach shows\nadvantages over several baseline methods based on the experiment results on two\nsynthetic datasets, with and without spatial interference. Our results on\nreal-world climate dataset also align with domain knowledge, further\ndemonstrating the effectiveness of our proposed method.\n","authors":["Sahara Ali","Omar Faruque","Jianwu Wang"],"pdf_url":"https://arxiv.org/pdf/2405.08174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16929v1","updated":"2024-08-29T22:08:07Z","published":"2024-08-29T22:08:07Z","title":"AI-driven Reverse Engineering of QML Models","summary":"  Quantum machine learning (QML) is a rapidly emerging area of research, driven\nby the capabilities of Noisy Intermediate-Scale Quantum (NISQ) devices. With\nthe progress in the research of QML models, there is a rise in third-party\nquantum cloud services to cater to the increasing demand for resources. New\nsecurity concerns surface, specifically regarding the protection of\nintellectual property (IP) from untrustworthy service providers. One of the\nmost pressing risks is the potential for reverse engineering (RE) by malicious\nactors who may steal proprietary quantum IPs such as trained parameters and QML\narchitecture, modify them to remove additional watermarks or signatures and\nre-transpile them for other quantum hardware. Prior work presents a brute force\napproach to RE the QML parameters which takes exponential time overhead. In\nthis paper, we introduce an autoencoder-based approach to extract the\nparameters from transpiled QML models deployed on untrusted third-party\nvendors. We experiment on multi-qubit classifiers and note that they can be\nreverse-engineered under restricted conditions with a mean error of order\n10^-1. The amount of time taken to prepare the dataset and train the model to\nreverse engineer the QML circuit being of the order 10^3 seconds (which is\n10^2x better than the previously reported value for 4-layered 4-qubit\nclassifiers) makes the threat of RE highly potent, underscoring the need for\ncontinued development of effective defenses.\n","authors":["Archisman Ghosh","Swaroop Ghosh"],"pdf_url":"https://arxiv.org/pdf/2408.16929v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.16913v1","updated":"2024-08-29T21:21:53Z","published":"2024-08-29T21:21:53Z","title":"Analyzing Inference Privacy Risks Through Gradients in Machine Learning","summary":"  In distributed learning settings, models are iteratively updated with shared\ngradients computed from potentially sensitive user data. While previous work\nhas studied various privacy risks of sharing gradients, our paper aims to\nprovide a systematic approach to analyze private information leakage from\ngradients. We present a unified game-based framework that encompasses a broad\nrange of attacks including attribute, property, distributional, and user\ndisclosures. We investigate how different uncertainties of the adversary affect\ntheir inferential power via extensive experiments on five datasets across\nvarious data modalities. Our results demonstrate the inefficacy of solely\nrelying on data aggregation to achieve privacy against inference attacks in\ndistributed learning. We further evaluate five types of defenses, namely,\ngradient pruning, signed gradient descent, adversarial perturbations,\nvariational information bottleneck, and differential privacy, under both static\nand adaptive adversary settings. We provide an information-theoretic view for\nanalyzing the effectiveness of these defenses against inference from gradients.\nFinally, we introduce a method for auditing attribute inference privacy,\nimproving the empirical estimation of worst-case privacy through crafting\nadversarial canary records.\n","authors":["Zhuohang Li","Andrew Lowy","Jing Liu","Toshiaki Koike-Akino","Kieran Parsons","Bradley Malin","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16896v1","updated":"2024-08-29T20:39:54Z","published":"2024-08-29T20:39:54Z","title":"DLFormer: Enhancing Explainability in Multivariate Time Series\n  Forecasting using Distributed Lag Embedding","summary":"  . Most real-world variables are multivariate time series influenced by past\nvalues and explanatory factors. Consequently, predicting these time series data\nusing artificial intelligence is ongoing. In particular, in fields such as\nhealthcare and finance, where reliability is crucial, having understandable\nexplanations for predictions is essential. However, achieving a balance between\nhigh prediction accuracy and intuitive explainability has proven challenging.\nAlthough attention-based models have limitations in representing the individual\ninfluences of each variable, these models can influence the temporal\ndependencies in time series prediction and the magnitude of the influence of\nindividual variables. To address this issue, this study introduced DLFormer, an\nattention-based architecture integrated with distributed lag embedding, to\ntemporally embed individual variables and capture their temporal influence.\nThrough validation against various real-world datasets, DLFormer showcased\nsuperior performance improvements compared to existing attention-based\nhigh-performance models. Furthermore, comparing the relationships between\nvariables enhanced the reliability of explainability.\n","authors":["Younghwi Kim","Dohee Kim","Sunghyun Sim"],"pdf_url":"https://arxiv.org/pdf/2408.16896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16893v1","updated":"2024-08-29T20:27:05Z","published":"2024-08-29T20:27:05Z","title":"Exploring Multiple Strategies to Improve Multilingual Coreference\n  Resolution in CorefUD","summary":"  Coreference resolution, the task of identifying expressions in text that\nrefer to the same entity, is a critical component in various natural language\nprocessing (NLP) applications. This paper presents our end-to-end neural\ncoreference resolution system, utilizing the CorefUD 1.1 dataset, which spans\n17 datasets across 12 languages. We first establish strong baseline models,\nincluding monolingual and cross-lingual variations, and then propose several\nextensions to enhance performance across diverse linguistic contexts. These\nextensions include cross-lingual training, incorporation of syntactic\ninformation, a Span2Head model for optimized headword prediction, and advanced\nsingleton modeling. We also experiment with headword span representation and\nlong-documents modeling through overlapping segments. The proposed extensions,\nparticularly the heads-only approach, singleton modeling, and long document\nprediction significantly improve performance across most datasets. We also\nperform zero-shot cross-lingual experiments, highlighting the potential and\nlimitations of cross-lingual transfer in coreference resolution. Our findings\ncontribute to the development of robust and scalable coreference systems for\nmultilingual coreference resolution. Finally, we evaluate our model on CorefUD\n1.1 test set and surpass the best model from CRAC 2023 shared task of a\ncomparable size by a large margin. Our nodel is available on GitHub:\n\\url{https://github.com/ondfa/coref-multiling}\n","authors":["OndÅej PraÅ¾Ã¡k","Miloslav KonopÃ­k"],"pdf_url":"https://arxiv.org/pdf/2408.16893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16892v1","updated":"2024-08-29T20:26:27Z","published":"2024-08-29T20:26:27Z","title":"Tex-ViT: A Generalizable, Robust, Texture-based dual-branch\n  cross-attention deepfake detector","summary":"  Deepfakes, which employ GAN to produce highly realistic facial modification,\nare widely regarded as the prevailing method. Traditional CNN have been able to\nidentify bogus media, but they struggle to perform well on different datasets\nand are vulnerable to adversarial attacks due to their lack of robustness.\nVision transformers have demonstrated potential in the realm of image\nclassification problems, but they require enough training data. Motivated by\nthese limitations, this publication introduces Tex-ViT (Texture-Vision\nTransformer), which enhances CNN features by combining ResNet with a vision\ntransformer. The model combines traditional ResNet features with a texture\nmodule that operates in parallel on sections of ResNet before each\ndown-sampling operation. The texture module then serves as an input to the dual\nbranch of the cross-attention vision transformer. It specifically focuses on\nimproving the global texture module, which extracts feature map correlation.\nEmpirical analysis reveals that fake images exhibit smooth textures that do not\nremain consistent over long distances in manipulations. Experiments were\nperformed on different categories of FF++, such as DF, f2f, FS, and NT,\ntogether with other types of GAN datasets in cross-domain scenarios.\nFurthermore, experiments also conducted on FF++, DFDCPreview, and Celeb-DF\ndataset underwent several post-processing situations, such as blurring,\ncompression, and noise. The model surpassed the most advanced models in terms\nof generalization, achieving a 98% accuracy in cross-domain scenarios. This\ndemonstrates its ability to learn the shared distinguishing textural\ncharacteristics in the manipulated samples. These experiments provide evidence\nthat the proposed model is capable of being applied to various situations and\nis resistant to many post-processing procedures.\n","authors":["Deepak Dagar","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2408.16892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06266v3","updated":"2024-08-29T20:26:19Z","published":"2024-08-12T16:24:51Z","title":"Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment","summary":"  Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.\n","authors":["Karel D'Oosterlinck","Winnie Xu","Chris Develder","Thomas Demeester","Amanpreet Singh","Christopher Potts","Douwe Kiela","Shikib Mehri"],"pdf_url":"https://arxiv.org/pdf/2408.06266v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16890v1","updated":"2024-08-29T20:22:22Z","published":"2024-08-29T20:22:22Z","title":"Robotic warehousing operations: a learn-then-optimize approach to\n  large-scale neighborhood search","summary":"  The rapid deployment of robotics technologies requires dedicated optimization\nalgorithms to manage large fleets of autonomous agents. This paper supports\nrobotic parts-to-picker operations in warehousing by optimizing\norder-workstation assignments, item-pod assignments and the schedule of order\nfulfillment at workstations. The model maximizes throughput, while managing\nhuman workload at the workstations and congestion in the facility. We solve it\nvia large-scale neighborhood search, with a novel learn-then-optimize approach\nto subproblem generation. The algorithm relies on an offline machine learning\nprocedure to predict objective improvements based on subproblem features, and\nan online optimization model to generate a new subproblem at each iteration. In\ncollaboration with Amazon Robotics, we show that our model and algorithm\ngenerate much stronger solutions for practical problems than state-of-the-art\napproaches. In particular, our solution enhances the utilization of robotic\nfleets by coordinating robotic tasks for human operators to pick multiple items\nat once, and by coordinating robotic routes to avoid congestion in the\nfacility.\n","authors":["Cynthia Barnhart","Alexandre Jacquillat","Alexandria Schmid"],"pdf_url":"https://arxiv.org/pdf/2408.16890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00303v3","updated":"2024-08-29T20:21:07Z","published":"2024-05-01T03:59:06Z","title":"Joint Optimization of Piecewise Linear Ensembles","summary":"  Tree ensembles achieve state-of-the-art performance on numerous prediction\ntasks. We propose $\\textbf{J}$oint $\\textbf{O}$ptimization of\n$\\textbf{P}$iecewise $\\textbf{L}$inear $\\textbf{En}$sembles (JOPLEn), which\njointly fits piecewise linear models at all leaf nodes of an existing tree\nensemble. In addition to enhancing the ensemble expressiveness, JOPLEn allows\nseveral common penalties, including sparsity-promoting and subspace-norms, to\nbe applied to nonlinear prediction. For example, JOPLEn with a nuclear norm\npenalty learns subspace-aligned functions. Additionally, JOPLEn (combined with\na Dirty LASSO penalty) is an effective feature selection method for nonlinear\nprediction in multitask learning. Finally, we demonstrate the performance of\nJOPLEn on 153 regression and classification datasets and with a variety of\npenalties. JOPLEn leads to improved prediction performance relative to not only\nstandard random forest and boosted tree ensembles, but also other methods for\nenhancing tree ensembles.\n","authors":["Matt Raymond","Angela Violi","Clayton Scott"],"pdf_url":"https://arxiv.org/pdf/2405.00303v3.pdf","comment":"7 pages, 4 figures, accepted to IEEE MLSP 2024 While preparing the\n  code release, we found minor bugs in the penalty gradient computation and the\n  validation set preprocessing. Fixing these bugs provides the updated results\n  shown in Figure 1 and Section 3.1. The conclusions of the paper remain the\n  same"},{"id":"http://arxiv.org/abs/2408.16889v1","updated":"2024-08-29T20:20:49Z","published":"2024-08-29T20:20:49Z","title":"LLaVA-Chef: A Multi-modal Generative Model for Food Recipes","summary":"  In the rapidly evolving landscape of online recipe sharing within a\nglobalized context, there has been a notable surge in research towards\ncomprehending and generating food recipes. Recent advancements in large\nlanguage models (LLMs) like GPT-2 and LLaVA have paved the way for Natural\nLanguage Processing (NLP) approaches to delve deeper into various facets of\nfood-related tasks, encompassing ingredient recognition and comprehensive\nrecipe generation. Despite impressive performance and multi-modal adaptability\nof LLMs, domain-specific training remains paramount for their effective\napplication. This work evaluates existing LLMs for recipe generation and\nproposes LLaVA-Chef, a novel model trained on a curated dataset of diverse\nrecipe prompts in a multi-stage approach. First, we refine the mapping of\nvisual food image embeddings to the language space. Second, we adapt LLaVA to\nthe food domain by fine-tuning it on relevant recipe data. Third, we utilize\ndiverse prompts to enhance the model's recipe comprehension. Finally, we\nimprove the linguistic quality of generated recipes by penalizing the model\nwith a custom loss function. LLaVA-Chef demonstrates impressive improvements\nover pretrained LLMs and prior works. A detailed qualitative analysis reveals\nthat LLaVA-Chef generates more detailed recipes with precise ingredient\nmentions, compared to existing approaches.\n","authors":["Fnu Mohbat","Mohammed J. Zaki"],"pdf_url":"https://arxiv.org/pdf/2408.16889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16883v1","updated":"2024-08-29T20:12:01Z","published":"2024-08-29T20:12:01Z","title":"Revising Multimodal VAEs with Diffusion Decoders","summary":"  Multimodal VAEs often struggle with generating high-quality outputs, a\nchallenge that extends beyond the inherent limitations of the VAE framework.\nThe core issue lies in the restricted joint representation of the latent space,\nparticularly when complex modalities like images are involved. Feedforward\ndecoders, commonly used for these intricate modalities, inadvertently constrain\nthe joint latent space, leading to a degradation in the quality of the other\nmodalities as well. Although recent studies have shown improvement by\nintroducing modality-specific representations, the issue remains significant.\nIn this work, we demonstrate that incorporating a flexible diffusion decoder\nspecifically for the image modality not only enhances the generation quality of\nthe images but also positively impacts the performance of the other modalities\nthat rely on feedforward decoders. This approach addresses the limitations\nimposed by conventional joint representations and opens up new possibilities\nfor improving multimodal generation tasks using the multimodal VAE framework.\nOur model provides state-of-the-art results compared to other multimodal VAEs\nin different datasets with higher coherence and superior quality in the\ngenerated modalities\n","authors":["Daniel Wesego","Amirmohammad Rooshenas"],"pdf_url":"https://arxiv.org/pdf/2408.16883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16882v1","updated":"2024-08-29T20:09:20Z","published":"2024-08-29T20:09:20Z","title":"Coverage Analysis of Multi-Environment Q-Learning Algorithms for\n  Wireless Network Optimization","summary":"  Q-learning is widely used to optimize wireless networks with unknown system\ndynamics. Recent advancements include ensemble multi-environment hybrid\nQ-learning algorithms, which utilize multiple Q-learning algorithms across\nstructurally related but distinct Markovian environments and outperform\nexisting Q-learning algorithms in terms of accuracy and complexity in\nlarge-scale wireless networks. We herein conduct a comprehensive coverage\nanalysis to ensure optimal data coverage conditions for these algorithms.\nInitially, we establish upper bounds on the expectation and variance of\ndifferent coverage coefficients. Leveraging these bounds, we present an\nalgorithm for efficient initialization of these algorithms. We test our\nalgorithm on two distinct real-world wireless networks. Numerical simulations\nshow that our algorithm can achieve %50 less policy error and %40 less runtime\ncomplexity than state-of-the-art reinforcement learning algorithms.\nFurthermore, our algorithm exhibits robustness to changes in network settings\nand parameters. We also numerically validate our theoretical results.\n","authors":["Talha Bozkus","Urbashi Mitra"],"pdf_url":"https://arxiv.org/pdf/2408.16882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16877v1","updated":"2024-08-29T19:58:46Z","published":"2024-08-29T19:58:46Z","title":"Longitudinal Modularity, a Modularity for Link Streams","summary":"  Temporal networks are commonly used to model real-life phenomena. When these\nphenomena represent interactions and are captured at a fine-grained temporal\nresolution, they are modeled as link streams. Community detection is an\nessential network analysis task. Although many methods exist for static\nnetworks, and some methods have been developed for temporal networks\nrepresented as sequences of snapshots, few works can handle link streams. This\narticle introduces the first adaptation of the well-known Modularity quality\nfunction to link streams. Unlike existing methods, it is independent of the\ntime scale of analysis. After introducing the quality function, and its\nrelation to existing static and dynamic definitions of Modularity, we show\nexperimentally its relevance for dynamic community evaluation.\n","authors":["Victor Brabant","Yasaman Asgari","Pierre Borgnat","Angela Bonifati","Remy Cazabet"],"pdf_url":"https://arxiv.org/pdf/2408.16877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16875v1","updated":"2024-08-29T19:57:52Z","published":"2024-08-29T19:57:52Z","title":"Learning Multi-agent Multi-machine Tending by Mobile Robots","summary":"  Robotics can help address the growing worker shortage challenge of the\nmanufacturing industry. As such, machine tending is a task collaborative robots\ncan tackle that can also highly boost productivity. Nevertheless, existing\nrobotics systems deployed in that sector rely on a fixed single-arm setup,\nwhereas mobile robots can provide more flexibility and scalability. In this\nwork, we introduce a multi-agent multi-machine tending learning framework by\nmobile robots based on Multi-agent Reinforcement Learning (MARL) techniques\nwith the design of a suitable observation and reward. Moreover, an\nattention-based encoding mechanism is developed and integrated into Multi-agent\nProximal Policy Optimization (MAPPO) algorithm to boost its performance for\nmachine tending scenarios. Our model (AB-MAPPO) outperformed MAPPO in this new\nchallenging scenario in terms of task success, safety, and resources\nutilization. Furthermore, we provided an extensive ablation study to support\nour various design decisions.\n","authors":["Abdalwhab Abdalwhab","Giovanni Beltrame","Samira Ebrahimi Kahou","David St-Onge"],"pdf_url":"https://arxiv.org/pdf/2408.16875v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.16871v1","updated":"2024-08-29T19:40:04Z","published":"2024-08-29T19:40:04Z","title":"GSTAM: Efficient Graph Distillation with Structural Attention-Matching","summary":"  Graph distillation has emerged as a solution for reducing large graph\ndatasets to smaller, more manageable, and informative ones. Existing methods\nprimarily target node classification, involve computationally intensive\nprocesses, and fail to capture the true distribution of the full graph dataset.\nTo address these issues, we introduce Graph Distillation with Structural\nAttention Matching (GSTAM), a novel method for condensing graph classification\ndatasets. GSTAM leverages the attention maps of GNNs to distill structural\ninformation from the original dataset into synthetic graphs. The structural\nattention-matching mechanism exploits the areas of the input graph that GNNs\nprioritize for classification, effectively distilling such information into the\nsynthetic graphs and improving overall distillation performance. Comprehensive\nexperiments demonstrate GSTAM's superiority over existing methods, achieving\n0.45% to 6.5% better performance in extreme condensation ratios, highlighting\nits potential use in advancing distillation for graph classification tasks\n(Code available at https://github.com/arashrasti96/GSTAM).\n","authors":["Arash Rasti-Meymandi","Ahmad Sajedi","Zhaopan Xu","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2408.16871v1.pdf","comment":"Accepted at ECCV-DD 2024"},{"id":"http://arxiv.org/abs/2406.05464v2","updated":"2024-08-29T19:30:55Z","published":"2024-06-08T12:58:13Z","title":"DAISY: Data Adaptive Self-Supervised Early Exit for Speech\n  Representation Models","summary":"  Self-supervised speech models have shown to be useful for various tasks, but\ntheir large size limits the use in devices with low computing power and memory.\nIn this work, we explore early exit, an approach for reducing latency by\nexiting the forward process of a network early. Most approaches of early exit\nneed a separate early exit model for each task, with some even requiring\nfine-tuning of the entire pretrained model. We introduce Data Adaptive\nSelf-Supervised Early Exit (DAISY), an approach that decides when to exit based\non the self-supervised loss, eliminating the need for multiple round of\ntraining and fine-tuning. DAISY matches the performance of HuBERT on the\nMiniSUPERB benchmark, but with much faster inference times. Our analysis on the\nadaptivity of DAISY shows that the model exits early (using fewer layers) on\nclean data while exits late (using more layers) on noisy data, dynamically\nadjusting the computational cost of inference based on the noise level of each\nsample.\n","authors":["Tzu-Quan Lin","Hung-yi Lee","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2406.05464v2.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2207.07174v2","updated":"2024-08-29T19:27:49Z","published":"2022-07-14T19:20:30Z","title":"Attribute Graphs Underlying Molecular Generative Models: Path to\n  Learning with Limited Data","summary":"  Training generative models that capture rich semantics of the data and\ninterpreting the latent representations encoded by such models are very\nimportant problems in un-/self-supervised learning. In this work, we provide a\nsimple algorithm that relies on perturbation experiments on latent codes of a\npre-trained generative autoencoder to uncover an attribute graph that is\nimplied by the generative model. We perform perturbation experiments to check\nfor influence of a given latent variable on a subset of attributes. Given this,\nwe show that one can fit an effective graphical model that models a structural\nequation model between latent codes taken as exogenous variables and attributes\ntaken as observed variables. One interesting aspect is that a single latent\nvariable controls multiple overlapping subsets of attributes unlike\nconventional approaches that try to impose full independence. Using a\npre-trained generative autoencoder trained on a large dataset of small\nmolecules, we demonstrate that the graphical model between various molecular\nattributes and latent codes learned by our algorithm can be used to predict a\nspecific property for molecules which are drawn from a different distribution.\nWe compare prediction models trained on various feature subsets chosen by\nsimple baselines, as well as existing causal discovery and sparse\nlearning/feature selection methods, with the ones in the derived Markov blanket\nfrom our method. Results show empirically that the predictor that relies on our\nMarkov blanket attributes is robust to distribution shifts when transferred or\nfine-tuned with a few samples from the new distribution, especially when\ntraining data is limited.\n","authors":["Samuel C. Hoffman","Payel Das","Karthikeyan Shanmugam","Kahini Wadhawan","Prasanna Sattigeri"],"pdf_url":"https://arxiv.org/pdf/2207.07174v2.pdf","comment":"New experiments; reframed contributions"},{"id":"http://arxiv.org/abs/2211.09944v3","updated":"2024-08-29T19:25:59Z","published":"2022-11-17T23:38:29Z","title":"MelHuBERT: A simplified HuBERT on Mel spectrograms","summary":"  Self-supervised models have had great success in learning speech\nrepresentations that can generalize to various downstream tasks. However, most\nself-supervised models require a large amount of compute and multiple GPUs to\ntrain, significantly hampering the development of self-supervised learning. In\nan attempt to reduce the computation of training, we revisit the training of\nHuBERT, a highly successful self-supervised model. We improve and simplify\nseveral key components, including the loss function, input representation, and\ntraining in multiple stages. Our model, MelHuBERT, is able to achieve favorable\nperformance on phone recognition, speaker identification, and automatic speech\nrecognition against HuBERT, while saving 31.2% of the pre-training time, or\nequivalently 33.5% MACs per one second speech. The code and pre-trained models\nare available in https://github.com/nervjack2/MelHuBERT.\n","authors":["Tzu-Quan Lin","Hung-yi Lee","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2211.09944v3.pdf","comment":"ASRU 2023"},{"id":"http://arxiv.org/abs/2406.11402v2","updated":"2024-08-29T19:24:29Z","published":"2024-06-17T10:45:36Z","title":"Are Small Language Models Ready to Compete with Large Language Models\n  for Practical Applications?","summary":"  The rapid rise of Language Models (LMs) has expanded their use in several\napplications. Yet, due to constraints of model size, associated cost, or\nproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always\nfeasible. With open, smaller LMs emerging, more applications can leverage their\ncapabilities, but selecting the right LM can be challenging as smaller LMs\ndon't perform well universally. This work tries to bridge this gap by proposing\na framework to experimentally evaluate small, open LMs in practical settings\nthrough measuring semantic correctness of outputs across three practical\naspects: task types, application domains and reasoning types, using diverse\nprompt styles. It also conducts an in-depth comparison of 10 small, open LMs to\nidentify best LM and prompt style depending on specific application requirement\nusing the proposed framework. We also show that if selected appropriately, they\ncan outperform SOTA LLMs like DeepSeek-v2, GPT-4o-mini, Gemini-1.5-Pro, and\neven compete with GPT-4o.\n","authors":["Neelabh Sinha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2406.11402v2.pdf","comment":"Submitted to ARR"},{"id":"http://arxiv.org/abs/2408.16868v1","updated":"2024-08-29T19:22:37Z","published":"2024-08-29T19:22:37Z","title":"Characterization of point-source transient events with a rolling-shutter\n  compressed sensing system","summary":"  Point-source transient events (PSTEs) - optical events that are both\nextremely fast and extremely small - pose several challenges to an imaging\nsystem. Due to their speed, accurately characterizing such events often\nrequires detectors with very high frame rates. Due to their size, accurately\ndetecting such events requires maintaining coverage over an extended\nfield-of-view, often through the use of imaging focal plane arrays (FPA) with a\nglobal shutter readout. Traditional imaging systems that meet these\nrequirements are costly in terms of price, size, weight, power consumption, and\ndata bandwidth, and there is a need for cheaper solutions with adequate\ntemporal and spatial coverage. To address these issues, we develop a novel\ncompressed sensing algorithm adapted to the rolling shutter readout of an\nimaging system. This approach enables reconstruction of a PSTE signature at the\nsampling rate of the rolling shutter, offering a 1-2 order of magnitude\ntemporal speedup and a proportional reduction in data bandwidth. We present\nempirical results demonstrating accurate recovery of PSTEs using measurements\nthat are spatially undersampled by a factor of 25, and our simulations show\nthat, relative to other compressed sensing algorithms, our algorithm is both\nfaster and yields higher quality reconstructions. We also present theoretical\nresults characterizing our algorithm and corroborating simulations. The\npotential impact of our work includes the development of much faster, cheaper\nsensor solutions for PSTE detection and characterization.\n","authors":["Frank Qiu","Joshua Michalenko","Lilian K. Casias","Cameron J. Radosevich","Jon Slater","Eric A. Shields"],"pdf_url":"https://arxiv.org/pdf/2408.16868v1.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.03886v2","updated":"2024-08-29T19:11:21Z","published":"2024-06-06T09:24:21Z","title":"BiomedBench: A benchmark suite of TinyML biomedical applications for\n  low-power wearables","summary":"  The design of low-power wearables for the biomedical domain has received a\nlot of attention in recent decades, as technological advances in chip\nmanufacturing have allowed real-time monitoring of patients using\nlow-complexity ML within the mW range. Despite advances in application and\nhardware design research, the domain lacks a systematic approach to hardware\nevaluation. In this work, we propose BiomedBench, a new benchmark suite\ncomposed of complete end-to-end TinyML biomedical applications for real-time\nmonitoring of patients using wearable devices. Each application presents\ndifferent requirements during typical signal acquisition and processing phases,\nincluding varying computational workloads and relations between active and idle\ntimes. Furthermore, our evaluation of five state-of-the-art low-power platforms\nin terms of energy efficiency shows that modern platforms cannot effectively\ntarget all types of biomedical applications. BiomedBench is released as an\nopen-source suite to standardize hardware evaluation and guide hardware and\napplication design in the TinyML wearable domain.\n","authors":["Dimitrios Samakovlis","Stefano Albini","RubÃ©n RodrÃ­guez Ãlvarez","Denisa-Andreea Constantinescu","Pasquale Davide Schiavone","Miguel PeÃ³n QuirÃ³s","David Atienza"],"pdf_url":"https://arxiv.org/pdf/2406.03886v2.pdf","comment":"7 pages, 5 figures. Sumbitted to Design & Test Special Issue TinyML"},{"id":"http://arxiv.org/abs/2310.12404v2","updated":"2024-08-29T19:08:54Z","published":"2023-10-19T01:20:12Z","title":"Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative\n  Editing","summary":"  Creating music is iterative, requiring varied methods at each stage. However,\nexisting AI music systems fall short in orchestrating multiple subsystems for\ndiverse needs. To address this gap, we introduce Loop Copilot, a novel system\nthat enables users to generate and iteratively refine music through an\ninteractive, multi-round dialogue interface. The system uses a large language\nmodel to interpret user intentions and select appropriate AI models for task\nexecution. Each backend model is specialized for a specific task, and their\noutputs are aggregated to meet the user's requirements. To ensure musical\ncoherence, essential attributes are maintained in a centralized table. We\nevaluate the effectiveness of the proposed system through semi-structured\ninterviews and questionnaires, highlighting its utility not only in\nfacilitating music creation but also its potential for broader applications.\n","authors":["Yixiao Zhang","Akira Maezawa","Gus Xia","Kazuhiko Yamamoto","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2310.12404v2.pdf","comment":"Source code and demo video are available at\n  \\url{https://sites.google.com/view/loop-copilot}"},{"id":"http://arxiv.org/abs/2408.16862v1","updated":"2024-08-29T18:58:39Z","published":"2024-08-29T18:58:39Z","title":"Probabilistic Decomposed Linear Dynamical Systems for Robust Discovery\n  of Latent Neural Dynamics","summary":"  Time-varying linear state-space models are powerful tools for obtaining\nmathematically interpretable representations of neural signals. For example,\nswitching and decomposed models describe complex systems using latent variables\nthat evolve according to simple locally linear dynamics. However, existing\nmethods for latent variable estimation are not robust to dynamical noise and\nsystem nonlinearity due to noise-sensitive inference procedures and limited\nmodel formulations. This can lead to inconsistent results on signals with\nsimilar dynamics, limiting the model's ability to provide scientific insight.\nIn this work, we address these limitations and propose a probabilistic approach\nto latent variable estimation in decomposed models that improves robustness\nagainst dynamical noise. Additionally, we introduce an extended latent dynamics\nmodel to improve robustness against system nonlinearities. We evaluate our\napproach on several synthetic dynamical systems, including an\nempirically-derived brain-computer interface experiment, and demonstrate more\naccurate latent variable inference in nonlinear systems with diverse noise\nconditions. Furthermore, we apply our method to a real-world clinical\nneurophysiology dataset, illustrating the ability to identify interpretable and\ncoherent structure where previous models cannot.\n","authors":["Yenho Chen","Noga Mudrik","Kyle A. Johnsen","Sankaraleengam Alagapan","Adam S. Charles","Christopher J. Rozell"],"pdf_url":"https://arxiv.org/pdf/2408.16862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16852v1","updated":"2024-08-29T18:34:59Z","published":"2024-08-29T18:34:59Z","title":"The Star Geometry of Critic-Based Regularizer Learning","summary":"  Variational regularization is a classical technique to solve statistical\ninference tasks and inverse problems, with modern data-driven approaches\nparameterizing regularizers via deep neural networks showcasing impressive\nempirical performance. Recent works along these lines learn task-dependent\nregularizers. This is done by integrating information about the measurements\nand ground-truth data in an unsupervised, critic-based loss function, where the\nregularizer attributes low values to likely data and high values to unlikely\ndata. However, there is little theory about the structure of regularizers\nlearned via this process and how it relates to the two data distributions. To\nmake progress on this challenge, we initiate a study of optimizing critic-based\nloss functions to learn regularizers over a particular family of regularizers:\ngauges (or Minkowski functionals) of star-shaped bodies. This family contains\nregularizers that are commonly employed in practice and shares properties with\nregularizers parameterized by deep neural networks. We specifically investigate\ncritic-based losses derived from variational representations of statistical\ndistances between probability measures. By leveraging tools from star geometry\nand dual Brunn-Minkowski theory, we illustrate how these losses can be\ninterpreted as dual mixed volumes that depend on the data distribution. This\nallows us to derive exact expressions for the optimal regularizer in certain\ncases. Finally, we identify which neural network architectures give rise to\nsuch star body gauges and when do such regularizers have favorable properties\nfor optimization. More broadly, this work highlights how the tools of star\ngeometry can aid in understanding the geometry of unsupervised regularizer\nlearning.\n","authors":["Oscar Leong","Eliza O'Reilly","Yong Sheng Soh"],"pdf_url":"https://arxiv.org/pdf/2408.16852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16849v1","updated":"2024-08-29T18:27:32Z","published":"2024-08-29T18:27:32Z","title":"Machine Learning-Based Research on the Adaptability of Adolescents to\n  Online Education","summary":"  With the rapid advancement of internet technology, the adaptability of\nadolescents to online learning has emerged as a focal point of interest within\nthe educational sphere. However, the academic community's efforts to develop\npredictive models for adolescent online learning adaptability require further\nrefinement and expansion. Utilizing data from the \"Chinese Adolescent Online\nEducation Survey\" spanning the years 2014 to 2016, this study implements five\nmachine learning algorithms - logistic regression, K-nearest neighbors, random\nforest, XGBoost, and CatBoost - to analyze the factors influencing adolescent\nonline learning adaptability and to determine the model best suited for\nprediction. The research reveals that the duration of courses, the financial\nstatus of the family, and age are the primary factors affecting students'\nadaptability in online learning environments. Additionally, age significantly\nimpacts students' adaptive capacities. Among the predictive models, the random\nforest, XGBoost, and CatBoost algorithms demonstrate superior forecasting\ncapabilities, with the random forest model being particularly adept at\ncapturing the characteristics of students' adaptability.\n","authors":["Mingwei Wang","Sitong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.16849v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.16625v1","updated":"2024-08-29T15:34:25Z","published":"2024-08-29T15:34:25Z","title":"MultiMediate'24: Multi-Domain Engagement Estimation","summary":"  Estimating the momentary level of participant's engagement is an important\nprerequisite for assistive systems that support human interactions. Previous\nwork has addressed this task in within-domain evaluation scenarios, i.e.\ntraining and testing on the same dataset. This is in contrast to real-life\nscenarios where domain shifts between training and testing data frequently\noccur. With MultiMediate'24, we present the first challenge addressing\nmulti-domain engagement estimation. As training data, we utilise the NOXI\ndatabase of dyadic novice-expert interactions. In addition to within-domain\ntest data, we add two new test domains. First, we introduce recordings\nfollowing the NOXI protocol but covering languages that are not present in the\nNOXI training data. Second, we collected novel engagement annotations on the\nMPIIGroupInteraction dataset which consists of group discussions between three\nto four people. In this way, MultiMediate'24 evaluates the ability of\napproaches to generalise across factors such as language and cultural\nbackground, group size, task, and screen-mediated vs. face-to-face interaction.\nThis paper describes the MultiMediate'24 challenge and presents baseline\nresults. In addition, we discuss selected challenge solutions.\n","authors":["Philipp MÃ¼ller","Michal Balazia","Tobias Baur","Michael Dietz","Alexander Heimerl","Anna Penzkofer","Dominik Schiller","FranÃ§ois BrÃ©mond","Jan Alexandersson","Elisabeth AndrÃ©","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2408.16625v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2308.08256"},{"id":"http://arxiv.org/abs/2408.16564v1","updated":"2024-08-29T14:30:56Z","published":"2024-08-29T14:30:56Z","title":"Human-Inspired Audio-Visual Speech Recognition: Spike Activity, Cueing\n  Interaction and Causal Processing","summary":"  Humans naturally perform audiovisual speech recognition (AVSR), enhancing the\naccuracy and robustness by integrating auditory and visual information. Spiking\nneural networks (SNNs), which mimic the brain's information-processing\nmechanisms, are well-suited for emulating the human capability of AVSR. Despite\ntheir potential, research on SNNs for AVSR is scarce, with most existing\naudio-visual multimodal methods focused on object or digit recognition. These\nmodels simply integrate features from both modalities, neglecting their unique\ncharacteristics and interactions. Additionally, they often rely on future\ninformation for current processing, which increases recognition latency and\nlimits real-time applicability. Inspired by human speech perception, this paper\nproposes a novel human-inspired SNN named HI-AVSNN for AVSR, incorporating\nthree key characteristics: cueing interaction, causal processing and spike\nactivity. For cueing interaction, we propose a visual-cued auditory attention\nmodule (VCA2M) that leverages visual cues to guide attention to auditory\nfeatures. We achieve causal processing by aligning the SNN's temporal dimension\nwith that of visual and auditory features and applying temporal masking to\nutilize only past and current information. To implement spike activity, in\naddition to using SNNs, we leverage the event camera to capture lip movement as\nspikes, mimicking the human retina and providing efficient visual data. We\nevaluate HI-AVSNN on an audiovisual speech recognition dataset combining the\nDVS-Lip dataset with its corresponding audio samples. Experimental results\ndemonstrate the superiority of our proposed fusion method, outperforming\nexisting audio-visual SNN fusion methods and achieving a 2.27% improvement in\naccuracy over the only existing SNN-based AVSR method.\n","authors":["Qianhui Liu","Jiadong Wang","Yang Wang","Xin Yang","Gang Pan","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2408.16564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16532v1","updated":"2024-08-29T13:43:36Z","published":"2024-08-29T13:43:36Z","title":"WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio\n  Language Modeling","summary":"  Language models have been effectively applied to modeling natural signals,\nsuch as images, video, speech, and audio. A crucial component of these models\nis the codec tokenizer, which compresses high-dimensional natural signals into\nlower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,\nwhich offers several advantages over previous SOTA acoustic codec models in the\naudio domain: 1)extreme compression. By compressing the layers of quantizers\nand the temporal dimension of the discrete codec, one-second audio of 24kHz\nsampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved\nsubjective quality. Despite the reduced number of tokens, WavTokenizer achieves\nstate-of-the-art reconstruction quality with outstanding UTMOS scores and\ninherently contains richer semantic information. Specifically, we achieve these\nresults by designing a broader VQ space, extended contextual windows, and\nimproved attention networks, as well as introducing a powerful multi-scale\ndiscriminator and an inverse Fourier transform structure. We conducted\nextensive reconstruction experiments in the domains of speech, audio, and\nmusic. WavTokenizer exhibited strong performance across various objective and\nsubjective metrics compared to state-of-the-art models. We also tested semantic\ninformation, VQ utilization, and adaptability to generative models.\nComprehensive ablation studies confirm the necessity of each module in\nWavTokenizer. The related code, demos, and pre-trained models are available at\nhttps://github.com/jishengpeng/WavTokenizer.\n","authors":["Shengpeng Ji","Ziyue Jiang","Xize Cheng","Yifu Chen","Minghui Fang","Jialong Zuo","Qian Yang","Ruiqi Li","Ziang Zhang","Xiaoda Yang","Rongjie Huang","Yidi Jiang","Qian Chen","Siqi Zheng","Wen Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.16532v1.pdf","comment":"Working in progress. arXiv admin note: text overlap with\n  arXiv:2402.12208"},{"id":"http://arxiv.org/abs/2408.16879v1","updated":"2024-08-29T20:05:02Z","published":"2024-08-29T20:05:02Z","title":"MSLIQA: Enhancing Learning Representations for Image Quality Assessment\n  through Multi-Scale Learning","summary":"  No-Reference Image Quality Assessment (NR-IQA) remains a challenging task due\nto the diversity of distortions and the lack of large annotated datasets. Many\nstudies have attempted to tackle these challenges by developing more accurate\nNR-IQA models, often employing complex and computationally expensive networks,\nor by bridging the domain gap between various distortions to enhance\nperformance on test datasets. In our work, we improve the performance of a\ngeneric lightweight NR-IQA model by introducing a novel augmentation strategy\nthat boosts its performance by almost 28\\%. This augmentation strategy enables\nthe network to better discriminate between different distortions in various\nparts of the image by zooming in and out. Additionally, the inclusion of\ntest-time augmentation further enhances performance, making our lightweight\nnetwork's results comparable to the current state-of-the-art models, simply\nthrough the use of augmentations.\n","authors":["Nasim Jamshidi Avanaki","Abhijay Ghildiyal","Nabajeet Barman","Saman Zadtootaghaj"],"pdf_url":"https://arxiv.org/pdf/2408.16879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16809v1","updated":"2024-08-29T17:59:57Z","published":"2024-08-29T17:59:57Z","title":"See or Guess: Counterfactually Regularized Image Captioning","summary":"  Image captioning, which generates natural language descriptions of the visual\ninformation in an image, is a crucial task in vision-language research.\nPrevious models have typically addressed this task by aligning the generative\ncapabilities of machines with human intelligence through statistical fitting of\nexisting datasets. While effective for normal images, they may struggle to\naccurately describe those where certain parts of the image are obscured or\nedited, unlike humans who excel in such cases. These weaknesses they exhibit,\nincluding hallucinations and limited interpretability, often hinder performance\nin scenarios with shifted association patterns. In this paper, we present a\ngeneric image captioning framework that employs causal inference to make\nexisting models more capable of interventional tasks, and counterfactually\nexplainable. Our approach includes two variants leveraging either total effect\nor natural direct effect. Integrating them into the training process enables\nmodels to handle counterfactual scenarios, increasing their generalizability.\nExtensive experiments on various datasets show that our method effectively\nreduces hallucinations and improves the model's faithfulness to images,\ndemonstrating high portability across both small-scale and large-scale\nimage-to-text models. The code is available at\nhttps://github.com/Aman-4-Real/See-or-Guess.\n","authors":["Qian Cao","Xu Chen","Ruihua Song","Xiting Wang","Xinting Huang","Yuchen Ren"],"pdf_url":"https://arxiv.org/pdf/2408.16809v1.pdf","comment":"Accepted by ACM MM 2024"}]},"2024-08-30T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2408.17443v1","updated":"2024-08-30T17:52:55Z","published":"2024-08-30T17:52:55Z","title":"Bridging Episodes and Semantics: A Novel Framework for Long-Form Video\n  Understanding","summary":"  While existing research often treats long-form videos as extended short\nvideos, we propose a novel approach that more accurately reflects human\ncognition. This paper introduces BREASE: BRidging Episodes And SEmantics for\nLong-Form Video Understanding, a model that simulates episodic memory\naccumulation to capture action sequences and reinforces them with semantic\nknowledge dispersed throughout the video. Our work makes two key contributions:\nFirst, we develop an Episodic COmpressor (ECO) that efficiently aggregates\ncrucial representations from micro to semi-macro levels. Second, we propose a\nSemantics reTRiever (SeTR) that enhances these aggregated representations with\nsemantic information by focusing on the broader context, dramatically reducing\nfeature dimensionality while preserving relevant macro-level information.\nExtensive experiments demonstrate that BREASE achieves state-of-the-art\nperformance across multiple long video understanding benchmarks in both\nzero-shot and fully-supervised settings. The project page and code are at:\nhttps://joslefaure.github.io/assets/html/hermes.html.\n","authors":["Gueter Josmy Faure","Jia-Fong Yeh","Min-Hung Chen","Hung-Ting Su","Winston H. Hsu","Shang-Hong Lai"],"pdf_url":"https://arxiv.org/pdf/2408.17443v1.pdf","comment":"Accepted to the EVAL-FoMo Workshop at ECCV'24. Project page:\n  https://joslefaure.github.io/assets/html/hermes.html"},{"id":"http://arxiv.org/abs/2408.17437v1","updated":"2024-08-30T17:41:30Z","published":"2024-08-30T17:41:30Z","title":"SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists","summary":"  Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList.\n","authors":["Raoyuan Zhao","Abdullatif KÃ¶ksal","Yihong Liu","Leonie Weissweiler","Anna Korhonen","Hinrich SchÃ¼tze"],"pdf_url":"https://arxiv.org/pdf/2408.17437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17428v1","updated":"2024-08-30T17:26:05Z","published":"2024-08-30T17:26:05Z","title":"CLOCR-C: Context Leveraging OCR Correction with Pre-trained Language\n  Models","summary":"  The digitisation of historical print media archives is crucial for increasing\naccessibility to contemporary records. However, the process of Optical\nCharacter Recognition (OCR) used to convert physical records to digital text is\nprone to errors, particularly in the case of newspapers and periodicals due to\ntheir complex layouts. This paper introduces Context Leveraging OCR Correction\n(CLOCR-C), which utilises the infilling and context-adaptive abilities of\ntransformer-based language models (LMs) to improve OCR quality. The study aims\nto determine if LMs can perform post-OCR correction, improve downstream NLP\ntasks, and the value of providing the socio-cultural context as part of the\ncorrection process. Experiments were conducted using seven LMs on three\ndatasets: the 19th Century Serials Edition (NCSE) and two datasets from the\nOverproof collection. The results demonstrate that some LMs can significantly\nreduce error rates, with the top-performing model achieving over a 60%\nreduction in character error rate on the NCSE dataset. The OCR improvements\nextend to downstream tasks, such as Named Entity Recognition, with increased\nCosine Named Entity Similarity. Furthermore, the study shows that providing\nsocio-cultural context in the prompts improves performance, while misleading\nprompts lower performance. In addition to the findings, this study releases a\ndataset of 91 transcribed articles from the NCSE, containing a total of 40\nthousand words, to support further research in this area. The findings suggest\nthat CLOCR-C is a promising approach for enhancing the quality of existing\ndigital archives by leveraging the socio-cultural information embedded in the\nLMs and the text requiring correction.\n","authors":["Jonathan Bourne"],"pdf_url":"https://arxiv.org/pdf/2408.17428v1.pdf","comment":"13 pages, 3 figures, currently under peer review"},{"id":"http://arxiv.org/abs/2403.12212v2","updated":"2024-08-30T17:02:11Z","published":"2024-03-18T19:53:56Z","title":"Evaluating Named Entity Recognition: A comparative analysis of mono- and\n  multilingual transformer models on a novel Brazilian corporate earnings call\n  transcripts dataset","summary":"  Since 2018, when the Transformer architecture was introduced, Natural\nLanguage Processing has gained significant momentum with pre-trained\nTransformer-based models that can be fine-tuned for various tasks. Most models\nare pre-trained on large English corpora, making them less applicable to other\nlanguages, such as Brazilian Portuguese. In our research, we identified two\nmodels pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two\nmultilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder\nmodule, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to\nevaluate their performance on a financial Named Entity Recognition (NER) task\nand determine the computational requirements for fine-tuning and inference. To\nthis end, we developed the Brazilian Financial NER (BraFiNER) dataset,\ncomprising sentences from Brazilian banks' earnings calls transcripts annotated\nusing a weakly supervised approach. Additionally, we introduced a novel\napproach that reframes the token classification task as a text generation\nproblem. After fine-tuning the models, we evaluated them using performance and\nerror metrics. Our findings reveal that BERT-based models consistently\noutperform T5-based models. While the multilingual models exhibit comparable\nmacro F1-scores, BERTimbau demonstrates superior performance over PTT5. In\nterms of error metrics, BERTimbau outperforms the other models. We also\nobserved that PTT5 and mT5 generated sentences with changes in monetary and\npercentage values, highlighting the importance of accuracy and consistency in\nthe financial domain. Our findings provide insights into the differing\nperformance of BERT- and T5-based models for the NER task.\n","authors":["Ramon Abilio","Guilherme Palermo Coelho","Ana Estela Antunes da Silva"],"pdf_url":"https://arxiv.org/pdf/2403.12212v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06120v2","updated":"2024-08-30T16:42:05Z","published":"2024-02-09T01:10:25Z","title":"Exploring Group and Symmetry Principles in Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released.\n","authors":["Shima Imani","Hamid Palangi"],"pdf_url":"https://arxiv.org/pdf/2402.06120v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02175v3","updated":"2024-08-30T16:40:15Z","published":"2024-05-03T15:25:48Z","title":"Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset","summary":"  Hoaxes are a recognised form of disinformation created deliberately, with\npotential serious implications in the credibility of reference knowledge\nresources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is that\nthey often are written according to the official style guidelines. In this\nwork, we first provide a systematic analysis of similarities and discrepancies\nbetween legitimate and hoax Wikipedia articles, and introduce Hoaxpedia, a\ncollection of 311 hoax articles (from existing literature and official\nWikipedia lists), together with semantically similar legitimate articles, which\ntogether form a binary text classification dataset aimed at fostering research\nin automated hoax detection. In this paper, We report results after analyzing\nseveral language models, hoax-to-legit ratios, and the amount of text\nclassifiers are exposed to (full article vs the article's definition alone).\nOur results suggest that detecting deceitful content in Wikipedia based on\ncontent alone is hard but feasible, and complement our analysis with a study on\nthe differences in distributions in edit histories, and find that looking at\nthis feature yields better classification results than context.\n","authors":["Hsuvas Borkakoty","Luis Espinosa-Anke"],"pdf_url":"https://arxiv.org/pdf/2405.02175v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15379v2","updated":"2024-08-30T16:30:39Z","published":"2024-08-27T19:33:15Z","title":"DualKanbaFormer: Kolmogorov-Arnold Networks and State Space Model\n  Transformer for Multimodal Aspect-based Sentiment Analysis","summary":"  Multimodal aspect-based sentiment analysis (MABSA) enhances sentiment\ndetection by combining text with other data types like images. However, despite\nsetting significant benchmarks, attention mechanisms exhibit limitations in\nefficiently modelling long-range dependencies between aspect and opinion\ntargets within the text. They also face challenges in capturing global-context\ndependencies for visual representations. To this end, we propose\nKolmogorov-Arnold Networks (KANs) and Selective State Space model (Mamba)\ntransformer (DualKanbaFormer), a novel architecture to address the above\nissues. We leverage the power of Mamba to capture global context dependencies,\nMulti-head Attention (MHA) to capture local context dependencies, and KANs to\ncapture non-linear modelling patterns for both textual representations (textual\nKanbaFormer) and visual representations (visual KanbaFormer). Furthermore, we\nfuse the textual KanbaFormer and visual KanbaFomer with a gated fusion layer to\ncapture the inter-modality dynamics. According to extensive experimental\nresults, our model outperforms some state-of-the-art (SOTA) studies on two\npublic datasets.\n","authors":["Adamu Lawan","Juhua Pu","Haruna Yunusa","Muhammad Lawan","Aliyu Umar","Adamu Sani Yahya"],"pdf_url":"https://arxiv.org/pdf/2408.15379v2.pdf","comment":"10 pages, 2 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2405.12363v2","updated":"2024-08-30T16:23:13Z","published":"2024-05-20T20:27:00Z","title":"Question-Based Retrieval using Atomic Units for Enterprise RAG","summary":"  Enterprise retrieval augmented generation (RAG) offers a highly flexible\nframework for combining powerful large language models (LLMs) with internal,\npossibly temporally changing, documents. In RAG, documents are first chunked.\nRelevant chunks are then retrieved for a user query, which are passed as\ncontext to a synthesizer LLM to generate the query response. However, the\nretrieval step can limit performance, as incorrect chunks can lead the\nsynthesizer LLM to generate a false response. This work applies a zero-shot\nadaptation of standard dense retrieval steps for more accurate chunk recall.\nSpecifically, a chunk is first decomposed into atomic statements. A set of\nsynthetic questions are then generated on these atoms (with the chunk as the\ncontext). Dense retrieval involves finding the closest set of synthetic\nquestions, and associated chunks, to the user query. It is found that retrieval\nwith the atoms leads to higher recall than retrieval with chunks. Further\nperformance gain is observed with retrieval using the synthetic questions\ngenerated over the atoms. Higher recall at the retrieval step enables higher\nperformance of the enterprise LLM using the RAG pipeline.\n","authors":["Vatsal Raina","Mark Gales"],"pdf_url":"https://arxiv.org/pdf/2405.12363v2.pdf","comment":"14 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2408.17377v1","updated":"2024-08-30T16:13:49Z","published":"2024-08-30T16:13:49Z","title":"NDP: Next Distribution Prediction as a More Broad Target","summary":"  Large language models (LLMs) trained on next-token prediction (NTP) paradigm\nhave demonstrated powerful capabilities. However, the existing NTP paradigm\ncontains several limitations, particularly related to planned task\ncomplications and error propagation during inference. In our work, we extend\nthe critique of NTP, highlighting its limitation also due to training with a\nnarrow objective: the prediction of a sub-optimal one-hot distribution. To\nsupport this critique, we conducted a pre-experiment treating the output\ndistribution from powerful LLMs as efficient world data compression. By\nevaluating the similarity between the $n$-gram distribution and the one-hot\ndistribution with LLMs, we observed that the $n$-gram distributions align more\nclosely with the output distribution of LLMs. Based on this insight, we\nintroduce Next Distribution Prediction (NDP), which uses $n$-gram distributions\nto replace the one-hot targets, enhancing learning without extra online\ntraining time. We conducted experiments across translation, general task,\nlanguage transfer, and medical domain adaptation. Compared to NTP, NDP can\nachieve up to +2.97 COMET improvement in translation tasks, +0.61 average\nimprovement in general tasks, and incredible +10.75 average improvement in the\nmedical domain. This demonstrates the concrete benefits of addressing the\ntarget narrowing problem, pointing to a new direction for future work on\nimproving NTP.\n","authors":["Junhao Ruan","Abudukeyumu Abudula","Xinyu Liu","Bei Li","Yinqiao Li","Chenglong Wang","Yuchun Fan","Yuan Ge","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.17377v1.pdf","comment":"8 pages,5 figures"},{"id":"http://arxiv.org/abs/2404.00458v2","updated":"2024-08-30T15:59:46Z","published":"2024-03-30T19:45:04Z","title":"Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for\n  Embedding Model Selection","summary":"  This position paper proposes a systematic approach towards developing a\nframework to help select the most effective embedding models for natural\nlanguage processing (NLP) tasks, addressing the challenge posed by the\nproliferation of both proprietary and open-source encoder models.\n","authors":["Vivek Khetan"],"pdf_url":"https://arxiv.org/pdf/2404.00458v2.pdf","comment":"It was an initial idea - we plan to work on a detailed version"},{"id":"http://arxiv.org/abs/2408.17362v1","updated":"2024-08-30T15:52:41Z","published":"2024-08-30T15:52:41Z","title":"Assessing Generative Language Models in Classification Tasks:\n  Performance and Self-Evaluation Capabilities in the Environmental and Climate\n  Change Domain","summary":"  This paper examines the performance of two Large Language Models (LLMs),\nGPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three\ndifferent classification tasks within the climate change (CC) and environmental\ndomain. Employing BERT-based models as a baseline, we compare their efficacy\nagainst these transformer-based models. Additionally, we assess the models'\nself-evaluation capabilities by analyzing the calibration of verbalized\nconfidence scores in these text classification tasks. Our findings reveal that\nwhile BERT-based models generally outperform both the LLMs and SLM, the\nperformance of the large generative models is still noteworthy. Furthermore,\nour calibration analysis reveals that although Gemma is well-calibrated in\ninitial tasks, it thereafter produces inconsistent results; Llama is reasonably\ncalibrated, and GPT consistently exhibits strong calibration. Through this\nresearch, we aim to contribute to the ongoing discussion on the utility and\neffectiveness of generative LMs in addressing some of the planet's most urgent\nissues, highlighting their strengths and limitations in the context of ecology\nand CC.\n","authors":["Francesca Grasso","Stefano Locci"],"pdf_url":"https://arxiv.org/pdf/2408.17362v1.pdf","comment":"11 pages, to be published in NLDB 2024"},{"id":"http://arxiv.org/abs/2408.09869v3","updated":"2024-08-30T15:05:58Z","published":"2024-08-19T10:20:06Z","title":"Docling Technical Report","summary":"  This technical report introduces Docling, an easy to use, self-contained,\nMIT-licensed open-source package for PDF document conversion. It is powered by\nstate-of-the-art specialized AI models for layout analysis (DocLayNet) and\ntable structure recognition (TableFormer), and runs efficiently on commodity\nhardware in a small resource budget. The code interface allows for easy\nextensibility and addition of new features and models.\n","authors":["Christoph Auer","Maksym Lysak","Ahmed Nassar","Michele Dolfi","Nikolaos Livathinos","Panos Vagenas","Cesar Berrospi Ramis","Matteo Omenetti","Fabian Lindlbauer","Kasper Dinkla","Lokesh Mishra","Yusik Kim","Shubham Gupta","Rafael Teixeira de Lima","Valery Weber","Lucas Morin","Ingmar Meijer","Viktor Kuropiatnyk","Peter W. J. Staar"],"pdf_url":"https://arxiv.org/pdf/2408.09869v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15569v2","updated":"2024-08-30T14:52:24Z","published":"2024-07-22T11:55:14Z","title":"An Empirical Study of Retrieval Augmented Generation with\n  Chain-of-Thought","summary":"  Since the launch of ChatGPT at the end of 2022, generative dialogue models\nrepresented by ChatGPT have quickly become essential tools in daily life. As\nuser expectations increase, enhancing the capability of generative dialogue\nmodels to solve complex problems has become a focal point of current research.\nThis paper delves into the effectiveness of the RAFT (Retrieval Augmented\nFine-Tuning) method in improving the performance of Generative dialogue models.\nRAFT combines chain-of-thought with model supervised fine-tuning (SFT) and\nretrieval augmented generation (RAG), which significantly enhanced the model's\ninformation extraction and logical reasoning abilities. We evaluated the RAFT\nmethod across multiple datasets and analysed its performance in various\nreasoning tasks, including long-form QA and short-form QA tasks, tasks in both\nChinese and English, and supportive and comparison reasoning tasks. Notably, it\naddresses the gaps in previous research regarding long-form QA tasks and\nChinese datasets. Moreover, we also evaluate the benefit of the\nchain-of-thought (CoT) in the RAFT method. This work offers valuable insights\nfor studies focused on enhancing the performance of generative dialogue models.\n","authors":["Yuetong Zhao","Hongyu Cao","Xianyu Zhao","Zhijian Ou"],"pdf_url":"https://arxiv.org/pdf/2407.15569v2.pdf","comment":"Accepted by ISCSLP 2024"},{"id":"http://arxiv.org/abs/2402.01676v2","updated":"2024-08-30T14:43:22Z","published":"2024-01-19T19:36:54Z","title":"Language models align with human judgments on key grammatical\n  constructions","summary":"  Do large language models (LLMs) make human-like linguistic generalizations?\nDentella et al. (2023) (\"DGL\") prompt several LLMs (\"Is the following sentence\ngrammatically correct in English?\") to elicit grammaticality judgments of 80\nEnglish sentences, concluding that LLMs demonstrate a \"yes-response bias\" and a\n\"failure to distinguish grammatical from ungrammatical sentences\". We\nre-evaluate LLM performance using well-established practices and find that\nDGL's data in fact provide evidence for just how well LLMs capture human\nbehaviors. Models not only achieve high accuracy overall, but also capture\nfine-grained variation in human linguistic judgments.\n","authors":["Jennifer Hu","Kyle Mahowald","Gary Lupyan","Anna Ivanova","Roger Levy"],"pdf_url":"https://arxiv.org/pdf/2402.01676v2.pdf","comment":"Published in PNAS at https://www.pnas.org/doi/10.1073/pnas.2400917121\n  as response to Dentella et al. (2023)"},{"id":"http://arxiv.org/abs/2408.17325v1","updated":"2024-08-30T14:37:10Z","published":"2024-08-30T14:37:10Z","title":"Impact of ChatGPT on the writing style of condensed matter physicists","summary":"  We apply a state-of-the-art difference-in-differences approach to estimate\nthe impact of ChatGPT's release on the writing style of condensed matter papers\non arXiv. Our analysis reveals a statistically significant improvement in the\nEnglish quality of abstracts written by non-native English speakers.\nImportantly, this improvement remains robust even after accounting for other\npotential factors, confirming that it can be attributed to the release of\nChatGPT. This indicates widespread adoption of the tool. Following the release\nof ChatGPT, there is a significant increase in the use of unique words, while\nthe frequency of rare words decreases. Across language families, the changes in\nwriting style are significant for authors from the Latin and Ural-Altaic\ngroups, but not for those from the Germanic or other Indo-European groups.\n","authors":["Shaojun Xu","Xiaohui Ye","Mengqi Zhang","Pei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.17325v1.pdf","comment":"9 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2408.17324v1","updated":"2024-08-30T14:35:01Z","published":"2024-08-30T14:35:01Z","title":"Modularity in Transformers: Investigating Neuron Separability &\n  Specialization","summary":"  Transformer models are increasingly prevalent in various applications, yet\nour understanding of their internal workings remains limited. This paper\ninvestigates the modularity and task specialization of neurons within\ntransformer architectures, focusing on both vision (ViT) and language (Mistral\n7B) models. Using a combination of selective pruning and MoEfication clustering\ntechniques, we analyze the overlap and specialization of neurons across\ndifferent tasks and data subsets. Our findings reveal evidence of task-specific\nneuron clusters, with varying degrees of overlap between related tasks. We\nobserve that neuron importance patterns persist to some extent even in randomly\ninitialized models, suggesting an inherent structure that training refines.\nAdditionally, we find that neuron clusters identified through MoEfication\ncorrespond more strongly to task-specific neurons in earlier and later layers\nof the models. This work contributes to a more nuanced understanding of\ntransformer internals and offers insights into potential avenues for improving\nmodel interpretability and efficiency.\n","authors":["Nicholas Pochinkov","Thomas Jones","Mohammed Rashidur Rahman"],"pdf_url":"https://arxiv.org/pdf/2408.17324v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.17322v1","updated":"2024-08-30T14:32:25Z","published":"2024-08-30T14:32:25Z","title":"Investigating Neuron Ablation in Attention Heads: The Case for Peak\n  Activation Centering","summary":"  The use of transformer-based models is growing rapidly throughout society.\nWith this growth, it is important to understand how they work, and in\nparticular, how the attention mechanisms represent concepts. Though there are\nmany interpretability methods, many look at models through their neuronal\nactivations, which are poorly understood. We describe different lenses through\nwhich to view neuron activations, and investigate the effectiveness in language\nmodels and vision transformers through various methods of neural ablation: zero\nablation, mean ablation, activation resampling, and a novel approach we term\n'peak ablation'. Through experimental analysis, we find that in different\nregimes and models, each method can offer the lowest degradation of model\nperformance compared to other methods, with resampling usually causing the most\nsignificant performance deterioration. We make our code available at\nhttps://github.com/nickypro/investigating-ablation.\n","authors":["Nicholas Pochinkov","Ben Pasero","Skylar Shibayama"],"pdf_url":"https://arxiv.org/pdf/2408.17322v1.pdf","comment":"9 pages, 2 figures, XAI World Conference 2024 Late-Breaking Work"},{"id":"http://arxiv.org/abs/2408.17316v1","updated":"2024-08-30T14:23:40Z","published":"2024-08-30T14:23:40Z","title":"Bridging Domain Knowledge and Process Discovery Using Large Language\n  Models","summary":"  Discovering good process models is essential for different process analysis\ntasks such as conformance checking and process improvements. Automated process\ndiscovery methods often overlook valuable domain knowledge. This knowledge,\nincluding insights from domain experts and detailed process documentation,\nremains largely untapped during process discovery. This paper leverages Large\nLanguage Models (LLMs) to integrate such knowledge directly into process\ndiscovery. We use rules derived from LLMs to guide model construction, ensuring\nalignment with both domain knowledge and actual process executions. By\nintegrating LLMs, we create a bridge between process knowledge expressed in\nnatural language and the discovery of robust process models, advancing process\ndiscovery methodologies significantly. To showcase the usability of our\nframework, we conducted a case study with the UWV employee insurance agency,\ndemonstrating its practical benefits and effectiveness.\n","authors":["Ali Norouzifar","Humam Kourani","Marcus Dees","Wil van der Aalst"],"pdf_url":"https://arxiv.org/pdf/2408.17316v1.pdf","comment":"This paper is accepted at the AI4BPM 2024 workshop and to be\n  published in their proceedings"},{"id":"http://arxiv.org/abs/2408.17308v1","updated":"2024-08-30T14:12:04Z","published":"2024-08-30T14:12:04Z","title":"Towards Tailored Recovery of Lexical Diversity in Literary Machine\n  Translation","summary":"  Machine translations are found to be lexically poorer than human\ntranslations. The loss of lexical diversity through MT poses an issue in the\nautomatic translation of literature, where it matters not only what is written,\nbut also how it is written. Current methods for increasing lexical diversity in\nMT are rigid. Yet, as we demonstrate, the degree of lexical diversity can vary\nconsiderably across different novels. Thus, rather than aiming for the rigid\nincrease of lexical diversity, we reframe the task as recovering what is lost\nin the machine translation process. We propose a novel approach that consists\nof reranking translation candidates with a classifier that distinguishes\nbetween original and translated text. We evaluate our approach on 31\nEnglish-to-Dutch book translations, and find that, for certain books, our\napproach retrieves lexical diversity scores that are close to human\ntranslation.\n","authors":["Esther Ploeger","Huiyuan Lai","Rik van Noord","Antonio Toral"],"pdf_url":"https://arxiv.org/pdf/2408.17308v1.pdf","comment":"Accepted to EAMT 2024"},{"id":"http://arxiv.org/abs/2310.09762v2","updated":"2024-08-30T13:39:56Z","published":"2023-10-15T07:20:28Z","title":"Diversifying the Mixture-of-Experts Representation for Language Models\n  with Orthogonal Optimizer","summary":"  The Mixture of Experts (MoE) has emerged as a highly successful technique in\ndeep learning, based on the principle of divide-and-conquer to maximize model\ncapacity without significant additional computational cost. Even in the era of\nlarge-scale language models (LLMs), MoE continues to play a crucial role, as\nsome researchers have indicated that GPT-4 adopts the MoE structure to ensure\ndiverse inference results. However, MoE is susceptible to performance\ndegeneracy, particularly evident in the issues of imbalance and homogeneous\nrepresentation among experts. While previous studies have extensively addressed\nthe problem of imbalance, the challenge of homogeneous representation remains\nunresolved. In this study, we shed light on the homogeneous representation\nproblem, wherein experts in the MoE fail to specialize and lack diversity,\nleading to frustratingly high similarities in their representations (up to 99\\%\nin a well-performed MoE model). This problem restricts the expressive power of\nthe MoE and, we argue, contradicts its original intention. To tackle this\nissue, we propose a straightforward yet highly effective solution: OMoE, an\northogonal expert optimizer. Additionally, we introduce an alternating training\nstrategy that encourages each expert to update in a direction orthogonal to the\nsubspace spanned by other experts. Our algorithm facilitates MoE training in\ntwo key ways: firstly, it explicitly enhances representation diversity, and\nsecondly, it implicitly fosters interaction between experts during orthogonal\nweights computation. Through extensive experiments, we demonstrate that our\nproposed optimization algorithm significantly improves the performance of\nfine-tuning the MoE model on the GLUE benchmark, SuperGLUE benchmark,\nquestion-answering task, and name entity recognition tasks.\n","authors":["Boan Liu","Liang Ding","Li Shen","Keqin Peng","Yu Cao","Dazhao Cheng","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2310.09762v2.pdf","comment":"ECAI 2024"},{"id":"http://arxiv.org/abs/2408.17280v1","updated":"2024-08-30T13:28:45Z","published":"2024-08-30T13:28:45Z","title":"Flexible and Effective Mixing of Large Language Models into a Mixture of\n  Domain Experts","summary":"  We present a toolkit for creating low-cost Mixture-of-Domain-Experts (MOE)\nfrom trained models. The toolkit can be used for creating a mixture from models\nor from adapters. We perform extensive tests and offer guidance on defining the\narchitecture of the resulting MOE using the toolkit. A public repository is\navailable.\n","authors":["Rhui Dih Lee","Laura Wynter","Raghu Kiran Ganti"],"pdf_url":"https://arxiv.org/pdf/2408.17280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12614v4","updated":"2024-08-30T12:40:04Z","published":"2024-06-18T13:43:22Z","title":"EUvsDisinfo: A Dataset for Multilingual Detection of Pro-Kremlin\n  Disinformation in News Articles","summary":"  This work introduces EUvsDisinfo, a multilingual dataset of disinformation\narticles originating from pro-Kremlin outlets, along with trustworthy articles\nfrom credible / less biased sources. It is sourced directly from the debunk\narticles written by experts leading the EUvsDisinfo project. Our dataset is the\nlargest to-date resource in terms of the overall number of articles and\ndistinct languages. It also provides the largest topical and temporal coverage.\nUsing this dataset, we investigate the dissemination of pro-Kremlin\ndisinformation across different languages, uncovering language-specific\npatterns targeting certain disinformation topics. We further analyse the\nevolution of topic distribution over an eight-year period, noting a significant\nsurge in disinformation content before the full-scale invasion of Ukraine in\n2022. Lastly, we demonstrate the dataset's applicability in training models to\neffectively distinguish between disinformation and trustworthy content in\nmultilingual settings.\n","authors":["JoÃ£o A. Leite","Olesya Razuvayevskaya","Kalina Bontcheva","Carolina Scarton"],"pdf_url":"https://arxiv.org/pdf/2406.12614v4.pdf","comment":"Published at CIKM 2024"},{"id":"http://arxiv.org/abs/2407.04295v2","updated":"2024-08-30T11:57:47Z","published":"2024-07-05T06:57:30Z","title":"Jailbreak Attacks and Defenses Against Large Language Models: A Survey","summary":"  Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof \"jailbreaking\", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs.\n","authors":["Sibo Yi","Yule Liu","Zhen Sun","Tianshuo Cong","Xinlei He","Jiaxing Song","Ke Xu","Qi Li"],"pdf_url":"https://arxiv.org/pdf/2407.04295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00023v2","updated":"2024-08-30T11:32:48Z","published":"2024-05-24T02:50:44Z","title":"Expert-Token Resonance: Redefining MoE Routing through Affinity-Driven\n  Active Selection","summary":"  Mixture-of-Experts (MoE) architectures have emerged as a paradigm-shifting\napproach for large language models (LLMs), offering unprecedented computational\nefficiency. However, these architectures grapple with challenges of token\ndistribution imbalance and expert homogenization, impeding optimal semantic\ngeneralization. We introduce a novel framework that redefines MoE routing\nthrough affinity-driven active selection. The innovations for the framework\nencompass: (1) A rigorous formulation of expert-token affinity metrics. (2) An\nadaptive bidirectional selection mechanism leveraging resonance between experts\nand tokens. (3) Theoretical derivation and experimental evidence of reduced\nexpert capacity bounds under dynamic token distribution evolution. It is also\nintegrated with orthogonal feature extraction module and an optimized loss\nfunction for expert localization. Our theoretical analysis demonstrates that\nthis approach mitigates expert homogenization while enabling substantial\ncapacity boundary reduction. Experimental validation corroborates these\nfindings: it achieves a 40% reduction in token processed by each expert without\ncompromising model convergence or efficacy. When coupled with communication\noptimizations, the training efficiency improvements of 5.4% to 46.6% can be\nobserved. After supervised fine-tuning, it exhibits performance gains of 9.7%\nto 14.1% across GDAD, C-Eval, and TeleQnA benchmarks.\n","authors":["Jing Li","Zhijie Sun","Dachao Lin","Xuan He","Yi Lin","Binfan Zheng","Li Zeng","Rongqian Zhao","Xin Chen"],"pdf_url":"https://arxiv.org/pdf/2406.00023v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05200v2","updated":"2024-08-30T11:14:17Z","published":"2024-08-09T17:44:45Z","title":"TaSL: Task Skill Localization and Consolidation for Language Model\n  Continual Learning","summary":"  Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Task Skill Localization and Consolidation\n(TaSL), which boosts knowledge transfer without depending on memory replay.\nTaSL initially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise skill localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained skill\nconsolidation strategy that retains task-specific knowledge, thereby preventing\nforgetting, and updates task-shared knowledge, which facilitates bi-directional\nknowledge transfer. As a result, TaSL achieves an optimal balance between\nretaining prior knowledge and excelling in new tasks. TaSL also demonstrates\nstrong generalizability, making it suitable for various base models and\nadaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of TaSL and\nits variants across different settings.\n","authors":["Yujie Feng","Xu Chu","Yongxin Xu","Zexin Lu","Bo Liu","Philip S. Yu","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2408.05200v2.pdf","comment":"Extension of ACL 2024 paper titled: Continual Dialog State Tracking\n  via Task Skill Localization and Consolidation"},{"id":"http://arxiv.org/abs/2408.17181v1","updated":"2024-08-30T10:28:49Z","published":"2024-08-30T10:28:49Z","title":"Improving Extraction of Clinical Event Contextual Properties from\n  Electronic Health Records: A Comparative Study","summary":"  Electronic Health Records are large repositories of valuable clinical data,\nwith a significant portion stored in unstructured text format. This textual\ndata includes clinical events (e.g., disorders, symptoms, findings, medications\nand procedures) in context that if extracted accurately at scale can unlock\nvaluable downstream applications such as disease prediction. Using an existing\nNamed Entity Recognition and Linking methodology, MedCAT, these identified\nconcepts need to be further classified (contextualised) for their relevance to\nthe patient, and their temporal and negated status for example, to be useful\ndownstream. This study performs a comparative analysis of various natural\nlanguage models for medical text classification. Extensive experimentation\nreveals the effectiveness of transformer-based language models, particularly\nBERT. When combined with class imbalance mitigation techniques, BERT\noutperforms Bi-LSTM models by up to 28% and the baseline BERT model by up to\n16% for recall of the minority classes. The method has been implemented as part\nof CogStack/MedCAT framework and made available to the community for further\nresearch.\n","authors":["Shubham Agarwal","Thomas Searle","Mart Ratas","Anthony Shek","James Teo","Richard Dobson"],"pdf_url":"https://arxiv.org/pdf/2408.17181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17175v1","updated":"2024-08-30T10:24:07Z","published":"2024-08-30T10:24:07Z","title":"Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model","summary":"  Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)\n","authors":["Zhen Ye","Peiwen Sun","Jiahe Lei","Hongzhan Lin","Xu Tan","Zheqi Dai","Qiuqiang Kong","Jianyi Chen","Jiahao Pan","Qifeng Liu","Yike Guo","Wei Xue"],"pdf_url":"https://arxiv.org/pdf/2408.17175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03387v2","updated":"2024-08-30T09:13:50Z","published":"2024-07-03T08:36:13Z","title":"ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages","summary":"  Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints.\n","authors":["Mehant Kammakomati","Sameer Pimparkhede","Srikanth Tamilselvam","Prince Kumar","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2407.03387v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17072v1","updated":"2024-08-30T07:57:30Z","published":"2024-08-30T07:57:30Z","title":"MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for\n  Retrieval-Augmented Large Language Models","summary":"  In a real-world RAG system, the current query often involves spoken ellipses\nand ambiguous references from dialogue contexts, necessitating query rewriting\nto better describe user's information needs. However, traditional context-based\nrewriting has minimal enhancement on downstream generation tasks due to the\nlengthy process from query rewriting to response generation. Some researchers\ntry to utilize reinforcement learning with generation feedback to assist the\nrewriter, but these sparse rewards provide little guidance in most cases,\nleading to unstable training and generation results. We find that user's needs\nare also reflected in the gold document, retrieved documents and ground truth.\nTherefore, by feeding back these multi-aspect dense rewards to query rewriting,\nmore stable and satisfactory responses can be achieved. In this paper, we\npropose a novel query rewriting method MaFeRw, which improves RAG performance\nby integrating multi-aspect feedback from both the retrieval process and\ngenerated results. Specifically, we first use manual data to train a T5 model\nfor the rewriter initialization. Next, we design three metrics as reinforcement\nlearning feedback: the similarity between the rewritten query and the gold\ndocument, the ranking metrics, and ROUGE between the generation and the ground\ntruth. Inspired by RLAIF, we train three kinds of reward models for the above\nmetrics to achieve more efficient training. Finally, we combine the scores of\nthese reward models as feedback, and use PPO algorithm to explore the optimal\nquery rewriting strategy. Experimental results on two conversational RAG\ndatasets demonstrate that MaFeRw achieves superior generation metrics and more\nstable training compared to baselines.\n","authors":["Yujing Wang","Hainan Zhang","Liang Pang","Liang Pang","Hongwei Zheng","Zhiming Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.17072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17070v1","updated":"2024-08-30T07:54:50Z","published":"2024-08-30T07:54:50Z","title":"Novel-WD: Exploring acquisition of Novel World Knowledge in LLMs Using\n  Prefix-Tuning","summary":"  Teaching new information to pre-trained large language models (PLM) is a\ncrucial but challenging task. Model adaptation techniques, such as fine-tuning\nand parameter-efficient training have been shown to store new facts at a slow\nrate; continual learning is an option but is costly and prone to catastrophic\nforgetting. This work studies and quantifies how PLM may learn and remember new\nworld knowledge facts that do not occur in their pre-training corpus, which\nonly contains world knowledge up to a certain date. To that purpose, we first\npropose Novel-WD, a new dataset consisting of sentences containing novel facts\nextracted from recent Wikidata updates, along with two evaluation tasks in the\nform of causal language modeling and multiple choice questions (MCQ). We make\nthis dataset freely available to the community, and release a procedure to\nlater build new versions of similar datasets with up-to-date information. We\nalso explore the use of prefix-tuning for novel information learning, and\nanalyze how much information can be stored within a given prefix. We show that\na single fact can reliably be encoded within a single prefix, and that the\nprefix capacity increases with its length and with the base model size.\n","authors":["Maxime MÃ©loux","Christophe Cerisara"],"pdf_url":"https://arxiv.org/pdf/2408.17070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13344v2","updated":"2024-08-30T07:43:00Z","published":"2024-05-22T05:03:39Z","title":"Contextualized Automatic Speech Recognition with Dynamic Vocabulary","summary":"  Deep biasing (DB) enhances the performance of end-to-end automatic speech\nrecognition (E2E-ASR) models for rare words or contextual phrases using a bias\nlist. However, most existing methods treat bias phrases as sequences of\nsubwords in a predefined static vocabulary. This naive sequence decomposition\nproduces unnatural token patterns, significantly lowering their occurrence\nprobability. More advanced techniques address this problem by expanding the\nvocabulary with additional modules, including the external language model\nshallow fusion or rescoring. However, they result in increasing the workload\ndue to the additional modules. This paper proposes a dynamic vocabulary where\nbias tokens can be added during inference. Each entry in a bias list is\nrepresented as a single token, unlike a sequence of existing subword tokens.\nThis approach eliminates the need to learn subword dependencies within the bias\nphrases. This method is easily applied to various architectures because it only\nexpands the embedding and output layers in common E2E-ASR architectures.\nExperimental results demonstrate that the proposed method improves the bias\nphrase WER on English and Japanese datasets by 3.1 -- 4.9 points compared with\nthe conventional DB method.\n","authors":["Yui Sudo","Yosuke Fukumoto","Muhammad Shakeel","Yifan Peng","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2405.13344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12942v2","updated":"2024-08-30T07:30:13Z","published":"2024-08-23T09:46:15Z","title":"Causal-Guided Active Learning for Debiasing Large Language Models","summary":"  Although achieving promising performance, recent analyses show that current\ngenerative large language models (LLMs) may still capture dataset biases and\nutilize them for generation, leading to poor generalizability and harmfulness\nof LLMs. However, due to the diversity of dataset biases and the\nover-optimization problem, previous prior-knowledge-based debiasing methods and\nfine-tuning-based debiasing methods may not be suitable for current LLMs. To\naddress this issue, we explore combining active learning with the causal\nmechanisms and propose a casual-guided active learning (CAL) framework, which\nutilizes LLMs itself to automatically and autonomously identify informative\nbiased samples and induce the bias patterns. Then a cost-effective and\nefficient in-context learning based method is employed to prevent LLMs from\nutilizing dataset biases during generation. Experimental results show that CAL\ncan effectively recognize typical biased instances and induce various bias\npatterns for debiasing LLMs.\n","authors":["Li Du","Zhouhao Sun","Xiao Ding","Yixuan Ma","Yang Zhao","Kaitao Qiu","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2408.12942v2.pdf","comment":"Accepted as ACL 2024 main conference & Rewared as Outstanding Paper"},{"id":"http://arxiv.org/abs/2407.21646v2","updated":"2024-08-30T06:50:51Z","published":"2024-07-31T14:48:27Z","title":"Towards Achieving Human Parity on End-to-end Simultaneous Speech\n  Translation via LLM Agent","summary":"  In this paper, we present Cross Language Agent -- Simultaneous\nInterpretation, CLASI, a high-quality and human-like Simultaneous Speech\nTranslation (SiST) System. Inspired by professional human interpreters, we\nutilize a novel data-driven read-write strategy to balance the translation\nquality and latency. To address the challenge of translating in-domain\nterminologies, CLASI employs a multi-modal retrieving module to obtain relevant\ninformation to augment the translation. Supported by LLMs, our approach can\ngenerate error-tolerated translation by considering the input audio, historical\ncontext, and retrieved information. Experimental results show that our system\noutperforms other systems by significant margins. Aligned with professional\nhuman interpreters, we evaluate CLASI with a better human evaluation metric,\nvalid information proportion (VIP), which measures the amount of information\nthat can be successfully conveyed to the listeners. In the real-world\nscenarios, where the speeches are often disfluent, informal, and unclear, CLASI\nachieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese\ntranslation directions, respectively. In contrast, state-of-the-art commercial\nor open-source systems only achieve 35.4% and 41.6%. On the extremely hard\ndataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%\nVIP.\n","authors":["Shanbo Cheng","Zhichao Huang","Tom Ko","Hang Li","Ningxin Peng","Lu Xu","Qini Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.21646v2.pdf","comment":"Authors are listed in alphabetical order by last name. Demonstrations\n  and human-annotated test sets are available at\n  https://byteresearchcla.github.io/clasi"},{"id":"http://arxiv.org/abs/2408.15545v2","updated":"2024-08-30T06:42:36Z","published":"2024-08-28T05:41:52Z","title":"SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding","summary":"  Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.\n","authors":["Sihang Li","Jin Huang","Jiaxi Zhuang","Yaorui Shi","Xiaochen Cai","Mingjun Xu","Xiang Wang","Linfeng Zhang","Guolin Ke","Hengxing Cai"],"pdf_url":"https://arxiv.org/pdf/2408.15545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02959v2","updated":"2024-08-30T05:54:15Z","published":"2024-03-05T13:30:02Z","title":"AgentsCourt: Building Judicial Decision-Making Agents with Court Debate\n  Simulation and Legal Knowledge Augmentation","summary":"  With the development of deep learning, natural language processing technology\nhas effectively improved the efficiency of various aspects of the traditional\njudicial industry. However, most current efforts focus on tasks within\nindividual judicial stages, making it difficult to handle complex tasks that\nspan multiple stages. As the autonomous agents powered by large language models\nare becoming increasingly smart and able to make complex decisions in\nreal-world settings, offering new insights for judicial intelligence. In this\npaper, (1) we propose a novel multi-agent framework, AgentsCourt, for judicial\ndecision-making. Our framework follows the classic court trial process,\nconsisting of court debate simulation, legal resources retrieval and\ndecision-making refinement to simulate the decision-making of judge. (2) we\nintroduce SimuCourt, a judicial benchmark that encompasses 420 Chinese judgment\ndocuments, spanning the three most common types of judicial cases. Furthermore,\nto support this task, we construct a large-scale legal knowledge base,\nLegal-KB, with multi-resource legal knowledge. (3) Extensive experiments show\nthat our framework outperforms the existing advanced methods in various\naspects, especially in generating legal articles, where our model achieves\nsignificant improvements of 8.6% and 9.1% F1 score in the first and second\ninstance settings, respectively.\n","authors":["Zhitao He","Pengfei Cao","Chenhao Wang","Zhuoran Jin","Yubo Chen","Jiexin Xu","Huaijun Li","Xiaojian Jiang","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.02959v2.pdf","comment":"This paper was first submitted to ACL ARR 2024 April (Under review)"},{"id":"http://arxiv.org/abs/2408.17026v1","updated":"2024-08-30T05:50:15Z","published":"2024-08-30T05:50:15Z","title":"From Text to Emotion: Unveiling the Emotion Annotation Capabilities of\n  LLMs","summary":"  Training emotion recognition models has relied heavily on human annotated\ndata, which present diversity, quality, and cost challenges. In this paper, we\nexplore the potential of Large Language Models (LLMs), specifically GPT4, in\nautomating or assisting emotion annotation. We compare GPT4 with supervised\nmodels and or humans in three aspects: agreement with human annotations,\nalignment with human perception, and impact on model training. We find that\ncommon metrics that use aggregated human annotations as ground truth can\nunderestimate the performance, of GPT-4 and our human evaluation experiment\nreveals a consistent preference for GPT-4 annotations over humans across\nmultiple datasets and evaluators. Further, we investigate the impact of using\nGPT-4 as an annotation filtering process to improve model training. Together,\nour findings highlight the great potential of LLMs in emotion annotation tasks\nand underscore the need for refined evaluation methodologies.\n","authors":["Minxue Niu","Mimansa Jaiswal","Emily Mower Provost"],"pdf_url":"https://arxiv.org/pdf/2408.17026v1.pdf","comment":"to be published in Interspeech 2024"},{"id":"http://arxiv.org/abs/2408.17024v1","updated":"2024-08-30T05:42:31Z","published":"2024-08-30T05:42:31Z","title":"InkubaLM: A small language model for low-resource African languages","summary":"  High-resource language models often fall short in the African context, where\nthere is a critical need for models that are efficient, accessible, and locally\nrelevant, even amidst significant computing and data constraints. This paper\nintroduces InkubaLM, a small language model with 0.4 billion parameters, which\nachieves performance comparable to models with significantly larger parameter\ncounts and more extensive training data on tasks such as machine translation,\nquestion-answering, AfriMMLU, and the AfriXnli task. Notably, InkubaLM\noutperforms many larger models in sentiment analysis and demonstrates\nremarkable consistency across multiple languages. This work represents a\npivotal advancement in challenging the conventional paradigm that effective\nlanguage models must rely on substantial resources. Our model and datasets are\npublicly available \\footnote{\\url{https://huggingface.co/lelapa}} to encourage\nresearch and development on low-resource languages.\n","authors":["Atnafu Lambebo Tonja","Bonaventure F. P. Dossou","Jessica Ojo","Jenalea Rajab","Fadel Thior","Eric Peter Wairagala","Aremu Anuoluwapo","Pelonomi Moiloa","Jade Abbott","Vukosi Marivate","Benjamin Rosman"],"pdf_url":"https://arxiv.org/pdf/2408.17024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17017v1","updated":"2024-08-30T05:14:59Z","published":"2024-08-30T05:14:59Z","title":"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM\n  Sampling","summary":"  Self-Consistency (SC) is a widely used method to mitigate hallucinations in\nLarge Language Models (LLMs) by sampling the LLM multiple times and outputting\nthe most frequent solution. Despite its benefits, SC results in significant\ncomputational costs proportional to the number of samples generated. Previous\nearly-stopping approaches, such as Early Stopping Self Consistency and Adaptive\nConsistency, have aimed to reduce these costs by considering output\nconsistency, but they do not analyze the quality of the reasoning paths (RPs)\nthemselves. To address this issue, we propose Reasoning-Aware Self-Consistency\n(RASC), an innovative early-stopping framework that dynamically adjusts the\nnumber of sample generations by considering both the output answer and the RPs\nfrom Chain of Thought (CoT) prompting. RASC assigns confidence scores\nsequentially to the generated samples, stops when certain criteria are met, and\nthen employs weighted majority voting to optimize sample usage and enhance\nanswer reliability. We comprehensively test RASC with multiple LLMs across\nvaried QA datasets. RASC outperformed existing methods and significantly\nreduces sample usage by an average of 80% while maintaining or improving\naccuracy up to 5% compared to the original SC\n","authors":["Guangya Wan","Yuqi Wu","Jie Chen","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2408.17017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10537v3","updated":"2024-08-30T04:51:28Z","published":"2022-12-20T18:46:28Z","title":"Does CLIP Bind Concepts? Probing Compositionality in Large Image Models","summary":"  Large-scale neural network models combining text and images have made\nincredible progress in recent years. However, it remains an open question to\nwhat extent such models encode compositional representations of the concepts\nover which they operate, such as correctly identifying \"red cube\" by reasoning\nover the constituents \"red\" and \"cube\". In this work, we focus on the ability\nof a large pretrained vision and language model (CLIP) to encode compositional\nconcepts and to bind variables in a structure-sensitive way (e.g.,\ndifferentiating \"cube behind sphere\" from \"sphere behind cube\"). To inspect the\nperformance of CLIP, we compare several architectures from research on\ncompositional distributional semantics models (CDSMs), a line of research that\nattempts to implement traditional compositional linguistic structures within\nembedding spaces. We benchmark them on three synthetic datasets -\nsingle-object, two-object, and relational - designed to test concept binding.\nWe find that CLIP can compose concepts in a single-object setting, but in\nsituations where concept binding is needed, performance drops dramatically. At\nthe same time, CDSMs also perform poorly, with best performance at chance\nlevel.\n","authors":["Martha Lewis","Nihal V. Nayak","Peilin Yu","Qinan Yu","Jack Merullo","Stephen H. Bach","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2212.10537v3.pdf","comment":"Lewis and Nayak contributed equally"},{"id":"http://arxiv.org/abs/2305.09548v3","updated":"2024-08-30T04:18:01Z","published":"2023-05-16T15:45:59Z","title":"Measuring Dimensions of Self-Presentation in Twitter Bios and their\n  Links to Misinformation Sharing","summary":"  Social media platforms provide users with a profile description field,\ncommonly known as a ``bio,\" where they can present themselves to the world. A\ngrowing literature shows that text in these bios can improve our understanding\nof online self-presentation and behavior, but existing work relies exclusively\non keyword-based approaches to do so. We here propose and evaluate a suite of\n\\hl{simple, effective, and theoretically motivated} approaches to embed bios in\nspaces that capture salient dimensions of social meaning, such as age and\npartisanship. We \\hl{evaluate our methods on four tasks, showing that the\nstrongest one out-performs several practical baselines.} We then show the\nutility of our method in helping understand associations between\nself-presentation and the sharing of URLs from low-quality news sites on\nTwitter\\hl{, with a particular focus on explore the interactions between age\nand partisanship, and exploring the effects of self-presentations of\nreligiosity}. Our work provides new tools to help computational social\nscientists make use of information in bios, and provides new insights into how\nmisinformation sharing may be perceived on Twitter.\n","authors":["Navid Madani","Rabiraj Bandyopadhyay","Briony Swire-Thompson","Michael Miller Yoder","Kenneth Joseph"],"pdf_url":"https://arxiv.org/pdf/2305.09548v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11999v5","updated":"2024-08-30T03:39:57Z","published":"2024-04-18T08:49:38Z","title":"Token-level Direct Preference Optimization","summary":"  Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.\n","authors":["Yongcheng Zeng","Guoqing Liu","Weiyu Ma","Ning Yang","Haifeng Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2404.11999v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16991v1","updated":"2024-08-30T03:38:37Z","published":"2024-08-30T03:38:37Z","title":"Tool-Assisted Agent on SQL Inspection and Refinement in Real-World\n  Scenarios","summary":"  Recent Text-to-SQL methods leverage large language models (LLMs) by\nincorporating feedback from the database management system. While these methods\neffectively address execution errors in SQL queries, they struggle with\ndatabase mismatches -- errors that do not trigger execution exceptions.\nDatabase mismatches include issues such as condition mismatches and stricter\nconstraint mismatches, both of which are more prevalent in real-world\nscenarios. To address these challenges, we propose a tool-assisted agent\nframework for SQL inspection and refinement, equipping the LLM-based agent with\ntwo specialized tools: a retriever and a detector, designed to diagnose and\ncorrect SQL queries with database mismatches. These tools enhance the\ncapability of LLMs to handle real-world queries more effectively. We also\nintroduce Spider-Mismatch, a new dataset specifically constructed to reflect\nthe condition mismatch problems encountered in real-world scenarios.\nExperimental results demonstrate that our method achieves the highest\nperformance on the averaged results of the Spider and Spider-Realistic datasets\nin few-shot settings, and it significantly outperforms baseline methods on the\nmore realistic dataset, Spider-Mismatch.\n","authors":["Zhongyuan Wang","Richong Zhang","Zhijie Nie","Jaein Kim"],"pdf_url":"https://arxiv.org/pdf/2408.16991v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2408.16725v2","updated":"2024-08-30T02:53:48Z","published":"2024-08-29T17:18:53Z","title":"Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming","summary":"  Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.\n","authors":["Zhifei Xie","Changqiao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16725v2.pdf","comment":"Technical report, work in progress. Demo and code:\n  https://github.com/gpt-omni/mini-omni"},{"id":"http://arxiv.org/abs/2403.04261v2","updated":"2024-08-30T02:47:43Z","published":"2024-03-07T06:52:51Z","title":"Advancing Chinese biomedical text mining with community challenges","summary":"  Objective: This study aims to review the recent advances in community\nchallenges for biomedical text mining in China. Methods: We collected\ninformation of evaluation tasks released in community challenges of biomedical\ntext mining, including task description, dataset description, data source, task\ntype and related links. A systematic summary and comparative analysis were\nconducted on various biomedical natural language processing tasks, such as\nnamed entity recognition, entity normalization, attribute extraction, relation\nextraction, event extraction, text classification, text similarity, knowledge\ngraph construction, question answering, text generation, and large language\nmodel evaluation. Results: We identified 39 evaluation tasks from 6 community\nchallenges that spanned from 2017 to 2023. Our analysis revealed the diverse\nrange of evaluation task types and data sources in biomedical text mining. We\nexplored the potential clinical applications of these community challenge tasks\nfrom a translational biomedical informatics perspective. We compared with their\nEnglish counterparts, and discussed the contributions, limitations, lessons and\nguidelines of these community challenges, while highlighting future directions\nin the era of large language models. Conclusion: Community challenge evaluation\ncompetitions have played a crucial role in promoting technology innovation and\nfostering interdisciplinary collaboration in the field of biomedical text\nmining. These challenges provide valuable platforms for researchers to develop\nstate-of-the-art solutions.\n","authors":["Hui Zong","Rongrong Wu","Jiaxue Cha","Weizhe Feng","Erman Wu","Jiakun Li","Aibin Shao","Liang Tao","Zuofeng Li","Buzhou Tang","Bairong Shen"],"pdf_url":"https://arxiv.org/pdf/2403.04261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16967v1","updated":"2024-08-30T02:01:56Z","published":"2024-08-30T02:01:56Z","title":"MemLong: Memory-Augmented Retrieval for Long Text Modeling","summary":"  Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong\n","authors":["Weijie Liu","Zecheng Tang","Juntao Li","Kehai Chen","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.16967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16966v1","updated":"2024-08-30T01:56:57Z","published":"2024-08-30T01:56:57Z","title":"UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches","summary":"  Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques.\n","authors":["Chao Wang","Neo Wu","Lin Ning","Luyang Liu","Jun Xie","Shawn O'Banion","Bradley Green"],"pdf_url":"https://arxiv.org/pdf/2408.16966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07000v2","updated":"2024-08-30T01:19:42Z","published":"2024-07-09T16:13:26Z","title":"Etalon: Holistic Performance Evaluation Framework for LLM Inference\n  Systems","summary":"  Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Etalon, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Etalon, discussing their strengths and weaknesses. Etalon is\navailable at https://github.com/project-etalon/etalon.\n","authors":["Amey Agrawal","Anmol Agarwal","Nitin Kedia","Jayashree Mohan","Souvik Kundu","Nipun Kwatra","Ramachandran Ramjee","Alexey Tumanov"],"pdf_url":"https://arxiv.org/pdf/2407.07000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09625v3","updated":"2024-08-30T01:07:08Z","published":"2023-12-15T09:08:14Z","title":"Weakly-Supervised 3D Visual Grounding based on Visual Linguistic\n  Alignment","summary":"  Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly\nsupervised approach for 3D visual grounding based on Visual Linguistic\nAlignment. Our 3D-VLA exploits the superior ability of current large-scale\nvision-language models (VLMs) on aligning the semantics between texts and 2D\nimages, as well as the naturally existing correspondences between 2D images and\n3D point clouds, and thus implicitly constructs correspondences between texts\nand 3D point clouds with no need for fine-grained box annotations in the\ntraining procedure. During the inference stage, the learned text-3D\ncorrespondence will help us ground the text queries to the 3D target objects\neven without 2D images. To the best of our knowledge, this is the first work to\ninvestigate 3D visual grounding in a weakly supervised manner by involving\nlarge scale vision-language models, and extensive experiments on ReferIt3D and\nScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even\nsuperior results over the fully supervised methods.\n","authors":["Xiaoxu Xu","Yitian Yuan","Qiudan Zhang","Wenhui Wu","Zequn Jie","Lin Ma","Xu Wang"],"pdf_url":"https://arxiv.org/pdf/2312.09625v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.17443v1","updated":"2024-08-30T17:52:55Z","published":"2024-08-30T17:52:55Z","title":"Bridging Episodes and Semantics: A Novel Framework for Long-Form Video\n  Understanding","summary":"  While existing research often treats long-form videos as extended short\nvideos, we propose a novel approach that more accurately reflects human\ncognition. This paper introduces BREASE: BRidging Episodes And SEmantics for\nLong-Form Video Understanding, a model that simulates episodic memory\naccumulation to capture action sequences and reinforces them with semantic\nknowledge dispersed throughout the video. Our work makes two key contributions:\nFirst, we develop an Episodic COmpressor (ECO) that efficiently aggregates\ncrucial representations from micro to semi-macro levels. Second, we propose a\nSemantics reTRiever (SeTR) that enhances these aggregated representations with\nsemantic information by focusing on the broader context, dramatically reducing\nfeature dimensionality while preserving relevant macro-level information.\nExtensive experiments demonstrate that BREASE achieves state-of-the-art\nperformance across multiple long video understanding benchmarks in both\nzero-shot and fully-supervised settings. The project page and code are at:\nhttps://joslefaure.github.io/assets/html/hermes.html.\n","authors":["Gueter Josmy Faure","Jia-Fong Yeh","Min-Hung Chen","Hung-Ting Su","Winston H. Hsu","Shang-Hong Lai"],"pdf_url":"https://arxiv.org/pdf/2408.17443v1.pdf","comment":"Accepted to the EVAL-FoMo Workshop at ECCV'24. Project page:\n  https://joslefaure.github.io/assets/html/hermes.html"},{"id":"http://arxiv.org/abs/2403.16210v2","updated":"2024-08-30T17:39:50Z","published":"2024-03-24T16:09:21Z","title":"Frankenstein: Generating Semantic-Compositional 3D Scenes in One\n  Tri-Plane","summary":"  We present Frankenstein, a diffusion-based framework that can generate\nsemantic-compositional 3D scenes in a single pass. Unlike existing methods that\noutput a single, unified 3D shape, Frankenstein simultaneously generates\nmultiple separated shapes, each corresponding to a semantically meaningful\npart. The 3D scene information is encoded in one single tri-plane tensor, from\nwhich multiple Singed Distance Function (SDF) fields can be decoded to\nrepresent the compositional shapes. During training, an auto-encoder compresses\ntri-planes into a latent space, and then the denoising diffusion process is\nemployed to approximate the distribution of the compositional scenes.\nFrankenstein demonstrates promising results in generating room interiors as\nwell as human avatars with automatically separated parts. The generated scenes\nfacilitate many downstream applications, such as part-wise re-texturing, object\nrearrangement in the room or avatar cloth re-targeting. Our project page is\navailable at: https://wolfball.github.io/frankenstein/.\n","authors":["Han Yan","Yang Li","Zhennan Wu","Shenzhou Chen","Weixuan Sun","Taizhang Shang","Weizhe Liu","Tian Chen","Xiaqiang Dai","Chao Ma","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.16210v2.pdf","comment":"SIGGRAPH Asia 2024 Conference Paper"},{"id":"http://arxiv.org/abs/2408.17433v1","updated":"2024-08-30T17:35:06Z","published":"2024-08-30T17:35:06Z","title":"DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised\n  Vector-LoRA of the Foundation Model","summary":"  Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D\nreconstruction and visualization. While foundation models like Depth Anything\nModels (DAM) show promise, directly applying them to surgery often yields\nsuboptimal results. Fully fine-tuning on limited surgical data can cause\noverfitting and catastrophic forgetting, compromising model robustness and\ngeneralization. Although Low-Rank Adaptation (LoRA) addresses some adaptation\nissues, its uniform parameter distribution neglects the inherent feature\nhierarchy, where earlier layers, learning more general features, require more\nparameters than later ones. To tackle this issue, we introduce Depth Anything\nin Robotic Endoscopic Surgery (DARES), a novel approach that employs a new\nadaptation technique, Vector Low-Rank Adaptation (Vector-LoRA) on the DAM V2 to\nperform self-supervised monocular depth estimation in RAS scenes. To enhance\nlearning efficiency, we introduce Vector-LoRA by integrating more parameters in\nearlier layers and gradually decreasing parameters in later layers. We also\ndesign a reprojection loss based on the multi-scale SSIM error to enhance depth\nperception by better tailoring the foundation model to the specific\nrequirements of the surgical environment. The proposed method is validated on\nthe SCARED dataset and demonstrates superior performance over recent\nstate-of-the-art self-supervised monocular depth estimation techniques,\nachieving an improvement of 13.3% in the absolute relative error metric. The\ncode and pre-trained weights are available at\nhttps://github.com/mobarakol/DARES.\n","authors":["Mona Sheikh Zeinoddin","Chiara Lena","Jiongqi Qu","Luca Carlini","Mattia Magro","Seunghoi Kim","Elena De Momi","Sophia Bano","Matthew Grech-Sollars","Evangelos Mazomenos","Daniel C. Alexander","Danail Stoyanov","Matthew J. Clarkson","Mobarakol Islam"],"pdf_url":"https://arxiv.org/pdf/2408.17433v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2408.17424v1","updated":"2024-08-30T17:16:18Z","published":"2024-08-30T17:16:18Z","title":"CinePreGen: Camera Controllable Video Previsualization via\n  Engine-powered Diffusion","summary":"  With advancements in video generative AI models (e.g., SORA), creators are\nincreasingly using these techniques to enhance video previsualization. However,\nthey face challenges with incomplete and mismatched AI workflows. Existing\nmethods mainly rely on text descriptions and struggle with camera placement, a\nkey component of previsualization. To address these issues, we introduce\nCinePreGen, a visual previsualization system enhanced with engine-powered\ndiffusion. It features a novel camera and storyboard interface that offers\ndynamic control, from global to local camera adjustments. This is combined with\na user-friendly AI rendering workflow, which aims to achieve consistent results\nthrough multi-masked IP-Adapter and engine simulation guidelines. In our\ncomprehensive evaluation study, we demonstrate that our system reduces\ndevelopment viscosity (i.e., the complexity and challenges in the development\nprocess), meets users' needs for extensive control and iteration in the design\nprocess, and outperforms other AI video production workflows in cinematic\ncamera movement, as shown by our experiments and a within-subjects user study.\nWith its intuitive camera controls and realistic rendering of camera motion,\nCinePreGen shows great potential for improving video production for both\nindividual creators and industry professionals.\n","authors":["Yiran Chen","Anyi Rao","Xuekun Jiang","Shishi Xiao","Ruiqing Ma","Zeyu Wang","Hui Xiong","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2408.17424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17422v1","updated":"2024-08-30T17:12:14Z","published":"2024-08-30T17:12:14Z","title":"Open-vocabulary Temporal Action Localization using VLMs","summary":"  Video action localization aims to find timings of a specific action from a\nlong video. Although existing learning-based approaches have been successful,\nthose require annotating videos that come with a considerable labor cost. This\npaper proposes a learning-free, open-vocabulary approach based on emerging\nvision-language models (VLM). The challenge stems from the fact that VLMs are\nneither designed to process long videos nor tailored for finding actions. We\novercome these problems by extending an iterative visual prompting technique.\nSpecifically, we sample video frames into a concatenated image with frame index\nlabels, making a VLM guess a frame that is considered to be closest to the\nstart/end of the action. Iterating this process by narrowing a sampling time\nwindow results in finding a specific frame of start and end of an action. We\ndemonstrate that this sampling technique yields reasonable results,\nillustrating a practical extension of VLMs for understanding videos.\n","authors":["Naoki Wake","Atsushi Kanehira","Kazuhiro Sasabuchi","Jun Takamatsu","Katsushi Ikeuchi"],"pdf_url":"https://arxiv.org/pdf/2408.17422v1.pdf","comment":"7 pages, 5 figures, 4 tables. Last updated on August 30th, 2024"},{"id":"http://arxiv.org/abs/2408.17421v1","updated":"2024-08-30T17:11:36Z","published":"2024-08-30T17:11:36Z","title":"Generative AI Enables Medical Image Segmentation in Ultra Low-Data\n  Regimes","summary":"  Semantic segmentation of medical images is pivotal in applications like\ndisease diagnosis and treatment planning. While deep learning has excelled in\nautomating this task, a major hurdle is the need for numerous annotated\nsegmentation masks, which are resource-intensive to produce due to the required\nexpertise and time. This scenario often leads to ultra low-data regimes, where\nannotated images are extremely limited, posing significant challenges for the\ngeneralization of conventional deep learning methods on test images. To address\nthis, we introduce a generative deep learning framework, which uniquely\ngenerates high-quality paired segmentation masks and medical images, serving as\nauxiliary data for training robust models in data-scarce environments. Unlike\ntraditional generative models that treat data generation and segmentation model\ntraining as separate processes, our method employs multi-level optimization for\nend-to-end data generation. This approach allows segmentation performance to\ndirectly influence the data generation process, ensuring that the generated\ndata is specifically tailored to enhance the performance of the segmentation\nmodel. Our method demonstrated strong generalization performance across 9\ndiverse medical image segmentation tasks and on 16 datasets, in ultra-low data\nregimes, spanning various diseases, organs, and imaging modalities. When\napplied to various segmentation models, it achieved performance improvements of\n10-20\\% (absolute), in both same-domain and out-of-domain scenarios. Notably,\nit requires 8 to 20 times less training data than existing methods to achieve\ncomparable results. This advancement significantly improves the feasibility and\ncost-effectiveness of applying deep learning in medical imaging, particularly\nin scenarios with limited data availability.\n","authors":["Li Zhang","Basu Jindal","Ahmed Alaa","Robert Weinreb","David Wilson","Eran Segal","James Zou","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2408.17421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17399v1","updated":"2024-08-30T16:35:28Z","published":"2024-08-30T16:35:28Z","title":"How Knowledge Distillation Mitigates the Synthetic Gap in Fair Face\n  Recognition","summary":"  Leveraging the capabilities of Knowledge Distillation (KD) strategies, we\ndevise a strategy to fight the recent retraction of face recognition datasets.\nGiven a pretrained Teacher model trained on a real dataset, we show that\ncarefully utilising synthetic datasets, or a mix between real and synthetic\ndatasets to distil knowledge from this teacher to smaller students can yield\nsurprising results. In this sense, we trained 33 different models with and\nwithout KD, on different datasets, with different architectures and losses. And\nour findings are consistent, using KD leads to performance gains across all\nethnicities and decreased bias. In addition, it helps to mitigate the\nperformance gap between real and synthetic datasets. This approach addresses\nthe limitations of synthetic data training, improving both the accuracy and\nfairness of face recognition models.\n","authors":["Pedro C. Neto","Ivona Colakovic","SaÅ¡o KarakatiÄ","Ana F. Sequeira"],"pdf_url":"https://arxiv.org/pdf/2408.17399v1.pdf","comment":"Accepted at ECCV 2024 Workshops"},{"id":"http://arxiv.org/abs/2405.18033v2","updated":"2024-08-30T16:14:57Z","published":"2024-05-28T10:34:28Z","title":"RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian\n  Representations of Radiance Fields","summary":"  Gaussian Splatting has revolutionized the world of novel view synthesis by\nachieving high rendering performance in real-time. Recently, studies have\nfocused on enriching these 3D representations with semantic information for\ndownstream tasks. In this paper, we introduce RT-GS2, the first generalizable\nsemantic segmentation method employing Gaussian Splatting. While existing\nGaussian Splatting-based approaches rely on scene-specific training, RT-GS2\ndemonstrates the ability to generalize to unseen scenes. Our method adopts a\nnew approach by first extracting view-independent 3D Gaussian features in a\nself-supervised manner, followed by a novel View-Dependent / View-Independent\n(VDVI) feature fusion to enhance semantic consistency over different views.\nExtensive experimentation on three different datasets showcases RT-GS2's\nsuperiority over the state-of-the-art methods in semantic segmentation quality,\nexemplified by a 8.01% increase in mIoU on the Replica dataset. Moreover, our\nmethod achieves real-time performance of 27.03 FPS, marking an astonishing 901\ntimes speedup compared to existing approaches. This work represents a\nsignificant advancement in the field by introducing, to the best of our\nknowledge, the first real-time generalizable semantic segmentation method for\n3D Gaussian representations of radiance fields.\n","authors":["Mihnea-Bogdan Jurca","Remco Royen","Ion Giosan","Adrian Munteanu"],"pdf_url":"https://arxiv.org/pdf/2405.18033v2.pdf","comment":"Accepted paper at BMVC 2024"},{"id":"http://arxiv.org/abs/2408.17363v1","updated":"2024-08-30T15:53:48Z","published":"2024-08-30T15:53:48Z","title":"Look, Learn and Leverage (L$^3$): Mitigating Visual-Domain Shift and\n  Discovering Intrinsic Relations via Symbolic Alignment","summary":"  Modern deep learning models have demonstrated outstanding performance on\ndiscovering the underlying mechanisms when both visual appearance and intrinsic\nrelations (e.g., causal structure) data are sufficient, such as Disentangled\nRepresentation Learning (DRL), Causal Representation Learning (CRL) and Visual\nQuestion Answering (VQA) methods. However, generalization ability of these\nmodels is challenged when the visual domain shifts and the relations data is\nabsent during finetuning. To address this challenge, we propose a novel\nlearning framework, Look, Learn and Leverage (L$^3$), which decomposes the\nlearning process into three distinct phases and systematically utilize the\nclass-agnostic segmentation masks as the common symbolic space to align visual\ndomains. Thus, a relations discovery model can be trained on the source domain,\nand when the visual domain shifts and the intrinsic relations are absent, the\npretrained relations discovery model can be directly reused and maintain a\nsatisfactory performance. Extensive performance evaluations are conducted on\nthree different tasks: DRL, CRL and VQA, and show outstanding results on all\nthree tasks, which reveals the advantages of L$^3$.\n","authors":["Hanchen Xie","Jiageng Zhu","Mahyar Khayatkhoei","Jiazhi Li","Wael AbdAlmageed"],"pdf_url":"https://arxiv.org/pdf/2408.17363v1.pdf","comment":"17 pages, 9 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.15119v3","updated":"2024-08-30T15:29:08Z","published":"2024-08-27T14:58:13Z","title":"A Permuted Autoregressive Approach to Word-Level Recognition for Urdu\n  Digital Text","summary":"  This research paper introduces a novel word-level Optical Character\nRecognition (OCR) model specifically designed for digital Urdu text, leveraging\ntransformer-based architectures and attention mechanisms to address the\ndistinct challenges of Urdu script recognition, including its diverse text\nstyles, fonts, and variations. The model employs a permuted autoregressive\nsequence (PARSeq) architecture, which enhances its performance by enabling\ncontext-aware inference and iterative refinement through the training of\nmultiple token permutations. This method allows the model to adeptly manage\ncharacter reordering and overlapping characters, commonly encountered in Urdu\nscript. Trained on a dataset comprising approximately 160,000 Urdu text images,\nthe model demonstrates a high level of accuracy in capturing the intricacies of\nUrdu script, achieving a CER of 0.178. Despite ongoing challenges in handling\ncertain text variations, the model exhibits superior accuracy and effectiveness\nin practical applications. Future work will focus on refining the model through\nadvanced data augmentation techniques and the integration of context-aware\nlanguage models to further enhance its performance and robustness in Urdu text\nrecognition.\n","authors":["Ahmed Mustafa","Muhammad Tahir Rafique","Muhammad Ijlal Baig","Hasan Sajid","Muhammad Jawad Khan","Karam Dad Kallu"],"pdf_url":"https://arxiv.org/pdf/2408.15119v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17347v1","updated":"2024-08-30T15:22:13Z","published":"2024-08-30T15:22:13Z","title":"LSMS: Language-guided Scale-aware MedSegmentor for Medical Image\n  Referring Segmentation","summary":"  Conventional medical image segmentation methods have been found inadequate in\nfacilitating physicians with the identification of specific lesions for\ndiagnosis and treatment. Given the utility of text as an instructional format,\nwe introduce a novel task termed Medical Image Referring Segmentation (MIRS),\nwhich requires segmenting specified lesions in images based on the given\nlanguage expressions. Due to the varying object scales in medical images, MIRS\ndemands robust vision-language modeling and comprehensive multi-scale\ninteraction for precise localization and segmentation under linguistic\nguidance. However, existing medical image segmentation methods fall short in\nmeeting these demands, resulting in insufficient segmentation accuracy. In\nresponse, we propose an approach named Language-guided Scale-aware MedSegmentor\n(LSMS), incorporating two appealing designs: (1)~a Scale-aware Vision-Language\nAttention module that leverages diverse convolutional kernels to acquire rich\nvisual knowledge and interact closely with linguistic features, thereby\nenhancing lesion localization capability; (2)~a Full-Scale Decoder that\nglobally models multi-modal features across various scales, capturing\ncomplementary information between scales to accurately outline lesion\nboundaries. Addressing the lack of suitable datasets for MIRS, we constructed a\nvision-language medical dataset called Reference Hepatic Lesion Segmentation\n(RefHL-Seg). This dataset comprises 2,283 abdominal CT slices from 231 cases,\nwith corresponding textual annotations and segmentation masks for various liver\nlesions in images. We validated the performance of LSMS for MIRS and\nconventional medical image segmentation tasks across various datasets. Our LSMS\nconsistently outperforms on all datasets with lower computational costs. The\ncode and datasets will be released.\n","authors":["Shuyi Ouyang","Jinyang Zhang","Xiangye Lin","Xilai Wang","Qingqing Chen","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2408.17347v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.00583v2","updated":"2024-08-30T15:16:43Z","published":"2023-11-30T18:53:03Z","title":"DeformGS: Scene Flow in Highly Deformable Scenes for Deformable Object\n  Manipulation","summary":"  Teaching robots to fold, drape, or reposition deformable objects such as\ncloth will unlock a variety of automation applications. While remarkable\nprogress has been made for rigid object manipulation, manipulating deformable\nobjects poses unique challenges, including frequent occlusions,\ninfinite-dimensional state spaces and complex dynamics. Just as object pose\nestimation and tracking have aided robots for rigid manipulation, dense 3D\ntracking (scene flow) of highly deformable objects will enable new applications\nin robotics while aiding existing approaches, such as imitation learning or\ncreating digital twins with real2sim transfer. We propose DeformGS, an approach\nto recover scene flow in highly deformable scenes, using simultaneous video\ncaptures of a dynamic scene from multiple cameras. DeformGS builds on recent\nadvances in Gaussian splatting, a method that learns the properties of a large\nnumber of Gaussians for state-of-the-art and fast novel-view synthesis.\nDeformGS learns a deformation function to project a set of Gaussians with\ncanonical properties into world space. The deformation function uses a\nneural-voxel encoding and a multilayer perceptron (MLP) to infer Gaussian\nposition, rotation, and a shadow scalar. We enforce physics-inspired\nregularization terms based on conservation of momentum and isometry, which\nleads to trajectories with smaller trajectory errors. We also leverage existing\nfoundation models SAM and XMEM to produce noisy masks, and learn a per-Gaussian\nmask for better physics-inspired regularization. DeformGS achieves high-quality\n3D tracking on highly deformable scenes with shadows and occlusions. In\nexperiments, DeformGS improves 3D tracking by an average of 55.8% compared to\nthe state-of-the-art. With sufficient texture, DeformGS achieves a median\ntracking error of 3.3 mm on a cloth of 1.5 x 1.5 m in area. Website:\nhttps://deformgs.github.io\n","authors":["Bardienus P. Duisterhof","Zhao Mandi","Yunchao Yao","Jia-Wei Liu","Jenny Seidenschwarz","Mike Zheng Shou","Deva Ramanan","Shuran Song","Stan Birchfield","Bowen Wen","Jeffrey Ichnowski"],"pdf_url":"https://arxiv.org/pdf/2312.00583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11933v3","updated":"2024-08-30T15:08:13Z","published":"2024-06-17T15:41:57Z","title":"OpticalRS-4M: Scaling Efficient Masked Autoencoder Learning on Large\n  Remote Sensing Dataset","summary":"  Masked Image Modeling (MIM) has become an essential method for building\nfoundational visual models in remote sensing (RS). However, the limitations in\nsize and diversity of existing RS datasets restrict the ability of MIM methods\nto learn generalizable representations. Additionally, conventional MIM\ntechniques, which require reconstructing all tokens, introduce unnecessary\ncomputational overhead. To address these issues, we present a new pre-training\npipeline for RS models, featuring the creation of a large-scale RS dataset and\nan efficient MIM approach. We curated a high-quality dataset named OpticalRS-4M\nby collecting publicly available RS datasets and processing them through\nexclusion, slicing, and deduplication. OpticalRS-4M comprises 4 million optical\nimages covering various RS tasks, such as object detection and pixel\nsegmentation. To enhance efficiency, we propose SelectiveMAE, a pre-training\nmethod that dynamically encodes and reconstructs semantically rich patch\ntokens, thereby reducing the inefficiencies of traditional MIM models caused by\nredundant background pixels in RS images. Extensive experiments demonstrate\nthat OpticalRS-4M significantly improves classification, detection, and\nsegmentation performance, while SelectiveMAE increases training efficiency over\n2 times. This highlights the effectiveness and scalability of our pipeline in\ndeveloping RS foundational models.\n","authors":["Fengxiang Wang","Hongzhen Wang","Di Wang","Zonghao Guo","Zhenyu Zhong","Long Lan","Jing Zhang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2406.11933v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17339v1","updated":"2024-08-30T15:06:45Z","published":"2024-08-30T15:06:45Z","title":"Enhancing Underwater Imaging with 4-D Light Fields: Dataset and Method","summary":"  In this paper, we delve into the realm of 4-D light fields (LFs) to enhance\nunderwater imaging plagued by light absorption, scattering, and other\nchallenges. Contrasting with conventional 2-D RGB imaging, 4-D LF imaging\nexcels in capturing scenes from multiple perspectives, thereby indirectly\nembedding geometric information. This intrinsic property is anticipated to\neffectively address the challenges associated with underwater imaging. By\nleveraging both explicit and implicit depth cues present in 4-D LF images, we\npropose a progressive, mutually reinforcing framework for underwater 4-D LF\nimage enhancement and depth estimation. Specifically, our framework explicitly\nutilizes estimated depth information alongside implicit depth-related dynamic\nconvolutional kernels to modulate output features. The entire framework\ndecomposes this complex task, iteratively optimizing the enhanced image and\ndepth information to progressively achieve optimal enhancement results. More\nimportantly, we construct the first 4-D LF-based underwater image dataset for\nquantitative evaluation and supervised training of learning-based methods,\ncomprising 75 underwater scenes and 3675 high-resolution 2K pairs. To craft\nvibrant and varied underwater scenes, we build underwater environments with\nvarious objects and adopt several types of degradation. Through extensive\nexperimentation, we showcase the potential and superiority of 4-D LF-based\nunderwater imaging vis-a-vis traditional 2-D RGB-based approaches. Moreover,\nour method effectively corrects color bias and achieves state-of-the-art\nperformance. The dataset and code will be publicly available at\nhttps://github.com/linlos1234/LFUIE.\n","authors":["Yuji Lin","Xianqiang Lyu","Junhui Hou","Qian Zhao","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2408.17339v1.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2408.09869v3","updated":"2024-08-30T15:05:58Z","published":"2024-08-19T10:20:06Z","title":"Docling Technical Report","summary":"  This technical report introduces Docling, an easy to use, self-contained,\nMIT-licensed open-source package for PDF document conversion. It is powered by\nstate-of-the-art specialized AI models for layout analysis (DocLayNet) and\ntable structure recognition (TableFormer), and runs efficiently on commodity\nhardware in a small resource budget. The code interface allows for easy\nextensibility and addition of new features and models.\n","authors":["Christoph Auer","Maksym Lysak","Ahmed Nassar","Michele Dolfi","Nikolaos Livathinos","Panos Vagenas","Cesar Berrospi Ramis","Matteo Omenetti","Fabian Lindlbauer","Kasper Dinkla","Lokesh Mishra","Yusik Kim","Shubham Gupta","Rafael Teixeira de Lima","Valery Weber","Lucas Morin","Ingmar Meijer","Viktor Kuropiatnyk","Peter W. J. Staar"],"pdf_url":"https://arxiv.org/pdf/2408.09869v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17337v1","updated":"2024-08-30T15:02:22Z","published":"2024-08-30T15:02:22Z","title":"Evaluating Reliability in Medical DNNs: A Critical Analysis of Feature\n  and Confidence-Based OOD Detection","summary":"  Reliable use of deep neural networks (DNNs) for medical image analysis\nrequires methods to identify inputs that differ significantly from the training\ndata, called out-of-distribution (OOD), to prevent erroneous predictions. OOD\ndetection methods can be categorised as either confidence-based (using the\nmodel's output layer for OOD detection) or feature-based (not using the output\nlayer). We created two new OOD benchmarks by dividing the D7P (dermatology) and\nBreastMNIST (ultrasound) datasets into subsets which either contain or don't\ncontain an artefact (rulers or annotations respectively). Models were trained\nwith artefact-free images, and images with the artefacts were used as OOD test\nsets. For each OOD image, we created a counterfactual by manually removing the\nartefact via image processing, to assess the artefact's impact on the model's\npredictions. We show that OOD artefacts can boost a model's softmax confidence\nin its predictions, due to correlations in training data among other factors.\nThis contradicts the common assumption that OOD artefacts should lead to more\nuncertain outputs, an assumption on which most confidence-based methods rely.\nWe use this to explain why feature-based methods (e.g. Mahalanobis score)\ntypically have greater OOD detection performance than confidence-based methods\n(e.g. MCP). However, we also show that feature-based methods typically perform\nworse at distinguishing between inputs that lead to correct and incorrect\npredictions (for both OOD and ID data). Following from these insights, we argue\nthat a combination of feature-based and confidence-based methods should be used\nwithin DNN pipelines to mitigate their respective weaknesses. These project's\ncode and OOD benchmarks are available at:\nhttps://github.com/HarryAnthony/Evaluating_OOD_detection.\n","authors":["Harry Anthony","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2408.17337v1.pdf","comment":"Accepted for the Uncertainty for Safe Utilization of Machine Learning\n  in Medical Imaging (UNSURE 2024) workshop at the MICCAI 2023"},{"id":"http://arxiv.org/abs/2406.18249v2","updated":"2024-08-30T14:36:08Z","published":"2024-06-26T10:51:44Z","title":"Foundational Models for Pathology and Endoscopy Images: Application for\n  Gastric Inflammation","summary":"  The integration of artificial intelligence (AI) in medical diagnostics\nrepresents a significant advancement in managing upper gastrointestinal (GI)\ncancer, a major cause of global cancer mortality. Specifically for gastric\ncancer (GC), chronic inflammation causes changes in the mucosa such as atrophy,\nintestinal metaplasia (IM), dysplasia and ultimately cancer. Early detection\nthrough endoscopic regular surveillance is essential for better outcomes.\nFoundation models (FM), which are machine or deep learning models trained on\ndiverse data and applicable to broad use cases, offer a promising solution to\nenhance the accuracy of endoscopy and its subsequent pathology image analysis.\nThis review explores the recent advancements, applications, and challenges\nassociated with FM in endoscopy and pathology imaging. We started by\nelucidating the core principles and architectures underlying these models,\nincluding their training methodologies and the pivotal role of large-scale data\nin developing their predictive capabilities. Moreover, this work discusses\nemerging trends and future research directions, emphasizing the integration of\nmultimodal data, the development of more robust and equitable models, and the\npotential for real-time diagnostic support. This review aims to provide a\nroadmap for researchers and practitioners in navigating the complexities of\nincorporating FM into clinical practice for prevention/management of GC cases,\nthereby improving patient outcomes.\n","authors":["Hamideh Kerdegari","Kyle Higgins","Dennis Veselkov","Ivan Laponogov","Inese Polaka","Miguel Coimbra","Junior Andrea Pescino","Marcis Leja","Mario Dinis-Ribeiro","Tania Fleitas Kanonnikoff","Kirill Veselkov"],"pdf_url":"https://arxiv.org/pdf/2406.18249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17322v1","updated":"2024-08-30T14:32:25Z","published":"2024-08-30T14:32:25Z","title":"Investigating Neuron Ablation in Attention Heads: The Case for Peak\n  Activation Centering","summary":"  The use of transformer-based models is growing rapidly throughout society.\nWith this growth, it is important to understand how they work, and in\nparticular, how the attention mechanisms represent concepts. Though there are\nmany interpretability methods, many look at models through their neuronal\nactivations, which are poorly understood. We describe different lenses through\nwhich to view neuron activations, and investigate the effectiveness in language\nmodels and vision transformers through various methods of neural ablation: zero\nablation, mean ablation, activation resampling, and a novel approach we term\n'peak ablation'. Through experimental analysis, we find that in different\nregimes and models, each method can offer the lowest degradation of model\nperformance compared to other methods, with resampling usually causing the most\nsignificant performance deterioration. We make our code available at\nhttps://github.com/nickypro/investigating-ablation.\n","authors":["Nicholas Pochinkov","Ben Pasero","Skylar Shibayama"],"pdf_url":"https://arxiv.org/pdf/2408.17322v1.pdf","comment":"9 pages, 2 figures, XAI World Conference 2024 Late-Breaking Work"},{"id":"http://arxiv.org/abs/2408.17311v1","updated":"2024-08-30T14:15:48Z","published":"2024-08-30T14:15:48Z","title":"Structuring a Training Strategy to Robustify Perception Models with\n  Realistic Image Augmentations","summary":"  Advancing Machine Learning (ML)-based perception models for autonomous\nsystems necessitates addressing weak spots within the models, particularly in\nchallenging Operational Design Domains (ODDs). These are environmental\noperating conditions of an autonomous vehicle which can contain difficult\nconditions, e.g., lens flare at night or objects reflected in a wet street.\nThis report introduces a novel methodology for training with augmentations to\nenhance model robustness and performance in such conditions. The proposed\napproach leverages customized physics-based augmentation functions, to generate\nrealistic training data that simulates diverse ODD scenarios.\n  We present a comprehensive framework that includes identifying weak spots in\nML models, selecting suitable augmentations, and devising effective training\nstrategies. The methodology integrates hyperparameter optimization and latent\nspace optimization to fine-tune augmentation parameters, ensuring they\nmaximally improve the ML models' performance. Experimental results demonstrate\nimprovements in model performance, as measured by commonly used metrics such as\nmean Average Precision (mAP) and mean Intersection over Union (mIoU) on\nopen-source object detection and semantic segmentation models and datasets.\n  Our findings emphasize that optimal training strategies are model- and\ndata-specific and highlight the benefits of integrating augmentations into the\ntraining pipeline. By incorporating augmentations, we observe enhanced\nrobustness of ML-based perception models, making them more resilient to edge\ncases encountered in real-world ODDs. This work underlines the importance of\ncustomized augmentations and offers an effective solution for improving the\nsafety and reliability of autonomous driving functions.\n","authors":["Ahmed Hammam","Bharathwaj Krishnaswami Sreedhar","Nura Kawa","Tim Patzelt","Oliver De Candido"],"pdf_url":"https://arxiv.org/pdf/2408.17311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01476v2","updated":"2024-08-30T14:09:36Z","published":"2024-06-03T16:05:25Z","title":"DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with\n  Video Diffusion Priors","summary":"  Dynamic 3D interaction has been attracting a lot of attention recently.\nHowever, creating such 4D content remains challenging. One solution is to\nanimate 3D scenes with physics-based simulation, which requires manually\nassigning precise physical properties to the object or the simulated results\nwould become unnatural. Another solution is to learn the deformation of 3D\nobjects with the distillation of video generative models, which, however, tends\nto produce 3D videos with small and discontinuous motions due to the\ninappropriate extraction and application of physical prior. In this work,\ncombining the strengths and complementing shortcomings of the above two\nsolutions, we propose to learn the physical properties of a material field with\nvideo diffusion priors, and then utilize a physics-based Material-Point-Method\n(MPM) simulator to generate 4D content with realistic motions. In particular,\nwe propose motion distillation sampling to emphasize video motion information\nduring distillation. Moreover, to facilitate the optimization, we further\npropose a KAN-based material field with frame boosting. Experimental results\ndemonstrate that our method enjoys more realistic motion than\nstate-of-the-arts. Codes are released at:\nhttps://github.com/tyhuang0428/DreamPhysics.\n","authors":["Tianyu Huang","Haoze Zhang","Yihan Zeng","Zhilu Zhang","Hui Li","Wangmeng Zuo","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2406.01476v2.pdf","comment":"Codes are released at: https://github.com/tyhuang0428/DreamPhysics"},{"id":"http://arxiv.org/abs/2408.03677v3","updated":"2024-08-30T13:55:50Z","published":"2024-08-07T10:36:26Z","title":"L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection","summary":"  LiDAR-based vision systems are integral for 3D object detection, which is\ncrucial for autonomous navigation. However, they suffer from performance\ndegradation in adverse weather conditions due to the quality deterioration of\nLiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is\nexpected to solve this problem. However, the fusion of LiDAR and 4D radar is\nchallenging because they differ significantly in terms of data quality and the\ndegree of degradation in adverse weather. To address these issues, we introduce\nL4DR, a weather-robust 3D object detection method that effectively achieves\nLiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and\nForeground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is\nthe first exploration of the complementarity of early fusion between LiDAR and\n4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )\nparallel feature extraction backbone coupled with a Multi-Scale Gated Fusion\n(MSGF) module to counteract the varying degrees of sensor degradation under\nadverse weather conditions. Experimental evaluation on a VoD dataset with\nsimulated fog proves that L4DR is more adaptable to changing weather\nconditions. It delivers a significant performance increase under different fog\nlevels, improving the 3D mAP by up to 20.0% over the traditional LiDAR-only\napproach. Moreover, the results on the K-Radar dataset validate the consistent\nperformance improvement of L4DR in real-world adverse weather conditions.\n","authors":["Xun Huang","Ziyu Xu","Hai Wu","Jinlong Wang","Qiming Xia","Yan Xia","Jonathan Li","Kyle Gao","Chenglu Wen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03677v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17297v1","updated":"2024-08-30T13:52:26Z","published":"2024-08-30T13:52:26Z","title":"BOP-D: Revisiting 6D Pose Estimation Benchmark for Better Evaluation\n  under Visual Ambiguities","summary":"  Currently, 6D pose estimation methods are benchmarked on datasets that\nconsider, for their ground truth annotations, visual ambiguities as only\nrelated to global object symmetries. However, as previously observed [26],\nvisual ambiguities can also happen depending on the viewpoint or the presence\nof occluding objects, when disambiguating parts become hidden. The visual\nambiguities are therefore actually different across images. We thus first\npropose an automatic method to re-annotate those datasets with a 6D pose\ndistribution specific to each image, taking into account the visibility of the\nobject surface in the image to correctly determine the visual ambiguities.\nGiven this improved ground truth, we re-evaluate the state-of-the-art methods\nand show this greatly modify the ranking of these methods. Our annotations also\nallow us to benchmark recent methods able to estimate a pose distribution on\nreal images for the first time. We will make our annotations for the T-LESS\ndataset and our code publicly available.\n","authors":["Boris Meden","Asma Brazi","Steve Bourgeois","Fabrice Mayran de Chamisso","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2408.17297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11748v3","updated":"2024-08-30T13:52:12Z","published":"2024-08-21T16:16:18Z","title":"GeoMeter: Probing Depth and Height Perception of Large Visual-Language\n  Models","summary":"  Geometric understanding is crucial for navigating and interacting with our\nenvironment. While large Vision Language Models (VLMs) demonstrate impressive\ncapabilities, deploying them in real-world scenarios necessitates a comparable\ngeometric understanding in visual perception. In this work, we focus on the\ngeometric comprehension of these models; specifically targeting the depths and\nheights of objects within a scene. Our observations reveal that, although VLMs\nexcel in basic geometric properties perception such as shape and size, they\nencounter significant challenges in reasoning about the depth and height of\nobjects. To address this, we introduce GeoMeter, a suite of benchmark datasets\nencompassing Synthetic 2D, Synthetic 3D, and Real-World scenarios to rigorously\nevaluate these aspects. We benchmark 17 state-of-the-art VLMs using these\ndatasets and find that they consistently struggle with both depth and height\nperception. Our key insights include detailed analyses of the shortcomings in\ndepth and height reasoning capabilities of VLMs and the inherent bias present\nin these models. This study aims to pave the way for the development of VLMs\nwith enhanced geometric understanding, crucial for real-world applications.\n","authors":["Shehreen Azad","Yash Jain","Rishit Garg","Yogesh S Rawat","Vibhav Vineet"],"pdf_url":"https://arxiv.org/pdf/2408.11748v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16227v2","updated":"2024-08-30T13:48:14Z","published":"2024-08-29T02:58:35Z","title":"Revisiting 360 Depth Estimation with PanoGabor: A New Fusion Perspective","summary":"  Depth estimation from a monocular 360 image is important to the perception of\nthe entire 3D environment. However, the inherent distortion and large field of\nview (FoV) in 360 images pose great challenges for this task. To this end,\nexisting mainstream solutions typically introduce additional perspective-based\n360 representations (\\textit{e.g.}, Cubemap) to achieve effective feature\nextraction. Nevertheless, regardless of the introduced representations, they\neventually need to be unified into the equirectangular projection (ERP) format\nfor the subsequent depth estimation, which inevitably reintroduces the\ntroublesome distortions. In this work, we propose an oriented distortion-aware\nGabor Fusion framework (PGFuse) to address the above challenges. First, we\nintroduce Gabor filters that analyze texture in the frequency domain, thereby\nextending the receptive fields and enhancing depth cues. To address the\nreintroduced distortions, we design a linear latitude-aware distortion\nrepresentation method to generate customized, distortion-aware Gabor filters\n(PanoGabor filters). Furthermore, we design a channel-wise and spatial-wise\nunidirectional fusion module (CS-UFM) that integrates the proposed PanoGabor\nfilters to unify other representations into the ERP format, delivering\neffective and distortion-free features. Considering the orientation sensitivity\nof the Gabor transform, we introduce a spherical gradient constraint to\nstabilize this sensitivity. Experimental results on three popular indoor 360\nbenchmarks demonstrate the superiority of the proposed PGFuse to existing\nstate-of-the-art solutions. Code can be available upon acceptance.\n","authors":["Zhijie Shen","Chunyu Lin","Lang Nie","Kang Liao"],"pdf_url":"https://arxiv.org/pdf/2408.16227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17284v1","updated":"2024-08-30T13:31:15Z","published":"2024-08-30T13:31:15Z","title":"DCUDF2: Improving Efficiency and Accuracy in Extracting Zero Level Sets\n  from Unsigned Distance Fields","summary":"  Unsigned distance fields (UDFs) allow for the representation of models with\ncomplex topologies, but extracting accurate zero level sets from these fields\nposes significant challenges, particularly in preserving topological accuracy\nand capturing fine geometric details. To overcome these issues, we introduce\nDCUDF2, an enhancement over DCUDF--the current state-of-the-art method--for\nextracting zero level sets from UDFs. Our approach utilizes an accuracy-aware\nloss function, enhanced with self-adaptive weights, to improve geometric\nquality significantly. We also propose a topology correction strategy that\nreduces the dependence on hyper-parameter, increasing the robustness of our\nmethod. Furthermore, we develop new operations leveraging self-adaptive weights\nto boost runtime efficiency. Extensive experiments on surface extraction across\ndiverse datasets demonstrate that DCUDF2 outperforms DCUDF and existing methods\nin both geometric fidelity and topological accuracy. We will make the source\ncode publicly available.\n","authors":["Xuhui Chen","Fugang Yu","Fei Hou","Wencheng Wang","Zhebin Zhang","Ying He"],"pdf_url":"https://arxiv.org/pdf/2408.17284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05735v3","updated":"2024-08-30T13:28:38Z","published":"2024-01-11T08:36:15Z","title":"Object-Centric Diffusion for Efficient Video Editing","summary":"  Diffusion-based video editing have reached impressive quality and can\ntransform either the global style, local structure, and attributes of given\nvideo inputs, following textual edit prompts. However, such solutions typically\nincur heavy memory and computational costs to generate temporally-coherent\nframes, either in the form of diffusion inversion and/or cross-frame attention.\nIn this paper, we conduct an analysis of such inefficiencies, and suggest\nsimple yet effective modifications that allow significant speed-ups whilst\nmaintaining quality. Moreover, we introduce Object-Centric Diffusion, to fix\ngeneration artifacts and further reduce latency by allocating more computations\ntowards foreground edited regions, arguably more important for perceptual\nquality. We achieve this by two novel proposals: i) Object-Centric Sampling,\ndecoupling the diffusion steps spent on salient or background regions and\nspending most on the former, and ii) Object-Centric Token Merging, which\nreduces cost of cross-frame attention by fusing redundant tokens in unimportant\nbackground regions. Both techniques are readily applicable to a given video\nediting model without retraining, and can drastically reduce its memory and\ncomputational cost. We evaluate our proposals on inversion-based and\ncontrol-signal-based editing pipelines, and show a latency reduction up to 10x\nfor a comparable synthesis quality. Project page:\nqualcomm-ai-research.github.io/object-centric-diffusion.\n","authors":["Kumara Kahatapitiya","Adil Karjauv","Davide Abati","Fatih Porikli","Yuki M. Asano","Amirhossein Habibian"],"pdf_url":"https://arxiv.org/pdf/2401.05735v3.pdf","comment":"ECCV24"},{"id":"http://arxiv.org/abs/2407.00697v3","updated":"2024-08-30T13:25:50Z","published":"2024-06-30T13:39:29Z","title":"CaFNet: A Confidence-Driven Framework for Radar Camera Depth Estimation","summary":"  Depth estimation is critical in autonomous driving for interpreting 3D scenes\naccurately. Recently, radar-camera depth estimation has become of sufficient\ninterest due to the robustness and low-cost properties of radar. Thus, this\npaper introduces a two-stage, end-to-end trainable Confidence-aware Fusion Net\n(CaFNet) for dense depth estimation, combining RGB imagery with sparse and\nnoisy radar point cloud data. The first stage addresses radar-specific\nchallenges, such as ambiguous elevation and noisy measurements, by predicting a\nradar confidence map and a preliminary coarse depth map. A novel approach is\npresented for generating the ground truth for the confidence map, which\ninvolves associating each radar point with its corresponding object to identify\npotential projection surfaces. These maps, together with the initial radar\ninput, are processed by a second encoder. For the final depth estimation, we\ninnovate a confidence-aware gated fusion mechanism to integrate radar and image\nfeatures effectively, thereby enhancing the reliability of the depth map by\nfiltering out radar noise. Our methodology, evaluated on the nuScenes dataset,\ndemonstrates superior performance, improving upon the current leading model by\n3.2% in Mean Absolute Error (MAE) and 2.7% in Root Mean Square Error (RMSE).\nCode: https://github.com/harborsarah/CaFNet\n","authors":["Huawei Sun","Hao Feng","Julius Ott","Lorenzo Servadei","Robert Wille"],"pdf_url":"https://arxiv.org/pdf/2407.00697v3.pdf","comment":"Accepted by IROS 2024"},{"id":"http://arxiv.org/abs/2408.17267v1","updated":"2024-08-30T13:13:35Z","published":"2024-08-30T13:13:35Z","title":"UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal\n  Models in Multi-View Urban Scenarios","summary":"  Recent evaluations of Large Multimodal Models (LMMs) have explored their\ncapabilities in various domains, with only few benchmarks specifically focusing\non urban environments. Moreover, existing urban benchmarks have been limited to\nevaluating LMMs with basic region-level urban tasks under singular views,\nleading to incomplete evaluations of LMMs' abilities in urban environments. To\naddress these issues, we present UrBench, a comprehensive benchmark designed\nfor evaluating LMMs in complex multi-view urban scenarios. UrBench contains\n11.6K meticulously curated questions at both region-level and role-level that\ncover 4 task dimensions: Geo-Localization, Scene Reasoning, Scene\nUnderstanding, and Object Understanding, totaling 14 task types. In\nconstructing UrBench, we utilize data from existing datasets and additionally\ncollect data from 11 cities, creating new annotations using a cross-view\ndetection-matching method. With these images and annotations, we then integrate\nLMM-based, rule-based, and human-based methods to construct large-scale\nhigh-quality questions. Our evaluations on 21 LMMs show that current LMMs\nstruggle in the urban environments in several aspects. Even the best performing\nGPT-4o lags behind humans in most tasks, ranging from simple tasks such as\ncounting to complex tasks such as orientation, localization and object\nattribute recognition, with an average performance gap of 17.4%. Our benchmark\nalso reveals that LMMs exhibit inconsistent behaviors with different urban\nviews, especially with respect to understanding cross-view relations. UrBench\ndatasets and benchmark results will be publicly available at\nhttps://opendatalab.github.io/UrBench/.\n","authors":["Baichuan Zhou","Haote Yang","Dairong Chen","Junyan Ye","Tianyi Bai","Jinhua Yu","Songyang Zhang","Dahua Lin","Conghui He","Weijia Li"],"pdf_url":"https://arxiv.org/pdf/2408.17267v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.15761v2","updated":"2024-08-30T13:13:04Z","published":"2024-08-28T12:56:00Z","title":"Addressing the challenges of loop detection in agricultural environments","summary":"  While visual SLAM systems are well studied and achieve impressive results in\nindoor and urban settings, natural, outdoor and open-field environments are\nmuch less explored and still present relevant research challenges. Visual\nnavigation and local mapping have shown a relatively good performance in\nopen-field environments. However, globally consistent mapping and long-term\nlocalization still depend on the robustness of loop detection and closure, for\nwhich the literature is scarce. In this work we propose a novel method to pave\nthe way towards robust loop detection in open fields, particularly in\nagricultural settings, based on local feature search and stereo geometric\nrefinement, with a final stage of relative pose estimation. Our method\nconsistently achieves good loop detections, with a median error of 15cm. We aim\nto characterize open fields as a novel environment for loop detection,\nunderstanding the limitations and problems that arise when dealing with them.\n","authors":["NicolÃ¡s Soncini","Javier Civera","TaihÃº Pire"],"pdf_url":"https://arxiv.org/pdf/2408.15761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08981v2","updated":"2024-08-30T13:06:28Z","published":"2024-04-13T12:09:37Z","title":"Fast Fishing: Approximating BAIT for Efficient and Scalable Deep Active\n  Image Classification","summary":"  Deep active learning (AL) seeks to minimize the annotation costs for training\ndeep neural networks. BAIT, a recently proposed AL strategy based on the Fisher\nInformation, has demonstrated impressive performance across various datasets.\nHowever, BAIT's high computational and memory requirements hinder its\napplicability on large-scale classification tasks, resulting in current\nresearch neglecting BAIT in their evaluation. This paper introduces two methods\nto enhance BAIT's computational efficiency and scalability. Notably, we\nsignificantly reduce its time complexity by approximating the Fisher\nInformation. In particular, we adapt the original formulation by i) taking the\nexpectation over the most probable classes, and ii) constructing a binary\nclassification task, leading to an alternative likelihood for gradient\ncomputations. Consequently, this allows the efficient use of BAIT on\nlarge-scale datasets, including ImageNet. Our unified and comprehensive\nevaluation across a variety of datasets demonstrates that our approximations\nachieve strong performance with considerably reduced time complexity.\nFurthermore, we provide an extensive open-source toolbox that implements recent\nstate-of-the-art AL strategies, available at\nhttps://github.com/dhuseljic/dal-toolbox.\n","authors":["Denis Huseljic","Paul Hahn","Marek Herde","Lukas Rauch","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2404.08981v2.pdf","comment":"Accepted at ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2408.17253v1","updated":"2024-08-30T12:51:55Z","published":"2024-08-30T12:51:55Z","title":"VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters","summary":"  Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either fine-tune large language models\n(LLMs) or build large-scale time-series datasets to develop TSF foundation\nmodels. However, these methods face challenges due to the severe cross-domain\ngap or in-domain heterogeneity. In this paper, we explore a new road to\nbuilding a TSF foundation model from rich and high-quality natural images,\nbased on the intrinsic similarities between images and time series. To bridge\nthe gap between the two domains, we reformulate the TSF task as an image\nreconstruction task, which is further processed by a visual masked autoencoder\n(MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly,\nwithout further adaptation in the time-series domain, the proposed VisionTS\ncould achieve superior zero-shot forecasting performance compared to existing\nTSF foundation models. With minimal fine-tuning, VisionTS could further improve\nthe forecasting and achieve state-of-the-art performance in most cases. These\nfindings suggest that visual models could be a free lunch for TSF and highlight\nthe potential for future cross-domain research between computer vision and TSF.\nOur code is publicly available at https://github.com/Keytoyze/VisionTS.\n","authors":["Mouxiang Chen","Lefei Shen","Zhuo Li","Xiaoyun Joy Wang","Jianling Sun","Chenghao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.17253v1.pdf","comment":"26 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.17251v1","updated":"2024-08-30T12:50:15Z","published":"2024-08-30T12:50:15Z","title":"Abstracted Gaussian Prototypes for One-Shot Concept Learning","summary":"  We introduce a cluster-based generative image segmentation framework to\nencode higher-level representations of visual concepts based on one-shot\nlearning inspired by the Omniglot Challenge. The inferred parameters of each\ncomponent of a Gaussian Mixture Model (GMM) represent a distinct topological\nsubpart of a visual concept. Sampling new data from these parameters generates\naugmented subparts to build a more robust prototype for each concept, i.e., the\nAbstracted Gaussian Prototype (AGP). This framework addresses one-shot\nclassification tasks using a cognitively-inspired similarity metric and\naddresses one-shot generative tasks through a novel AGP-VAE pipeline employing\nvariational autoencoders (VAEs) to generate new class variants. Results from\nhuman judges reveal that the generative pipeline produces novel examples and\nclasses of visual concepts that are broadly indistinguishable from those made\nby humans. The proposed framework leads to impressive but not state-of-the-art\nclassification accuracy; thus, the contribution is two-fold: 1) the system is\nuniquely low in theoretical and computational complexity and operates in a\ncompletely standalone manner compared while existing approaches draw heavily on\npre-training or knowledge engineering; and 2) in contrast with competing neural\nnetwork models, the AGP approach addresses the importance of breadth of task\ncapability emphasized in the Omniglot challenge (i.e., successful performance\non generative tasks). These two points are critical as we advance toward an\nunderstanding of how learning/reasoning systems can produce viable, robust, and\nflexible concepts based on literally nothing more than a single example.\n","authors":["Chelsea Zou","Kenneth J. Kurtz"],"pdf_url":"https://arxiv.org/pdf/2408.17251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02252v2","updated":"2024-08-30T12:44:44Z","published":"2024-07-02T13:17:49Z","title":"GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion\n  Models and Large Language Models","summary":"  Posters play a crucial role in marketing and advertising by enhancing visual\ncommunication and brand visibility, making significant contributions to\nindustrial design. With the latest advancements in controllable T2I diffusion\nmodels, increasing research has focused on rendering text within synthesized\nimages. Despite improvements in text rendering accuracy, the field of automatic\nposter generation remains underexplored. In this paper, we propose an automatic\nposter generation framework with text rendering capabilities leveraging LLMs,\nutilizing a triple-cross attention mechanism based on alignment learning. This\nframework aims to create precise poster text within a detailed contextual\nbackground. Additionally, the framework supports controllable fonts, adjustable\nimage resolution, and the rendering of posters with descriptions and text in\nboth English and Chinese.Furthermore, we introduce a high-resolution font\ndataset and a poster dataset with resolutions exceeding 1024 pixels. Our\napproach leverages the SDXL architecture. Extensive experiments validate our\nmethod's capability in generating poster images with complex and contextually\nrich backgrounds.Codes is available at\nhttps://github.com/OPPO-Mente-Lab/GlyphDraw2.\n","authors":["Jian Ma","Yonglin Deng","Chen Chen","Haonan Lu","Zhenyu Yang"],"pdf_url":"https://arxiv.org/pdf/2407.02252v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09353v2","updated":"2024-08-30T12:41:06Z","published":"2024-05-15T14:03:38Z","title":"Large coordinate kernel attention network for lightweight image\n  super-resolution","summary":"  The multi-scale receptive field and large kernel attention (LKA) module have\nbeen shown to significantly improve performance in the lightweight image\nsuper-resolution task. However, existing lightweight super-resolution (SR)\nmethods seldom pay attention to designing efficient building block with\nmulti-scale receptive field for local modeling, and their LKA modules face a\nquadratic increase in computational and memory footprints as the convolutional\nkernel size increases. To address the first issue, we propose the multi-scale\nblueprint separable convolutions (MBSConv) as highly efficient building block\nwith multi-scale receptive field, it can focus on the learning for the\nmulti-scale information which is a vital component of discriminative\nrepresentation. As for the second issue, we revisit the key properties of LKA\nin which we find that the adjacent direct interaction of local information and\nlong-distance dependencies is crucial to provide remarkable performance. Thus,\ntaking this into account and in order to mitigate the complexity of LKA, we\npropose a large coordinate kernel attention (LCKA) module which decomposes the\n2D convolutional kernels of the depth-wise convolutional layers in LKA into\nhorizontal and vertical 1-D kernels. LCKA enables the adjacent direct\ninteraction of local information and long-distance dependencies not only in the\nhorizontal direction but also in the vertical. Besides, LCKA allows for the\ndirect use of extremely large kernels in the depth-wise convolutional layers to\ncapture more contextual information, which helps to significantly improve the\nreconstruction performance, and it incurs lower computational complexity and\nmemory footprints. Integrating MBSConv and LCKA, we propose a large coordinate\nkernel attention network (LCAN).\n","authors":["Fangwei Hao","Jiesheng Wu","Haotian Lu","Ji Du","Jing Xu","Xiaoxuan Xu"],"pdf_url":"https://arxiv.org/pdf/2405.09353v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2310.19258v3","updated":"2024-08-30T12:31:40Z","published":"2023-10-30T04:04:02Z","title":"Improving Online Source-free Domain Adaptation for Object Detection by\n  Unsupervised Data Acquisition","summary":"  Effective object detection in autonomous vehicles is challenged by deployment\nin diverse and unfamiliar environments. Online Source-Free Domain Adaptation\n(O-SFDA) offers model adaptation using a stream of unlabeled data from a target\ndomain in an online manner. However, not all captured frames contain\ninformation beneficial for adaptation, especially in the presence of redundant\ndata and class imbalance issues. This paper introduces a novel approach to\nenhance O-SFDA for adaptive object detection through unsupervised data\nacquisition. Our methodology prioritizes the most informative unlabeled frames\nfor inclusion in the online training process. Empirical evaluation on a\nreal-world dataset reveals that our method outperforms existing\nstate-of-the-art O-SFDA techniques, demonstrating the viability of unsupervised\ndata acquisition for improving the adaptive object detector.\n","authors":["Xiangyu Shi","Yanyuan Qiao","Qi Wu","Lingqiao Liu","Feras Dayoub"],"pdf_url":"https://arxiv.org/pdf/2310.19258v3.pdf","comment":"Accepted by ECCV workshop ROAM 2024; 12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.17237v1","updated":"2024-08-30T12:27:22Z","published":"2024-08-30T12:27:22Z","title":"A nonlinear elasticity model in computer vision","summary":"  The purpose of this paper is to analyze a nonlinear elasticity model\npreviously introduced by the authors for comparing two images, regarded as\nbounded open subsets of $\\R^n$ together with associated vector-valued intensity\nmaps. Optimal transformations between the images are sought as minimisers of an\nintegral functional among orientation-preserving homeomorphisms. The existence\nof minimisers is proved under natural coercivity and polyconvexity conditions,\nassuming only that the intensity functions are bounded measurable. Variants of\nthe existence theorem are also proved, first under the constraint that finite\nsets of landmark points in the two images are mapped one to the other, and\nsecond when one image is to be compared to an unknown part of another.\n  The question is studied as to whether for images related by a linear mapping\nthe unique minimizer is given by that linear mapping. For a natural class of\nfunctional integrands an example is given guaranteeing that this property holds\nfor pairs of images in which the second is a scaling of the first by a constant\nfactor. However for the property to hold for arbitrary pairs of linearly\nrelated images it is shown that the integrand has to depend on the gradient of\nthe transformation as a convex function of its determinant alone. This suggests\na new model in which the integrand depends also on second derivatives of the\ntransformation, and an example is given for which both existence of minimizers\nis assured and the above property holds for all pairs of linearly related\nimages.\n","authors":["John M. Ball","Christopher L. Horner"],"pdf_url":"https://arxiv.org/pdf/2408.17237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17231v1","updated":"2024-08-30T12:17:49Z","published":"2024-08-30T12:17:49Z","title":"CondSeg: Ellipse Estimation of Pupil and Iris via Conditioned\n  Segmentation","summary":"  Parsing of eye components (i.e. pupil, iris and sclera) is fundamental for\neye tracking and gaze estimation for AR/VR products. Mainstream approaches\ntackle this problem as a multi-class segmentation task, providing only visible\npart of pupil/iris, other methods regress elliptical parameters using\nhuman-annotated full pupil/iris parameters. In this paper, we consider two\npriors: projected full pupil/iris circle can be modelled with ellipses (ellipse\nprior), and the visibility of pupil/iris is controlled by openness of\neye-region (condition prior), and design a novel method CondSeg to estimate\nelliptical parameters of pupil/iris directly from segmentation labels, without\nexplicitly annotating full ellipses, and use eye-region mask to control the\nvisibility of estimated pupil/iris ellipses. Conditioned segmentation loss is\nused to optimize the parameters by transforming parameterized ellipses into\npixel-wise soft masks in a differentiable way. Our method is tested on public\ndatasets (OpenEDS-2019/-2020) and shows competitive results on segmentation\nmetrics, and provides accurate elliptical parameters for further applications\nof eye tracking simultaneously.\n","authors":["Zhuang Jia","Jiangfan Deng","Liying Chi","Xiang Long","Daniel K. Du"],"pdf_url":"https://arxiv.org/pdf/2408.17231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17223v1","updated":"2024-08-30T12:01:59Z","published":"2024-08-30T12:01:59Z","title":"OG-Mapping: Octree-based Structured 3D Gaussians for Online Dense\n  Mapping","summary":"  3D Gaussian splatting (3DGS) has recently demonstrated promising advancements\nin RGB-D online dense mapping. Nevertheless, existing methods excessively rely\non per-pixel depth cues to perform map densification, which leads to\nsignificant redundancy and increased sensitivity to depth noise. Additionally,\nexplicitly storing 3D Gaussian parameters of room-scale scene poses a\nsignificant storage challenge. In this paper, we introduce OG-Mapping, which\nleverages the robust scene structural representation capability of sparse\noctrees, combined with structured 3D Gaussian representations, to achieve\nefficient and robust online dense mapping. Moreover, OG-Mapping employs an\nanchor-based progressive map refinement strategy to recover the scene\nstructures at multiple levels of detail. Instead of maintaining a small number\nof active keyframes with a fixed keyframe window as previous approaches do, a\ndynamic keyframe window is employed to allow OG-Mapping to better tackle false\nlocal minima and forgetting issues. Experimental results demonstrate that\nOG-Mapping delivers more robust and superior realism mapping results than\nexisting Gaussian-based RGB-D online mapping methods with a compact model, and\nno additional post-processing is required.\n","authors":["Meng Wang","Junyi Wang","Changqun Xia","Chen Wang","Yue Qi"],"pdf_url":"https://arxiv.org/pdf/2408.17223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17222v1","updated":"2024-08-30T12:01:06Z","published":"2024-08-30T12:01:06Z","title":"How Could Generative AI Support Compliance with the EU AI Act? A Review\n  for Safe Automated Driving Perception","summary":"  Deep Neural Networks (DNNs) have become central for the perception functions\nof autonomous vehicles, substantially enhancing their ability to understand and\ninterpret the environment. However, these systems exhibit inherent limitations\nsuch as brittleness, opacity, and unpredictable behavior in out-of-distribution\nscenarios. The European Union (EU) Artificial Intelligence (AI) Act, as a\npioneering legislative framework, aims to address these challenges by\nestablishing stringent norms and standards for AI systems, including those used\nin autonomous driving (AD), which are categorized as high-risk AI. In this\nwork, we explore how the newly available generative AI models can potentially\nsupport addressing upcoming regulatory requirements in AD perception,\nparticularly with respect to safety. This short review paper summarizes the\nrequirements arising from the EU AI Act regarding DNN-based perception systems\nand systematically categorizes existing generative AI applications in AD. While\ngenerative AI models show promise in addressing some of the EU AI Acts\nrequirements, such as transparency and robustness, this review examines their\npotential benefits and discusses how developers could leverage these methods to\nenhance compliance with the Act. The paper also highlights areas where further\nresearch is needed to ensure reliable and safe integration of these\ntechnologies.\n","authors":["Mert Keser","Youssef Shoeb","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2408.17222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13126v2","updated":"2024-08-30T11:45:09Z","published":"2024-08-23T14:54:49Z","title":"CathAction: A Benchmark for Endovascular Intervention Understanding","summary":"  Real-time visual feedback from catheterization analysis is crucial for\nenhancing surgical safety and efficiency during endovascular interventions.\nHowever, existing datasets are often limited to specific tasks, small scale,\nand lack the comprehensive annotations necessary for broader endovascular\nintervention understanding. To tackle these limitations, we introduce\nCathAction, a large-scale dataset for catheterization understanding. Our\nCathAction dataset encompasses approximately 500,000 annotated frames for\ncatheterization action understanding and collision detection, and 25,000 ground\ntruth masks for catheter and guidewire segmentation. For each task, we\nbenchmark recent related works in the field. We further discuss the challenges\nof endovascular intentions compared to traditional computer vision tasks and\npoint out open research questions. We hope that CathAction will facilitate the\ndevelopment of endovascular intervention understanding methods that can be\napplied to real-world applications. The dataset is available at\nhttps://airvlab.github.io/cathaction/.\n","authors":["Baoru Huang","Tuan Vo","Chayun Kongtongvattana","Giulio Dagnino","Dennis Kundrat","Wenqiang Chi","Mohamed Abdelaziz","Trevor Kwok","Tudor Jianu","Tuong Do","Hieu Le","Minh Nguyen","Hoan Nguyen","Erman Tjiputra","Quang Tran","Jianyang Xie","Yanda Meng","Binod Bhattarai","Zhaorui Tan","Hongbin Liu","Hong Seng Gan","Wei Wang","Xi Yang","Qiufeng Wang","Jionglong Su","Kaizhu Huang","Angelos Stefanidis","Min Guo","Bo Du","Rong Tao","Minh Vu","Guoyan Zheng","Yalin Zheng","Francisco Vasconcelos","Danail Stoyanov","Daniel Elson","Ferdinando Rodriguez y Baena","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2408.13126v2.pdf","comment":"10 pages. Webpage: https://airvlab.github.io/cathaction/"},{"id":"http://arxiv.org/abs/2408.17207v1","updated":"2024-08-30T11:22:09Z","published":"2024-08-30T11:22:09Z","title":"NanoMVG: USV-Centric Low-Power Multi-Task Visual Grounding based on\n  Prompt-Guided Camera and 4D mmWave Radar","summary":"  Recently, visual grounding and multi-sensors setting have been incorporated\ninto perception system for terrestrial autonomous driving systems and Unmanned\nSurface Vehicles (USVs), yet the high complexity of modern learning-based\nvisual grounding model using multi-sensors prevents such model to be deployed\non USVs in the real-life. To this end, we design a low-power multi-task model\nnamed NanoMVG for waterway embodied perception, guiding both camera and 4D\nmillimeter-wave radar to locate specific object(s) through natural language.\nNanoMVG can perform both box-level and mask-level visual grounding tasks\nsimultaneously. Compared to other visual grounding models, NanoMVG achieves\nhighly competitive performance on the WaterVG dataset, particularly in harsh\nenvironments and boasts ultra-low power consumption for long endurance.\n","authors":["Runwei Guan","Jianan Liu","Liye Jia","Haocheng Zhao","Shanliang Yao","Xiaohui Zhu","Ka Lok Man","Eng Gee Lim","Jeremy Smith","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2408.17207v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.18197v2","updated":"2024-08-30T11:19:02Z","published":"2024-06-26T09:29:05Z","title":"Human-Free Automated Prompting for Vision-Language Anomaly Detection:\n  Prompt Optimization with Meta-guiding Prompt Scheme","summary":"  Pre-trained vision-language models (VLMs) are highly adaptable to various\ndownstream tasks through few-shot learning, making prompt-based anomaly\ndetection a promising approach. Traditional methods depend on human-crafted\nprompts that require prior knowledge of specific anomaly types. Our goal is to\ndevelop a human-free prompt-based anomaly detection framework that optimally\nlearns prompts through data-driven methods, eliminating the need for human\nintervention. The primary challenge in this approach is the lack of anomalous\nsamples during the training phase. Additionally, the Vision Transformer\n(ViT)-based image encoder in VLMs is not ideal for pixel-wise anomaly\nsegmentation due to a locality feature mismatch between the original image and\nthe output feature map. To tackle the first challenge, we have developed the\nObject-Attention Anomaly Generation Module (OAGM) to synthesize anomaly samples\nfor training. Furthermore, our Meta-Guiding Prompt-Tuning Scheme (MPTS)\niteratively adjusts the gradient-based optimization direction of learnable\nprompts to avoid overfitting to the synthesized anomalies. For the second\nchallenge, we propose Locality-Aware Attention, which ensures that each local\npatch feature attends only to nearby patch features, preserving the locality\nfeatures corresponding to their original locations. This framework allows for\nthe optimal prompt embeddings by searching in the continuous latent space via\nbackpropagation, free from human semantic constraints. Additionally, the\nmodified locality-aware attention improves the precision of pixel-wise anomaly\nsegmentation.\n","authors":["Pi-Wei Chen","Jerry Chun-Wei Lin","Jia Ji","Feng-Hao Yeh","Chao-Chun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17197v1","updated":"2024-08-30T10:49:33Z","published":"2024-08-30T10:49:33Z","title":"Covariance-corrected Whitening Alleviates Network Degeneration on\n  Imbalanced Classification","summary":"  Class imbalance is a critical issue in image classification that\nsignificantly affects the performance of deep recognition models. In this work,\nwe first identify a network degeneration dilemma that hinders the model\nlearning by introducing a high linear dependence among the features inputted\ninto the classifier. To overcome this challenge, we propose a novel framework\ncalled Whitening-Net to mitigate the degenerate solutions, in which ZCA\nwhitening is integrated before the linear classifier to normalize and\ndecorrelate the batch samples. However, in scenarios with extreme class\nimbalance, the batch covariance statistic exhibits significant fluctuations,\nimpeding the convergence of the whitening operation. Therefore, we propose two\ncovariance-corrected modules, the Group-based Relatively Balanced Batch Sampler\n(GRBS) and the Batch Embedded Training (BET), to get more accurate and stable\nbatch covariance, thereby reinforcing the capability of whitening. Our modules\ncan be trained end-to-end without incurring substantial computational costs.\nComprehensive empirical evaluations conducted on benchmark datasets, including\nCIFAR-LT-10/100, ImageNet-LT, and iNaturalist-LT, validate the effectiveness of\nour proposed approaches.\n","authors":["Zhiwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.17197v1.pdf","comment":"20 pages, 10 figures, 10 tables. arXiv admin note: text overlap with\n  arXiv:2112.05958"},{"id":"http://arxiv.org/abs/2408.17182v1","updated":"2024-08-30T10:31:39Z","published":"2024-08-30T10:31:39Z","title":"Hybrid Classification-Regression Adaptive Loss for Dense Object\n  Detection","summary":"  For object detection detectors, enhancing model performance hinges on the\nability to simultaneously consider inconsistencies across tasks and focus on\ndifficult-to-train samples. Achieving this necessitates incorporating\ninformation from both the classification and regression tasks. However, prior\nwork tends to either emphasize difficult-to-train samples within their\nrespective tasks or simply compute classification scores with IoU, often\nleading to suboptimal model performance. In this paper, we propose a Hybrid\nClassification-Regression Adaptive Loss, termed as HCRAL. Specifically, we\nintroduce the Residual of Classification and IoU (RCI) module for cross-task\nsupervision, addressing task inconsistencies, and the Conditioning Factor (CF)\nto focus on difficult-to-train samples within each task. Furthermore, we\nintroduce a new strategy named Expanded Adaptive Training Sample Selection\n(EATSS) to provide additional samples that exhibit classification and\nregression inconsistencies. To validate the effectiveness of the proposed\nmethod, we conduct extensive experiments on COCO test-dev. Experimental\nevaluations demonstrate the superiority of our approachs. Additionally, we\ndesigned experiments by separately combining the classification and regression\nloss with regular loss functions in popular one-stage models, demonstrating\nimproved performance.\n","authors":["Yanquan Huang","Liu Wei Zhen","Yun Hao","Mengyuan Zhang","Qingyao Wu","Zikun Deng","Xueming Liu","Hong Deng"],"pdf_url":"https://arxiv.org/pdf/2408.17182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17168v1","updated":"2024-08-30T10:12:13Z","published":"2024-08-30T10:12:13Z","title":"EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and\n  Body-Worn IMUs","summary":"  Egocentric human pose estimation (HPE) using wearable sensors is essential\nfor VR/AR applications. Most methods rely solely on either egocentric-view\nimages or sparse Inertial Measurement Unit (IMU) signals, leading to\ninaccuracies due to self-occlusion in images or the sparseness and drift of\ninertial sensors. Most importantly, the lack of real-world datasets containing\nboth modalities is a major obstacle to progress in this field. To overcome the\nbarrier, we propose EMHI, a multimodal \\textbf{E}gocentric human\n\\textbf{M}otion dataset with \\textbf{H}ead-Mounted Display (HMD) and body-worn\n\\textbf{I}MUs, with all data collected under the real VR product suite.\nSpecifically, EMHI provides synchronized stereo images from downward-sloping\ncameras on the headset and IMU data from body-worn sensors, along with pose\nannotations in SMPL format. This dataset consists of 885 sequences captured by\n58 subjects performing 39 actions, totaling about 28.5 hours of recording. We\nevaluate the annotations by comparing them with optical marker-based SMPL\nfitting results. To substantiate the reliability of our dataset, we introduce\nMEPoser, a new baseline method for multimodal egocentric HPE, which employs a\nmultimodal fusion encoder, temporal feature encoder, and MLP-based regression\nheads. The experiments on EMHI show that MEPoser outperforms existing\nsingle-modal methods and demonstrates the value of our dataset in solving the\nproblem of egocentric HPE. We believe the release of EMHI and the method could\nadvance the research of egocentric HPE and expedite the practical\nimplementation of this technology in VR/AR products.\n","authors":["Zhen Fan","Peng Dai","Zhuo Su","Xu Gao","Zheng Lv","Jiarui Zhang","Tianyuan Du","Guidong Wang","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.17168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17154v1","updated":"2024-08-30T09:48:47Z","published":"2024-08-30T09:48:47Z","title":"Self-supervised Anomaly Detection Pretraining Enhances Long-tail ECG\n  Diagnosis","summary":"  Current computer-aided ECG diagnostic systems struggle with the\nunderdetection of rare but critical cardiac anomalies due to the imbalanced\nnature of ECG datasets. This study introduces a novel approach using\nself-supervised anomaly detection pretraining to address this limitation. The\nanomaly detection model is specifically designed to detect and localize subtle\ndeviations from normal cardiac patterns, capturing the nuanced details\nessential for accurate ECG interpretation. Validated on an extensive dataset of\nover one million ECG records from clinical practice, characterized by a\nlong-tail distribution across 116 distinct categories, the anomaly\ndetection-pretrained ECG diagnostic model has demonstrated a significant\nimprovement in overall accuracy. Notably, our approach yielded a 94.7% AUROC,\n92.2% sensitivity, and 92.5\\% specificity for rare ECG types, significantly\noutperforming traditional methods and narrowing the performance gap with common\nECG types. The integration of anomaly detection pretraining into ECG analysis\nrepresents a substantial contribution to the field, addressing the\nlong-standing challenge of long-tail data distributions in clinical\ndiagnostics. Furthermore, prospective validation in real-world clinical\nsettings revealed that our AI-driven approach enhances diagnostic efficiency,\nprecision, and completeness by 32%, 6.7%, and 11.8% respectively, when compared\nto standard practices. This advancement marks a pivotal step forward in the\nintegration of AI within clinical cardiology, with particularly profound\nimplications for emergency care, where rapid and accurate ECG interpretation is\ncrucial. The contributions of this study not only push the boundaries of\ncurrent ECG diagnostic capabilities but also lay the groundwork for more\nreliable and accessible cardiovascular care.\n","authors":["Aofan Jiang","Chaoqin Huang","Qing Cao","Yuchen Xu","Zi Zeng","Kang Chen","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.17154v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2404.04935"},{"id":"http://arxiv.org/abs/2408.17150v1","updated":"2024-08-30T09:40:10Z","published":"2024-08-30T09:40:10Z","title":"Look, Compare, Decide: Alleviating Hallucination in Large\n  Vision-Language Models via Multi-View Multi-Path Reasoning","summary":"  Recently, Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities in multi-modal context comprehension. However, they still suffer\nfrom hallucination problems referring to generating inconsistent outputs with\nthe image content. To mitigate hallucinations, previous studies mainly focus on\nretraining LVLMs with custom datasets. Although effective, they inherently come\nwith additional computational costs. In this paper, we propose a training-free\nframework, \\textbf{MVP}, that aims to reduce hallucinations by making the most\nof the innate capabilities of the LVLMs via \\textbf{M}ulti-\\textbf{V}iew\nMulti-\\textbf{P}ath Reasoning. Specifically, we first devise a multi-view\ninformation-seeking strategy to thoroughly perceive the comprehensive\ninformation in the image, which enriches the general global information\ncaptured by the original vision encoder in LVLMs. Furthermore, during the\nanswer decoding, we observe that the occurrence of hallucinations has a strong\ncorrelation with the certainty of the answer tokens. Thus, we propose\nmulti-path reasoning for each information view to quantify and aggregate the\ncertainty scores for each potential answer among multiple decoding paths and\nfinally decide the output answer. By fully grasping the information in the\nimage and carefully considering the certainty of the potential answers when\ndecoding, our MVP can effectively reduce hallucinations in LVLMs.The extensive\nexperiments verify that our proposed MVP significantly mitigates the\nhallucination problem across four well-known LVLMs. The source code is\navailable at: \\url{https://github.com/GasolSun36/MVP}.\n","authors":["Xiaoye Qu","Jiashuo Sun","Wei Wei","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.17150v1.pdf","comment":"13 pages, 7 tables, 7 figures"},{"id":"http://arxiv.org/abs/2408.17149v1","updated":"2024-08-30T09:39:59Z","published":"2024-08-30T09:39:59Z","title":"GMM-IKRS: Gaussian Mixture Models for Interpretable Keypoint Refinement\n  and Scoring","summary":"  The extraction of keypoints in images is at the basis of many computer vision\napplications, from localization to 3D reconstruction. Keypoints come with a\nscore permitting to rank them according to their quality. While learned\nkeypoints often exhibit better properties than handcrafted ones, their scores\nare not easily interpretable, making it virtually impossible to compare the\nquality of individual keypoints across methods. We propose a framework that can\nrefine, and at the same time characterize with an interpretable score, the\nkeypoints extracted by any method. Our approach leverages a modified robust\nGaussian Mixture Model fit designed to both reject non-robust keypoints and\nrefine the remaining ones. Our score comprises two components: one relates to\nthe probability of extracting the same keypoint in an image captured from\nanother viewpoint, the other relates to the localization accuracy of the\nkeypoint. These two interpretable components permit a comparison of individual\nkeypoints extracted across different methods. Through extensive experiments we\ndemonstrate that, when applied to popular keypoint detectors, our framework\nconsistently improves the repeatability of keypoints as well as their\nperformance in homography and two/multiple-view pose recovery tasks.\n","authors":["Emanuele Santellani","Martin Zach","Christian Sormann","Mattia Rossi","Andreas Kuhn","Friedrich Fraundorfer"],"pdf_url":"https://arxiv.org/pdf/2408.17149v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2408.17143v1","updated":"2024-08-30T09:34:36Z","published":"2024-08-30T09:34:36Z","title":"RenDetNet: Weakly-supervised Shadow Detection with Shadow Caster\n  Verification","summary":"  Existing shadow detection models struggle to differentiate dark image areas\nfrom shadows. In this paper, we tackle this issue by verifying that all\ndetected shadows are real, i.e. they have paired shadow casters. We perform\nthis step in a physically-accurate manner by differentiably re-rendering the\nscene and observing the changes stemming from carving out estimated shadow\ncasters. Thanks to this approach, the RenDetNet proposed in this paper is the\nfirst learning-based shadow detection model whose supervisory signals can be\ncomputed in a self-supervised manner. The developed system compares favourably\nagainst recent models trained on our data. As part of this publication, we\nrelease our code on github.\n","authors":["Nikolina Kubiak","Elliot Wortman","Armin Mustafa","Graeme Phillipson","Stephen Jolly","Simon Hadfield"],"pdf_url":"https://arxiv.org/pdf/2408.17143v1.pdf","comment":"AIM @ ECCV 2024 / code available at\n  https://github.com/n-kubiak/RenDetNet"},{"id":"http://arxiv.org/abs/2408.16005v2","updated":"2024-08-30T09:26:10Z","published":"2024-08-13T20:00:36Z","title":"Many-Worlds Inverse Rendering","summary":"  Discontinuous visibility changes remain a major bottleneck when optimizing\nsurfaces within a physically-based inverse renderer. Many previous works have\nproposed sophisticated algorithms and data structures to sample visibility\nsilhouettes more efficiently.\n  Our work presents another solution: instead of differentiating a tentative\nsurface locally, we differentiate a volumetric perturbation of a surface. We\nrefer this as a many-worlds representation because it models a non-interacting\nsuperposition of conflicting explanations (worlds) of the input dataset. Each\nworld is optically isolated from others, leading to a new transport law that\ndistinguishes our method from prior work based on exponential random media.\n  The resulting Monte Carlo algorithm is simpler and more efficient than prior\nmethods. We demonstrate that our method promotes rapid convergence, both in\nterms of the total iteration count and the cost per iteration.\n","authors":["Ziyi Zhang","Nicolas Roussel","Wenzel Jakob"],"pdf_url":"https://arxiv.org/pdf/2408.16005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17135v1","updated":"2024-08-30T09:22:07Z","published":"2024-08-30T09:22:07Z","title":"Temporal and Interactive Modeling for Efficient Human-Human Motion\n  Generation","summary":"  Human-human motion generation is essential for understanding humans as social\nbeings. Although several transformer-based methods have been proposed, they\ntypically model each individual separately and overlook the causal\nrelationships in temporal motion sequences. Furthermore, the attention\nmechanism in transformers exhibits quadratic computational complexity,\nsignificantly reducing their efficiency when processing long sequences. In this\npaper, we introduce TIM (Temporal and Interactive Modeling), an efficient and\neffective approach that presents the pioneering human-human motion generation\nmodel utilizing RWKV. Specifically, we first propose Causal Interactive\nInjection to leverage the temporal properties of motion sequences and avoid\nnon-causal and cumbersome modeling. Then we present Role-Evolving Mixing to\nadjust to the ever-evolving roles throughout the interaction. Finally, to\ngenerate smoother and more rational motion, we design Localized Pattern\nAmplification to capture short-term motion patterns. Extensive experiments on\nInterHuman demonstrate that our method achieves superior performance. Notably,\nTIM has achieved state-of-the-art results using only 32% of InterGen's\ntrainable parameters. Code will be available soon. Homepage:\nhttps://aigc-explorer.github.io/TIM-page/\n","authors":["Yabiao Wang","Shuo Wang","Jiangning Zhang","Ke Fan","Jiafu Wu","Zhengkai Jiang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.17135v1.pdf","comment":"Homepage: https://aigc-explorer.github.io/TIM-page/"},{"id":"http://arxiv.org/abs/2408.17131v1","updated":"2024-08-30T09:15:54Z","published":"2024-08-30T09:15:54Z","title":"VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion\n  Transformers","summary":"  The Diffusion Transformers Models (DiTs) have transitioned the network\narchitecture from traditional UNets to transformers, demonstrating exceptional\ncapabilities in image generation. Although DiTs have been widely applied to\nhigh-definition video generation tasks, their large parameter size hinders\ninference on edge devices. Vector quantization (VQ) can decompose model weight\ninto a codebook and assignments, allowing extreme weight quantization and\nsignificantly reducing memory usage. In this paper, we propose VQ4DiT, a fast\npost-training vector quantization method for DiTs. We found that traditional VQ\nmethods calibrate only the codebook without calibrating the assignments. This\nleads to weight sub-vectors being incorrectly assigned to the same assignment,\nproviding inconsistent gradients to the codebook and resulting in a suboptimal\nresult. To address this challenge, VQ4DiT calculates the candidate assignment\nset for each weight sub-vector based on Euclidean distance and reconstructs the\nsub-vector based on the weighted average. Then, using the zero-data and\nblock-wise calibration method, the optimal assignment from the set is\nefficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT\nXL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending\non the different quantization settings. Experiments show that VQ4DiT\nestablishes a new state-of-the-art in model size and performance trade-offs,\nquantizing weights to 2-bit precision while retaining acceptable image\ngeneration quality.\n","authors":["Juncan Deng","Shuaiting Li","Zeyu Wang","Hong Gu","Kedong Xu","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2408.17131v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.19341v3","updated":"2024-08-30T09:07:26Z","published":"2024-02-29T16:47:54Z","title":"RoadRunner -- Learning Traversability Estimation for Autonomous Off-road\n  Driving","summary":"  Autonomous navigation at high speeds in off-road environments necessitates\nrobots to comprehensively understand their surroundings using onboard sensing\nonly. The extreme conditions posed by the off-road setting can cause degraded\ncamera image quality due to poor lighting and motion blur, as well as limited\nsparse geometric information available from LiDAR sensing when driving at high\nspeeds. In this work, we present RoadRunner, a novel framework capable of\npredicting terrain traversability and an elevation map directly from camera and\nLiDAR sensor inputs. RoadRunner enables reliable autonomous navigation, by\nfusing sensory information, handling of uncertainty, and generation of\ncontextually informed predictions about the geometry and traversability of the\nterrain while operating at low latency. In contrast to existing methods relying\non classifying handcrafted semantic classes and using heuristics to predict\ntraversability costs, our method is trained end-to-end in a self-supervised\nfashion. The RoadRunner network architecture builds upon popular sensor fusion\nnetwork architectures from the autonomous driving domain, which embed LiDAR and\ncamera information into a common Bird's Eye View perspective. Training is\nenabled by utilizing an existing traversability estimation stack to generate\ntraining data in hindsight in a scalable manner from real-world off-road\ndriving datasets. Furthermore, RoadRunner improves the system latency by a\nfactor of roughly 4, from 500 ms to 140 ms, while improving the accuracy for\ntraversability costs and elevation map predictions. We demonstrate the\neffectiveness of RoadRunner in enabling safe and reliable off-road navigation\nat high speeds in multiple real-world driving scenarios through unstructured\ndesert environments.\n","authors":["Jonas Frey","Manthan Patel","Deegan Atha","Julian Nubert","David Fan","Ali Agha","Curtis Padgett","Patrick Spieler","Marco Hutter","Shehryar Khattak"],"pdf_url":"https://arxiv.org/pdf/2402.19341v3.pdf","comment":"accepted for IEEE Transactions on Field Robotics (T-FR)"},{"id":"http://arxiv.org/abs/2408.13123v3","updated":"2024-08-30T09:06:06Z","published":"2024-08-23T14:50:49Z","title":"Evidential Deep Partial Multi-View Classification With Discount Fusion","summary":"  Incomplete multi-view data classification poses significant challenges due to\nthe common issue of missing views in real-world scenarios. Despite\nadvancements, existing methods often fail to provide reliable predictions,\nlargely due to the uncertainty of missing views and the inconsistent quality of\nimputed data. To tackle these problems, we propose a novel framework called\nEvidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we use\nK-means imputation to address missing views, creating a complete set of\nmulti-view data. However, the potential conflicts and uncertainties within this\nimputed data can affect the reliability of downstream inferences. To manage\nthis, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), which\ndynamically adjusts based on the reliability of the evidence, ensuring\ntrustworthy discount fusion and producing reliable inference outcomes.\nComprehensive experiments on various benchmark datasets reveal EDP-MVC not only\nmatches but often surpasses the performance of state-of-the-art methods.\n","authors":["Haojian Huang","Zhe Liu","Sukumar Letchmunan","Muhammet Deveci","Mingwei Lin","Weizhong Wang"],"pdf_url":"https://arxiv.org/pdf/2408.13123v3.pdf","comment":"Ongoing work. 13 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2404.12966v3","updated":"2024-08-30T09:00:38Z","published":"2024-04-19T15:53:27Z","title":"Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of\n  Multi-modal Large Language Models","summary":"  Counterfactual reasoning, as a crucial manifestation of human intelligence,\nrefers to making presuppositions based on established facts and extrapolating\npotential outcomes. Existing multimodal large language models (MLLMs) have\nexhibited impressive cognitive and reasoning capabilities, which have been\nexamined across a wide range of Visual Question Answering (VQA) benchmarks.\nNevertheless, how will existing MLLMs perform when faced with counterfactual\nquestions? To answer this question, we first curate a novel\n\\textbf{C}ounter\\textbf{F}actual \\textbf{M}ulti\\textbf{M}odal reasoning\nbenchmark, abbreviated as \\textbf{CFMM}, to systematically assess the\ncounterfactual reasoning capabilities of MLLMs. Our CFMM comprises six\nchallenging tasks, each including hundreds of carefully human-labeled and\nGPT-generated counterfactual questions, to evaluate MLLM's counterfactual\nreasoning capabilities across diverse aspects. Through experiments,\ninterestingly, we find that existing MLLMs prefer to believe what they see, but\nignore the counterfactual presuppositions presented in the question, thereby\nleading to inaccurate responses. Furthermore, we evaluate a wide range of\nprevalent MLLMs on our proposed CFMM. The significant gap between their\nperformance on our CFMM and that on several VQA benchmarks indicates that there\nis still considerable room for improvement in existing MLLMs toward approaching\nhuman-level intelligence. On the other hand, through boosting MLLMs\nperformances on our CFMM in the future, potential avenues toward developing\nMLLMs with advanced intelligence can be explored.\n","authors":["Yian Li","Wentao Tian","Yang Jiao","Jingjing Chen"],"pdf_url":"https://arxiv.org/pdf/2404.12966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17115v1","updated":"2024-08-30T08:57:04Z","published":"2024-08-30T08:57:04Z","title":"Multi-centric AI Model for Unruptured Intracranial Aneurysm Detection\n  and Volumetric Segmentation in 3D TOF-MRI","summary":"  Purpose: To develop an open-source nnU-Net-based AI model for combined\ndetection and segmentation of unruptured intracranial aneurysms (UICA) in 3D\nTOF-MRI, and compare models trained on datasets with aneurysm-like differential\ndiagnoses. Methods: This retrospective study (2020-2023) included 385\nanonymized 3D TOF-MRI images from 364 patients (mean age 59 years, 60% female)\nat multiple centers plus 113 subjects from the ADAM challenge. Images featured\nuntreated or possible UICAs and differential diagnoses. Four distinct training\ndatasets were created, and the nnU-Net framework was used for model\ndevelopment. Performance was assessed on a separate test set using sensitivity\nand False Positive (FP)/case rate for detection, and DICE score and NSD\n(Normalized Surface Distance) with a 0.5mm threshold for segmentation.\nStatistical analysis included chi-square, Mann-Whitney-U, and Kruskal-Wallis\ntests, with significance set at p < 0.05. Results: Models achieved overall\nsensitivity between 82% and 85% and a FP/case rate of 0.20 to 0.31, with no\nsignificant differences (p = 0.90 and p = 0.16). The primary model showed 85%\nsensitivity and 0.23 FP/case rate, outperforming the ADAM-challenge winner\n(61%) and a nnU-Net trained on ADAM data (51%) in sensitivity (p < 0.05). It\nachieved a mean DICE score of 0.73 and an NSD of 0.84 for correctly detected\nUICA. Conclusions: Our open-source, nnU-Net-based AI model (available at\n10.5281/zenodo.13386859) demonstrates high sensitivity, low false positive\nrates, and consistent segmentation accuracy for UICA detection and segmentation\nin 3D TOF-MRI, suggesting its potential to improve clinical diagnosis and for\nmonitoring of UICA.\n","authors":["Ashraya K. Indrakanti","Jakob Wasserthal","Martin Segeroth","Shan Yang","Victor Schulze-Zachau","Joshy Cyriac","Michael Bach","Marios Psychogios","Matthias A. Mutke"],"pdf_url":"https://arxiv.org/pdf/2408.17115v1.pdf","comment":"14 pages, 5 figures, 3 tables, 2 supplementary tables"},{"id":"http://arxiv.org/abs/2408.17108v1","updated":"2024-08-30T08:49:27Z","published":"2024-08-30T08:49:27Z","title":"Sparse Uncertainty-Informed Sampling from Federated Streaming Data","summary":"  We present a numerically robust, computationally efficient approach for\nnon-I.I.D. data stream sampling in federated client systems, where resources\nare limited and labeled data for local model adaptation is sparse and\nexpensive. The proposed method identifies relevant stream observations to\noptimize the underlying client model, given a local labeling budget, and\nperforms instantaneous labeling decisions without relying on any memory\nbuffering strategies. Our experiments show enhanced training batch diversity\nand an improved numerical robustness of the proposal compared to existing\nstrategies over large-scale data streams, making our approach an effective and\nconvenient solution in FL environments.\n","authors":["Manuel RÃ¶der","Frank-Michael Schleif"],"pdf_url":"https://arxiv.org/pdf/2408.17108v1.pdf","comment":"Preprint, 6 pages, 3 figures, Accepted for ESANN 2024"},{"id":"http://arxiv.org/abs/2407.07605v3","updated":"2024-08-30T08:36:36Z","published":"2024-07-10T12:44:22Z","title":"Early Explorations of Lightweight Models for Wound Segmentation on\n  Mobile Devices","summary":"  The aging population poses numerous challenges to healthcare, including the\nincrease in chronic wounds in the elderly. The current approach to wound\nassessment by therapists based on photographic documentation is subjective,\nhighlighting the need for computer-aided wound recognition from smartphone\nphotos. This offers objective and convenient therapy monitoring, while being\naccessible to patients from their home at any time. However, despite research\nin mobile image segmentation, there is a lack of focus on mobile wound\nsegmentation. To address this gap, we conduct initial research on three\nlightweight architectures to investigate their suitability for smartphone-based\nwound segmentation. Using public datasets and UNet as a baseline, our results\nare promising, with both ENet and TopFormer, as well as the larger UNeXt\nvariant, showing comparable performance to UNet. Furthermore, we deploy the\nmodels into a smartphone app for visual assessment of live segmentation, where\nresults demonstrate the effectiveness of TopFormer in distinguishing wounds\nfrom wound-coloured objects. While our study highlights the potential of\ntransformer models for mobile wound segmentation, future work should aim to\nfurther improve the mask contours.\n","authors":["Vanessa Borst","Timo Dittus","Konstantin MÃ¼ller","Samuel Kounev"],"pdf_url":"https://arxiv.org/pdf/2407.07605v3.pdf","comment":"Extended version of our paper that was published in the \"47th German\n  Conference on Artificial Intelligence (KI 2024)\""},{"id":"http://arxiv.org/abs/2408.17098v1","updated":"2024-08-30T08:34:51Z","published":"2024-08-30T08:34:51Z","title":"UTrack: Multi-Object Tracking with Uncertain Detections","summary":"  The tracking-by-detection paradigm is the mainstream in multi-object\ntracking, associating tracks to the predictions of an object detector. Although\nexhibiting uncertainty through a confidence score, these predictions do not\ncapture the entire variability of the inference process. For safety and\nsecurity critical applications like autonomous driving, surveillance, etc.,\nknowing this predictive uncertainty is essential though. Therefore, we\nintroduce, for the first time, a fast way to obtain the empirical predictive\ndistribution during object detection and incorporate that knowledge in\nmulti-object tracking. Our mechanism can easily be integrated into\nstate-of-the-art trackers, enabling them to fully exploit the uncertainty in\nthe detections. Additionally, novel association methods are introduced that\nleverage the proposed mechanism. We demonstrate the effectiveness of our\ncontribution on a variety of benchmarks, such as MOT17, MOT20, DanceTrack, and\nKITTI.\n","authors":["Edgardo Solano-Carrillo","Felix Sattler","Antje Alex","Alexander Klein","Bruno Pereira Costa","Angel Bueno Rodriguez","Jannis Stoppe"],"pdf_url":"https://arxiv.org/pdf/2408.17098v1.pdf","comment":"Accepted for the ECCV 2024 Workshop on Uncertainty Quantification for\n  Computer Vision"},{"id":"http://arxiv.org/abs/2408.17095v1","updated":"2024-08-30T08:26:55Z","published":"2024-08-30T08:26:55Z","title":"RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation\n  and Retrieval-Guidance","summary":"  Diffusion-based models demonstrate impressive generation capabilities.\nHowever, they also have a massive number of parameters, resulting in enormous\nmodel sizes, thus making them unsuitable for deployment on resource-constraint\ndevices. Block-wise generation can be a promising alternative for designing\ncompact-sized (parameter-efficient) deep generative models since the model can\ngenerate one block at a time instead of generating the whole image at once.\nHowever, block-wise generation is also considerably challenging because\nensuring coherence across generated blocks can be non-trivial. To this end, we\ndesign a retrieval-augmented generation (RAG) approach and leverage the\ncorresponding blocks of the images retrieved by the RAG module to condition the\ntraining and generation stages of a block-wise denoising diffusion model. Our\nconditioning schemes ensure coherence across the different blocks during\ntraining and, consequently, during generation. While we showcase our approach\nusing the latent diffusion model (LDM) as the base model, it can be used with\nother variants of denoising diffusion models. We validate the solution of the\ncoherence problem through the proposed approach by reporting substantive\nexperiments to demonstrate our approach's effectiveness in compact model size\nand excellent generation quality.\n","authors":["Avideep Mukherjee","Soumya Banerjee","Vinay P. Namboodiri","Piyush Rai"],"pdf_url":"https://arxiv.org/pdf/2408.17095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17090v1","updated":"2024-08-30T08:22:30Z","published":"2024-08-30T08:22:30Z","title":"FissionVAE: Federated Non-IID Image Generation with Latent Space and\n  Decoder Decomposition","summary":"  Federated learning is a machine learning paradigm that enables decentralized\nclients to collaboratively learn a shared model while keeping all the training\ndata local. While considerable research has focused on federated image\ngeneration, particularly Generative Adversarial Networks, Variational\nAutoencoders have received less attention. In this paper, we address the\nchallenges of non-IID (independently and identically distributed) data\nenvironments featuring multiple groups of images of different types.\nSpecifically, heterogeneous data distributions can lead to difficulties in\nmaintaining a consistent latent space and can also result in local generators\nwith disparate texture features being blended during aggregation. We introduce\na novel approach, FissionVAE, which decomposes the latent space and constructs\ndecoder branches tailored to individual client groups. This method allows for\ncustomized learning that aligns with the unique data distributions of each\ngroup. Additionally, we investigate the incorporation of hierarchical VAE\narchitectures and demonstrate the use of heterogeneous decoder architectures\nwithin our model. We also explore strategies for setting the latent prior\ndistributions to enhance the decomposition process. To evaluate our approach,\nwe assemble two composite datasets: the first combines MNIST and FashionMNIST;\nthe second comprises RGB datasets of cartoon and human faces, wild animals,\nmarine vessels, and remote sensing images of Earth. Our experiments demonstrate\nthat FissionVAE greatly improves generation quality on these datasets compared\nto baseline federated VAE models.\n","authors":["Chen Hu","Jingjing Deng","Xianghua Xie","Xiaoke Ma"],"pdf_url":"https://arxiv.org/pdf/2408.17090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17083v1","updated":"2024-08-30T08:13:06Z","published":"2024-08-30T08:13:06Z","title":"Focus-Consistent Multi-Level Aggregation for Compositional Zero-Shot\n  Learning","summary":"  To transfer knowledge from seen attribute-object compositions to recognize\nunseen ones, recent compositional zero-shot learning (CZSL) methods mainly\ndiscuss the optimal classification branches to identify the elements, leading\nto the popularity of employing a three-branch architecture. However, these\nmethods mix up the underlying relationship among the branches, in the aspect of\nconsistency and diversity. Specifically, consistently providing the\nhighest-level features for all three branches increases the difficulty in\ndistinguishing classes that are superficially similar. Furthermore, a single\nbranch may focus on suboptimal regions when spatial messages are not shared\nbetween the personalized branches. Recognizing these issues and endeavoring to\naddress them, we propose a novel method called Focus-Consistent Multi-Level\nAggregation (FOMA). Our method incorporates a Multi-Level Feature Aggregation\n(MFA) module to generate personalized features for each branch based on the\nimage content. Additionally, a Focus-Consistent Constraint encourages a\nconsistent focus on the informative regions, thereby implicitly exchanging\nspatial information between all branches. Extensive experiments on three\nbenchmark datasets (UT-Zappos, C-GQA, and Clothing16K) demonstrate that our\nFOMA outperforms SOTA.\n","authors":["Fengyuan Dai","Siteng Huang","Min Zhang","Biao Gong","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.17083v1.pdf","comment":"Compositional Zero-Shot Learning"},{"id":"http://arxiv.org/abs/2408.17081v1","updated":"2024-08-30T08:09:19Z","published":"2024-08-30T08:09:19Z","title":"Stochastic Layer-Wise Shuffle: A Good Practice to Improve Vision Mamba\n  Training","summary":"  Recent Vision Mamba models not only have much lower complexity for processing\nhigher resolution images and longer videos but also the competitive performance\nwith Vision Transformers (ViTs). However, they are stuck into overfitting and\nthus only present up to base size (about 80M). It is still unclear how vanilla\nVision Mamba (Vim) can be efficiently scaled up to larger sizes, which is\nessentially for further exploitation. In this paper, we propose a stochastic\nlayer-wise shuffle regularization, which empowers successfully scaling\nnon-hierarchical Vision Mamba to a large size (about 300M) in a supervised\nsetting. Specifically, our base and large-scale ShuffleMamba models can\noutperform the supervised ViTs of similar size by 0.8\\% and 1.0\\%\nclassification accuracy on ImageNet1k, respectively, without auxiliary data.\nWhen evaluated on the ADE20K semantic segmentation and COCO detection tasks,\nour ShuffleMamba models also show significant improvements. Without bells and\nwhistles, the stochastic layer-wise shuffle has the following highlights: (1)\n\\textit{Plug and play:} it does not change model architectures and will be\nomitted in inference. (2) \\textit{Simple but effective:} it can improve the\noverfitting in Vim training and only introduce random token permutation\noperations. (3) \\textit{Intuitive:} the token sequences in deeper layers are\nmore likely to be shuffled as they are expected to be more semantic and less\nsensitive to patch positions. Code and models will be available at\nhttps://github.com/huangzizheng01/ShuffleMamba.\n","authors":["Zizheng Huang","Haoxing Chen","Jiaqi Li","Jun Lan","Huijia Zhu","Weiqiang Wang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.17081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17073v1","updated":"2024-08-30T07:57:47Z","published":"2024-08-30T07:57:47Z","title":"Approximately Invertible Neural Network for Learned Image Compression","summary":"  Learned image compression have attracted considerable interests in recent\nyears. It typically comprises an analysis transform, a synthesis transform,\nquantization and an entropy coding model. The analysis transform and synthesis\ntransform are used to encode an image to latent feature and decode the\nquantized feature to reconstruct the image, and can be regarded as coupled\ntransforms. However, the analysis transform and synthesis transform are\ndesigned independently in the existing methods, making them unreliable in\nhigh-quality image compression. Inspired by the invertible neural networks in\ngenerative modeling, invertible modules are used to construct the coupled\nanalysis and synthesis transforms. Considering the noise introduced in the\nfeature quantization invalidates the invertible process, this paper proposes an\nApproximately Invertible Neural Network (A-INN) framework for learned image\ncompression. It formulates the rate-distortion optimization in lossy image\ncompression when using INN with quantization, which differentiates from using\nINN for generative modelling. Generally speaking, A-INN can be used as the\ntheoretical foundation for any INN based lossy compression method. Based on\nthis formulation, A-INN with a progressive denoising module (PDM) is developed\nto effectively reduce the quantization noise in the decoding. Moreover, a\nCascaded Feature Recovery Module (CFRM) is designed to learn high-dimensional\nfeature recovery from low-dimensional ones to further reduce the noise in\nfeature channel compression. In addition, a Frequency-enhanced Decomposition\nand Synthesis Module (FDSM) is developed by explicitly enhancing the\nhigh-frequency components in an image to address the loss of high-frequency\ninformation inherent in neural network based image compression. Extensive\nexperiments demonstrate that the proposed A-INN outperforms the existing\nlearned image compression methods.\n","authors":["Yanbo Gao","Meng Fu","Shuai Li","Chong Lv","Xun Cai","Hui Yuan","Mao Ye"],"pdf_url":"https://arxiv.org/pdf/2408.17073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17065v1","updated":"2024-08-30T07:49:57Z","published":"2024-08-30T07:49:57Z","title":"Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level\n  Blending and Spatiotemporal Adapter Tuning","summary":"  Three key challenges hinder the development of current deepfake video\ndetection: (1) Temporal features can be complex and diverse: how can we\nidentify general temporal artifacts to enhance model generalization? (2)\nSpatiotemporal models often lean heavily on one type of artifact and ignore the\nother: how can we ensure balanced learning from both? (3) Videos are naturally\nresource-intensive: how can we tackle efficiency without compromising accuracy?\n  This paper attempts to tackle the three challenges jointly. First, inspired\nby the notable generality of using image-level blending data for image forgery\ndetection, we investigate whether and how video-level blending can be effective\nin video. We then perform a thorough analysis and identify a previously\nunderexplored temporal forgery artifact: Facial Feature Drift (FFD), which\ncommonly exists across different forgeries. To reproduce FFD, we then propose a\nnovel Video-level Blending data (VB), where VB is implemented by blending the\noriginal image and its warped version frame-by-frame, serving as a hard\nnegative sample to mine more general artifacts. Second, we carefully design a\nlightweight Spatiotemporal Adapter (StA) to equip a pretrained image model\n(both ViTs and CNNs) with the ability to capture both spatial and temporal\nfeatures jointly and efficiently. StA is designed with two-stream 3D-Conv with\nvarying kernel sizes, allowing it to process spatial and temporal features\nseparately. Extensive experiments validate the effectiveness of the proposed\nmethods; and show our approach can generalize well to previously unseen forgery\nvideos, even the just-released (in 2024) SoTAs. We release our code and\npretrained weights at \\url{https://github.com/YZY-stack/StA4Deepfake}.\n","authors":["Zhiyuan Yan","Yandan Zhao","Shen Chen","Xinghe Fu","Taiping Yao","Shouhong Ding","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.17065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17064v1","updated":"2024-08-30T07:49:35Z","published":"2024-08-30T07:49:35Z","title":"Instant Adversarial Purification with Adversarial Consistency\n  Distillation","summary":"  Neural networks, despite their remarkable performance in widespread\napplications, including image classification, are also known to be vulnerable\nto subtle adversarial noise. Although some diffusion-based purification methods\nhave been proposed, for example, DiffPure, those methods are time-consuming. In\nthis paper, we propose One Step Control Purification (OSCP), a diffusion-based\npurification model that can purify the adversarial image in one Neural Function\nEvaluation (NFE) in diffusion models. We use Latent Consistency Model (LCM) and\nControlNet for our one-step purification. OSCP is computationally friendly and\ntime efficient compared to other diffusion-based purification methods; we\nachieve defense success rate of 74.19\\% on ImageNet, only requiring 0.1s for\neach purification. Moreover, there is a fundamental incongruence between\nconsistency distillation and adversarial perturbation. To address this\nontological dissonance, we propose Gaussian Adversarial Noise Distillation\n(GAND), a novel consistency distillation framework that facilitates a more\nnuanced reconciliation of the latent space dynamics, effectively bridging the\nnatural and adversarial manifolds. Our experiments show that the GAND does not\nneed a Full Fine Tune (FFT); PEFT, e.g., LoRA is sufficient.\n","authors":["Chun Tong Lei","Hon Ming Yam","Zhongliang Guo","Chun Pong Lau"],"pdf_url":"https://arxiv.org/pdf/2408.17064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17062v1","updated":"2024-08-30T07:48:05Z","published":"2024-08-30T07:48:05Z","title":"Vote&Mix: Plug-and-Play Token Reduction for Efficient Vision Transformer","summary":"  Despite the remarkable success of Vision Transformers (ViTs) in various\nvisual tasks, they are often hindered by substantial computational cost. In\nthis work, we introduce Vote\\&Mix (\\textbf{VoMix}), a plug-and-play and\nparameter-free token reduction method, which can be readily applied to\noff-the-shelf ViT models \\textit{without any training}. VoMix tackles the\ncomputational redundancy of ViTs by identifying tokens with high homogeneity\nthrough a layer-wise token similarity voting mechanism. Subsequently, the\nselected tokens are mixed into the retained set, thereby preserving visual\ninformation. Experiments demonstrate VoMix significantly improves the\nspeed-accuracy tradeoff of ViTs on both images and videos. Without any\ntraining, VoMix achieves a 2$\\times$ increase in throughput of existing ViT-H\non ImageNet-1K and a 2.4$\\times$ increase in throughput of existing ViT-L on\nKinetics-400 video dataset, with a mere 0.3\\% drop in top-1 accuracy.\n","authors":["Shuai Peng","Di Fu","Baole Wei","Yong Cao","Liangcai Gao","Zhi Tang"],"pdf_url":"https://arxiv.org/pdf/2408.17062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10306v3","updated":"2024-08-30T07:39:50Z","published":"2023-01-25T11:00:32Z","title":"Deep Convolutional Framelet Denoising for Panoramic by Mixed Wavelet\n  Integration","summary":"  Enhancing quality and removing noise during preprocessing is one of the most\ncritical steps in image processing. X-ray images are created by photons\ncolliding with atoms and the variation in scattered noise absorption. This\nnoise leads to a deterioration in the graph's medical quality and, at times,\nresults in repetition, thereby increasing the patient's effective dose. One of\nthe most critical challenges in this area has consistently been lowering the\nimage noise. Techniques like BM3d, low-pass filters, and Autoencoder have taken\nthis step. Owing to their structural design and high rate of repetition, neural\nnetworks employing diverse architectures have, over the past decade, achieved\nnoise reduction with satisfactory outcomes, surpassing the traditional BM3D and\nlow-pass filters. The combination of the Hankel matrix with neural networks\nrepresents one of these configurations. The Hankel matrix aims to identify a\nlocal circle by separating individual values into local and non-local\ncomponents, utilizing a non-local matrix. A non-local matrix can be created\nusing the wave or DCT. This paper suggests integrating the waveform with the\nDaubechies (D4) wavelet due to its higher energy concentration and employs the\nu-Net neural network architecture, which incorporates the waveform exclusively\nat each stage. The outcomes were evaluated using the PSNR and SSIM criteria,\nand the outcomes were verified by using various waves. The effectiveness of a\none-wave network has increased from 0.5% to 1.2%, according to studies done on\nother datasets\n","authors":["Masoud Shahraki Mohammadi","Seyed Javad Seyed Mahdavi Chabok"],"pdf_url":"https://arxiv.org/pdf/2302.10306v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17060v1","updated":"2024-08-30T07:38:46Z","published":"2024-08-30T07:38:46Z","title":"Efficient Image Restoration through Low-Rank Adaptation and Stable\n  Diffusion XL","summary":"  In this study, we propose an enhanced image restoration model, SUPIR, based\non the integration of two low-rank adaptive (LoRA) modules with the Stable\nDiffusion XL (SDXL) framework. Our method leverages the advantages of LoRA to\nfine-tune SDXL models, thereby significantly improving image restoration\nquality and efficiency. We collect 2600 high-quality real-world images, each\nwith detailed descriptive text, for training the model. The proposed method is\nevaluated on standard benchmarks and achieves excellent performance,\ndemonstrated by higher peak signal-to-noise ratio (PSNR), lower learned\nperceptual image patch similarity (LPIPS), and higher structural similarity\nindex measurement (SSIM) scores. These results underscore the effectiveness of\ncombining LoRA with SDXL for advanced image restoration tasks, highlighting the\npotential of our approach in generating high-fidelity restored images.\n","authors":["Haiyang Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.17060v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2408.17059v1","updated":"2024-08-30T07:38:28Z","published":"2024-08-30T07:38:28Z","title":"A Survey of the Self Supervised Learning Mechanisms for Vision\n  Transformers","summary":"  Deep supervised learning models require high volume of labeled data to attain\nsufficiently good results. Although, the practice of gathering and annotating\nsuch big data is costly and laborious. Recently, the application of self\nsupervised learning (SSL) in vision tasks has gained significant attention. The\nintuition behind SSL is to exploit the synchronous relationships within the\ndata as a form of self-supervision, which can be versatile. In the current big\ndata era, most of the data is unlabeled, and the success of SSL thus relies in\nfinding ways to improve this vast amount of unlabeled data available. Thus its\nbetter for deep learning algorithms to reduce reliance on human supervision and\ninstead focus on self-supervision based on the inherent relationships within\nthe data. With the advent of ViTs, which have achieved remarkable results in\ncomputer vision, it is crucial to explore and understand the various SSL\nmechanisms employed for training these models specifically in scenarios where\nthere is less label data available. In this survey we thus develop a\ncomprehensive taxonomy of systematically classifying the SSL techniques based\nupon their representations and pre-training tasks being applied. Additionally,\nwe discuss the motivations behind SSL, review popular pre-training tasks, and\nhighlight the challenges and advancements in this field. Furthermore, we\npresent a comparative analysis of different SSL methods, evaluate their\nstrengths and limitations, and identify potential avenues for future research.\n","authors":["Asifullah Khan","Anabia Sohail","Mustansar Fiaz","Mehdi Hassan","Tariq Habib Afridi","Sibghat Ullah Marwat","Farzeen Munir","Safdar Ali","Hannan Naseem","Muhammad Zaigham Zaheer","Kamran Ali","Tangina Sultana","Ziaurrehman Tanoli","Naeem Akhter"],"pdf_url":"https://arxiv.org/pdf/2408.17059v1.pdf","comment":"34 Pages, 5 Figures, 7 Tables"},{"id":"http://arxiv.org/abs/2308.09990v4","updated":"2024-08-30T07:32:50Z","published":"2023-08-19T11:40:57Z","title":"TSAR-MVS: Textureless-aware Segmentation and Correlative Refinement\n  Guided Multi-View Stereo","summary":"  The reconstruction of textureless areas has long been a challenging problem\nin MVS due to lack of reliable pixel correspondences between images. In this\npaper, we propose the Textureless-aware Segmentation And Correlative Refinement\nguided Multi-View Stereo (TSAR-MVS), a novel method that effectively tackles\nchallenges posed by textureless areas in 3D reconstruction through filtering,\nrefinement and segmentation. First, we implement the joint hypothesis\nfiltering, a technique that merges a confidence estimator with a disparity\ndiscontinuity detector to eliminate incorrect depth estimations. Second, to\nspread the pixels with confident depth, we introduce an iterative correlation\nrefinement strategy that leverages RANSAC to generate 3D planes based on\nsuperpixels, succeeded by a weighted median filter for broadening the influence\nof accurately determined pixels. Finally, we present a textureless-aware\nsegmentation method that leverages edge detection and line detection for\naccurately identify large textureless regions for further depth completion.\nExperiments on ETH3D, Tanks & Temples and Strecha datasets demonstrate the\nsuperior performance and strong generalization capability of our proposed\nmethod.\n","authors":["Zhenlong Yuan","Jiakai Cao","Zhaoqi Wang","Zhaoxin Li"],"pdf_url":"https://arxiv.org/pdf/2308.09990v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17057v1","updated":"2024-08-30T07:32:19Z","published":"2024-08-30T07:32:19Z","title":"LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality\n  Assessment Model","summary":"  Recent advancements in the field of No-Reference Image Quality Assessment\n(NR-IQA) using deep learning techniques demonstrate high performance across\nmultiple open-source datasets. However, such models are typically very large\nand complex making them not so suitable for real-world deployment, especially\non resource- and battery-constrained mobile devices. To address this\nlimitation, we propose a compact, lightweight NR-IQA model that achieves\nstate-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation\nand test datasets while being also nearly 5.7 times faster than the fastest\nSOTA model. Our model features a dual-branch architecture, with each branch\nseparately trained on synthetically and authentically distorted images which\nenhances the model's generalizability across different distortion types. To\nimprove robustness under diverse real-world visual conditions, we additionally\nincorporate multiple color spaces during the training process. We also\ndemonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks\n(KANs) for final quality regression as compared to the conventional Multi-Layer\nPerceptrons (MLPs). Our evaluation considering various open-source datasets\nhighlights the practical, high-accuracy, and robust performance of our proposed\nlightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA.\n","authors":["Nasim Jamshidi Avanaki","Abhijay Ghildiyal","Nabajeet Barman","Saman Zadtootaghaj"],"pdf_url":"https://arxiv.org/pdf/2408.17057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19323v2","updated":"2024-08-30T07:30:11Z","published":"2024-07-27T19:00:44Z","title":"MSP-MVS: Multi-granularity Segmentation Prior Guided Multi-View Stereo","summary":"  Reconstructing textureless areas in MVS poses challenges due to the absence\nof reliable pixel correspondences within fixed patch. Although certain methods\nemploy patch deformation to expand the receptive field, their patches\nmistakenly skip depth edges to calculate areas with depth discontinuity,\nthereby causing ambiguity. Consequently, we introduce Multi-granularity\nSegmentation Prior Multi-View Stereo (MSP-MVS). Specifically, we first propose\nmulti-granularity segmentation prior by integrating multi-granularity depth\nedges to restrict patch deformation within homogeneous areas. Moreover, we\npresent anchor equidistribution that bring deformed patches with more uniformly\ndistributed anchors to ensure an adequate coverage of their own homogeneous\nareas. Furthermore, we introduce iterative local search optimization to\nrepresent larger patch with sparse representative candidates, significantly\nboosting the expressive capacity for each patch. The state-of-the-art results\non ETH3D and Tanks & Temples benchmarks demonstrate the effectiveness and\nrobust generalization ability of our proposed method.\n","authors":["Zhenlong Yuan","Cong Liu","Fei Shen","Zhaoxin Li","Tianlu Mao","Zhaoqi Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19323v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2308.09990"},{"id":"http://arxiv.org/abs/2408.17054v1","updated":"2024-08-30T07:25:53Z","published":"2024-08-30T07:25:53Z","title":"BTMuda: A Bi-level Multi-source unsupervised domain adaptation framework\n  for breast cancer diagnosis","summary":"  Deep learning has revolutionized the early detection of breast cancer,\nresulting in a significant decrease in mortality rates. However, difficulties\nin obtaining annotations and huge variations in distribution between training\nsets and real scenes have limited their clinical applications. To address these\nlimitations, unsupervised domain adaptation (UDA) methods have been used to\ntransfer knowledge from one labeled source domain to the unlabeled target\ndomain, yet these approaches suffer from severe domain shift issues and often\nignore the potential benefits of leveraging multiple relevant sources in\npractical applications. To address these limitations, in this work, we\nconstruct a Three-Branch Mixed extractor and propose a Bi-level Multi-source\nunsupervised domain adaptation method called BTMuda for breast cancer\ndiagnosis. Our method addresses the problems of domain shift by dividing domain\nshift issues into two levels: intra-domain and inter-domain. To reduce the\nintra-domain shift, we jointly train a CNN and a Transformer as two paths of a\ndomain mixed feature extractor to obtain robust representations rich in both\nlow-level local and high-level global information. As for the inter-domain\nshift, we redesign the Transformer delicately to a three-branch architecture\nwith cross-attention and distillation, which learns domain-invariant\nrepresentations from multiple domains. Besides, we introduce two alignment\nmodules - one for feature alignment and one for classifier alignment - to\nimprove the alignment process. Extensive experiments conducted on three public\nmammographic datasets demonstrate that our BTMuda outperforms state-of-the-art\nmethods.\n","authors":["Yuxiang Yang","Xinyi Zeng","Pinxian Zeng","Binyu Yan","Xi Wu","Jiliu Zhou","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.17054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17052v1","updated":"2024-08-30T07:22:11Z","published":"2024-08-30T07:22:11Z","title":"Can We Leave Deepfake Data Behind in Training Deepfake Detector?","summary":"  The generalization ability of deepfake detectors is vital for their\napplications in real-world scenarios. One effective solution to enhance this\nability is to train the models with manually-blended data, which we termed\n\"blendfake\", encouraging models to learn generic forgery artifacts like\nblending boundary. Interestingly, current SoTA methods utilize blendfake\nwithout incorporating any deepfake data in their training process. This is\nlikely because previous empirical observations suggest that vanilla hybrid\ntraining (VHT), which combines deepfake and blendfake data, results in inferior\nperformance to methods using only blendfake data (so-called \"1+1<2\").\nTherefore, a critical question arises: Can we leave deepfake behind and rely\nsolely on blendfake data to train an effective deepfake detector? Intuitively,\nas deepfakes also contain additional informative forgery clues (e.g., deep\ngenerative artifacts), excluding all deepfake data in training deepfake\ndetectors seems counter-intuitive. In this paper, we rethink the role of\nblendfake in detecting deepfakes and formulate the process from \"real to\nblendfake to deepfake\" to be a progressive transition. Specifically, blendfake\nand deepfake can be explicitly delineated as the oriented pivot anchors between\n\"real-to-fake\" transitions. The accumulation of forgery information should be\noriented and progressively increasing during this transition process. To this\nend, we propose an Oriented Progressive Regularizor (OPR) to establish the\nconstraints that compel the distribution of anchors to be discretely arranged.\nFurthermore, we introduce feature bridging to facilitate the smooth transition\nbetween adjacent anchors. Extensive experiments confirm that our design allows\nleveraging forgery information from both blendfake and deepfake effectively and\ncomprehensively.\n","authors":["Jikang Cheng","Zhiyuan Yan","Ying Zhang","Yuhao Luo","Zhongyuan Wang","Chen Li"],"pdf_url":"https://arxiv.org/pdf/2408.17052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17046v1","updated":"2024-08-30T07:08:01Z","published":"2024-08-30T07:08:01Z","title":"Text-to-Image Generation Via Energy-Based CLIP","summary":"  Joint Energy Models (JEMs), while drawing significant research attention,\nhave not been successfully scaled to real-world, high-resolution datasets. We\npresent EB-CLIP, a novel approach extending JEMs to the multimodal\nvision-language domain using CLIP, integrating both generative and\ndiscriminative objectives. For the generative objective, we introduce an\nimage-text joint-energy function based on Cosine similarity in the CLIP space,\ntraining CLIP to assign low energy to real image-caption pairs and high energy\notherwise. For the discriminative objective, we employ contrastive adversarial\nloss, extending the adversarial training objective to the multimodal domain.\nEB-CLIP not only generates realistic images from text but also achieves\ncompetitive results on the compositionality benchmark, outperforming leading\nmethods with fewer parameters. Additionally, we demonstrate the superior\nguidance capability of EB-CLIP by enhancing CLIP-based generative frameworks\nand converting unconditional diffusion models to text-based ones. Lastly, we\nshow that EB-CLIP can serve as a more robust evaluation metric for\ntext-to-image generative tasks than CLIP.\n","authors":["Roy Ganz","Michael Elad"],"pdf_url":"https://arxiv.org/pdf/2408.17046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17036v1","updated":"2024-08-30T06:13:49Z","published":"2024-08-30T06:13:49Z","title":"CP-VoteNet: Contrastive Prototypical VoteNet for Few-Shot Point Cloud\n  Object Detection","summary":"  Few-shot point cloud 3D object detection (FS3D) aims to identify and localise\nobjects of novel classes from point clouds, using knowledge learnt from\nannotated base classes and novel classes with very few annotations. Thus far,\nthis challenging task has been approached using prototype learning, but the\nperformance remains far from satisfactory. We find that in existing methods,\nthe prototypes are only loosely constrained and lack of fine-grained awareness\nof the semantic and geometrical correlation embedded within the point cloud\nspace. To mitigate these issues, we propose to leverage the inherent\ncontrastive relationship within the semantic and geometrical subspaces to learn\nmore refined and generalisable prototypical representations. To this end, we\nfirst introduce contrastive semantics mining, which enables the network to\nextract discriminative categorical features by constructing positive and\nnegative pairs within training batches. Meanwhile, since point features\nrepresenting local patterns can be clustered into geometric components, we\nfurther propose to impose contrastive relationship at the primitive level.\nThrough refined primitive geometric structures, the transferability of feature\nencoding from base to novel classes is significantly enhanced. The above\ndesigns and insights lead to our novel Contrastive Prototypical VoteNet\n(CP-VoteNet). Extensive experiments on two FS3D benchmarks FS-ScanNet and\nFS-SUNRGBD demonstrate that CP-VoteNet surpasses current state-of-the-art\nmethods by considerable margins across different FS3D settings. Further\nablation studies conducted corroborate the rationale and effectiveness of our\ndesigns.\n","authors":["Xuejing Li","Weijia Zhang","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2408.17036v1.pdf","comment":"Accepted by PRCV 2024"},{"id":"http://arxiv.org/abs/2408.17027v1","updated":"2024-08-30T05:57:01Z","published":"2024-08-30T05:57:01Z","title":"ConDense: Consistent 2D/3D Pre-training for Dense and Sparse Features\n  from Multi-View Images","summary":"  To advance the state of the art in the creation of 3D foundation models, this\npaper introduces the ConDense framework for 3D pre-training utilizing existing\npre-trained 2D networks and large-scale multi-view datasets. We propose a novel\n2D-3D joint training scheme to extract co-embedded 2D and 3D features in an\nend-to-end pipeline, where 2D-3D feature consistency is enforced through a\nvolume rendering NeRF-like ray marching process. Using dense per pixel features\nwe are able to 1) directly distill the learned priors from 2D models to 3D\nmodels and create useful 3D backbones, 2) extract more consistent and less\nnoisy 2D features, 3) formulate a consistent embedding space where 2D, 3D, and\nother modalities of data (e.g., natural language prompts) can be jointly\nqueried. Furthermore, besides dense features, ConDense can be trained to\nextract sparse features (e.g., key points), also with 2D-3D consistency --\ncondensing 3D NeRF representations into compact sets of decorated key points.\nWe demonstrate that our pre-trained model provides good initialization for\nvarious 3D tasks including 3D classification and segmentation, outperforming\nother 3D pre-training methods by a significant margin. It also enables, by\nexploiting our sparse features, additional useful downstream tasks, such as\nmatching 2D images to 3D scenes, detecting duplicate 3D scenes, and querying a\nrepository of 3D scenes through natural language -- all quite efficiently and\nwithout any per-scene fine-tuning.\n","authors":["Xiaoshuai Zhang","Zhicheng Wang","Howard Zhou","Soham Ghosh","Danushen Gnanapragasam","Varun Jampani","Hao Su","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2408.17027v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2310.13019v4","updated":"2024-08-30T05:50:56Z","published":"2023-10-18T18:50:39Z","title":"Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class\n  Manipulation Using DeepFool Algorithm","summary":"  The susceptibility of deep neural networks (DNNs) to adversarial attacks\nundermines their reliability across numerous applications, underscoring the\nnecessity for an in-depth exploration of these vulnerabilities and the\nformulation of robust defense strategies. The DeepFool algorithm by\nMoosavi-Dezfooli et al. (2016) represents a pivotal step in identifying minimal\nperturbations required to induce misclassification of input images.\nNonetheless, its generic methodology falls short in scenarios necessitating\ntargeted interventions. Additionally, previous research studies have\npredominantly concentrated on the success rate of attacks without adequately\naddressing the consequential distortion of images, the maintenance of image\nquality, or the confidence threshold required for misclassification. To bridge\nthese gaps, we introduce the Enhanced Targeted DeepFool (ET DeepFool)\nalgorithm, an evolution of DeepFool that not only facilitates the specification\nof desired misclassification targets but also incorporates a configurable\nminimum confidence score. Our empirical investigations demonstrate the\nsuperiority of this refined approach in maintaining the integrity of images and\nminimizing perturbations across a variety of DNN architectures. Unlike previous\niterations, such as the Targeted DeepFool by Gajjar et al. (2022), our method\ngrants unparalleled control over the perturbation process, enabling precise\nmanipulation of model responses. Preliminary outcomes reveal that certain\nmodels, including AlexNet and the advanced Vision Transformer, display\ncommendable robustness to such manipulations. This discovery of varying levels\nof model robustness, as unveiled through our confidence level adjustments,\ncould have far-reaching implications for the field of image recognition. Our\ncode will be made public upon acceptance of the paper.\n","authors":["S. M. Fazle Rabby Labib","Joyanta Jyoti Mondal","Meem Arafat Manab","Sarfaraz Newaz","Xi Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.13019v4.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.14628v2","updated":"2024-08-30T05:34:25Z","published":"2024-03-21T17:59:59Z","title":"Zero-Shot Multi-Object Scene Completion","summary":"  We present a 3D scene completion method that recovers the complete geometry\nof multiple unseen objects in complex scenes from a single RGB-D image. Despite\nnotable advancements in single-object 3D shape completion, high-quality\nreconstructions in highly cluttered real-world multi-object scenes remains a\nchallenge. To address this issue, we propose OctMAE, an architecture that\nleverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near\nreal-time multi-object scene completion through both local and global geometric\nreasoning. Because a naive 3D MAE can be computationally intractable and memory\nintensive even in the latent space, we introduce a novel occlusion masking\nstrategy and adopt 3D rotary embeddings, which significantly improves the\nruntime and scene completion quality. To generalize to a wide range of objects\nin diverse scenes, we create a large-scale photorealistic dataset, featuring a\ndiverse set of 12K 3D object models from the Objaverse dataset which are\nrendered in multi-object scenes with physics-based positioning. Our method\noutperforms the current state-of-the-art on both synthetic and real-world\ndatasets and demonstrates a strong zero-shot capability.\n","authors":["Shun Iwase","Katherine Liu","Vitor Guizilini","Adrien Gaidon","Kris Kitani","Rares Ambrus","Sergey Zakharov"],"pdf_url":"https://arxiv.org/pdf/2403.14628v2.pdf","comment":"Published at ECCV 2024, Webpage: https://sh8.io/#/oct_mae"},{"id":"http://arxiv.org/abs/2212.10537v3","updated":"2024-08-30T04:51:28Z","published":"2022-12-20T18:46:28Z","title":"Does CLIP Bind Concepts? Probing Compositionality in Large Image Models","summary":"  Large-scale neural network models combining text and images have made\nincredible progress in recent years. However, it remains an open question to\nwhat extent such models encode compositional representations of the concepts\nover which they operate, such as correctly identifying \"red cube\" by reasoning\nover the constituents \"red\" and \"cube\". In this work, we focus on the ability\nof a large pretrained vision and language model (CLIP) to encode compositional\nconcepts and to bind variables in a structure-sensitive way (e.g.,\ndifferentiating \"cube behind sphere\" from \"sphere behind cube\"). To inspect the\nperformance of CLIP, we compare several architectures from research on\ncompositional distributional semantics models (CDSMs), a line of research that\nattempts to implement traditional compositional linguistic structures within\nembedding spaces. We benchmark them on three synthetic datasets -\nsingle-object, two-object, and relational - designed to test concept binding.\nWe find that CLIP can compose concepts in a single-object setting, but in\nsituations where concept binding is needed, performance drops dramatically. At\nthe same time, CDSMs also perform poorly, with best performance at chance\nlevel.\n","authors":["Martha Lewis","Nihal V. Nayak","Peilin Yu","Qinan Yu","Jack Merullo","Stephen H. Bach","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2212.10537v3.pdf","comment":"Lewis and Nayak contributed equally"},{"id":"http://arxiv.org/abs/2408.17011v1","updated":"2024-08-30T04:51:19Z","published":"2024-08-30T04:51:19Z","title":"Disease Classification and Impact of Pretrained Deep Convolution Neural\n  Networks on Diverse Medical Imaging Datasets across Imaging Modalities","summary":"  Imaging techniques such as Chest X-rays, whole slide images, and optical\ncoherence tomography serve as the initial screening and detection for a wide\nvariety of medical pulmonary and ophthalmic conditions respectively. This paper\ninvestigates the intricacies of using pretrained deep convolutional neural\nnetworks with transfer learning across diverse medical imaging datasets with\nvarying modalities for binary and multiclass classification. We conducted a\ncomprehensive performance analysis with ten network architectures and model\nfamilies each with pretraining and random initialization. Our finding showed\nthat the use of pretrained models as fixed feature extractors yields poor\nperformance irrespective of the datasets. Contrary, histopathology microscopy\nwhole slide images have better performance. It is also found that deeper and\nmore complex architectures did not necessarily result in the best performance.\nThis observation implies that the improvements in ImageNet are not parallel to\nthe medical imaging tasks. Within a medical domain, the performance of the\nnetwork architectures varies within model families with shifts in datasets.\nThis indicates that the performance of models within a specific modality may\nnot be conclusive for another modality within the same domain. This study\nprovides a deeper understanding of the applications of deep learning techniques\nin medical imaging and highlights the impact of pretrained networks across\ndifferent medical imaging datasets under five different experimental settings.\n","authors":["Jutika Borah","Kumaresh Sarmah","Hidam Kumarjit Singh"],"pdf_url":"https://arxiv.org/pdf/2408.17011v1.pdf","comment":"15 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.17006v1","updated":"2024-08-30T04:39:43Z","published":"2024-08-30T04:39:43Z","title":"Retrieval-Augmented Natural Language Reasoning for Explainable Visual\n  Question Answering","summary":"  Visual Question Answering with Natural Language Explanation (VQA-NLE) task is\nchallenging due to its high demand for reasoning-based inference. Recent\nVQA-NLE studies focus on enhancing model networks to amplify the model's\nreasoning capability but this approach is resource-consuming and unstable. In\nthis work, we introduce a new VQA-NLE model, ReRe (Retrieval-augmented natural\nlanguage Reasoning), using leverage retrieval information from the memory to\naid in generating accurate answers and persuasive explanations without relying\non complex networks and extra datasets. ReRe is an encoder-decoder architecture\nmodel using a pre-trained clip vision encoder and a pre-trained GPT-2 language\nmodel as a decoder. Cross-attention layers are added in the GPT-2 for\nprocessing retrieval features. ReRe outperforms previous methods in VQA\naccuracy and explanation score and shows improvement in NLE with more\npersuasive, reliability.\n","authors":["Su Hyeon Lim","Minkuk Kim","Hyeon Bae Kim","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2408.17006v1.pdf","comment":"ICIP Workshop 2024"},{"id":"http://arxiv.org/abs/2408.17005v1","updated":"2024-08-30T04:37:52Z","published":"2024-08-30T04:37:52Z","title":"Efficient Camera Exposure Control for Visual Odometry via Deep\n  Reinforcement Learning","summary":"  The stability of visual odometry (VO) systems is undermined by degraded image\nquality, especially in environments with significant illumination changes. This\nstudy employs a deep reinforcement learning (DRL) framework to train agents for\nexposure control, aiming to enhance imaging performance in challenging\nconditions. A lightweight image simulator is developed to facilitate the\ntraining process, enabling the diversification of image exposure and sequence\ntrajectory. This setup enables completely offline training, eliminating the\nneed for direct interaction with camera hardware and the real environments.\nDifferent levels of reward functions are crafted to enhance the VO systems,\nequipping the DRL agents with varying intelligence. Extensive experiments have\nshown that our exposure control agents achieve superior efficiency-with an\naverage inference duration of 1.58 ms per frame on a CPU-and respond more\nquickly than traditional feedback control schemes. By choosing an appropriate\nreward function, agents acquire an intelligent understanding of motion trends\nand anticipate future illumination changes. This predictive capability allows\nVO systems to deliver more stable and precise odometry results. The codes and\ndatasets are available at https://github.com/ShuyangUni/drl_exposure_ctrl.\n","authors":["Shuyang Zhang","Jinhao He","Yilong Zhu","Jin Wu","Jie Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.17005v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.09826v2","updated":"2024-08-30T03:58:50Z","published":"2024-07-13T09:39:11Z","title":"3D Weakly Supervised Semantic Segmentation with 2D Vision-Language\n  Guidance","summary":"  In this paper, we propose 3DSS-VLG, a weakly supervised approach for 3D\nSemantic Segmentation with 2D Vision-Language Guidance, an alternative approach\nthat a 3D model predicts dense-embedding for each point which is co-embedded\nwith both the aligned image and text spaces from the 2D vision-language model.\nSpecifically, our method exploits the superior generalization ability of the 2D\nvision-language models and proposes the Embeddings Soft-Guidance Stage to\nutilize it to implicitly align 3D embeddings and text embeddings. Moreover, we\nintroduce the Embeddings Specialization Stage to purify the feature\nrepresentation with the help of a given scene-level label, specifying a better\nfeature supervised by the corresponding text embedding. Thus, the 3D model is\nable to gain informative supervisions both from the image embedding and text\nembedding, leading to competitive segmentation performances. To the best of our\nknowledge, this is the first work to investigate 3D weakly supervised semantic\nsegmentation by using the textual semantic information of text category labels.\nMoreover, with extensive quantitative and qualitative experiments, we present\nthat our 3DSS-VLG is able not only to achieve the state-of-the-art performance\non both S3DIS and ScanNet datasets, but also to maintain strong generalization\ncapability.\n","authors":["Xiaoxu Xu","Yitian Yuan","Jinlong Li","Qiudan Zhang","Zequn Jie","Lin Ma","Hao Tang","Nicu Sebe","Xu Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09826v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10571v2","updated":"2024-08-30T03:48:40Z","published":"2024-08-20T06:17:56Z","title":"Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models","summary":"  Diffusion models have revolutionized customized text-to-image generation,\nallowing for efficient synthesis of photos from personal data with textual\ndescriptions. However, these advancements bring forth risks including privacy\nbreaches and unauthorized replication of artworks. Previous researches\nprimarily center around using prompt-specific methods to generate adversarial\nexamples to protect personal images, yet the effectiveness of existing methods\nis hindered by constrained adaptability to different prompts. In this paper, we\nintroduce a Prompt-Agnostic Adversarial Perturbation (PAP) method for\ncustomized diffusion models. PAP first models the prompt distribution using a\nLaplace Approximation, and then produces prompt-agnostic perturbations by\nmaximizing a disturbance expectation based on the modeled distribution. This\napproach effectively tackles the prompt-agnostic attacks, leading to improved\ndefense stability. Extensive experiments in face privacy and artistic style\nprotection, demonstrate the superior generalization of our method in comparison\nto existing techniques.\n","authors":["Cong Wan","Yuhang He","Xiang Song","Yihong Gong"],"pdf_url":"https://arxiv.org/pdf/2408.10571v2.pdf","comment":"The experiments are insufficient and need to be completed"},{"id":"http://arxiv.org/abs/2406.06978v4","updated":"2024-08-30T03:37:36Z","published":"2024-06-11T06:18:26Z","title":"Hydra-MDP: End-to-end Multimodal Planning with Multi-target\n  Hydra-Distillation","summary":"  We propose Hydra-MDP, a novel paradigm employing multiple teachers in a\nteacher-student model. This approach uses knowledge distillation from both\nhuman and rule-based teachers to train the student model, which features a\nmulti-head decoder to learn diverse trajectory candidates tailored to various\nevaluation metrics. With the knowledge of rule-based teachers, Hydra-MDP learns\nhow the environment influences the planning in an end-to-end manner instead of\nresorting to non-differentiable post-processing. This method achieves the\n$1^{st}$ place in the Navsim challenge, demonstrating significant improvements\nin generalization across diverse driving environments and conditions. More\ndetails by visiting \\url{https://github.com/NVlabs/Hydra-MDP}.\n","authors":["Zhenxin Li","Kailin Li","Shihao Wang","Shiyi Lan","Zhiding Yu","Yishen Ji","Zhiqi Li","Ziyue Zhu","Jan Kautz","Zuxuan Wu","Yu-Gang Jiang","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2406.06978v4.pdf","comment":"The 1st place solution of End-to-end Driving at Scale at the CVPR\n  2024 Autonomous Grand Challenge"},{"id":"http://arxiv.org/abs/2408.16986v1","updated":"2024-08-30T03:16:49Z","published":"2024-08-30T03:16:49Z","title":"AdaptVision: Dynamic Input Scaling in MLLMs for Versatile Scene\n  Understanding","summary":"  Over the past few years, the advancement of Multimodal Large Language Models\n(MLLMs) has captured the wide interest of researchers, leading to numerous\ninnovations to enhance MLLMs' comprehension. In this paper, we present\nAdaptVision, a multimodal large language model specifically designed to\ndynamically process input images at varying resolutions. We hypothesize that\nthe requisite number of visual tokens for the model is contingent upon both the\nresolution and content of the input image. Generally, natural images with a\nlower information density can be effectively interpreted by the model using\nfewer visual tokens at reduced resolutions. In contrast, images containing\ntextual content, such as documents with rich text, necessitate a higher number\nof visual tokens for accurate text interpretation due to their higher\ninformation density. Building on this insight, we devise a dynamic image\npartitioning module that adjusts the number of visual tokens according to the\nsize and aspect ratio of images. This method mitigates distortion effects that\narise from resizing images to a uniform resolution and dynamically optimizing\nthe visual tokens input to the LLMs. Our model is capable of processing images\nwith resolutions up to $1008\\times 1008$. Extensive experiments across various\ndatasets demonstrate that our method achieves impressive performance in\nhandling vision-language tasks in both natural and text-related scenes. The\nsource code and dataset are now publicly available at\n\\url{https://github.com/harrytea/AdaptVision}.\n","authors":["Yonghui Wang","Wengang Zhou","Hao Feng","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2408.16986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15026v3","updated":"2024-08-30T03:10:59Z","published":"2024-03-22T08:16:59Z","title":"VRSO: Visual-Centric Reconstruction for Static Object Annotation","summary":"  As a part of the perception results of intelligent driving systems, static\nobject detection (SOD) in 3D space provides crucial cues for driving\nenvironment understanding. With the rapid deployment of deep neural networks\nfor SOD tasks, the demand for high-quality training samples soars. The\ntraditional, also reliable, way is manual labelling over the dense LiDAR point\nclouds and reference images. Though most public driving datasets adopt this\nstrategy to provide SOD ground truth (GT), it is still expensive and\ntime-consuming in practice. This paper introduces VRSO, a visual-centric\napproach for static object annotation. Experiments on the Waymo Open Dataset\nshow that the mean reprojection error from VRSO annotation is only 2.6 pixels,\naround four times lower than the Waymo Open Dataset labels (10.6 pixels). VRSO\nis distinguished in low cost, high efficiency, and high quality: (1) It\nrecovers static objects in 3D space with only camera images as input, and (2)\nmanual annotation is barely involved since GT for SOD tasks is generated based\non an automatic reconstruction and annotation pipeline.\n","authors":["Chenyao Yu","Yingfeng Cai","Jiaxin Zhang","Hui Kong","Wei Sui","Cong Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15026v3.pdf","comment":"Accepted at 2024 IEEE International Conference on Intelligent Robots\n  and Systems (IROS)"},{"id":"http://arxiv.org/abs/2309.01159v2","updated":"2024-08-30T03:08:42Z","published":"2023-09-03T12:37:59Z","title":"An Asynchronous Linear Filter Architecture for Hybrid Event-Frame\n  Cameras","summary":"  Event cameras are ideally suited to capture High Dynamic Range (HDR) visual\ninformation without blur but provide poor imaging capability for static or\nslowly varying scenes. Conversely, conventional image sensors measure absolute\nintensity of slowly changing scenes effectively but do poorly on HDR or quickly\nchanging scenes. In this paper, we present an asynchronous linear filter\narchitecture, fusing event and frame camera data, for HDR video reconstruction\nand spatial convolution that exploits the advantages of both sensor modalities.\nThe key idea is the introduction of a state that directly encodes the\nintegrated or convolved image information and that is updated asynchronously as\neach event or each frame arrives from the camera. The state can be read-off\nas-often-as and whenever required to feed into subsequent vision modules for\nreal-time robotic systems. Our experimental results are evaluated on both\npublicly available datasets with challenging lighting conditions and fast\nmotions, along with a new dataset with HDR reference that we provide. The\nproposed AKF pipeline outperforms other state-of-the-art methods in both\nabsolute intensity error (69.4% reduction) and image similarity indexes\n(average 35.5% improvement). We also demonstrate the integration of image\nconvolution with linear spatial kernels Gaussian, Sobel, and Laplacian as an\napplication of our architecture.\n","authors":["Ziwei Wang","Yonhon Ng","Cedric Scheerlinck","Robert Mahony"],"pdf_url":"https://arxiv.org/pdf/2309.01159v2.pdf","comment":"17 pages, 10 figures. Date of Publication: 04 September 2023"},{"id":"http://arxiv.org/abs/2408.16982v1","updated":"2024-08-30T03:04:11Z","published":"2024-08-30T03:04:11Z","title":"2DGH: 2D Gaussian-Hermite Splatting for High-quality Rendering and\n  Better Geometry Reconstruction","summary":"  2D Gaussian Splatting has recently emerged as a significant method in 3D\nreconstruction, enabling novel view synthesis and geometry reconstruction\nsimultaneously. While the well-known Gaussian kernel is broadly used, its lack\nof anisotropy and deformation ability leads to dim and vague edges at object\nsilhouettes, limiting the reconstruction quality of current Gaussian splatting\nmethods. To enhance the representation power, we draw inspiration from quantum\nphysics and propose to use the Gaussian-Hermite kernel as the new primitive in\nGaussian splatting. The new kernel takes a unified mathematical form and\nextends the Gaussian function, which serves as the zero-rank term in the\nupdated formulation. Our experiments demonstrate the extraordinary performance\nof Gaussian-Hermite kernel in both geometry reconstruction and novel-view\nsynthesis tasks. The proposed kernel outperforms traditional Gaussian Splatting\nkernels, showcasing its potential for high-quality 3D reconstruction and\nrendering.\n","authors":["Ruihan Yu","Tianyu Huang","Jingwang Ling","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2408.16982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16224v2","updated":"2024-08-30T02:49:40Z","published":"2024-08-29T02:43:20Z","title":"LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in\n  Vision-Language Models","summary":"  Recent advances in large vision-language models (VLMs) typically employ\nvision encoders based on the Vision Transformer (ViT) architecture. The\ndivision of the images into patches by ViT results in a fragmented perception,\nthereby hindering the visual understanding capabilities of VLMs. In this paper,\nwe propose an innovative enhancement to address this limitation by introducing\na Scene Graph Expression (SGE) module in VLMs. This module extracts and\nstructurally expresses the complex semantic information within images, thereby\nimproving the foundational perception and understanding abilities of VLMs.\nExtensive experiments demonstrate that integrating our SGE module significantly\nenhances the VLM's performance in vision-language tasks, indicating its\neffectiveness in preserving intricate semantic details and facilitating better\nvisual understanding.\n","authors":["Jingyi Wang","Jianzhong Ju","Jian Luan","Zhidong Deng"],"pdf_url":"https://arxiv.org/pdf/2408.16224v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16979v1","updated":"2024-08-30T02:45:56Z","published":"2024-08-30T02:45:56Z","title":"Cross Fusion RGB-T Tracking with Bi-directional Adapter","summary":"  Many state-of-the-art RGB-T trackers have achieved remarkable results through\nmodality fusion. However, these trackers often either overlook temporal\ninformation or fail to fully utilize it, resulting in an ineffective balance\nbetween multi-modal and temporal information. To address this issue, we propose\na novel Cross Fusion RGB-T Tracking architecture (CFBT) that ensures the full\nparticipation of multiple modalities in tracking while dynamically fusing\ntemporal information. The effectiveness of CFBT relies on three newly designed\ncross spatio-temporal information fusion modules: Cross Spatio-Temporal\nAugmentation Fusion (CSTAF), Cross Spatio-Temporal Complementarity Fusion\n(CSTCF), and Dual-Stream Spatio-Temporal Adapter (DSTA). CSTAF employs a\ncross-attention mechanism to enhance the feature representation of the template\ncomprehensively. CSTCF utilizes complementary information between different\nbranches to enhance target features and suppress background features. DSTA\nadopts the adapter concept to adaptively fuse complementary information from\nmultiple branches within the transformer layer, using the RGB modality as a\nmedium. These ingenious fusions of multiple perspectives introduce only less\nthan 0.3\\% of the total modal parameters, but they indeed enable an efficient\nbalance between multi-modal and temporal information. Extensive experiments on\nthree popular RGB-T tracking benchmarks demonstrate that our method achieves\nnew state-of-the-art performance.\n","authors":["Zhirong Zeng","Xiaotao Liu","Meng Sun","Hongyu Wang","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2408.16979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16757v2","updated":"2024-08-30T02:26:01Z","published":"2024-08-29T17:55:07Z","title":"Dissecting Out-of-Distribution Detection and Open-Set Recognition: A\n  Critical Analysis of Methods and Benchmarks","summary":"  Detecting test-time distribution shift has emerged as a key capability for\nsafely deployed machine learning models, with the question being tackled under\nvarious guises in recent years. In this paper, we aim to provide a consolidated\nview of the two largest sub-fields within the community: out-of-distribution\n(OOD) detection and open-set recognition (OSR). In particular, we aim to\nprovide rigorous empirical analysis of different methods across settings and\nprovide actionable takeaways for practitioners and researchers. Concretely, we\nmake the following contributions: (i) We perform rigorous cross-evaluation\nbetween state-of-the-art methods in the OOD detection and OSR settings and\nidentify a strong correlation between the performances of methods for them;\n(ii) We propose a new, large-scale benchmark setting which we suggest better\ndisentangles the problem tackled by OOD detection and OSR, re-evaluating\nstate-of-the-art OOD detection and OSR methods in this setting; (iii) We\nsurprisingly find that the best performing method on standard benchmarks\n(Outlier Exposure) struggles when tested at scale, while scoring rules which\nare sensitive to the deep feature magnitude consistently show promise; and (iv)\nWe conduct empirical analysis to explain these phenomena and highlight\ndirections for future research. Code:\nhttps://github.com/Visual-AI/Dissect-OOD-OSR\n","authors":["Hongjun Wang","Sagar Vaze","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2408.16757v2.pdf","comment":"Accepted to IJCV, preprint version; v2: add supplementary"},{"id":"http://arxiv.org/abs/2408.16971v1","updated":"2024-08-30T02:14:33Z","published":"2024-08-30T02:14:33Z","title":"Synthetic Lunar Terrain: A Multimodal Open Dataset for Training and\n  Evaluating Neuromorphic Vision Algorithms","summary":"  Synthetic Lunar Terrain (SLT) is an open dataset collected from an analogue\ntest site for lunar missions, featuring synthetic craters in a high-contrast\nlighting setup. It includes several side-by-side captures from event-based and\nconventional RGB cameras, supplemented with a high-resolution 3D laser scan for\ndepth estimation. The event-stream recorded from the neuromorphic vision sensor\nof the event-based camera is of particular interest as this emerging technology\nprovides several unique advantages, such as high data rates, low energy\nconsumption and resilience towards scenes of high dynamic range. SLT provides a\nsolid foundation to analyse the limits of RGB-cameras and potential advantages\nor synergies in utilizing neuromorphic visions with the goal of enabling and\nimproving lunar specific applications like rover navigation, landing in\ncratered environments or similar.\n","authors":["Marcus MÃ¤rtens","Kevin Farries","John Culton","Tat-Jun Chin"],"pdf_url":"https://arxiv.org/pdf/2408.16971v1.pdf","comment":"7 pages, 5 figures, to be published at \"International Symposium on\n  Artificial Intelligence, Robotics and Automation in Space, i-SAIRAS, 2024"},{"id":"http://arxiv.org/abs/2406.00971v2","updated":"2024-08-30T01:50:37Z","published":"2024-06-03T03:59:29Z","title":"MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing\n  MiniGPT-4","summary":"  Vision-Language Models (VLMs) have recently seen significant advancements\nthrough integrating with Large Language Models (LLMs). The VLMs, which process\nimage and text modalities simultaneously, have demonstrated the ability to\nlearn and understand the interaction between images and texts across various\nmulti-modal tasks. Reverse designing, which could be defined as a complex\nvision-language task, aims to predict the edits and their parameters, given a\nsource image, an edited version, and an optional high-level textual edit\ndescription. This task requires VLMs to comprehend the interplay between the\nsource image, the edited version, and the optional textual context\nsimultaneously, going beyond traditional vision-language tasks. In this paper,\nwe extend and fine-tune MiniGPT-4 for the reverse designing task. Our\nexperiments demonstrate the extensibility of off-the-shelf VLMs, specifically\nMiniGPT-4, for more complex tasks such as reverse designing. Code is available\nat this \\href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}\n","authors":["Vahid Azizi","Fatemeh Koochaki"],"pdf_url":"https://arxiv.org/pdf/2406.00971v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.16965v1","updated":"2024-08-30T01:47:43Z","published":"2024-08-30T01:47:43Z","title":"Contrastive Learning with Synthetic Positives","summary":"  Contrastive learning with the nearest neighbor has proved to be one of the\nmost efficient self-supervised learning (SSL) techniques by utilizing the\nsimilarity of multiple instances within the same class. However, its efficacy\nis constrained as the nearest neighbor algorithm primarily identifies ``easy''\npositive pairs, where the representations are already closely located in the\nembedding space. In this paper, we introduce a novel approach called\nContrastive Learning with Synthetic Positives (CLSP) that utilizes synthetic\nimages, generated by an unconditional diffusion model, as the additional\npositives to help the model learn from diverse positives. Through feature\ninterpolation in the diffusion model sampling process, we generate images with\ndistinct backgrounds yet similar semantic content to the anchor image. These\nimages are considered ``hard'' positives for the anchor image, and when\nincluded as supplementary positives in the contrastive loss, they contribute to\na performance improvement of over 2\\% and 1\\% in linear evaluation compared to\nthe previous NNCLR and All4One methods across multiple benchmark datasets such\nas CIFAR10, achieving state-of-the-art methods. On transfer learning\nbenchmarks, CLSP outperforms existing SSL frameworks on 6 out of 8 downstream\ndatasets. We believe CLSP establishes a valuable baseline for future SSL\nstudies incorporating synthetic data in the training process.\n","authors":["Dewen Zeng","Yawen Wu","Xinrong Hu","Xiaowei Xu","Yiyu Shi"],"pdf_url":"https://arxiv.org/pdf/2408.16965v1.pdf","comment":"8 pages, conference"},{"id":"http://arxiv.org/abs/2408.16964v1","updated":"2024-08-30T01:45:22Z","published":"2024-08-30T01:45:22Z","title":"Causal Representation-Based Domain Generalization on Gaze Estimation","summary":"  The availability of extensive datasets containing gaze information for each\nsubject has significantly enhanced gaze estimation accuracy. However, the\ndiscrepancy between domains severely affects a model's performance explicitly\ntrained for a particular domain. In this paper, we propose the Causal\nRepresentation-Based Domain Generalization on Gaze Estimation (CauGE) framework\ndesigned based on the general principle of causal mechanisms, which is\nconsistent with the domain difference. We employ an adversarial training manner\nand an additional penalizing term to extract domain-invariant features. After\nextracting features, we position the attention layer to make features\nsufficient for inferring the actual gaze. By leveraging these modules, CauGE\nensures that the neural networks learn from representations that meet the\ncausal mechanisms' general principles. By this, CauGE generalizes across\ndomains by extracting domain-invariant features, and spurious correlations\ncannot influence the model. Our method achieves state-of-the-art performance in\nthe domain generalization on gaze estimation benchmark.\n","authors":["Younghan Kim","Kangryun Moon","Yongjun Park","Yonggyu Kim"],"pdf_url":"https://arxiv.org/pdf/2408.16964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16959v1","updated":"2024-08-30T01:16:29Z","published":"2024-08-30T01:16:29Z","title":"HiTSR: A Hierarchical Transformer for Reference-based Super-Resolution","summary":"  In this paper, we propose HiTSR, a hierarchical transformer model for\nreference-based image super-resolution, which enhances low-resolution input\nimages by learning matching correspondences from high-resolution reference\nimages. Diverging from existing multi-network, multi-stage approaches, we\nstreamline the architecture and training pipeline by incorporating the double\nattention block from GAN literature. Processing two visual streams\nindependently, we fuse self-attention and cross-attention blocks through a\ngating attention strategy. The model integrates a squeeze-and-excitation module\nto capture global context from the input images, facilitating long-range\nspatial interactions within window-based attention blocks. Long skip\nconnections between shallow and deep layers further enhance information flow.\nOur model demonstrates superior performance across three datasets including\nSUN80, Urban100, and Manga109. Specifically, on the SUN80 dataset, our model\nachieves PSNR/SSIM values of 30.24/0.821. These results underscore the\neffectiveness of attention mechanisms in reference-based image\nsuper-resolution. The transformer-based model attains state-of-the-art results\nwithout the need for purpose-built subnetworks, knowledge distillation, or\nmulti-stage training, emphasizing the potency of attention in meeting\nreference-based image super-resolution requirements.\n","authors":["Masoomeh Aslahishahri","Jordan Ubbens","Ian Stavness"],"pdf_url":"https://arxiv.org/pdf/2408.16959v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2307.08837"},{"id":"http://arxiv.org/abs/2312.09625v3","updated":"2024-08-30T01:07:08Z","published":"2023-12-15T09:08:14Z","title":"Weakly-Supervised 3D Visual Grounding based on Visual Linguistic\n  Alignment","summary":"  Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly\nsupervised approach for 3D visual grounding based on Visual Linguistic\nAlignment. Our 3D-VLA exploits the superior ability of current large-scale\nvision-language models (VLMs) on aligning the semantics between texts and 2D\nimages, as well as the naturally existing correspondences between 2D images and\n3D point clouds, and thus implicitly constructs correspondences between texts\nand 3D point clouds with no need for fine-grained box annotations in the\ntraining procedure. During the inference stage, the learned text-3D\ncorrespondence will help us ground the text queries to the 3D target objects\neven without 2D images. To the best of our knowledge, this is the first work to\ninvestigate 3D visual grounding in a weakly supervised manner by involving\nlarge scale vision-language models, and extensive experiments on ReferIt3D and\nScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even\nsuperior results over the fully supervised methods.\n","authors":["Xiaoxu Xu","Yitian Yuan","Qiudan Zhang","Wenhui Wu","Zequn Jie","Lin Ma","Xu Wang"],"pdf_url":"https://arxiv.org/pdf/2312.09625v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16952v1","updated":"2024-08-30T00:27:46Z","published":"2024-08-30T00:27:46Z","title":"Transient Fault Tolerant Semantic Segmentation for Autonomous Driving","summary":"  Deep learning models are crucial for autonomous vehicle perception, but their\nreliability is challenged by algorithmic limitations and hardware faults. We\naddress the latter by examining fault-tolerance in semantic segmentation\nmodels. Using established hardware fault models, we evaluate existing hardening\ntechniques both in terms of accuracy and uncertainty and introduce ReLUMax, a\nnovel simple activation function designed to enhance resilience against\ntransient faults. ReLUMax integrates seamlessly into existing architectures\nwithout time overhead. Our experiments demonstrate that ReLUMax effectively\nimproves robustness, preserving performance and boosting prediction confidence,\nthus contributing to the development of reliable autonomous driving systems.\n","authors":["Leonardo Iurada","NiccolÃ² Cavagnero","Fernando Fernandes Dos Santos","Giuseppe Averta","Paolo Rech","Tatiana Tommasi"],"pdf_url":"https://arxiv.org/pdf/2408.16952v1.pdf","comment":"Accepted ECCV 2024 UnCV Workshop -\n  https://github.com/iurada/neutron-segmentation"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.00458v2","updated":"2024-08-30T15:59:46Z","published":"2024-03-30T19:45:04Z","title":"Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for\n  Embedding Model Selection","summary":"  This position paper proposes a systematic approach towards developing a\nframework to help select the most effective embedding models for natural\nlanguage processing (NLP) tasks, addressing the challenge posed by the\nproliferation of both proprietary and open-source encoder models.\n","authors":["Vivek Khetan"],"pdf_url":"https://arxiv.org/pdf/2404.00458v2.pdf","comment":"It was an initial idea - we plan to work on a detailed version"},{"id":"http://arxiv.org/abs/2408.17344v1","updated":"2024-08-30T15:16:52Z","published":"2024-08-30T15:16:52Z","title":"rerankers: A Lightweight Python Library to Unify Ranking Methods","summary":"  This paper presents rerankers, a Python library which provides an easy-to-use\ninterface to the most commonly used re-ranking approaches. Re-ranking is an\nintegral component of many retrieval pipelines; however, there exist numerous\napproaches to it, relying on different implementation methods.\n\\texttt{rerankers} unifies these methods into a single user-friendly interface,\nallowing practitioners and researchers alike to explore different methods while\nonly changing a single line of Python code. Moreover ,rerankers ensures that\nits implementations are done with the fewest dependencies possible, and re-uses\nthe original implementation whenever possible, guaranteeing that our simplified\ninterface results in no performance degradation compared to more complex ones.\nThe full source code and list of supported models are updated regularly and\navailable at https://github.com/answerdotai/rerankers.\n","authors":["Benjamin ClaviÃ©"],"pdf_url":"https://arxiv.org/pdf/2408.17344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17332v1","updated":"2024-08-30T14:48:52Z","published":"2024-08-30T14:48:52Z","title":"Not All Videos Become Outdated: Short-Video Recommendation by Learning\n  to Deconfound Release Interval Bias","summary":"  Short-video recommender systems often exhibit a biased preference to recently\nreleased videos. However, not all videos become outdated; certain classic\nvideos can still attract user's attention. Such bias along temporal dimension\ncan be further aggravated by the matching model between users and videos,\nbecause the model learns from preexisting interactions. From real data, we\nobserve that different videos have varying sensitivities to recency in\nattracting users' attention. Our analysis, based on a causal graph modeling\nshort-video recommendation, suggests that the release interval serves as a\nconfounder, establishing a backdoor path between users and videos. To address\nthis confounding effect, we propose a model-agnostic causal architecture called\nLearning to Deconfound the Release Interval Bias (LDRI). LDRI enables jointly\nlearning of the matching model and the video recency sensitivity perceptron. In\nthe inference stage, we apply a backdoor adjustment, effectively blocking the\nbackdoor path by intervening on each video. Extensive experiments on two\nbenchmarks demonstrate that LDRI consistently outperforms backbone models and\nexhibits superior performance against state-of-the-art models. Additional\ncomprehensive analyses confirm the deconfounding capability of LDRI.\n","authors":["Lulu Dong","Guoxiu He","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2408.17332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17309v1","updated":"2024-08-30T14:12:31Z","published":"2024-08-30T14:12:31Z","title":"Metadata practices for simulation workflows","summary":"  Computer simulations are an essential pillar of knowledge generation in\nscience. Understanding, reproducing, and exploring the results of simulations\nrelies on tracking and organizing metadata describing numerical experiments.\nHowever, the models used to understand real-world systems, and the\ncomputational machinery required to simulate them, are typically complex, and\nproduce large amounts of heterogeneous metadata. Here, we present general\npractices for acquiring and handling metadata that are agnostic to software and\nhardware, and highly flexible for the user. These consist of two steps: 1)\nrecording and storing raw metadata, and 2) selecting and structuring metadata.\nAs a proof of concept, we develop the Archivist, a Python tool to help with the\nsecond step, and use it to apply our practices to distinct high-performance\ncomputing use cases from neuroscience and hydrology. Our practices and the\nArchivist can readily be applied to existing workflows without the need for\nsubstantial restructuring. They support sustainable numerical workflows,\nfacilitating reproducibility and data reuse in generic simulation-based\nresearch.\n","authors":["Jose Villamar","Matthias Kelbling","Heather L. More","Michael Denker","Tom Tetzlaff","Johanna Senk","Stephan Thober"],"pdf_url":"https://arxiv.org/pdf/2408.17309v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.16312v2","updated":"2024-08-30T11:48:40Z","published":"2024-08-29T07:20:56Z","title":"SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval","summary":"  Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings.\n","authors":["Hossein A. Rahmani","Xi Wang","Emine Yilmaz","Nick Craswell","Bhaskar Mitra","Paul Thomas"],"pdf_url":"https://arxiv.org/pdf/2408.16312v2.pdf","comment":"9 pages, resource paper"},{"id":"http://arxiv.org/abs/2408.17214v1","updated":"2024-08-30T11:38:51Z","published":"2024-08-30T11:38:51Z","title":"Efficient Multi-task Prompt Tuning for Recommendation","summary":"  With the expansion of business scenarios, real recommender systems are facing\nchallenges in dealing with the constantly emerging new tasks in multi-task\nlearning frameworks. In this paper, we attempt to improve the generalization\nability of multi-task recommendations when dealing with new tasks. We find that\njoint training will enhance the performance of the new task but always\nnegatively impact existing tasks in most multi-task learning methods. Besides,\nsuch a re-training mechanism with new tasks increases the training costs,\nlimiting the generalization ability of multi-task recommendation models. Based\non this consideration, we aim to design a suitable sharing mechanism among\ndifferent tasks while maintaining joint optimization efficiency in new task\nlearning. A novel two-stage prompt-tuning MTL framework (MPT-Rec) is proposed\nto address task irrelevance and training efficiency problems in multi-task\nrecommender systems. Specifically, we disentangle the task-specific and\ntask-sharing information in the multi-task pre-training stage, then use\ntask-aware prompts to transfer knowledge from other tasks to the new task\neffectively. By freezing parameters in the pre-training tasks, MPT-Rec solves\nthe negative impacts that may be brought by the new task and greatly reduces\nthe training costs. Extensive experiments on three real-world datasets show the\neffectiveness of our proposed multi-task learning framework. MPT-Rec achieves\nthe best performance compared to the SOTA multi-task learning method. Besides,\nit maintains comparable model performance but vastly improves the training\nefficiency (i.e., with up to 10% parameters in the full training way) in the\nnew task learning.\n","authors":["Ting Bai","Le Huang","Yue Yu","Cheng Yang","Cheng Hou","Zhe Zhao","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.17214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17180v1","updated":"2024-08-30T10:28:36Z","published":"2024-08-30T10:28:36Z","title":"Identifying and Clustering Counter Relationships of Team Compositions in\n  PvP Games for Efficient Balance Analysis","summary":"  How can balance be quantified in game settings? This question is crucial for\ngame designers, especially in player-versus-player (PvP) games, where analyzing\nthe strength relations among predefined team compositions-such as hero\ncombinations in multiplayer online battle arena (MOBA) games or decks in card\ngames-is essential for enhancing gameplay and achieving balance. We have\ndeveloped two advanced measures that extend beyond the simplistic win rate to\nquantify balance in zero-sum competitive scenarios. These measures are derived\nfrom win value estimations, which employ strength rating approximations via the\nBradley-Terry model and counter relationship approximations via vector\nquantization, significantly reducing the computational complexity associated\nwith traditional win value estimations. Throughout the learning process of\nthese models, we identify useful categories of compositions and pinpoint their\ncounter relationships, aligning with the experiences of human players without\nrequiring specific game knowledge. Our methodology hinges on a simple technique\nto enhance codebook utilization in discrete representation with a deterministic\nvector quantization process for an extremely small state space. Our framework\nhas been validated in popular online games, including Age of Empires II,\nHearthstone, Brawl Stars, and League of Legends. The accuracy of the observed\nstrength relations in these games is comparable to traditional pairwise win\nvalue predictions, while also offering a more manageable complexity for\nanalysis. Ultimately, our findings contribute to a deeper understanding of PvP\ngame dynamics and present a methodology that significantly improves game\nbalance evaluation and design.\n","authors":["Chiu-Chou Lin","Yu-Wei Shih","Kuei-Ting Kuo","Yu-Cheng Chen","Chien-Hua Chen","Wei-Chen Chiu","I-Chen Wu"],"pdf_url":"https://arxiv.org/pdf/2408.17180v1.pdf","comment":"TMLR 09/2024 https://openreview.net/forum?id=2D36otXvBE"},{"id":"http://arxiv.org/abs/2408.17103v1","updated":"2024-08-30T08:40:59Z","published":"2024-08-30T08:40:59Z","title":"Understanding the User: An Intent-Based Ranking Dataset","summary":"  As information retrieval systems continue to evolve, accurate evaluation and\nbenchmarking of these systems become pivotal. Web search datasets, such as MS\nMARCO, primarily provide short keyword queries without accompanying intent or\ndescriptions, posing a challenge in comprehending the underlying information\nneed. This paper proposes an approach to augmenting such datasets to annotate\ninformative query descriptions, with a focus on two prominent benchmark\ndatasets: TREC-DL-21 and TREC-DL-22. Our methodology involves utilizing\nstate-of-the-art LLMs to analyze and comprehend the implicit intent within\nindividual queries from benchmark datasets. By extracting key semantic\nelements, we construct detailed and contextually rich descriptions for these\nqueries. To validate the generated query descriptions, we employ crowdsourcing\nas a reliable means of obtaining diverse human perspectives on the accuracy and\ninformativeness of the descriptions. This information can be used as an\nevaluation set for tasks such as ranking, query rewriting, or others.\n","authors":["Abhijit Anand","Jurek Leonhardt","V Venktesh","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2408.17103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12492v2","updated":"2024-08-30T07:58:46Z","published":"2024-08-22T15:33:46Z","title":"The Importance of Cognitive Biases in the Recommendation Ecosystem","summary":"  Cognitive biases have been studied in psychology, sociology, and behavioral\neconomics for decades. Traditionally, they have been considered a negative\nhuman trait that leads to inferior decision-making, reinforcement of\nstereotypes, or can be exploited to manipulate consumers, respectively. We\nargue that cognitive biases also manifest in different parts of the\nrecommendation ecosystem and at different stages of the recommendation process.\nMore importantly, we contest this traditional detrimental perspective on\ncognitive biases and claim that certain cognitive biases can be beneficial when\naccounted for by recommender systems. Concretely, we provide empirical evidence\nthat biases such as feature-positive effect, Ikea effect, and cultural\nhomophily can be observed in various components of the recommendation pipeline,\nincluding input data (such as ratings or side information), recommendation\nalgorithm or model (and consequently recommended items), and user interactions\nwith the system. In three small experiments covering recruitment and\nentertainment domains, we study the pervasiveness of the aforementioned biases.\nWe ultimately advocate for a prejudice-free consideration of cognitive biases\nto improve user and item models as well as recommendation algorithms.\n","authors":["Markus Schedl","Oleg Lesota","Stefan Brandl","Mohammad Lotfi","Gustavo Junior Escobedo Ticona","Shahed Masoudian"],"pdf_url":"https://arxiv.org/pdf/2408.12492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17008v1","updated":"2024-08-30T04:40:35Z","published":"2024-08-30T04:40:35Z","title":"Evaluation of Table Representations to Answer Questions from Tables in\n  Documents : A Case Study using 3GPP Specifications","summary":"  With the ubiquitous use of document corpora for question answering, one\nimportant aspect which is especially relevant for technical documents is the\nability to extract information from tables which are interspersed with text.\nThe major challenge in this is that unlike free-flow text or isolated set of\ntables, the representation of a table in terms of what is a relevant chunk is\nnot obvious. We conduct a series of experiments examining various\nrepresentations of tabular data interspersed with text to understand the\nrelative benefits of different representations. We choose a corpus of $3^{rd}$\nGeneration Partnership Project (3GPP) documents since they are heavily\ninterspersed with tables. We create expert curated dataset of question answers\nto evaluate our approach. We conclude that row level representations with\ncorresponding table header information being included in every cell improves\nthe performance of the retrieval, thus leveraging the structural information\npresent in the tabular data.\n","authors":["Sujoy Roychowdhury","Sumit Soman","HG Ranjani","Avantika Sharma","Neeraj Gunda","Sai Krishna Bala"],"pdf_url":"https://arxiv.org/pdf/2408.17008v1.pdf","comment":"10 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.06051v2","updated":"2024-08-30T03:19:26Z","published":"2024-08-12T10:55:42Z","title":"Perceptual Similarity for Measuring Decision-Making Style and Policy\n  Diversity in Games","summary":"  Defining and measuring decision-making styles, also known as playstyles, is\ncrucial in gaming, where these styles reflect a broad spectrum of individuality\nand diversity. However, finding a universally applicable measure for these\nstyles poses a challenge. Building on Playstyle Distance, the first\nunsupervised metric to measure playstyle similarity based on game screens and\nraw actions, we introduce three enhancements to increase accuracy: multiscale\nanalysis with varied state granularity, a perceptual kernel rooted in\npsychology, and the utilization of the intersection-over-union method for\nefficient evaluation. These innovations not only advance measurement precision\nbut also offer insights into human cognition of similarity. Across two racing\ngames and seven Atari games, our techniques significantly improve the precision\nof zero-shot playstyle classification, achieving an accuracy exceeding 90\npercent with fewer than 512 observation-action pairs, which is less than half\nan episode of these games. Furthermore, our experiments with 2048 and Go\ndemonstrate the potential of discrete playstyle measures in puzzle and board\ngames. We also develop an algorithm for assessing decision-making diversity\nusing these measures. Our findings improve the measurement of end-to-end game\nanalysis and the evolution of artificial intelligence for diverse playstyles.\n","authors":["Chiu-Chou Lin","Wei-Chen Chiu","I-Chen Wu"],"pdf_url":"https://arxiv.org/pdf/2408.06051v2.pdf","comment":"TMLR 08/2024 https://openreview.net/forum?id=30C9AWBW49"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2402.17295v2","updated":"2024-08-30T17:36:49Z","published":"2024-02-27T08:16:17Z","title":"Quantum Distance Approximation for Persistence Diagrams","summary":"  Topological Data Analysis methods can be useful for classification and\nclustering tasks in many different fields as they can provide two dimensional\npersistence diagrams that summarize important information about the shape of\npotentially complex and high dimensional data sets. The space of persistence\ndiagrams can be endowed with various metrics such as the Wasserstein distance\nwhich admit a statistical structure and allow to use these summaries for\nmachine learning algorithms. However, computing the distance between two\npersistence diagrams involves finding an optimal way to match the points of the\ntwo diagrams and may not always be an easy task for classical computers. In\nthis work we explore the potential of quantum computers to estimate the\ndistance between persistence diagrams, in particular we propose variational\nquantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$\ndistance. Our implementation is a weighted version of the Quantum Approximate\nOptimization Algorithm that relies on control clauses to encode the constraints\nof the optimization problem.\n","authors":["Bernardo Ameneyro","Rebekah Herrman","George Siopsis","Vasileios Maroulas"],"pdf_url":"https://arxiv.org/pdf/2402.17295v2.pdf","comment":"39 pages, 12 figures, 2 tables, submitted to Journal of Physics:\n  Complexity"},{"id":"http://arxiv.org/abs/2408.17432v1","updated":"2024-08-30T17:34:46Z","published":"2024-08-30T17:34:46Z","title":"SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame\n  Selection","summary":"  Synthesizing the voices of unseen speakers is a persisting challenge in\nmulti-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on\nmodeling speaker characteristics through speaker conditioning during training.\nModeling unseen speaker attributes through this approach has necessitated an\nincrease in model complexity, which makes it challenging to reproduce results\nand improve upon them. We design a simple alternative to this. We propose\nSelectTTS, a novel method to select the appropriate frames from the target\nspeaker and decode using frame-level self-supervised learning (SSL) features.\nWe show that this approach can effectively capture speaker characteristics for\nunseen speakers, and achieves comparable results to other multi-speaker TTS\nframeworks in both objective and subjective metrics. With SelectTTS, we show\nthat frame selection from the target speaker's speech is a direct way to\nachieve generalization in unseen speakers with low model complexity. We achieve\nbetter speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E\nwith over an 8x reduction in model parameters and a 270x reduction in training\ndata\n","authors":["Ismail Rasim Ulgen","Shreeram Suresh Chandra","Junchen Lu","Berrak Sisman"],"pdf_url":"https://arxiv.org/pdf/2408.17432v1.pdf","comment":"Submitted to IEEE Signal Processing Letters"},{"id":"http://arxiv.org/abs/2310.19704v3","updated":"2024-08-30T17:22:01Z","published":"2023-10-30T16:29:47Z","title":"A Survey on Knowledge Editing of Neural Networks","summary":"  Deep neural networks are becoming increasingly pervasive in academia and\nindustry, matching and surpassing human performance on a wide variety of fields\nand related tasks. However, just as humans, even the largest artificial neural\nnetworks make mistakes, and once-correct predictions can become invalid as the\nworld progresses in time. Augmenting datasets with samples that account for\nmistakes or up-to-date information has become a common workaround in practical\napplications. However, the well-known phenomenon of catastrophic forgetting\nposes a challenge in achieving precise changes in the implicitly memorized\nknowledge of neural network parameters, often requiring a full model\nre-training to achieve desired behaviors. That is expensive, unreliable, and\nincompatible with the current trend of large self-supervised pre-training,\nmaking it necessary to find more efficient and effective methods for adapting\nneural network models to changing data. To address this need, knowledge editing\nis emerging as a novel area of research that aims to enable reliable,\ndata-efficient, and fast changes to a pre-trained target model, without\naffecting model behaviors on previously learned tasks. In this survey, we\nprovide a brief review of this recent artificial intelligence field of\nresearch. We first introduce the problem of editing neural networks, formalize\nit in a common framework and differentiate it from more notorious branches of\nresearch such as continuous learning. Next, we provide a review of the most\nrelevant knowledge editing approaches and datasets proposed so far, grouping\nworks under four different families: regularization techniques, meta-learning,\ndirect model editing, and architectural strategies. Finally, we outline some\nintersections with other fields of research and potential directions for future\nworks.\n","authors":["Vittorio Mazzia","Alessandro Pedrani","Andrea Caciolai","Kay Rottmann","Davide Bernardi"],"pdf_url":"https://arxiv.org/pdf/2310.19704v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12212v2","updated":"2024-08-30T17:02:11Z","published":"2024-03-18T19:53:56Z","title":"Evaluating Named Entity Recognition: A comparative analysis of mono- and\n  multilingual transformer models on a novel Brazilian corporate earnings call\n  transcripts dataset","summary":"  Since 2018, when the Transformer architecture was introduced, Natural\nLanguage Processing has gained significant momentum with pre-trained\nTransformer-based models that can be fine-tuned for various tasks. Most models\nare pre-trained on large English corpora, making them less applicable to other\nlanguages, such as Brazilian Portuguese. In our research, we identified two\nmodels pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two\nmultilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder\nmodule, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to\nevaluate their performance on a financial Named Entity Recognition (NER) task\nand determine the computational requirements for fine-tuning and inference. To\nthis end, we developed the Brazilian Financial NER (BraFiNER) dataset,\ncomprising sentences from Brazilian banks' earnings calls transcripts annotated\nusing a weakly supervised approach. Additionally, we introduced a novel\napproach that reframes the token classification task as a text generation\nproblem. After fine-tuning the models, we evaluated them using performance and\nerror metrics. Our findings reveal that BERT-based models consistently\noutperform T5-based models. While the multilingual models exhibit comparable\nmacro F1-scores, BERTimbau demonstrates superior performance over PTT5. In\nterms of error metrics, BERTimbau outperforms the other models. We also\nobserved that PTT5 and mT5 generated sentences with changes in monetary and\npercentage values, highlighting the importance of accuracy and consistency in\nthe financial domain. Our findings provide insights into the differing\nperformance of BERT- and T5-based models for the NER task.\n","authors":["Ramon Abilio","Guilherme Palermo Coelho","Ana Estela Antunes da Silva"],"pdf_url":"https://arxiv.org/pdf/2403.12212v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03496v9","updated":"2024-08-30T16:45:58Z","published":"2024-02-05T20:15:19Z","title":"Can We Remove the Square-Root in Adaptive Gradient Methods? A\n  Second-Order Perspective","summary":"  Adaptive gradient optimizers like Adam(W) are the default training algorithms\nfor many deep learning architectures, such as transformers. Their diagonal\npreconditioner is based on the gradient outer product which is incorporated\ninto the parameter update via a square root. While these methods are often\nmotivated as approximate second-order methods, the square root represents a\nfundamental difference. In this work, we investigate how the behavior of\nadaptive methods changes when we remove the root, i.e., strengthen their\nsecond-order motivation. Surprisingly, we find that such square-root-free\nadaptive methods close the generalization gap to SGD on convolutional\narchitectures, while maintaining their root-based counterpart's performance on\ntransformers. The second-order perspective also has practical benefits for\ndeveloping non-diagonal methods that can incorporate arbitrary curvature\napproximations through the concept of preconditioner invariance. In contrast to\nroot-based methods like Shampoo, root-free counterparts work well and fast with\nhalf-precision since they do not require numerically unstable matrix root\ndecompositions and inversions. Overall, our findings provide new insights into\nthe development of adaptive methods and raise important questions regarding the\noverlooked role of adaptivity in their success. (experiment code:\nhttps://github.com/yorkerlin/remove-the-square-root optimizer code:\nhttps://github.com/f-dangel/sirfshampoo)\n","authors":["Wu Lin","Felix Dangel","Runa Eschenhagen","Juhan Bae","Richard E. Turner","Alireza Makhzani"],"pdf_url":"https://arxiv.org/pdf/2402.03496v9.pdf","comment":"A long version of the ICML 2024 paper. Added root-free update schemes\n  for n-dim tensor cases"},{"id":"http://arxiv.org/abs/2405.02175v3","updated":"2024-08-30T16:40:15Z","published":"2024-05-03T15:25:48Z","title":"Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset","summary":"  Hoaxes are a recognised form of disinformation created deliberately, with\npotential serious implications in the credibility of reference knowledge\nresources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is that\nthey often are written according to the official style guidelines. In this\nwork, we first provide a systematic analysis of similarities and discrepancies\nbetween legitimate and hoax Wikipedia articles, and introduce Hoaxpedia, a\ncollection of 311 hoax articles (from existing literature and official\nWikipedia lists), together with semantically similar legitimate articles, which\ntogether form a binary text classification dataset aimed at fostering research\nin automated hoax detection. In this paper, We report results after analyzing\nseveral language models, hoax-to-legit ratios, and the amount of text\nclassifiers are exposed to (full article vs the article's definition alone).\nOur results suggest that detecting deceitful content in Wikipedia based on\ncontent alone is hard but feasible, and complement our analysis with a study on\nthe differences in distributions in edit histories, and find that looking at\nthis feature yields better classification results than context.\n","authors":["Hsuvas Borkakoty","Luis Espinosa-Anke"],"pdf_url":"https://arxiv.org/pdf/2405.02175v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17396v1","updated":"2024-08-30T16:30:00Z","published":"2024-08-30T16:30:00Z","title":"Fairness-Aware Estimation of Graphical Models","summary":"  This paper examines the issue of fairness in the estimation of graphical\nmodels (GMs), particularly Gaussian, Covariance, and Ising models. These models\nplay a vital role in understanding complex relationships in high-dimensional\ndata. However, standard GMs can result in biased outcomes, especially when the\nunderlying data involves sensitive characteristics or protected groups. To\naddress this, we introduce a comprehensive framework designed to reduce bias in\nthe estimation of GMs related to protected attributes. Our approach involves\nthe integration of the pairwise graph disparity error and a tailored loss\nfunction into a nonsmooth multi-objective optimization problem, striving to\nachieve fairness across different sensitive groups while maintaining the\neffectiveness of the GMs. Experimental evaluations on synthetic and real-world\ndatasets demonstrate that our framework effectively mitigates bias without\nundermining GMs' performance.\n","authors":["Zhuoping Zhou","Davoud Ataee Tarzanagh","Bojian Hou","Qi Long","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2408.17396v1.pdf","comment":"32 Pages, 9 Figures"},{"id":"http://arxiv.org/abs/2408.17394v1","updated":"2024-08-30T16:29:09Z","published":"2024-08-30T16:29:09Z","title":"Continual learning with the neural tangent ensemble","summary":"  A natural strategy for continual learning is to weigh a Bayesian ensemble of\nfixed functions. This suggests that if a (single) neural network could be\ninterpreted as an ensemble, one could design effective algorithms that learn\nwithout forgetting. To realize this possibility, we observe that a neural\nnetwork classifier with N parameters can be interpreted as a weighted ensemble\nof N classifiers, and that in the lazy regime limit these classifiers are fixed\nthroughout learning. We term these classifiers the neural tangent experts and\nshow they output valid probability distributions over the labels. We then\nderive the likelihood and posterior probability of each expert given past data.\nSurprisingly, we learn that the posterior updates for these experts are\nequivalent to a scaled and projected form of stochastic gradient descent (SGD)\nover the network weights. Away from the lazy regime, networks can be seen as\nensembles of adaptive experts which improve over time. These results offer a\nnew interpretation of neural networks as Bayesian ensembles of experts,\nproviding a principled framework for understanding and mitigating catastrophic\nforgetting in continual learning settings.\n","authors":["Ari S. Benjamin","Christian Pehle","Kyle Daruwalla"],"pdf_url":"https://arxiv.org/pdf/2408.17394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17387v1","updated":"2024-08-30T16:26:31Z","published":"2024-08-30T16:26:31Z","title":"Bayesian Optimization for Non-Convex Two-Stage Stochastic Optimization\n  Problems","summary":"  Bayesian optimization is a sample-efficient method for solving expensive,\nblack-box optimization problems. Stochastic programming concerns optimization\nunder uncertainty where, typically, average performance is the quantity of\ninterest. In the first stage of a two-stage problem, here-and-now decisions\nmust be made in the face of this uncertainty, while in the second stage,\nwait-and-see decisions are made after the uncertainty has been resolved. Many\nmethods in stochastic programming assume that the objective is cheap to\nevaluate and linear or convex. In this work, we apply Bayesian optimization to\nsolve non-convex, two-stage stochastic programs which are expensive to\nevaluate. We formulate a knowledge-gradient-based acquisition function to\njointly optimize the first- and second-stage variables, establish a guarantee\nof asymptotic consistency and provide a computationally efficient\napproximation. We demonstrate comparable empirical results to an alternative we\nformulate which alternates its focus between the two variable types, and\nsuperior empirical results over the standard, naive, two-step benchmark. We\nshow that differences in the dimension and length scales between the variable\ntypes can lead to inefficiencies of the two-step algorithm, while the joint and\nalternating acquisition functions perform well in all problems tested.\nExperiments are conducted on both synthetic and real-world examples.\n","authors":["Jack M. Buckingham","Ivo Couckuyt","Juergen Branke"],"pdf_url":"https://arxiv.org/pdf/2408.17387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17384v1","updated":"2024-08-30T16:26:04Z","published":"2024-08-30T16:26:04Z","title":"LASSO-MOGAT: A Multi-Omics Graph Attention Framework for Cancer\n  Classification","summary":"  The application of machine learning methods to analyze changes in gene\nexpression patterns has recently emerged as a powerful approach in cancer\nresearch, enhancing our understanding of the molecular mechanisms underpinning\ncancer development and progression. Combining gene expression data with other\ntypes of omics data has been reported by numerous works to improve cancer\nclassification outcomes. Despite these advances, effectively integrating\nhigh-dimensional multi-omics data and capturing the complex relationships\nacross different biological layers remains challenging. This paper introduces\nLASSO-MOGAT (LASSO-Multi-Omics Gated ATtention), a novel graph-based deep\nlearning framework that integrates messenger RNA, microRNA, and DNA methylation\ndata to classify 31 cancer types. Utilizing differential expression analysis\nwith LIMMA and LASSO regression for feature selection, and leveraging Graph\nAttention Networks (GATs) to incorporate protein-protein interaction (PPI)\nnetworks, LASSO-MOGAT effectively captures intricate relationships within\nmulti-omics data. Experimental validation using five-fold cross-validation\ndemonstrates the method's precision, reliability, and capacity for providing\ncomprehensive insights into cancer molecular mechanisms. The computation of\nattention coefficients for the edges in the graph by the proposed\ngraph-attention architecture based on protein-protein interactions proved\nbeneficial for identifying synergies in multi-omics data for cancer\nclassification.\n","authors":["Fadi Alharbi","Aleksandar Vakanski","Murtada K. Elbashir","Mohanad Mohammed"],"pdf_url":"https://arxiv.org/pdf/2408.17384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17383v1","updated":"2024-08-30T16:24:27Z","published":"2024-08-30T16:24:27Z","title":"MoRe Fine-Tuning with 10x Fewer Parameters","summary":"  Parameter-efficient fine-tuning (PEFT) techniques have unlocked the potential\nto cheaply and easily specialize large pretrained models. However, the most\nprominent approaches, like low-rank adapters (LoRA), depend on heuristics or\nrules-of-thumb for their architectural choices -- potentially limiting their\nperformance for new models and architectures. This limitation suggests that\ntechniques from neural architecture search could be used to obtain optimal\nadapter architectures, but these are often expensive and difficult to\nimplement. We address this challenge with Monarch Rectangular Fine-tuning\n(MoRe), a simple framework to search over adapter architectures that relies on\nthe Monarch matrix class. Theoretically, we show that MoRe is more expressive\nthan LoRA. Empirically, our approach is more parameter-efficient and performant\nthan state-of-the-art PEFTs on a range of tasks and models, with as few as 5\\%\nof LoRA's parameters.\n","authors":["Wenxuan Tan","Nicholas Roberts","Tzu-Heng Huang","Jitian Zhao","John Cooper","Samuel Guo","Chengyu Duan","Frederic Sala"],"pdf_url":"https://arxiv.org/pdf/2408.17383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17380v1","updated":"2024-08-30T16:16:57Z","published":"2024-08-30T16:16:57Z","title":"Traffic expertise meets residual RL: Knowledge-informed model-based\n  residual reinforcement learning for CAV trajectory control","summary":"  Model-based reinforcement learning (RL) is anticipated to exhibit higher\nsample efficiency compared to model-free RL by utilizing a virtual environment\nmodel. However, it is challenging to obtain sufficiently accurate\nrepresentations of the environmental dynamics due to uncertainties in complex\nsystems and environments. An inaccurate environment model may degrade the\nsample efficiency and performance of model-based RL. Furthermore, while\nmodel-based RL can improve sample efficiency, it often still requires\nsubstantial training time to learn from scratch, potentially limiting its\nadvantages over model-free approaches. To address these challenges, this paper\nintroduces a knowledge-informed model-based residual reinforcement learning\nframework aimed at enhancing learning efficiency by infusing established expert\nknowledge into the learning process and avoiding the issue of beginning from\nzero. Our approach integrates traffic expert knowledge into a virtual\nenvironment model, employing the Intelligent Driver Model (IDM) for basic\ndynamics and neural networks for residual dynamics, thus ensuring adaptability\nto complex scenarios. We propose a novel strategy that combines traditional\ncontrol methods with residual RL, facilitating efficient learning and policy\noptimization without the need to learn from scratch. The proposed approach is\napplied to CAV trajectory control tasks for the dissipation of stop-and-go\nwaves in mixed traffic flow. Experimental results demonstrate that our proposed\napproach enables the CAV agent to achieve superior performance in trajectory\ncontrol compared to the baseline agents in terms of sample efficiency, traffic\nflow smoothness and traffic mobility. The source code and supplementary\nmaterials are available at https://github.com/zihaosheng/traffic-expertise-RL/.\n","authors":["Zihao Sheng","Zilin Huang","Sikai Chen"],"pdf_url":"https://arxiv.org/pdf/2408.17380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17376v1","updated":"2024-08-30T16:12:57Z","published":"2024-08-30T16:12:57Z","title":"Exploring the Impact of Environmental Pollutants on Multiple Sclerosis\n  Progression","summary":"  Multiple Sclerosis (MS) is a chronic autoimmune and inflammatory neurological\ndisorder characterised by episodes of symptom exacerbation, known as relapses.\nIn this study, we investigate the role of environmental factors in relapse\noccurrence among MS patients, using data from the H2020 BRAINTEASER project. We\nemployed predictive models, including Random Forest (RF) and Logistic\nRegression (LR), with varying sets of input features to predict the occurrence\nof relapses based on clinical and pollutant data collected over a week. The RF\nyielded the best result, with an AUC-ROC score of 0.713. Environmental\nvariables, such as precipitation, NO2, PM2.5, humidity, and temperature, were\nfound to be relevant to the prediction.\n","authors":["Elena Marinello","Erica Tavazzi","Enrico Longato","Pietro Bosoni","Arianna Dagliati","Mahin Vazifehdan","Riccardo Bellazzi","Isotta Trescato","Alessandro Guazzo","Martina Vettoretti","Eleonora Tavazzi","Lara Ahmad","Roberto Bergamaschi","Paola Cavalla","Umberto Manera","Adriano Chio","Barbara Di Camillo"],"pdf_url":"https://arxiv.org/pdf/2408.17376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05955v2","updated":"2024-08-30T16:04:04Z","published":"2023-02-12T16:55:58Z","title":"Recursive Estimation of Conditional Kernel Mean Embeddings","summary":"  Kernel mean embeddings, a widely used technique in machine learning, map\nprobability distributions to elements of a reproducing kernel Hilbert space\n(RKHS). For supervised learning problems, where input-output pairs are\nobserved, the conditional distribution of outputs given the inputs is a key\nobject. The input dependent conditional distribution of an output can be\nencoded with an RKHS valued function, the conditional kernel mean map. In this\npaper we present a new recursive algorithm to estimate the conditional kernel\nmean map in a Hilbert space valued $L_2$ space, that is in a Bochner space. We\nprove the weak and strong $L_2$ consistency of our recursive estimator under\nmild conditions. The idea is to generalize Stone's theorem for Hilbert space\nvalued regression in a locally compact Polish space. We present new insights\nabout conditional kernel mean embeddings and give strong asymptotic bounds\nregarding the convergence of the proposed recursive method. Finally, the\nresults are demonstrated on three application domains: for inputs coming from\nEuclidean spaces, Riemannian manifolds and locally compact subsets of function\nspaces.\n","authors":["Ambrus TamÃ¡s","BalÃ¡zs CsanÃ¡d CsÃ¡ji"],"pdf_url":"https://arxiv.org/pdf/2302.05955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09102v3","updated":"2024-08-30T16:03:53Z","published":"2022-07-19T06:49:24Z","title":"Complexity of High-Dimensional Identity Testing with Coordinate\n  Conditional Sampling","summary":"  We study the identity testing problem for high-dimensional distributions.\nGiven as input an explicit distribution $\\mu$, an $\\varepsilon>0$, and access\nto sampling oracle(s) for a hidden distribution $\\pi$, the goal in identity\ntesting is to distinguish whether the two distributions $\\mu$ and $\\pi$ are\nidentical or are at least $\\varepsilon$-far apart. When there is only access to\nfull samples from the hidden distribution $\\pi$, it is known that exponentially\nmany samples (in the dimension) may be needed for identity testing, and hence\nprevious works have studied identity testing with additional access to various\n\"conditional\" sampling oracles. We consider a significantly weaker conditional\nsampling oracle, which we call the $\\mathsf{Coordinate\\ Oracle}$, and provide a\ncomputational and statistical characterization of the identity testing problem\nin this new model.\n  We prove that if an analytic property known as approximate tensorization of\nentropy holds for an $n$-dimensional visible distribution $\\mu$, then there is\nan efficient identity testing algorithm for any hidden distribution $\\pi$ using\n$\\tilde{O}(n/\\varepsilon)$ queries to the $\\mathsf{Coordinate\\ Oracle}$.\nApproximate tensorization of entropy is a pertinent condition as recent works\nhave established it for a large class of high-dimensional distributions. We\nalso prove a computational phase transition: for a well-studied class of\n$n$-dimensional distributions, specifically sparse antiferromagnetic Ising\nmodels over $\\{+1,-1\\}^n$, we show that in the regime where approximate\ntensorization of entropy fails, there is no efficient identity testing\nalgorithm unless $\\mathsf{RP}=\\mathsf{NP}$. We complement our results with a\nmatching $\\Omega(n/\\varepsilon)$ statistical lower bound for the sample\ncomplexity of identity testing in the $\\mathsf{Coordinate\\ Oracle}$ model.\n","authors":["Antonio Blanca","Zongchen Chen","Daniel Å tefankoviÄ","Eric Vigoda"],"pdf_url":"https://arxiv.org/pdf/2207.09102v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17366v1","updated":"2024-08-30T15:54:50Z","published":"2024-08-30T15:54:50Z","title":"Leveraging Graph Neural Networks to Forecast Electricity Consumption","summary":"  Accurate electricity demand forecasting is essential for several reasons,\nespecially as the integration of renewable energy sources and the transition to\na decentralized network paradigm introduce greater complexity and uncertainty.\nThe proposed methodology leverages graph-based representations to effectively\ncapture the spatial distribution and relational intricacies inherent in this\ndecentralized network structure. This research work offers a novel approach\nthat extends beyond the conventional Generalized Additive Model framework by\nconsidering models like Graph Convolutional Networks or Graph SAGE. These\ngraph-based models enable the incorporation of various levels of\ninterconnectedness and information sharing among nodes, where each node\ncorresponds to the combined load (i.e. consumption) of a subset of consumers\n(e.g. the regions of a country). More specifically, we introduce a range of\nmethods for inferring graphs tailored to consumption forecasting, along with a\nframework for evaluating the developed models in terms of both performance and\nexplainability. We conduct experiments on electricity forecasting, in both a\nsynthetic and a real framework considering the French mainland regions, and the\nperformance and merits of our approach are discussed.\n","authors":["Eloi Campagne","Yvenn Amara-Ouali","Yannig Goude","Argyris Kalogeratos"],"pdf_url":"https://arxiv.org/pdf/2408.17366v1.pdf","comment":"17 pages, ECML PKDD 2024 Workshop paper"},{"id":"http://arxiv.org/abs/2408.17358v1","updated":"2024-08-30T15:49:31Z","published":"2024-08-30T15:49:31Z","title":"Hold Me Tight: Stable Encoder-Decoder Design for Speech Enhancement","summary":"  Convolutional layers with 1-D filters are often used as frontend to encode\naudio signals. Unlike fixed time-frequency representations, they can adapt to\nthe local characteristics of input data. However, 1-D filters on raw audio are\nhard to train and often suffer from instabilities. In this paper, we address\nthese problems with hybrid solutions, i.e., combining theory-driven and\ndata-driven approaches. First, we preprocess the audio signals via a auditory\nfilterbank, guaranteeing good frequency localization for the learned encoder.\nSecond, we use results from frame theory to define an unsupervised learning\nobjective that encourages energy conservation and perfect reconstruction.\nThird, we adapt mixed compressed spectral norms as learning objectives to the\nencoder coefficients. Using these solutions in a low-complexity\nencoder-mask-decoder model significantly improves the perceptual evaluation of\nspeech quality (PESQ) in speech enhancement.\n","authors":["Daniel Haider","Felix Perfler","Vincent Lostanlen","Martin Ehler","Peter Balazs"],"pdf_url":"https://arxiv.org/pdf/2408.17358v1.pdf","comment":"Accepted at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.17585v2","updated":"2024-08-30T15:45:11Z","published":"2024-06-25T14:28:17Z","title":"Learning Dynamic Bayesian Networks from Data: Foundations, First\n  Principles and Numerical Comparisons","summary":"  In this paper, we present a guide to the foundations of learning Dynamic\nBayesian Networks (DBNs) from data in the form of multiple samples of\ntrajectories for some length of time. We present the formalism for a generic as\nwell as a set of common types of DBNs for particular variable distributions. We\npresent the analytical form of the models, with a comprehensive discussion on\nthe interdependence between structure and weights in a DBN model and their\nimplications for learning. Next, we give a broad overview of learning methods\nand describe and categorize them based on the most important statistical\nfeatures, and how they treat the interplay between learning structure and\nweights. We give the analytical form of the likelihood and Bayesian score\nfunctions, emphasizing the distinction from the static case. We discuss\nfunctions used in optimization to enforce structural requirements. We briefly\ndiscuss more complex extensions and representations. Finally we present a set\nof comparisons in different settings for various distinct but representative\nalgorithms across the variants.\n","authors":["Vyacheslav Kungurtsev","Fadwa Idlahcen","Petr Rysavy","Pavel Rytir","Ales Wodecki"],"pdf_url":"https://arxiv.org/pdf/2406.17585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17356v1","updated":"2024-08-30T15:39:37Z","published":"2024-08-30T15:39:37Z","title":"C-RADAR: A Centralized Deep Learning System for Intrusion Detection in\n  Software Defined Networks","summary":"  The popularity of Software Defined Networks (SDNs) has grown in recent years,\nmainly because of their ability to simplify network management and improve\nnetwork flexibility. However, this also makes them vulnerable to various types\nof cyber attacks. SDNs work on a centralized control plane which makes them\nmore prone to network attacks. Research has demonstrated that deep learning\n(DL) methods can be successful in identifying intrusions in conventional\nnetworks, but their application in SDNs is still an open research area. In this\nresearch, we propose the use of DL techniques for intrusion detection in SDNs.\nWe measure the effectiveness of our method by experimentation on a dataset of\nnetwork traffic and comparing it to existing techniques. Our results show that\nthe DL-based approach outperforms traditional methods in terms of detection\naccuracy and computational efficiency. The deep learning architecture that has\nbeen used in this research is a Long Short Term Memory Network and\nSelf-Attention based architecture i.e. LSTM-Attn which achieves an Fl-score of\n0.9721. Furthermore, this technique can be trained to detect new attack\npatterns and improve the overall security of SDNs.\n","authors":["Osama Mustafa","Khizer Ali","Talha Naqash"],"pdf_url":"https://arxiv.org/pdf/2408.17356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17355v1","updated":"2024-08-30T15:39:34Z","published":"2024-08-30T15:39:34Z","title":"Bidirectional Decoding: Improving Action Chunking via Closed-Loop\n  Resampling","summary":"  Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. However, its effects on learned policies remain\npuzzling: some studies highlight its importance for achieving strong\nperformance, while others observe detrimental effects. In this paper, we first\ndissect the role of action chunking by analyzing the divergence between the\nlearner and the demonstrator. We find that longer action chunks enable a policy\nto better capture temporal dependencies by taking into account more past states\nand actions within the chunk. However, this advantage comes at the cost of\nexacerbating errors in stochastic environments due to fewer observations of\nrecent states. To address this, we propose Bidirectional Decoding (BID), a\ntest-time inference algorithm that bridges action chunking with closed-loop\noperations. BID samples multiple predictions at each time step and searches for\nthe optimal one based on two criteria: (i) backward coherence, which favors\nsamples aligned with previous decisions, (ii) forward contrast, which favors\nsamples close to outputs of a stronger policy and distant from those of a\nweaker policy. By coupling decisions within and across action chunks, BID\nenhances temporal consistency over extended sequences while enabling adaptive\nreplanning in stochastic environments. Experimental results show that BID\nsubstantially outperforms conventional closed-loop operations of two\nstate-of-the-art generative policies across seven simulation benchmarks and two\nreal-world tasks.\n","authors":["Yuejiang Liu","Jubayer Ibn Hamid","Annie Xie","Yoonho Lee","Maximilian Du","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2408.17355v1.pdf","comment":"Project website: https://bid-robot.github.io/"},{"id":"http://arxiv.org/abs/2408.17354v1","updated":"2024-08-30T15:35:09Z","published":"2024-08-30T15:35:09Z","title":"Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language\n  Models for Privacy Leakage","summary":"  Fine-tuning large language models on private data for downstream applications\nposes significant privacy risks in potentially exposing sensitive information.\nSeveral popular community platforms now offer convenient distribution of a\nlarge variety of pre-trained models, allowing anyone to publish without\nrigorous verification. This scenario creates a privacy threat, as pre-trained\nmodels can be intentionally crafted to compromise the privacy of fine-tuning\ndatasets. In this study, we introduce a novel poisoning technique that uses\nmodel-unlearning as an attack tool. This approach manipulates a pre-trained\nlanguage model to increase the leakage of private data during the fine-tuning\nprocess. Our method enhances both membership inference and data extraction\nattacks while preserving model utility. Experimental results across different\nmodels, datasets, and fine-tuning setups demonstrate that our attacks\nsignificantly surpass baseline performance. This work serves as a cautionary\nnote for users who download pre-trained models from unverified sources,\nhighlighting the potential risks involved.\n","authors":["Md Rafi Ur Rashid","Jing Liu","Toshiaki Koike-Akino","Shagufta Mehnaz","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2408.17354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05241v6","updated":"2024-08-30T15:17:11Z","published":"2024-04-08T07:11:33Z","title":"LightFF: Lightweight Inference for Forward-Forward Algorithm","summary":"  The human brain performs tasks with an outstanding energy efficiency, i.e.,\nwith approximately 20 Watts. The state-of-the-art Artificial/Deep Neural\nNetworks (ANN/DNN), on the other hand, have recently been shown to consume\nmassive amounts of energy. The training of these ANNs/DNNs is done almost\nexclusively based on the back-propagation algorithm, which is known to be\nbiologically implausible. This has led to a new generation of forward-only\ntechniques, including the Forward-Forward algorithm. In this paper, we propose\na lightweight inference scheme specifically designed for DNNs trained using the\nForward-Forward algorithm. We have evaluated our proposed lightweight inference\nscheme in the case of the MNIST and CIFAR datasets, as well as two real-world\napplications, namely, epileptic seizure detection and cardiac arrhythmia\nclassification using wearable technologies, where complexity overheads/energy\nconsumption is a major constraint, and demonstrate its relevance. Our code is\navailable at https://github.com/AminAminifar/LightFF.\n","authors":["Amin Aminifar","Baichuan Huang","Azra Abtahi","Amir Aminifar"],"pdf_url":"https://arxiv.org/pdf/2404.05241v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17337v1","updated":"2024-08-30T15:02:22Z","published":"2024-08-30T15:02:22Z","title":"Evaluating Reliability in Medical DNNs: A Critical Analysis of Feature\n  and Confidence-Based OOD Detection","summary":"  Reliable use of deep neural networks (DNNs) for medical image analysis\nrequires methods to identify inputs that differ significantly from the training\ndata, called out-of-distribution (OOD), to prevent erroneous predictions. OOD\ndetection methods can be categorised as either confidence-based (using the\nmodel's output layer for OOD detection) or feature-based (not using the output\nlayer). We created two new OOD benchmarks by dividing the D7P (dermatology) and\nBreastMNIST (ultrasound) datasets into subsets which either contain or don't\ncontain an artefact (rulers or annotations respectively). Models were trained\nwith artefact-free images, and images with the artefacts were used as OOD test\nsets. For each OOD image, we created a counterfactual by manually removing the\nartefact via image processing, to assess the artefact's impact on the model's\npredictions. We show that OOD artefacts can boost a model's softmax confidence\nin its predictions, due to correlations in training data among other factors.\nThis contradicts the common assumption that OOD artefacts should lead to more\nuncertain outputs, an assumption on which most confidence-based methods rely.\nWe use this to explain why feature-based methods (e.g. Mahalanobis score)\ntypically have greater OOD detection performance than confidence-based methods\n(e.g. MCP). However, we also show that feature-based methods typically perform\nworse at distinguishing between inputs that lead to correct and incorrect\npredictions (for both OOD and ID data). Following from these insights, we argue\nthat a combination of feature-based and confidence-based methods should be used\nwithin DNN pipelines to mitigate their respective weaknesses. These project's\ncode and OOD benchmarks are available at:\nhttps://github.com/HarryAnthony/Evaluating_OOD_detection.\n","authors":["Harry Anthony","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2408.17337v1.pdf","comment":"Accepted for the Uncertainty for Safe Utilization of Machine Learning\n  in Medical Imaging (UNSURE 2024) workshop at the MICCAI 2023"},{"id":"http://arxiv.org/abs/2403.06087v2","updated":"2024-08-30T14:46:41Z","published":"2024-03-10T04:17:42Z","title":"Learning the irreversible progression trajectory of Alzheimer's disease","summary":"  Alzheimer's disease (AD) is a progressive and irreversible brain disorder\nthat unfolds over the course of 30 years. Therefore, it is critical to capture\nthe disease progression in an early stage such that intervention can be applied\nbefore the onset of symptoms. Machine learning (ML) models have been shown\neffective in predicting the onset of AD. Yet for subjects with follow-up\nvisits, existing techniques for AD classification only aim for accurate group\nassignment, where the monotonically increasing risk across follow-up visits is\nusually ignored. Resulted fluctuating risk scores across visits violate the\nirreversibility of AD, hampering the trustworthiness of models and also\nproviding little value to understanding the disease progression. To address\nthis issue, we propose a novel regularization approach to predict AD\nlongitudinally. Our technique aims to maintain the expected monotonicity of\nincreasing disease risk during progression while preserving expressiveness.\nSpecifically, we introduce a monotonicity constraint that encourages the model\nto predict disease risk in a consistent and ordered manner across follow-up\nvisits. We evaluate our method using the longitudinal structural MRI and\namyloid-PET imaging data from the Alzheimer's Disease Neuroimaging Initiative\n(ADNI). Our model outperforms existing techniques in capturing the\nprogressiveness of disease risk, and at the same time preserves prediction\naccuracy.\n","authors":["Yipei Wang","Bing He","Shannon Risacher","Andrew Saykin","Jingwen Yan","Xiaoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06087v2.pdf","comment":"accepted by ISBI 2024"},{"id":"http://arxiv.org/abs/2408.17329v1","updated":"2024-08-30T14:42:03Z","published":"2024-08-30T14:42:03Z","title":"Estimation of Cardiac and Non-cardiac Diagnosis from Electrocardiogram\n  Features","summary":"  Introduction: Ensuring timely and accurate diagnosis of medical conditions is\nparamount for effective patient care. Electrocardiogram (ECG) signals are\nfundamental for evaluating a patient's cardiac health and are readily\navailable. Despite this, little attention has been given to the remarkable\npotential of ECG data in detecting non-cardiac conditions.\n  Methods: In our study, we used publicly available datasets (MIMIC-IV-ECG-ICD\nand ECG-VIEW II) to investigate the feasibility of inferring general diagnostic\nconditions from ECG features. To this end, we trained a tree-based model\n(XGBoost) based on ECG features and basic demographic features to estimate a\nwide range of diagnoses, encompassing both cardiac and non-cardiac conditions.\n  Results: Our results demonstrate the reliability of estimating 23 cardiac as\nwell as 21 non-cardiac conditions above 0.7 AUROC in a statistically\nsignificant manner across a wide range of physiological categories. Our\nfindings underscore the predictive potential of ECG data in identifying\nwell-known cardiac conditions. However, even more striking, this research\nrepresents a pioneering effort in systematically expanding the scope of\nECG-based diagnosis to conditions not traditionally associated with the cardiac\nsystem.\n","authors":["Juan Miguel Lopez Alcaraz","Nils Strodthoff"],"pdf_url":"https://arxiv.org/pdf/2408.17329v1.pdf","comment":"4 pages, source code under https://github.com/AI4HealthUOL/CardioDiag"},{"id":"http://arxiv.org/abs/2408.05990v2","updated":"2024-08-30T14:39:24Z","published":"2024-08-12T08:33:09Z","title":"Parameters Inference for Nonlinear Wave Equations with Markovian\n  Switching","summary":"  Traditional partial differential equations with constant coefficients often\nstruggle to capture abrupt changes in real-world phenomena, leading to the\ndevelopment of variable coefficient PDEs and Markovian switching models.\nRecently, research has introduced the concept of PDEs with Markov switching\nmodels, established their well-posedness and presented numerical methods.\nHowever, there has been limited discussion on parameter estimation for the jump\ncoefficients in these models. This paper addresses this gap by focusing on\nparameter inference for the wave equation with Markovian switching. We propose\na Bayesian statistical framework using discrete sparse Bayesian learning to\nestablish its convergence and a uniform error bound. Our method requires fewer\nassumptions and enables independent parameter inference for each segment by\nallowing different underlying structures for the parameter estimation problem\nwithin each segmented time interval. The effectiveness of our approach is\ndemonstrated through three numerical cases, which involve noisy spatiotemporal\ndata from different wave equations with Markovian switching. The results show\nstrong performance in parameter estimation for variable coefficient PDEs.\n","authors":["Yi Zhang","Zhikun Zhang","Xiangjun Wang"],"pdf_url":"https://arxiv.org/pdf/2408.05990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00910v2","updated":"2024-08-30T14:37:26Z","published":"2023-12-01T20:19:12Z","title":"Effectiveness of probabilistic contact tracing in epidemic containment:\n  the role of super-spreaders and transmission path reconstruction","summary":"  The recent COVID-19 pandemic underscores the significance of early-stage\nnon-pharmacological intervention strategies. The widespread use of masks and\nthe systematic implementation of contact tracing strategies provide a\npotentially equally effective and socially less impactful alternative to more\nconventional approaches, such as large-scale mobility restrictions. However,\nmanual contact tracing faces strong limitations in accessing the network of\ncontacts, and the scalability of currently implemented protocols for\nsmartphone-based digital contact tracing becomes impractical during the rapid\nexpansion phases of the outbreaks, due to the surge in exposure notifications\nand associated tests. A substantial improvement in digital contact tracing can\nbe obtained through the integration of probabilistic techniques for risk\nassessment that can more effectively guide the allocation of new diagnostic\ntests. In this study, we first quantitatively analyze the diagnostic and social\ncosts associated with these containment measures based on contact tracing,\nemploying three state-of-the-art models of SARS-CoV-2 spreading. Our results\nsuggest that probabilistic techniques allow for more effective mitigation at a\nlower cost. Secondly, our findings reveal a remarkable efficacy of\nprobabilistic contact-tracing techniques in performing backward and multi-step\ntracing and capturing super-spreading events.\n","authors":["A. P. Muntoni","F. Mazza","A. Braunstein","G. Catania","L. Dall'Asta"],"pdf_url":"https://arxiv.org/pdf/2312.00910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18249v2","updated":"2024-08-30T14:36:08Z","published":"2024-06-26T10:51:44Z","title":"Foundational Models for Pathology and Endoscopy Images: Application for\n  Gastric Inflammation","summary":"  The integration of artificial intelligence (AI) in medical diagnostics\nrepresents a significant advancement in managing upper gastrointestinal (GI)\ncancer, a major cause of global cancer mortality. Specifically for gastric\ncancer (GC), chronic inflammation causes changes in the mucosa such as atrophy,\nintestinal metaplasia (IM), dysplasia and ultimately cancer. Early detection\nthrough endoscopic regular surveillance is essential for better outcomes.\nFoundation models (FM), which are machine or deep learning models trained on\ndiverse data and applicable to broad use cases, offer a promising solution to\nenhance the accuracy of endoscopy and its subsequent pathology image analysis.\nThis review explores the recent advancements, applications, and challenges\nassociated with FM in endoscopy and pathology imaging. We started by\nelucidating the core principles and architectures underlying these models,\nincluding their training methodologies and the pivotal role of large-scale data\nin developing their predictive capabilities. Moreover, this work discusses\nemerging trends and future research directions, emphasizing the integration of\nmultimodal data, the development of more robust and equitable models, and the\npotential for real-time diagnostic support. This review aims to provide a\nroadmap for researchers and practitioners in navigating the complexities of\nincorporating FM into clinical practice for prevention/management of GC cases,\nthereby improving patient outcomes.\n","authors":["Hamideh Kerdegari","Kyle Higgins","Dennis Veselkov","Ivan Laponogov","Inese Polaka","Miguel Coimbra","Junior Andrea Pescino","Marcis Leja","Mario Dinis-Ribeiro","Tania Fleitas Kanonnikoff","Kirill Veselkov"],"pdf_url":"https://arxiv.org/pdf/2406.18249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17324v1","updated":"2024-08-30T14:35:01Z","published":"2024-08-30T14:35:01Z","title":"Modularity in Transformers: Investigating Neuron Separability &\n  Specialization","summary":"  Transformer models are increasingly prevalent in various applications, yet\nour understanding of their internal workings remains limited. This paper\ninvestigates the modularity and task specialization of neurons within\ntransformer architectures, focusing on both vision (ViT) and language (Mistral\n7B) models. Using a combination of selective pruning and MoEfication clustering\ntechniques, we analyze the overlap and specialization of neurons across\ndifferent tasks and data subsets. Our findings reveal evidence of task-specific\nneuron clusters, with varying degrees of overlap between related tasks. We\nobserve that neuron importance patterns persist to some extent even in randomly\ninitialized models, suggesting an inherent structure that training refines.\nAdditionally, we find that neuron clusters identified through MoEfication\ncorrespond more strongly to task-specific neurons in earlier and later layers\nof the models. This work contributes to a more nuanced understanding of\ntransformer internals and offers insights into potential avenues for improving\nmodel interpretability and efficiency.\n","authors":["Nicholas Pochinkov","Thomas Jones","Mohammed Rashidur Rahman"],"pdf_url":"https://arxiv.org/pdf/2408.17324v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.17322v1","updated":"2024-08-30T14:32:25Z","published":"2024-08-30T14:32:25Z","title":"Investigating Neuron Ablation in Attention Heads: The Case for Peak\n  Activation Centering","summary":"  The use of transformer-based models is growing rapidly throughout society.\nWith this growth, it is important to understand how they work, and in\nparticular, how the attention mechanisms represent concepts. Though there are\nmany interpretability methods, many look at models through their neuronal\nactivations, which are poorly understood. We describe different lenses through\nwhich to view neuron activations, and investigate the effectiveness in language\nmodels and vision transformers through various methods of neural ablation: zero\nablation, mean ablation, activation resampling, and a novel approach we term\n'peak ablation'. Through experimental analysis, we find that in different\nregimes and models, each method can offer the lowest degradation of model\nperformance compared to other methods, with resampling usually causing the most\nsignificant performance deterioration. We make our code available at\nhttps://github.com/nickypro/investigating-ablation.\n","authors":["Nicholas Pochinkov","Ben Pasero","Skylar Shibayama"],"pdf_url":"https://arxiv.org/pdf/2408.17322v1.pdf","comment":"9 pages, 2 figures, XAI World Conference 2024 Late-Breaking Work"},{"id":"http://arxiv.org/abs/2301.04204v2","updated":"2024-08-30T14:30:13Z","published":"2023-01-10T20:43:29Z","title":"A Newton-CG based barrier-augmented Lagrangian method for general\n  nonconvex conic optimization","summary":"  In this paper we consider finding an approximate second-order stationary\npoint (SOSP) of general nonconvex conic optimization that minimizes a twice\ndifferentiable function subject to nonlinear equality constraints and also a\nconvex conic constraint. In particular, we propose a Newton-conjugate gradient\n(Newton-CG) based barrier-augmented Lagrangian method for finding an\napproximate SOSP of this problem. Under some mild assumptions, we show that our\nmethod enjoys a total inner iteration complexity of $\\widetilde{\\cal\nO}(\\epsilon^{-11/2})$ and an operation complexity of $\\widetilde{\\cal\nO}(\\epsilon^{-11/2}\\min\\{n,\\epsilon^{-5/4}\\})$ for finding an\n$(\\epsilon,\\sqrt{\\epsilon})$-SOSP of general nonconvex conic optimization with\nhigh probability. Moreover, under a constraint qualification, these complexity\nbounds are improved to $\\widetilde{\\cal O}(\\epsilon^{-7/2})$ and\n$\\widetilde{\\cal O}(\\epsilon^{-7/2}\\min\\{n,\\epsilon^{-3/4}\\})$, respectively.\nTo the best of our knowledge, this is the first study on the complexity of\nfinding an approximate SOSP of general nonconvex conic optimization.\nPreliminary numerical results are presented to demonstrate superiority of the\nproposed method over first-order methods in terms of solution quality.\n","authors":["Chuan He","Heng Huang","Zhaosong Lu"],"pdf_url":"https://arxiv.org/pdf/2301.04204v2.pdf","comment":"To appear in Computational Optimization and Applications. arXiv admin\n  note: text overlap with arXiv:2301.03139"},{"id":"http://arxiv.org/abs/2401.05218v2","updated":"2024-08-30T14:27:33Z","published":"2024-01-10T15:34:42Z","title":"Invariant Causal Prediction with Local Models","summary":"  We consider the task of identifying the causal parents of a target variable\namong a set of candidates from observational data. Our main assumption is that\nthe candidate variables are observed in different environments which may, under\ncertain assumptions, be regarded as interventions on the observed system. We\nassume a linear relationship between target and candidates, which can be\ndifferent in each environment with the only restriction that the causal\nstructure is invariant across environments. Within our proposed setting we\nprovide sufficient conditions for identifiability of the causal parents and\nintroduce a practical method called L-ICP ($\\textbf{L}$ocalized\n$\\textbf{I}$nvariant $\\textbf{Ca}$usal $\\textbf{P}$rediction), which is based\non a hypothesis test for parent identification using a ratio of minimum and\nmaximum statistics. We then show in a simplified setting that the statistical\npower of L-ICP converges exponentially fast in the sample size, and finally we\nanalyze the behavior of L-ICP experimentally in more general settings.\n","authors":["Alexander Mey","Rui Manuel Castro"],"pdf_url":"https://arxiv.org/pdf/2401.05218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17313v1","updated":"2024-08-30T14:18:34Z","published":"2024-08-30T14:18:34Z","title":"Fair Best Arm Identification with Fixed Confidence","summary":"  In this work, we present a novel framework for Best Arm Identification (BAI)\nunder fairness constraints, a setting that we refer to as \\textit{F-BAI} (fair\nBAI). Unlike traditional BAI, which solely focuses on identifying the optimal\narm with minimal sample complexity, F-BAI also includes a set of fairness\nconstraints. These constraints impose a lower limit on the selection rate of\neach arm and can be either model-agnostic or model-dependent. For this setting,\nwe establish an instance-specific sample complexity lower bound and analyze the\n\\textit{price of fairness}, quantifying how fairness impacts sample complexity.\nBased on the sample complexity lower bound, we propose F-TaS, an algorithm\nprovably matching the sample complexity lower bound, while ensuring that the\nfairness constraints are satisfied. Numerical results, conducted using both a\nsynthetic model and a practical wireless scheduling application, show the\nefficiency of F-TaS in minimizing the sample complexity while achieving low\nfairness violations.\n","authors":["Alessio Russo","Filippo Vannella"],"pdf_url":"https://arxiv.org/pdf/2408.17313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17311v1","updated":"2024-08-30T14:15:48Z","published":"2024-08-30T14:15:48Z","title":"Structuring a Training Strategy to Robustify Perception Models with\n  Realistic Image Augmentations","summary":"  Advancing Machine Learning (ML)-based perception models for autonomous\nsystems necessitates addressing weak spots within the models, particularly in\nchallenging Operational Design Domains (ODDs). These are environmental\noperating conditions of an autonomous vehicle which can contain difficult\nconditions, e.g., lens flare at night or objects reflected in a wet street.\nThis report introduces a novel methodology for training with augmentations to\nenhance model robustness and performance in such conditions. The proposed\napproach leverages customized physics-based augmentation functions, to generate\nrealistic training data that simulates diverse ODD scenarios.\n  We present a comprehensive framework that includes identifying weak spots in\nML models, selecting suitable augmentations, and devising effective training\nstrategies. The methodology integrates hyperparameter optimization and latent\nspace optimization to fine-tune augmentation parameters, ensuring they\nmaximally improve the ML models' performance. Experimental results demonstrate\nimprovements in model performance, as measured by commonly used metrics such as\nmean Average Precision (mAP) and mean Intersection over Union (mIoU) on\nopen-source object detection and semantic segmentation models and datasets.\n  Our findings emphasize that optimal training strategies are model- and\ndata-specific and highlight the benefits of integrating augmentations into the\ntraining pipeline. By incorporating augmentations, we observe enhanced\nrobustness of ML-based perception models, making them more resilient to edge\ncases encountered in real-world ODDs. This work underlines the importance of\ncustomized augmentations and offers an effective solution for improving the\nsafety and reliability of autonomous driving functions.\n","authors":["Ahmed Hammam","Bharathwaj Krishnaswami Sreedhar","Nura Kawa","Tim Patzelt","Oliver De Candido"],"pdf_url":"https://arxiv.org/pdf/2408.17311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2009.07799v3","updated":"2024-08-30T14:12:30Z","published":"2020-09-16T16:48:28Z","title":"On the Curse of Memory in Recurrent Neural Networks: Approximation and\n  Optimization Analysis","summary":"  We study the approximation properties and optimization dynamics of recurrent\nneural networks (RNNs) when applied to learn input-output relationships in\ntemporal data. We consider the simple but representative setting of using\ncontinuous-time linear RNNs to learn from data generated by linear\nrelationships. Mathematically, the latter can be understood as a sequence of\nlinear functionals. We prove a universal approximation theorem of such linear\nfunctionals, and characterize the approximation rate and its relation with\nmemory. Moreover, we perform a fine-grained dynamical analysis of training\nlinear RNNs, which further reveal the intricate interactions between memory and\nlearning. A unifying theme uncovered is the non-trivial effect of memory, a\nnotion that can be made precise in our framework, on approximation and\noptimization: when there is long term memory in the target, it takes a large\nnumber of neurons to approximate it. Moreover, the training process will suffer\nfrom slow downs. In particular, both of these effects become exponentially more\npronounced with memory - a phenomenon we call the \"curse of memory\". These\nanalyses represent a basic step towards a concrete mathematical understanding\nof new phenomenon that may arise in learning temporal relationships using\nrecurrent architectures.\n","authors":["Zhong Li","Jiequn Han","Weinan E","Qianxiao Li"],"pdf_url":"https://arxiv.org/pdf/2009.07799v3.pdf","comment":"Updated to include the condition $\\sup_n \\| \\boldsymbol{x}(n)\n  \\|_{\\mathcal{X}} \\leq 1$ in the definition of regularity, which excludes the\n  trivial case where only the zero functional is regular. Fixed various typos\n  and improved clarity"},{"id":"http://arxiv.org/abs/2408.17307v1","updated":"2024-08-30T14:11:12Z","published":"2024-08-30T14:11:12Z","title":"Hybridizing Base-Line 2D-CNN Model with Cat Swarm Optimization for\n  Enhanced Advanced Persistent Threat Detection","summary":"  In the realm of cyber-security, detecting Advanced Persistent Threats (APTs)\nremains a formidable challenge due to their stealthy and sophisticated nature.\nThis research paper presents an innovative approach that leverages\nConvolutional Neural Networks (CNNs) with a 2D baseline model, enhanced by the\ncutting-edge Cat Swarm Optimization (CSO) algorithm, to significantly improve\nAPT detection accuracy. By seamlessly integrating the 2D-CNN baseline model\nwith CSO, we unlock the potential for unprecedented accuracy and efficiency in\nAPT detection. The results unveil an impressive accuracy score of $98.4\\%$,\nmarking a significant enhancement in APT detection across various attack\nstages, illuminating a path forward in combating these relentless and\nsophisticated threats.\n","authors":["Ali M. Bakhiet","Salah A. Aly"],"pdf_url":"https://arxiv.org/pdf/2408.17307v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.05442v3","updated":"2024-08-30T14:11:08Z","published":"2022-07-12T10:18:36Z","title":"Wasserstein multivariate auto-regressive models for modeling\n  distributional time series","summary":"  This paper is focused on the statistical analysis of data consisting of a\ncollection of multiple series of probability measures that are indexed by\ndistinct time instants and supported over a bounded interval of the real line.\nBy modeling these time-dependent probability measures as random objects in the\nWasserstein space, we propose a new auto-regressive model for the statistical\nanalysis of multivariate distributional time series. Using the theory of\niterated random function systems, results on the existence, uniqueness and\nstationarity of the solution of such a model are provided. We also propose a\nconsistent estimator for the auto-regressive coefficients of this model. Due to\nthe simplex constraints that we impose on the model coefficients, the proposed\nestimator that is learned under these constraints, naturally has a sparse\nstructure. The sparsity allows the application of the proposed model in\nlearning a graph of temporal dependency from multivariate distributional time\nseries. We explore the numerical performances of our estimation procedure using\nsimulated data. To shed some light on the benefits of our approach for real\ndata analysis, we also apply this methodology to a data set made of\nobservations from age distribution in different countries.\n","authors":["Yiye Jiang","JÃ©rÃ©mie Bigot"],"pdf_url":"https://arxiv.org/pdf/2207.05442v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.11498v3","updated":"2024-08-30T14:07:35Z","published":"2022-12-22T06:18:41Z","title":"Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with\n  Robotic and Human Co-Workers","summary":"  We consider a warehouse in which dozens of mobile robots and human pickers\nwork together to collect and deliver items within the warehouse. The\nfundamental problem we tackle, called the order-picking problem, is how these\nworker agents must coordinate their movement and actions in the warehouse to\nmaximise performance in this task. Established industry methods using heuristic\napproaches require large engineering efforts to optimise for innately variable\nwarehouse configurations. In contrast, multi-agent reinforcement learning\n(MARL) can be flexibly applied to diverse warehouse configurations (e.g. size,\nlayout, number/types of workers, item replenishment frequency), and different\ntypes of order-picking paradigms (e.g. Goods-to-Person and Person-to-Goods), as\nthe agents can learn how to cooperate optimally through experience. We develop\nhierarchical MARL algorithms in which a manager agent assigns goals to worker\nagents, and the policies of the manager and workers are co-trained toward\nmaximising a global objective (e.g. pick rate). Our hierarchical algorithms\nachieve significant gains in sample efficiency over baseline MARL algorithms\nand overall pick rates over multiple established industry heuristics in a\ndiverse set of warehouse configurations and different order-picking paradigms.\n","authors":["Aleksandar Krnjaic","Raul D. Steleac","Jonathan D. Thomas","Georgios Papoudakis","Lukas SchÃ¤fer","Andrew Wing Keung To","Kuan-Ho Lao","Murat Cubuktepe","Matthew Haley","Peter BÃ¶rsting","Stefano V. Albrecht"],"pdf_url":"https://arxiv.org/pdf/2212.11498v3.pdf","comment":"IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2024"},{"id":"http://arxiv.org/abs/2402.19037v2","updated":"2024-08-30T13:58:55Z","published":"2024-02-29T11:02:47Z","title":"A Deep-Learning Technique to Locate Cryptographic Operations in\n  Side-Channel Traces","summary":"  Side-channel attacks allow extracting secret information from the execution\nof cryptographic primitives by correlating the partially known computed data\nand the measured side-channel signal. However, to set up a successful\nside-channel attack, the attacker has to perform i) the challenging task of\nlocating the time instant in which the target cryptographic primitive is\nexecuted inside a side-channel trace and then ii)the time-alignment of the\nmeasured data on that time instant. This paper presents a novel deep-learning\ntechnique to locate the time instant in which the target computed cryptographic\noperations are executed in the side-channel trace. In contrast to\nstate-of-the-art solutions, the proposed methodology works even in the presence\nof trace deformations obtained through random delay insertion techniques. We\nvalidated our proposal through a successful attack against a variety of\nunprotected and protected cryptographic primitives that have been executed on\nan FPGA-implemented system-on-chip featuring a RISC-V CPU.\n","authors":["Giuseppe Chiari","Davide Galli","Francesco Lattari","Matteo Matteucci","Davide Zoni"],"pdf_url":"https://arxiv.org/pdf/2402.19037v2.pdf","comment":"6 pages, 3 figures. Presented at DATE24"},{"id":"http://arxiv.org/abs/2408.17298v1","updated":"2024-08-30T13:55:19Z","published":"2024-08-30T13:55:19Z","title":"Accelerating the discovery of steady-states of planetary interior\n  dynamics with machine learning","summary":"  Simulating mantle convection often requires reaching a computationally\nexpensive steady-state, crucial for deriving scaling laws for thermal and\ndynamical flow properties and benchmarking numerical solutions. The strong\ntemperature dependence of the rheology of mantle rocks causes viscosity\nvariations of several orders of magnitude, leading to a slow-evolving stagnant\nlid where heat conduction dominates, overlying a rapidly-evolving and strongly\nconvecting region. Time-stepping methods, while effective for fluids with\nconstant viscosity, are hindered by the Courant criterion, which restricts the\ntime step based on the system's maximum velocity and grid size. Consequently,\nachieving steady-state requires a large number of time steps due to the\ndisparate time scales governing the stagnant and convecting regions.\n  We present a concept for accelerating mantle convection simulations using\nmachine learning. We generate a dataset of 128 two-dimensional simulations with\nmixed basal and internal heating, and pressure- and temperature-dependent\nviscosity. We train a feedforward neural network on 97 simulations to predict\nsteady-state temperature profiles. These can then be used to initialize\nnumerical time stepping methods for different simulation parameters. Compared\nto typical initializations, the number of time steps required to reach\nsteady-state is reduced by a median factor of 3.75. The benefit of this method\nlies in requiring very few simulations to train on, providing a solution with\nno prediction error as we initialize a numerical method, and posing minimal\ncomputational overhead at inference time. We demonstrate the effectiveness of\nour approach and discuss the potential implications for accelerated simulations\nfor advancing mantle convection research.\n","authors":["Siddhant Agarwal","Nicola Tosi","Christian HÃ¼ttig","David S. Greenberg","Ali Can Bekar"],"pdf_url":"https://arxiv.org/pdf/2408.17298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17286v1","updated":"2024-08-30T13:33:18Z","published":"2024-08-30T13:33:18Z","title":"Stationary Policies are Optimal in Risk-averse Total-reward MDPs with\n  EVaR","summary":"  Optimizing risk-averse objectives in discounted MDPs is challenging because\nmost models do not admit direct dynamic programming equations and require\ncomplex history-dependent policies. In this paper, we show that the risk-averse\n{\\em total reward criterion}, under the Entropic Risk Measure (ERM) and\nEntropic Value at Risk (EVaR) risk measures, can be optimized by a stationary\npolicy, making it simple to analyze, interpret, and deploy. We propose\nexponential value iteration, policy iteration, and linear programming to\ncompute optimal policies. In comparison with prior work, our results only\nrequire the relatively mild condition of transient MDPs and allow for {\\em\nboth} positive and negative rewards. Our results indicate that the total reward\ncriterion may be preferable to the discounted criterion in a broad range of\nrisk-averse reinforcement learning domains.\n","authors":["Xihong Su","Marek Petrik","Julien Grand-ClÃ©ment"],"pdf_url":"https://arxiv.org/pdf/2408.17286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17285v1","updated":"2024-08-30T13:33:07Z","published":"2024-08-30T13:33:07Z","title":"Image-Perfect Imperfections: Safety, Bias, and Authenticity in the\n  Shadow of Text-To-Image Model Evolution","summary":"  Text-to-image models, such as Stable Diffusion (SD), undergo iterative\nupdates to improve image quality and address concerns such as safety.\nImprovements in image quality are straightforward to assess. However, how model\nupdates resolve existing concerns and whether they raise new questions remain\nunexplored. This study takes an initial step in investigating the evolution of\ntext-to-image models from the perspectives of safety, bias, and authenticity.\nOur findings, centered on Stable Diffusion, indicate that model updates paint a\nmixed picture. While updates progressively reduce the generation of unsafe\nimages, the bias issue, particularly in gender, intensifies. We also find that\nnegative stereotypes either persist within the same Non-White race group or\nshift towards other Non-White race groups through SD updates, yet with minimal\nassociation of these traits with the White race group. Additionally, our\nevaluation reveals a new concern stemming from SD updates: State-of-the-art\nfake image detectors, initially trained for earlier SD versions, struggle to\nidentify fake images generated by updated versions. We show that fine-tuning\nthese detectors on fake images generated by updated versions achieves at least\n96.6\\% accuracy across various SD versions, addressing this issue. Our insights\nhighlight the importance of continued efforts to mitigate biases and\nvulnerabilities in evolving text-to-image models.\n","authors":["Yixin Wu","Yun Shen","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.17285v1.pdf","comment":"To Appear in the ACM Conference on Computer and Communications\n  Security, October 14-18, 2024"},{"id":"http://arxiv.org/abs/2401.05735v3","updated":"2024-08-30T13:28:38Z","published":"2024-01-11T08:36:15Z","title":"Object-Centric Diffusion for Efficient Video Editing","summary":"  Diffusion-based video editing have reached impressive quality and can\ntransform either the global style, local structure, and attributes of given\nvideo inputs, following textual edit prompts. However, such solutions typically\nincur heavy memory and computational costs to generate temporally-coherent\nframes, either in the form of diffusion inversion and/or cross-frame attention.\nIn this paper, we conduct an analysis of such inefficiencies, and suggest\nsimple yet effective modifications that allow significant speed-ups whilst\nmaintaining quality. Moreover, we introduce Object-Centric Diffusion, to fix\ngeneration artifacts and further reduce latency by allocating more computations\ntowards foreground edited regions, arguably more important for perceptual\nquality. We achieve this by two novel proposals: i) Object-Centric Sampling,\ndecoupling the diffusion steps spent on salient or background regions and\nspending most on the former, and ii) Object-Centric Token Merging, which\nreduces cost of cross-frame attention by fusing redundant tokens in unimportant\nbackground regions. Both techniques are readily applicable to a given video\nediting model without retraining, and can drastically reduce its memory and\ncomputational cost. We evaluate our proposals on inversion-based and\ncontrol-signal-based editing pipelines, and show a latency reduction up to 10x\nfor a comparable synthesis quality. Project page:\nqualcomm-ai-research.github.io/object-centric-diffusion.\n","authors":["Kumara Kahatapitiya","Adil Karjauv","Davide Abati","Fatih Porikli","Yuki M. Asano","Amirhossein Habibian"],"pdf_url":"https://arxiv.org/pdf/2401.05735v3.pdf","comment":"ECCV24"},{"id":"http://arxiv.org/abs/2408.17276v1","updated":"2024-08-30T13:22:08Z","published":"2024-08-30T13:22:08Z","title":"Minimax and Communication-Efficient Distributed Best Subset Selection\n  with Oracle Property","summary":"  The explosion of large-scale data in fields such as finance, e-commerce, and\nsocial media has outstripped the processing capabilities of single-machine\nsystems, driving the need for distributed statistical inference methods.\nTraditional approaches to distributed inference often struggle with achieving\ntrue sparsity in high-dimensional datasets and involve high computational\ncosts. We propose a novel, two-stage, distributed best subset selection\nalgorithm to address these issues. Our approach starts by efficiently\nestimating the active set while adhering to the $\\ell_0$ norm-constrained\nsurrogate likelihood function, effectively reducing dimensionality and\nisolating key variables. A refined estimation within the active set follows,\nensuring sparse estimates and matching the minimax $\\ell_2$ error bound. We\nintroduce a new splicing technique for adaptive parameter selection to tackle\nsubproblems under $\\ell_0$ constraints and a Generalized Information Criterion\n(GIC). Our theoretical and numerical studies show that the proposed algorithm\ncorrectly finds the true sparsity pattern, has the oracle property, and greatly\nlowers communication costs. This is a big step forward in distributed sparse\nestimation.\n","authors":["Jingguo Lan","Hongmei Lin","Xueqin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.17276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17274v1","updated":"2024-08-30T13:19:20Z","published":"2024-08-30T13:19:20Z","title":"The Transferability of Downsampling Sparse Graph Convolutional Networks","summary":"  In this paper, we propose a large-scale sparse graph downsampling method\nbased on a sparse random graph model, which allows for the adjustment of\ndifferent sparsity levels. We combine sparsity and topological similarity: the\nsparse graph model reduces the node connection probability as the graph size\nincreases, while the downsampling method preserves a specific topological\nconnection pattern during this change. Based on the downsampling method, we\nderive a theoretical transferability bound about downsampling sparse graph\nconvolutional networks (GCNs), that higher sampling rates, greater average\ndegree expectations, and smaller initial graph sizes lead to better\ndownsampling transferability performance.\n","authors":["Qinji Shu","Hang Sheng","Hui Feng","Bo Hu"],"pdf_url":"https://arxiv.org/pdf/2408.17274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17271v1","updated":"2024-08-30T13:17:57Z","published":"2024-08-30T13:17:57Z","title":"Equation identification for fluid flows via physics-informed neural\n  networks","summary":"  Scientific machine learning (SciML) methods such as physics-informed neural\nnetworks (PINNs) are used to estimate parameters of interest from governing\nequations and small quantities of data. However, there has been little work in\nassessing how well PINNs perform for inverse problems across wide ranges of\ngoverning equations across the mathematical sciences. We present a new and\nchallenging benchmark problem for inverse PINNs based on a parametric sweep of\nthe 2D Burgers' equation with rotational flow. We show that a novel strategy\nthat alternates between first- and second-order optimization proves superior to\ntypical first-order strategies for estimating parameters. In addition, we\npropose a novel data-driven method to characterize PINN effectiveness in the\ninverse setting. PINNs' physics-informed regularization enables them to\nleverage small quantities of data more efficiently than the data-driven\nbaseline. However, both PINNs and the baseline can fail to recover parameters\nfor highly inviscid flows, motivating the need for further development of PINN\nmethods.\n","authors":["Alexander New","Marisel VillafaÃ±e-Delgado","Charles Shugert"],"pdf_url":"https://arxiv.org/pdf/2408.17271v1.pdf","comment":"Published at ICML 2024 AI4Science:\n  https://openreview.net/forum?id=XsvCLEYH3O"},{"id":"http://arxiv.org/abs/2404.08981v2","updated":"2024-08-30T13:06:28Z","published":"2024-04-13T12:09:37Z","title":"Fast Fishing: Approximating BAIT for Efficient and Scalable Deep Active\n  Image Classification","summary":"  Deep active learning (AL) seeks to minimize the annotation costs for training\ndeep neural networks. BAIT, a recently proposed AL strategy based on the Fisher\nInformation, has demonstrated impressive performance across various datasets.\nHowever, BAIT's high computational and memory requirements hinder its\napplicability on large-scale classification tasks, resulting in current\nresearch neglecting BAIT in their evaluation. This paper introduces two methods\nto enhance BAIT's computational efficiency and scalability. Notably, we\nsignificantly reduce its time complexity by approximating the Fisher\nInformation. In particular, we adapt the original formulation by i) taking the\nexpectation over the most probable classes, and ii) constructing a binary\nclassification task, leading to an alternative likelihood for gradient\ncomputations. Consequently, this allows the efficient use of BAIT on\nlarge-scale datasets, including ImageNet. Our unified and comprehensive\nevaluation across a variety of datasets demonstrates that our approximations\nachieve strong performance with considerably reduced time complexity.\nFurthermore, we provide an extensive open-source toolbox that implements recent\nstate-of-the-art AL strategies, available at\nhttps://github.com/dhuseljic/dal-toolbox.\n","authors":["Denis Huseljic","Paul Hahn","Marek Herde","Lukas Rauch","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2404.08981v2.pdf","comment":"Accepted at ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2408.17258v1","updated":"2024-08-30T12:56:17Z","published":"2024-08-30T12:56:17Z","title":"Joint Estimation and Prediction of City-wide Delivery Demand: A Large\n  Language Model Empowered Graph-based Learning Approach","summary":"  The proliferation of e-commerce and urbanization has significantly\nintensified delivery operations in urban areas, boosting the volume and\ncomplexity of delivery demand. Data-driven predictive methods, especially those\nutilizing machine learning techniques, have emerged to handle these\ncomplexities in urban delivery demand management problems. One particularly\npressing problem that has not yet been sufficiently studied is the joint\nestimation and prediction of city-wide delivery demand. To this end, we\nformulate this problem as a graph-based spatiotemporal learning task. First, a\nmessage-passing neural network model is formalized to capture the interaction\nbetween demand patterns of associated regions. Second, by exploiting recent\nadvances in large language models, we extract general geospatial knowledge\nencodings from the unstructured locational data and integrate them into the\ndemand predictor. Last, to encourage the cross-city transferability of the\nmodel, an inductive training scheme is developed in an end-to-end routine.\nExtensive empirical results on two real-world delivery datasets, including\neight cities in China and the US, demonstrate that our model significantly\noutperforms state-of-the-art baselines in these challenging tasks.\n","authors":["Tong Nie","Junlin He","Yuewen Mei","Guoyang Qin","Guilong Li","Jian Sun","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2408.17258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17255v1","updated":"2024-08-30T12:53:40Z","published":"2024-08-30T12:53:40Z","title":"Self-supervised learning for crystal property prediction via denoising","summary":"  Accurate prediction of the properties of crystalline materials is crucial for\ntargeted discovery, and this prediction is increasingly done with data-driven\nmodels. However, for many properties of interest, the number of materials for\nwhich a specific property has been determined is much smaller than the number\nof known materials. To overcome this disparity, we propose a novel\nself-supervised learning (SSL) strategy for material property prediction. Our\napproach, crystal denoising self-supervised learning (CDSSL), pretrains\npredictive models (e.g., graph networks) with a pretext task based on\nrecovering valid material structures when given perturbed versions of these\nstructures. We demonstrate that CDSSL models out-perform models trained without\nSSL, across material types, properties, and dataset sizes.\n","authors":["Alexander New","Nam Q. Le","Michael J. Pekala","Christopher D. Stiles"],"pdf_url":"https://arxiv.org/pdf/2408.17255v1.pdf","comment":"Published at ICML 2024 AI4Science:\n  https://openreview.net/forum?id=yML9ufAEoV"},{"id":"http://arxiv.org/abs/2304.01762v3","updated":"2024-08-30T12:51:53Z","published":"2023-04-04T12:51:35Z","title":"Incorporating Unlabelled Data into Bayesian Neural Networks","summary":"  Conventional Bayesian Neural Networks (BNNs) are unable to leverage\nunlabelled data to improve their predictions. To overcome this limitation, we\nintroduce Self-Supervised Bayesian Neural Networks, which use unlabelled data\nto learn models with suitable prior predictive distributions. This is achieved\nby leveraging contrastive pretraining techniques and optimising a variational\nlower bound. We then show that the prior predictive distributions of\nself-supervised BNNs capture problem semantics better than conventional BNN\npriors. In turn, our approach offers improved predictive performance over\nconventional BNNs, especially in low-budget regimes.\n","authors":["Mrinank Sharma","Tom Rainforth","Yee Whye Teh","Vincent Fortuin"],"pdf_url":"https://arxiv.org/pdf/2304.01762v3.pdf","comment":"Published in the Transactions on Machine Learning Research"},{"id":"http://arxiv.org/abs/2408.17246v1","updated":"2024-08-30T12:40:12Z","published":"2024-08-30T12:40:12Z","title":"Learning and Verifying Maximal Taylor-Neural Lyapunov functions","summary":"  We introduce a novel neural network architecture, termed Taylor-neural\nLyapunov functions, designed to approximate Lyapunov functions with formal\ncertification. This architecture innovatively encodes local approximations and\nextends them globally by leveraging neural networks to approximate the\nresiduals. Our method recasts the problem of estimating the largest region of\nattraction - specifically for maximal Lyapunov functions - into a learning\nproblem, ensuring convergence around the origin through robust control theory.\nPhysics-informed machine learning techniques further refine the estimation of\nthe largest region of attraction. Remarkably, this method is versatile,\noperating effectively even without simulated data points. We validate the\nefficacy of our approach by providing numerical certificates of convergence\nacross multiple examples. Our proposed methodology not only competes closely\nwith state-of-the-art approaches, such as sum-of-squares and LyZNet, but also\nachieves comparable results even in the absence of simulated data. This work\nrepresents a significant advancement in control theory, with broad potential\napplications in the design of stable control systems and beyond.\n","authors":["Matthieu Barreau","Nicola Bastianello"],"pdf_url":"https://arxiv.org/pdf/2408.17246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12614v4","updated":"2024-08-30T12:40:04Z","published":"2024-06-18T13:43:22Z","title":"EUvsDisinfo: A Dataset for Multilingual Detection of Pro-Kremlin\n  Disinformation in News Articles","summary":"  This work introduces EUvsDisinfo, a multilingual dataset of disinformation\narticles originating from pro-Kremlin outlets, along with trustworthy articles\nfrom credible / less biased sources. It is sourced directly from the debunk\narticles written by experts leading the EUvsDisinfo project. Our dataset is the\nlargest to-date resource in terms of the overall number of articles and\ndistinct languages. It also provides the largest topical and temporal coverage.\nUsing this dataset, we investigate the dissemination of pro-Kremlin\ndisinformation across different languages, uncovering language-specific\npatterns targeting certain disinformation topics. We further analyse the\nevolution of topic distribution over an eight-year period, noting a significant\nsurge in disinformation content before the full-scale invasion of Ukraine in\n2022. Lastly, we demonstrate the dataset's applicability in training models to\neffectively distinguish between disinformation and trustworthy content in\nmultilingual settings.\n","authors":["JoÃ£o A. Leite","Olesya Razuvayevskaya","Kalina Bontcheva","Carolina Scarton"],"pdf_url":"https://arxiv.org/pdf/2406.12614v4.pdf","comment":"Published at CIKM 2024"},{"id":"http://arxiv.org/abs/2408.17244v1","updated":"2024-08-30T12:36:00Z","published":"2024-08-30T12:36:00Z","title":"Categorical data clustering: 25 years beyond K-modes","summary":"  The clustering of categorical data is a common and important task in computer\nscience, offering profound implications across a spectrum of applications.\nUnlike purely numerical datasets, categorical data often lack inherent ordering\nas in nominal data, or have varying levels of order as in ordinal data, thus\nrequiring specialized methodologies for efficient organization and analysis.\nThis review provides a comprehensive synthesis of categorical data clustering\nin the past twenty-five years, starting from the introduction of K-modes. It\nelucidates the pivotal role of categorical data clustering in diverse fields\nsuch as health sciences, natural sciences, social sciences, education,\nengineering and economics. Practical comparisons are conducted for algorithms\nhaving public implementations, highlighting distinguishing clustering\nmethodologies and revealing the performance of recent algorithms on several\nbenchmark categorical datasets. Finally, challenges and opportunities in the\nfield are discussed.\n","authors":["Tai Dinh","Wong Hauchi","Philippe Fournier-Viger","Daniil Lisik","Minh-Quyet Ha","Hieu-Chi Dam","Van-Nam Huynh"],"pdf_url":"https://arxiv.org/pdf/2408.17244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17240v1","updated":"2024-08-30T12:31:25Z","published":"2024-08-30T12:31:25Z","title":"Using Quantum Solved Deep Boltzmann Machines to Increase the Data\n  Efficiency of RL Agents","summary":"  Deep Learning algorithms, such as those used in Reinforcement Learning, often\nrequire large quantities of data to train effectively. In most cases, the\navailability of data is not a significant issue. However, for some contexts,\nsuch as in autonomous cyber defence, we require data efficient methods.\nRecently, Quantum Machine Learning and Boltzmann Machines have been proposed as\nsolutions to this challenge. In this work we build upon the pre-existing work\nto extend the use of Deep Boltzmann Machines to the cutting edge algorithm\nProximal Policy Optimisation in a Reinforcement Learning cyber defence\nenvironment. We show that this approach, when solved using a D-WAVE quantum\nannealer, can lead to a two-fold increase in data efficiency. We therefore\nexpect it to be used by the machine learning and quantum communities who are\nhoping to capitalise on data-efficient Reinforcement Learning methods.\n","authors":["Daniel Kent","Clement O'Rourke","Jake Southall","Kirsty Duncan","Adrian Bedford"],"pdf_url":"https://arxiv.org/pdf/2408.17240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17235v1","updated":"2024-08-30T12:26:23Z","published":"2024-08-30T12:26:23Z","title":"AI-Driven Intrusion Detection Systems (IDS) on the ROAD dataset: A\n  Comparative Analysis for automotive Controller Area Network (CAN)","summary":"  The integration of digital devices in modern vehicles has revolutionized\nautomotive technology, enhancing safety and the overall driving experience. The\nController Area Network (CAN) bus is a central system for managing in-vehicle\ncommunication between the electronic control units (ECUs). However, the CAN\nprotocol poses security challenges due to inherent vulnerabilities, lacking\nencryption and authentication, which, combined with an expanding attack\nsurface, necessitates robust security measures. In response to this challenge,\nnumerous Intrusion Detection Systems (IDS) have been developed and deployed.\nNonetheless, an open, comprehensive, and realistic dataset to test the\neffectiveness of such IDSs remains absent in the existing literature. This\npaper addresses this gap by considering the latest ROAD dataset, containing\nstealthy and sophisticated injections. The methodology involves dataset\nlabelling and the implementation of both state-of-the-art deep learning models\nand traditional machine learning models to show the discrepancy in performance\nbetween the datasets most commonly used in the literature and the ROAD dataset,\na more realistic alternative.\n","authors":["Lorenzo Guerra","Linhan Xu","Pavlo Mozharovskyi","Paolo Bellavista","Thomas Chapuis","Guillaume Duc","Van-Tam Nguyen"],"pdf_url":"https://arxiv.org/pdf/2408.17235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17221v1","updated":"2024-08-30T12:00:36Z","published":"2024-08-30T12:00:36Z","title":"Geometry of Lightning Self-Attention: Identifiability and Dimension","summary":"  We consider function spaces defined by self-attention networks without\nnormalization, and theoretically analyze their geometry. Since these networks\nare polynomial, we rely on tools from algebraic geometry. In particular, we\nstudy the identifiability of deep attention by providing a description of the\ngeneric fibers of the parametrization for an arbitrary number of layers and, as\na consequence, compute the dimension of the function space. Additionally, for a\nsingle-layer model, we characterize the singular and boundary points. Finally,\nwe formulate a conjectural extension of our results to normalized\nself-attention networks, prove it for a single layer, and numerically verify it\nin the deep case.\n","authors":["Nathan W. Henry","Giovanni Luca Marchetti","KathlÃ©n Kohn"],"pdf_url":"https://arxiv.org/pdf/2408.17221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04295v2","updated":"2024-08-30T11:57:47Z","published":"2024-07-05T06:57:30Z","title":"Jailbreak Attacks and Defenses Against Large Language Models: A Survey","summary":"  Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof \"jailbreaking\", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs.\n","authors":["Sibo Yi","Yule Liu","Zhen Sun","Tianshuo Cong","Xinlei He","Jiaxing Song","Ke Xu","Qi Li"],"pdf_url":"https://arxiv.org/pdf/2407.04295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17216v1","updated":"2024-08-30T11:46:39Z","published":"2024-08-30T11:46:39Z","title":"Democratizing AI in Africa: FL for Low-Resource Edge Devices","summary":"  Africa faces significant challenges in healthcare delivery due to limited\ninfrastructure and access to advanced medical technologies. This study explores\nthe use of federated learning to overcome these barriers, focusing on perinatal\nhealth. We trained a fetal plane classifier using perinatal data from five\nAfrican countries: Algeria, Ghana, Egypt, Malawi, and Uganda, along with data\nfrom Spanish hospitals. To incorporate the lack of computational resources in\nthe analysis, we considered a heterogeneous set of devices, including a\nRaspberry Pi and several laptops, for model training. We demonstrate\ncomparative performance between a centralized and a federated model, despite\nthe compute limitations, and a significant improvement in model\ngeneralizability when compared to models trained only locally. These results\nshow the potential for a future implementation at a large scale of a federated\nlearning platform to bridge the accessibility gap and improve model\ngeneralizability with very little requirements.\n","authors":["Jorge Fabila","VÃ­ctor M. Campello","Carlos MartÃ­n-Isla","Johnes Obungoloch","Kinyera Leo","Amodoi Ronald","Karim Lekadir"],"pdf_url":"https://arxiv.org/pdf/2408.17216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15874v2","updated":"2024-08-30T11:18:08Z","published":"2024-08-28T15:44:34Z","title":"Robust Statistical Scaling of Outlier Scores: Improving the Quality of\n  Outlier Probabilities for Outliers (Extended Version)","summary":"  Outlier detection algorithms typically assign an outlier score to each\nobservation in a dataset, indicating the degree to which an observation is an\noutlier. However, these scores are often not comparable across algorithms and\ncan be difficult for humans to interpret. Statistical scaling addresses this\nproblem by transforming outlier scores into outlier probabilities without using\nground-truth labels, thereby improving interpretability and comparability\nacross algorithms. However, the quality of this transformation can be different\nfor outliers and inliers. Missing outliers in scenarios where they are of\nparticular interest - such as healthcare, finance, or engineering - can be\ncostly or dangerous. Thus, ensuring good probabilities for outliers is\nessential. This paper argues that statistical scaling, as commonly used in the\nliterature, does not produce equally good probabilities for outliers as for\ninliers. Therefore, we propose robust statistical scaling, which uses robust\nestimators to improve the probabilities for outliers. We evaluate several\nvariants of our method against other outlier score transformations for\nreal-world datasets and outlier detection algorithms, where it can improve the\nprobabilities for outliers.\n","authors":["Philipp RÃ¶chner","Henrique O. Marques","Ricardo J. G. B. Campello","Arthur Zimek","Franz Rothlauf"],"pdf_url":"https://arxiv.org/pdf/2408.15874v2.pdf","comment":"15 pages, 4 figures, extended version of an original article accepted\n  for publication in SISAP 2024 by Springer Nature"},{"id":"http://arxiv.org/abs/2303.11428v3","updated":"2024-08-30T11:15:27Z","published":"2023-03-20T20:18:04Z","title":"Lamarr: LHCb ultra-fast simulation based on machine learning models\n  deployed within Gauss","summary":"  About 90% of the computing resources available to the LHCb experiment has\nbeen spent to produce simulated data samples for Run 2 of the Large Hadron\nCollider at CERN. The upgraded LHCb detector will be able to collect larger\ndata samples, requiring many more simulated events to analyze the data to be\ncollected in Run 3. Simulation is a key necessity of analysis to interpret\nsignal, reject background and measure efficiencies. The needed simulation will\nfar exceed the pledged resources, requiring an evolution in technologies and\ntechniques to produce these simulated data samples. In this contribution, we\ndiscuss Lamarr, a Gaudi-based framework to speed-up the simulation production\nparameterizing both the detector response and the reconstruction algorithms of\nthe LHCb experiment. Deep Generative Models powered by several algorithms and\nstrategies are employed to effectively parameterize the high-level response of\nthe single components of the LHCb detector, encoding within neural networks the\nexperimental errors and uncertainties introduced in the detection and\nreconstruction phases. Where possible, models are trained directly on real\ndata, statistically subtracting any background components by applying\nappropriate reweighing procedures. Embedding Lamarr in the general LHCb Gauss\nSimulation framework allows to combine its execution with any of the available\ngenerators in a seamless way. The resulting software package enables a\nsimulation process independent of the detailed simulation used to date.\n","authors":["Matteo Barbetti"],"pdf_url":"https://arxiv.org/pdf/2303.11428v3.pdf","comment":"To be published in Journal of Physics: Conference Series (ACAT 2022)"},{"id":"http://arxiv.org/abs/2408.12594v3","updated":"2024-08-30T10:55:58Z","published":"2024-08-22T17:57:31Z","title":"Non-Homophilic Graph Pre-Training and Prompt Learning","summary":"  Graphs are ubiquitous for modeling complex relationships between objects\nacross various fields. Graph neural networks (GNNs) have become a mainstream\ntechnique for graph-based applications, but their performance heavily relies on\nabundant labeled data. To reduce labeling requirement, pre-training and prompt\nlearning has become a popular alternative. However, most existing prompt\nmethods do not differentiate homophilic and heterophilic characteristics of\nreal-world graphs. In particular, many real-world graphs are non-homophilic,\nnot strictly or uniformly homophilic with mixing homophilic and heterophilic\npatterns, exhibiting varying non-homophilic characteristics across graphs and\nnodes. In this paper, we propose ProNoG, a novel pre-training and prompt\nlearning framework for such non-homophilic graphs. First, we analyze existing\ngraph pre-training methods, providing theoretical insights into the choice of\npre-training tasks. Second, recognizing that each node exhibits unique\nnon-homophilic characteristics, we propose a conditional network to\ncharacterize the node-specific patterns in downstream tasks. Finally, we\nthoroughly evaluate and analyze ProNoG through extensive experiments on ten\npublic datasets.\n","authors":["Xingtong Yu","Jie Zhang","Yuan Fang","Renhe Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.12594v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2408.17198v1","updated":"2024-08-30T10:52:18Z","published":"2024-08-30T10:52:18Z","title":"Towards Symbolic XAI -- Explanation Through Human Understandable Logical\n  Relationships Between Features","summary":"  Explainable Artificial Intelligence (XAI) plays a crucial role in fostering\ntransparency and trust in AI systems, where traditional XAI approaches\ntypically offer one level of abstraction for explanations, often in the form of\nheatmaps highlighting single or multiple input features. However, we ask\nwhether abstract reasoning or problem-solving strategies of a model may also be\nrelevant, as these align more closely with how humans approach solutions to\nproblems. We propose a framework, called Symbolic XAI, that attributes\nrelevance to symbolic queries expressing logical relationships between input\nfeatures, thereby capturing the abstract reasoning behind a model's\npredictions. The methodology is built upon a simple yet general multi-order\ndecomposition of model predictions. This decomposition can be specified using\nhigher-order propagation-based relevance methods, such as GNN-LRP, or\nperturbation-based explanation methods commonly used in XAI. The effectiveness\nof our framework is demonstrated in the domains of natural language processing\n(NLP), vision, and quantum chemistry (QC), where abstract symbolic domain\nknowledge is abundant and of significant interest to users. The Symbolic XAI\nframework provides an understanding of the model's decision-making process that\nis both flexible for customization by the user and human-readable through\nlogical formulas.\n","authors":["Thomas Schnake","Farnoush Rezaei Jafaria","Jonas Lederer","Ping Xiong","Shinichi Nakajima","Stefan Gugler","GrÃ©goire Montavon","Klaus-Robert MÃ¼ller"],"pdf_url":"https://arxiv.org/pdf/2408.17198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01001v4","updated":"2024-08-30T10:41:55Z","published":"2023-05-31T05:04:50Z","title":"DiffLoad: Uncertainty Quantification in Electrical Load Forecasting with\n  Diffusion Model","summary":"  Electrical load forecasting plays a crucial role in decision-making for power\nsystems, including unit commitment and economic dispatch. The integration of\nrenewable energy sources and the occurrence of external events, such as the\nCOVID-19 pandemic, have rapidly increased uncertainties in load forecasting.\nThe uncertainties in load forecasting can be divided into two types: epistemic\nuncertainty and aleatoric uncertainty. Separating these types of uncertainties\ncan help decision-makers better understand where and to what extent the\nuncertainty is, thereby enhancing their confidence in the following\ndecision-making. This paper proposes a diffusion-based Seq2Seq structure to\nestimate epistemic uncertainty and employs the robust additive Cauchy\ndistribution to estimate aleatoric uncertainty. Our method not only ensures the\naccuracy of load forecasting but also demonstrates the ability to separate the\ntwo types of uncertainties and be applicable to different levels of loads. The\nrelevant code can be found at\n\\url{https://anonymous.4open.science/r/DiffLoad-4714/}.\n","authors":["Zhixian Wang","Qingsong Wen","Chaoli Zhang","Liang Sun","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2306.01001v4.pdf","comment":"Accepted by IEEE Transactions on Power Systems, 2024"},{"id":"http://arxiv.org/abs/2301.05522v3","updated":"2024-08-30T10:39:34Z","published":"2023-01-13T12:57:48Z","title":"Hyperparameter Optimization as a Service on INFN Cloud","summary":"  The simplest and often most effective way of parallelizing the training of\ncomplex machine learning models is to execute several training instances on\nmultiple machines, scanning the hyperparameter space to optimize the underlying\nstatistical model and the learning procedure. Often, such a meta-learning\nprocedure is limited by the ability of accessing securely a common database\norganizing the knowledge of the previous and ongoing trials. Exploiting\nopportunistic GPUs provided in different environments represents a further\nchallenge when designing such optimization campaigns. In this contribution, we\ndiscuss how a set of REST APIs can be used to access a dedicated service based\non INFN Cloud to monitor and coordinate multiple training instances, with\ngradient-less optimization techniques, via simple HTTP requests. The service,\ncalled Hopaas (Hyperparameter OPtimization As A Service), is made of a web\ninterface and sets of APIs implemented with a FastAPI backend running through\nUvicorn and NGINX in a virtual instance of INFN Cloud. The optimization\nalgorithms are currently based on Bayesian techniques as provided by Optuna. A\nPython frontend is also made available for quick prototyping. We present\napplications to hyperparameter optimization campaigns performed by combining\nprivate, INFN Cloud, and CINECA resources. Such multi-node multi-site\noptimization studies have given a significant boost to the development of a set\nof parameterizations for the ultra-fast simulation of the LHCb experiment.\n","authors":["Matteo Barbetti","Lucio Anderlini"],"pdf_url":"https://arxiv.org/pdf/2301.05522v3.pdf","comment":"To be published in Journal of Physics: Conference Series (ACAT 2022)"},{"id":"http://arxiv.org/abs/2408.17185v1","updated":"2024-08-30T10:35:59Z","published":"2024-08-30T10:35:59Z","title":"Short-term Wind Speed Forecasting for Power Integration in Smart Grids\n  based on Hybrid LSSVM-SVMD Method","summary":"  Owing to its minimal pollution and efficient energy use, wind energy has\nbecome one of the most widely exploited renewable energy resources. The\nsuccessful integration of wind power into the grid system is contingent upon\naccurate wind speed forecasting models. However, the task of wind speed\nforecasting is challenging due to the inherent intermittent characteristics of\nwind speed. In this paper, a hybrid machine learning approach is developed for\npredicting short-term wind speed. First, the wind data was decomposed into\nmodal components using Successive Variational Mode Decomposition (SVMD). Then,\neach sub-signal was fitted into a Least Squares Support Vector Machines (LSSVM)\nmodel, with its hyperparameter optimized by a novel variant of Quantum-behaved\nParticle Swarm Optimization (QPSO), QPSO with elitist breeding (EBQPSO).\nSecond, the residuals making up for the differences between the original wind\nseries and the aggregate of the SVMD modes were modeled using long short-term\nmodel (LSTM). Then, the overall predicted values were computed using the\naggregate of the LSSVM and the LSTM models. Finally, the performance of the\nproposed model was compared against state-of-the-art benchmark models for\nforecasting wind speed using two separate data sets collected from a local wind\nfarm. Empirical results show significant improvement in performance by the\nproposed method, achieving a 1.21% to 32.76% reduction in root mean square\nerror (RMSE) and a 2.05% to 40.75% reduction in mean average error (MAE)\ncompared to the benchmark methods. The entire code implementation of this work\nis freely available in Github.\n","authors":["Ephrem Admasu Yekun","Alem H. Fitwib","Selvi Karpaga Subramaniand","Anubhav Kumard","Teshome Goa Tella"],"pdf_url":"https://arxiv.org/pdf/2408.17185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14058v2","updated":"2024-08-30T10:30:55Z","published":"2024-07-19T06:35:49Z","title":"On the Causal Sufficiency and Necessity of Multi-Modal Representation\n  Learning","summary":"  An effective paradigm of multi-modal learning (MML) is to learn unified\nrepresentations among modalities. From a causal perspective, constraining the\nconsistency between different modalities can mine causal representations that\nconvey primary events. However, such simple consistency may face the risk of\nlearning insufficient or unnecessary information: a necessary but insufficient\ncause is invariant across modalities but may not have the required accuracy; a\nsufficient but unnecessary cause tends to adapt well to specific modalities but\nmay be hard to adapt to new data. To address this issue, in this paper, we aim\nto learn representations that are both causal sufficient and necessary, i.e.,\nCausal Complete Cause ($C^3$), for MML. Firstly, we define the concept of $C^3$\nfor MML, which reflects the probability of being causal sufficiency and\nnecessity. We also propose the identifiability and measurement of $C^3$, i.e.,\n$C^3$ risk, to ensure calculating the learned representations' $C^3$ scores in\npractice. Then, we theoretically prove the effectiveness of $C^3$ risk by\nestablishing the performance guarantee of MML with a tight generalization\nbound. Based on these theoretical results, we propose a plug-and-play method,\nnamely Causal Complete Cause Regularization ($C^3$R), to learn causal complete\nrepresentations by constraining the $C^3$ risk bound. Extensive experiments\nconducted on various benchmark datasets empirically demonstrate the\neffectiveness of $C^3$R.\n","authors":["Jingyao Wang","Wenwen Qiang","Jiangmeng Li","Lingyu Si","Changwen Zheng","Bing Su"],"pdf_url":"https://arxiv.org/pdf/2407.14058v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17180v1","updated":"2024-08-30T10:28:36Z","published":"2024-08-30T10:28:36Z","title":"Identifying and Clustering Counter Relationships of Team Compositions in\n  PvP Games for Efficient Balance Analysis","summary":"  How can balance be quantified in game settings? This question is crucial for\ngame designers, especially in player-versus-player (PvP) games, where analyzing\nthe strength relations among predefined team compositions-such as hero\ncombinations in multiplayer online battle arena (MOBA) games or decks in card\ngames-is essential for enhancing gameplay and achieving balance. We have\ndeveloped two advanced measures that extend beyond the simplistic win rate to\nquantify balance in zero-sum competitive scenarios. These measures are derived\nfrom win value estimations, which employ strength rating approximations via the\nBradley-Terry model and counter relationship approximations via vector\nquantization, significantly reducing the computational complexity associated\nwith traditional win value estimations. Throughout the learning process of\nthese models, we identify useful categories of compositions and pinpoint their\ncounter relationships, aligning with the experiences of human players without\nrequiring specific game knowledge. Our methodology hinges on a simple technique\nto enhance codebook utilization in discrete representation with a deterministic\nvector quantization process for an extremely small state space. Our framework\nhas been validated in popular online games, including Age of Empires II,\nHearthstone, Brawl Stars, and League of Legends. The accuracy of the observed\nstrength relations in these games is comparable to traditional pairwise win\nvalue predictions, while also offering a more manageable complexity for\nanalysis. Ultimately, our findings contribute to a deeper understanding of PvP\ngame dynamics and present a methodology that significantly improves game\nbalance evaluation and design.\n","authors":["Chiu-Chou Lin","Yu-Wei Shih","Kuei-Ting Kuo","Yu-Cheng Chen","Chien-Hua Chen","Wei-Chen Chiu","I-Chen Wu"],"pdf_url":"https://arxiv.org/pdf/2408.17180v1.pdf","comment":"TMLR 09/2024 https://openreview.net/forum?id=2D36otXvBE"},{"id":"http://arxiv.org/abs/2408.17171v1","updated":"2024-08-30T10:17:37Z","published":"2024-08-30T10:17:37Z","title":"SafeTail: Efficient Tail Latency Optimization in Edge Service Scheduling\n  via Computational Redundancy Management","summary":"  Optimizing tail latency while efficiently managing computational resources is\ncrucial for delivering high-performance, latency-sensitive services in edge\ncomputing. Emerging applications, such as augmented reality, require\nlow-latency computing services with high reliability on user devices, which\noften have limited computational capabilities. Consequently, these devices\ndepend on nearby edge servers for processing. However, inherent uncertainties\nin network and computation latencies stemming from variability in wireless\nnetworks and fluctuating server loads make service delivery on time\nchallenging. Existing approaches often focus on optimizing median latency but\nfall short of addressing the specific challenges of tail latency in edge\nenvironments, particularly under uncertain network and computational\nconditions. Although some methods do address tail latency, they typically rely\non fixed or excessive redundancy and lack adaptability to dynamic network\nconditions, often being designed for cloud environments rather than the unique\ndemands of edge computing. In this paper, we introduce SafeTail, a framework\nthat meets both median and tail response time targets, with tail latency\ndefined as latency beyond the 90^th percentile threshold. SafeTail addresses\nthis challenge by selectively replicating services across multiple edge servers\nto meet target latencies. SafeTail employs a reward-based deep learning\nframework to learn optimal placement strategies, balancing the need to achieve\ntarget latencies with minimizing additional resource usage. Through\ntrace-driven simulations, SafeTail demonstrated near-optimal performance and\noutperformed most baseline strategies across three diverse services.\n","authors":["Jyoti Shokhanda","Utkarsh Pal","Aman Kumar","Soumi Chattopadhyay","Arani Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2408.17171v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.17166v1","updated":"2024-08-30T10:09:12Z","published":"2024-08-30T10:09:12Z","title":"Learning Multi-Target TDOA Features for Sound Event Localization and\n  Detection","summary":"  Sound event localization and detection (SELD) systems using audio recordings\nfrom a microphone array rely on spatial cues for determining the location of\nsound events. As a consequence, the localization performance of such systems is\nto a large extent determined by the quality of the audio features that are used\nas inputs to the system. We propose a new feature, based on neural generalized\ncross-correlations with phase-transform (NGCC-PHAT), that learns audio\nrepresentations suitable for localization. Using permutation invariant training\nfor the time-difference of arrival (TDOA) estimation problem enables NGCC-PHAT\nto learn TDOA features for multiple overlapping sound events. These features\ncan be used as a drop-in replacement for GCC-PHAT inputs to a SELD-network. We\ntest our method on the STARSS23 dataset and demonstrate improved localization\nperformance compared to using standard GCC-PHAT or SALSA-Lite input features.\n","authors":["Axel Berg","Johanna Engman","Jens Gulin","Karl ÃstrÃ¶m","Magnus Oskarsson"],"pdf_url":"https://arxiv.org/pdf/2408.17166v1.pdf","comment":"DCASE 2024"},{"id":"http://arxiv.org/abs/2408.17165v1","updated":"2024-08-30T10:08:21Z","published":"2024-08-30T10:08:21Z","title":"Efficient Testable Learning of General Halfspaces with Adversarial Label\n  Noise","summary":"  We study the task of testable learning of general -- not necessarily\nhomogeneous -- halfspaces with adversarial label noise with respect to the\nGaussian distribution. In the testable learning framework, the goal is to\ndevelop a tester-learner such that if the data passes the tester, then one can\ntrust the output of the robust learner on the data.Our main result is the first\npolynomial time tester-learner for general halfspaces that achieves\ndimension-independent misclassification error. At the heart of our approach is\na new methodology to reduce testable learning of general halfspaces to testable\nlearning of nearly homogeneous halfspaces that may be of broader interest.\n","authors":["Ilias Diakonikolas","Daniel M. Kane","Sihan Liu","Nikos Zarifis"],"pdf_url":"https://arxiv.org/pdf/2408.17165v1.pdf","comment":"Presented to COLT'24"},{"id":"http://arxiv.org/abs/2408.17163v1","updated":"2024-08-30T10:06:26Z","published":"2024-08-30T10:06:26Z","title":"The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by\n  Leveraging Second-Order Information","summary":"  The rising footprint of machine learning has led to a focus on imposing\n\\emph{model sparsity} as a means of reducing computational and memory costs.\nFor deep neural networks (DNNs), the state-of-the-art accuracy-vs-sparsity is\nachieved by heuristics inspired by the classical Optimal Brain Surgeon (OBS)\nframework~\\citep{lecun90brain, hassibi1992second, hassibi1993optimal}, which\nleverages loss curvature information to make better pruning decisions. Yet,\nthese results still lack a solid theoretical understanding, and it is unclear\nwhether they can be improved by leveraging connections to the wealth of work on\nsparse recovery algorithms. In this paper, we draw new connections between\nthese two areas and present new sparse recovery algorithms inspired by the OBS\nframework that comes with theoretical guarantees under reasonable assumptions\nand have strong practical performance. Specifically, our work starts from the\nobservation that we can leverage curvature information in OBS-like fashion upon\nthe projection step of classic iterative sparse recovery algorithms such as\nIHT. We show for the first time that this leads both to improved convergence\nbounds under standard assumptions. Furthermore, we present extensions of this\napproach to the practical task of obtaining accurate sparse DNNs, and validate\nit experimentally at scale for Transformer-based models on vision and language\ntasks.\n","authors":["Diyuan Wu","Ionut-Vlad Modoranu","Mher Safaryan","Denis Kuznedelev","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2408.17163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17162v1","updated":"2024-08-30T10:05:24Z","published":"2024-08-30T10:05:24Z","title":"Deep Feature Embedding for Tabular Data","summary":"  Tabular data learning has extensive applications in deep learning but its\nexisting embedding techniques are limited in numerical and categorical features\nsuch as the inability to capture complex relationships and engineering. This\npaper proposes a novel deep embedding framework with leverages lightweight deep\nneural networks to generate effective feature embeddings for tabular data in\nmachine learning research. For numerical features, a two-step feature expansion\nand deep transformation technique is used to capture copious semantic\ninformation. For categorical features, a unique identification vector for each\nentity is referred by a compact lookup table with a parameterized deep\nembedding function to uniform the embedding size dimensions, and transformed\ninto a embedding vector using deep neural network. Experiments are conducted on\nreal-world datasets for performance evaluation.\n","authors":["Yuqian Wu","Hengyi Luo","Raymond S. T. Lee"],"pdf_url":"https://arxiv.org/pdf/2408.17162v1.pdf","comment":"15 pages, 2figures, accepted to ICONIP 2024, Paper ID: 1399"},{"id":"http://arxiv.org/abs/2401.10726v3","updated":"2024-08-30T10:04:21Z","published":"2024-01-19T14:43:04Z","title":"Empowering Aggregators with Practical Data-Driven Tools: Harnessing\n  Aggregated and Disaggregated Flexibility for Demand Response","summary":"  This study explores the interaction between aggregators and building\noccupants in activating flexibility through Demand Response (DR) programs, with\na focus on reinforcing the resilience of the energy system considering the\nuncertainties presented by Renewable Energy Sources (RES). Firstly, it\nintroduces a methodology of optimizing aggregated flexibility provision\nstrategies in environments with limited data, utilizing Discrete Fourier\nTransformation (DFT) and clustering techniques to identify building occupants'\nactivity patterns. Secondly, the study assesses the disaggregated flexibility\nprovision of Heating Ventilation and Air Conditioning (HVAC) systems during DR\nevents, employing machine learning and optimization techniques for precise,\ndevice-level analysis. The first approach offers a non-intrusive pathway for\naggregators to provide flexibility services in environments of a single smart\nmeter for the whole building's consumption, while the second approach maximizes\nthe amount of flexibility in the case of dedicated metering devices to the HVAC\nsystems by carefully considering building occupants' thermal comfort profiles.\nThrough the application of data-driven techniques and encompassing case studies\nfrom both industrial and residential buildings, this paper not only unveils\npivotal opportunities for aggregators in the balancing and emerging flexibility\nmarkets but also successfully develops and demonstrates end-to-end practical\ntools for aggregators.\n","authors":["Costas Mylonas","Donata Boric","Leila Luttenberger Maric","Alexandros Tsitsanis","Eleftheria Petrianou","Magda Foti"],"pdf_url":"https://arxiv.org/pdf/2401.10726v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00877v3","updated":"2024-08-30T10:02:22Z","published":"2024-05-01T21:42:38Z","title":"Markov flow policy -- deep MC","summary":"  Discounted algorithms often encounter evaluation errors due to their reliance\non short-term estimations, which can impede their efficacy in addressing\nsimple, short-term tasks and impose undesired temporal discounts (\\(\\gamma\\)).\nInterestingly, these algorithms are often tested without applying a discount, a\nphenomenon we refer as the \\textit{train-test bias}. In response to these\nchallenges, we propose the Markov Flow Policy, which utilizes a non-negative\nneural network flow to enable comprehensive forward-view predictions. Through\nintegration into the TD7 codebase and evaluation using the MuJoCo benchmark, we\nobserve significant performance improvements, positioning MFP as a\nstraightforward, practical, and easily implementable solution within the domain\nof average rewards algorithms.\n","authors":["Nitsan Soffair","Gilad Katz"],"pdf_url":"https://arxiv.org/pdf/2405.00877v3.pdf","comment":"Paper has been not finished"},{"id":"http://arxiv.org/abs/2211.15411v6","updated":"2024-08-30T10:01:25Z","published":"2022-11-09T08:11:16Z","title":"Solving Collaborative Dec-POMDPs with Deep Reinforcement Learning\n  Heuristics","summary":"  WQMIX, QMIX, QTRAN, and VDN are SOTA algorithms for Dec-POMDP. All of them\ncannot solve complex agents' cooperation domains. We give an algorithm to solve\nsuch problems. In the first stage, we solve a single-agent problem and get a\npolicy. In the second stage, we solve the multi-agent problem with the\nsingle-agent policy. SA2MA has a clear advantage over all competitors in\ncomplex agents' cooperative domains.\n","authors":["Nitsan Soffair"],"pdf_url":"https://arxiv.org/pdf/2211.15411v6.pdf","comment":"Paper has been not finished"},{"id":"http://arxiv.org/abs/2408.17151v1","updated":"2024-08-30T09:40:52Z","published":"2024-08-30T09:40:52Z","title":"Investigating Privacy Leakage in Dimensionality Reduction Methods via\n  Reconstruction Attack","summary":"  This study investigates privacy leakage in dimensionality reduction methods\nthrough a novel machine learning-based reconstruction attack. Employing an\n\\emph{informed adversary} threat model, we develop a neural network capable of\nreconstructing high-dimensional data from low-dimensional embeddings.\n  We evaluate six popular dimensionality reduction techniques: PCA, sparse\nrandom projection (SRP), multidimensional scaling (MDS), Isomap, $t$-SNE, and\nUMAP. Using both MNIST and NIH Chest X-ray datasets, we perform a qualitative\nanalysis to identify key factors affecting reconstruction quality. Furthermore,\nwe assess the effectiveness of an additive noise mechanism in mitigating these\nreconstruction attacks.\n","authors":["Chayadon Lumbut","Donlapark Ponnoprat"],"pdf_url":"https://arxiv.org/pdf/2408.17151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17148v1","updated":"2024-08-30T09:38:51Z","published":"2024-08-30T09:38:51Z","title":"The Many Faces of Optimal Weak-to-Strong Learning","summary":"  Boosting is an extremely successful idea, allowing one to combine multiple\nlow accuracy classifiers into a much more accurate voting classifier. In this\nwork, we present a new and surprisingly simple Boosting algorithm that obtains\na provably optimal sample complexity. Sample optimal Boosting algorithms have\nonly recently been developed, and our new algorithm has the fastest runtime\namong all such algorithms and is the simplest to describe: Partition your\ntraining data into 5 disjoint pieces of equal size, run AdaBoost on each, and\ncombine the resulting classifiers via a majority vote. In addition to this\ntheoretical contribution, we also perform the first empirical comparison of the\nproposed sample optimal Boosting algorithms. Our pilot empirical study suggests\nthat our new algorithm might outperform previous algorithms on large data sets.\n","authors":["Mikael MÃ¸ller HÃ¸gsgaard","Kasper Green Larsen","Markus Engelund Mathiasen"],"pdf_url":"https://arxiv.org/pdf/2408.17148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17145v1","updated":"2024-08-30T09:35:36Z","published":"2024-08-30T09:35:36Z","title":"Towards Hyper-parameter-free Federated Learning","summary":"  The adaptive synchronization techniques in federated learning (FL) for scaled\nglobal model updates show superior performance over the vanilla federated\naveraging (FedAvg) scheme. However, existing methods employ additional tunable\nhyperparameters on the server to determine the scaling factor. A contrasting\napproach is automated scaling analogous to tuning-free step-size schemes in\nstochastic gradient descent (SGD) methods, which offer competitive convergence\nrates and exhibit good empirical performance. In this work, we introduce two\nalgorithms for automated scaling of global model updates. In our first\nalgorithm, we establish that a descent-ensuring step-size regime at the clients\nensures descent for the server objective. We show that such a scheme enables\nlinear convergence for strongly convex federated objectives. Our second\nalgorithm shows that the average of objective values of sampled clients is a\npractical and effective substitute for the objective function value at the\nserver required for computing the scaling factor, whose computation is\notherwise not permitted. Our extensive empirical results show that the proposed\nmethods perform at par or better than the popular federated learning algorithms\nfor both convex and non-convex problems. Our work takes a step towards\ndesigning hyper-parameter-free federated learning.\n","authors":[" Geetika","Drishya Uniyal","Bapi Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2408.17145v1.pdf","comment":"28 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.15799v5","updated":"2024-08-30T09:33:53Z","published":"2023-03-28T08:07:28Z","title":"FedAgg: Adaptive Federated Learning with Aggregated Gradients","summary":"  Federated Learning (FL) has emerged as a crucial distributed training\nparadigm, enabling discrete devices to collaboratively train a shared model\nunder the coordination of a central server, while leveraging their locally\nstored private data. Nonetheless, the\nnon-independent-and-identically-distributed (Non-IID) data generated on\nheterogeneous clients and the incessant information exchange among participants\nmay significantly impede training efficacy, retard the model convergence rate\nand increase the risk of privacy leakage. To alleviate the divergence between\nthe local and average model parameters and obtain a fast model convergence\nrate, we propose an adaptive FEDerated learning algorithm called FedAgg by\nrefining the conventional stochastic gradient descent (SGD) methodology with an\nAGgregated Gradient term at each local training epoch and adaptively adjusting\nthe learning rate based on a penalty term that quantifies the local model\ndeviation. To tackle the challenge of information exchange among clients during\nlocal training and design a decentralized adaptive learning rate for each\nclient, we introduce two mean-field terms to approximate the average local\nparameters and gradients over time. Through rigorous theoretical analysis, we\ndemonstrate the existence and convergence of the mean-field terms and provide a\nrobust upper bound on the convergence of our proposed algorithm. The extensive\nexperimental results on real-world datasets substantiate the superiority of our\nframework in comparison with existing state-of-the-art FL strategies for\nenhancing model performance and accelerating convergence rate under IID and\nNon-IID datasets.\n","authors":["Wenhao Yuan","Xuehe Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15799v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17139v1","updated":"2024-08-30T09:28:32Z","published":"2024-08-30T09:28:32Z","title":"Flow Matching for Optimal Reaction Coordinates of Biomolecular System","summary":"  We present Flow Matching for Reaction Coordinates (FMRC), a novel deep\nlearning algorithm designed to identify optimal reaction coordinates (RC) in\nbiomolecular reversible dynamics. FMRC is based on the mathematical principles\nof lumpability and decomposability, which we reformulate into a conditional\nprobability framework for efficient data-driven optimization using deep\ngenerative models. While FMRC does not explicitly learn the well-established\ntransfer operator or its eigenfunctions, it can effectively encode the dynamics\nof leading eigenfunctions of the system transfer operator into its\nlow-dimensional RC space. We further quantitatively compare its performance\nwith several state-of-the-art algorithms by evaluating the quality of Markov\nState Models (MSM) constructed in their respective RC spaces, demonstrating the\nsuperiority of FMRC in three increasingly complex biomolecular systems.\nFinally, we discuss its potential applications in downstream applications such\nas enhanced sampling methods and MSM construction.\n","authors":["Mingyuan Zhang","Zhicheng Zhang","Yong Wang","Hao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.17139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17129v1","updated":"2024-08-30T09:14:38Z","published":"2024-08-30T09:14:38Z","title":"Controllable Edge-Type-Specific Interpretation in Multi-Relational Graph\n  Neural Networks for Drug Response Prediction","summary":"  Graph Neural Networks have been widely applied in critical decision-making\nareas that demand interpretable predictions, leading to the flourishing\ndevelopment of interpretability algorithms. However, current graph\ninterpretability algorithms tend to emphasize generality and often overlook\nbiological significance, thereby limiting their applicability in predicting\ncancer drug responses. In this paper, we propose a novel post-hoc\ninterpretability algorithm for cancer drug response prediction, CETExplainer,\nwhich incorporates a controllable edge-type-specific weighting mechanism. It\nconsiders the mutual information between subgraphs and predictions, proposing a\nstructural scoring approach to provide fine-grained, biologically meaningful\nexplanations for predictive models. We also introduce a method for constructing\nground truth based on real-world datasets to quantitatively evaluate the\nproposed interpretability algorithm. Empirical analysis on the real-world\ndataset demonstrates that CETExplainer achieves superior stability and improves\nexplanation quality compared to leading algorithms, thereby offering a robust\nand insightful tool for cancer drug prediction.\n","authors":["Xiaodi Li","Jianfeng Gui","Qian Gao","Haoyuan Shi","Zhenyu Yue"],"pdf_url":"https://arxiv.org/pdf/2408.17129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02895v2","updated":"2024-08-30T09:14:18Z","published":"2022-12-06T11:38:22Z","title":"Training Neural Networks on Data Sources with Unknown Reliability","summary":"  When data is generated by multiple sources, conventional training methods\nupdate models assuming equal reliability for each source and do not consider\ntheir individual data quality during training. However, in many applications,\nsources have varied levels of reliability that can have negative effects on the\nperformance of a neural network. A key issue is that often the quality of data\nfor individual sources is not known during training. Focusing on supervised\nlearning, this work presents a solution that aims to train neural networks on\neach data source for a number of steps proportional to the source's estimated\nrelative reliability. This way, we allow training on all sources during the\nwarm-up, and reduce learning on less reliable sources during the final training\nstages, when it has been shown models overfit to noise. We show through diverse\nexperiments, this can significantly improve model performance when trained on\nmixtures of reliable and unreliable data sources, and maintain performance when\nmodels are trained on reliable sources only.\n","authors":["Alexander Capstick","Francesca Palermo","Tianyu Cui","Payam Barnaghi"],"pdf_url":"https://arxiv.org/pdf/2212.02895v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17118v1","updated":"2024-08-30T09:01:04Z","published":"2024-08-30T09:01:04Z","title":"Efficient Estimation of Unique Components in Independent Component\n  Analysis by Matrix Representation","summary":"  Independent component analysis (ICA) is a widely used method in various\napplications of signal processing and feature extraction. It extends principal\ncomponent analysis (PCA) and can extract important and complicated components\nwith small variances. One of the major problems of ICA is that the uniqueness\nof the solution is not guaranteed, unlike PCA. That is because there are many\nlocal optima in optimizing the objective function of ICA. It has been shown\npreviously that the unique global optimum of ICA can be estimated from many\nrandom initializations by handcrafted thread computation. In this paper, the\nunique estimation of ICA is highly accelerated by reformulating the algorithm\nin matrix representation and reducing redundant calculations. Experimental\nresults on artificial datasets and EEG data verified the efficiency of the\nproposed method.\n","authors":["Yoshitatsu Matsuda","Kazunori Yamaguch"],"pdf_url":"https://arxiv.org/pdf/2408.17118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17108v1","updated":"2024-08-30T08:49:27Z","published":"2024-08-30T08:49:27Z","title":"Sparse Uncertainty-Informed Sampling from Federated Streaming Data","summary":"  We present a numerically robust, computationally efficient approach for\nnon-I.I.D. data stream sampling in federated client systems, where resources\nare limited and labeled data for local model adaptation is sparse and\nexpensive. The proposed method identifies relevant stream observations to\noptimize the underlying client model, given a local labeling budget, and\nperforms instantaneous labeling decisions without relying on any memory\nbuffering strategies. Our experiments show enhanced training batch diversity\nand an improved numerical robustness of the proposal compared to existing\nstrategies over large-scale data streams, making our approach an effective and\nconvenient solution in FL environments.\n","authors":["Manuel RÃ¶der","Frank-Michael Schleif"],"pdf_url":"https://arxiv.org/pdf/2408.17108v1.pdf","comment":"Preprint, 6 pages, 3 figures, Accepted for ESANN 2024"},{"id":"http://arxiv.org/abs/2408.17095v1","updated":"2024-08-30T08:26:55Z","published":"2024-08-30T08:26:55Z","title":"RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation\n  and Retrieval-Guidance","summary":"  Diffusion-based models demonstrate impressive generation capabilities.\nHowever, they also have a massive number of parameters, resulting in enormous\nmodel sizes, thus making them unsuitable for deployment on resource-constraint\ndevices. Block-wise generation can be a promising alternative for designing\ncompact-sized (parameter-efficient) deep generative models since the model can\ngenerate one block at a time instead of generating the whole image at once.\nHowever, block-wise generation is also considerably challenging because\nensuring coherence across generated blocks can be non-trivial. To this end, we\ndesign a retrieval-augmented generation (RAG) approach and leverage the\ncorresponding blocks of the images retrieved by the RAG module to condition the\ntraining and generation stages of a block-wise denoising diffusion model. Our\nconditioning schemes ensure coherence across the different blocks during\ntraining and, consequently, during generation. While we showcase our approach\nusing the latent diffusion model (LDM) as the base model, it can be used with\nother variants of denoising diffusion models. We validate the solution of the\ncoherence problem through the proposed approach by reporting substantive\nexperiments to demonstrate our approach's effectiveness in compact model size\nand excellent generation quality.\n","authors":["Avideep Mukherjee","Soumya Banerjee","Vinay P. Namboodiri","Piyush Rai"],"pdf_url":"https://arxiv.org/pdf/2408.17095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17090v1","updated":"2024-08-30T08:22:30Z","published":"2024-08-30T08:22:30Z","title":"FissionVAE: Federated Non-IID Image Generation with Latent Space and\n  Decoder Decomposition","summary":"  Federated learning is a machine learning paradigm that enables decentralized\nclients to collaboratively learn a shared model while keeping all the training\ndata local. While considerable research has focused on federated image\ngeneration, particularly Generative Adversarial Networks, Variational\nAutoencoders have received less attention. In this paper, we address the\nchallenges of non-IID (independently and identically distributed) data\nenvironments featuring multiple groups of images of different types.\nSpecifically, heterogeneous data distributions can lead to difficulties in\nmaintaining a consistent latent space and can also result in local generators\nwith disparate texture features being blended during aggregation. We introduce\na novel approach, FissionVAE, which decomposes the latent space and constructs\ndecoder branches tailored to individual client groups. This method allows for\ncustomized learning that aligns with the unique data distributions of each\ngroup. Additionally, we investigate the incorporation of hierarchical VAE\narchitectures and demonstrate the use of heterogeneous decoder architectures\nwithin our model. We also explore strategies for setting the latent prior\ndistributions to enhance the decomposition process. To evaluate our approach,\nwe assemble two composite datasets: the first combines MNIST and FashionMNIST;\nthe second comprises RGB datasets of cartoon and human faces, wild animals,\nmarine vessels, and remote sensing images of Earth. Our experiments demonstrate\nthat FissionVAE greatly improves generation quality on these datasets compared\nto baseline federated VAE models.\n","authors":["Chen Hu","Jingjing Deng","Xianghua Xie","Xiaoke Ma"],"pdf_url":"https://arxiv.org/pdf/2408.17090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02606v2","updated":"2024-08-30T08:12:23Z","published":"2023-10-04T06:42:33Z","title":"Mending of Spatio-Temporal Dependencies in Block Adjacency Matrix","summary":"  In the realm of applications where data dynamically evolves across spatial\nand temporal dimensions, Graph Neural Networks (GNNs) are often complemented by\nsequence modeling architectures, such as RNNs and transformers, to effectively\nmodel temporal changes. These hybrid models typically arrange the spatial and\ntemporal learning components in series. A pioneering effort to jointly model\nthe spatio-temporal dependencies using only GNNs was the introduction of the\nBlock Adjacency Matrix \\(\\mathbf{A_B}\\) \\cite{1}, which was constructed by\ndiagonally concatenating adjacency matrices from graphs at different time\nsteps. This approach resulted in a single graph encompassing complete\nspatio-temporal data; however, the graphs from different time steps remained\ndisconnected, limiting GNN message-passing to spatially connected nodes only.\nAddressing this critical challenge, we propose a novel end-to-end learning\narchitecture specifically designed to mend the temporal dependencies, resulting\nin a well-connected graph. Thus, we provide a framework for the learnable\nrepresentation of spatio-temporal data as graphs. Our methodology demonstrates\nsuperior performance on benchmark datasets, such as SurgVisDom and C2D2,\nsurpassing existing state-of-the-art graph models in terms of accuracy. Our\nmodel also achieves significantly lower computational complexity, having far\nfewer parameters than methods reliant on CLIP and 3D CNN architectures.\n","authors":["Osama Ahmad","Omer Abdul Jalil","Usman Nazir","Murtaza Taj"],"pdf_url":"https://arxiv.org/pdf/2310.02606v2.pdf","comment":"Accepted at ICONIP 2024"},{"id":"http://arxiv.org/abs/2406.04099v2","updated":"2024-08-30T08:05:08Z","published":"2024-06-06T14:15:12Z","title":"Enhancing Weather Predictions: Super-Resolution via Deep Diffusion\n  Models","summary":"  This study investigates the application of deep-learning diffusion models for\nthe super-resolution of weather data, a novel approach aimed at enhancing the\nspatial resolution and detail of meteorological variables. Leveraging the\ncapabilities of diffusion models, specifically the SR3 and ResDiff\narchitectures, we present a methodology for transforming low-resolution weather\ndata into high-resolution outputs. Our experiments, conducted using the\nWeatherBench dataset, focus on the super-resolution of the two-meter\ntemperature variable, demonstrating the models' ability to generate detailed\nand accurate weather maps. The results indicate that the ResDiff model, further\nimproved by incorporating physics-based modifications, significantly\noutperforms traditional SR3 methods in terms of Mean Squared Error (MSE),\nStructural Similarity Index (SSIM), and Peak Signal-to-Noise Ratio (PSNR). This\nresearch highlights the potential of diffusion models in meteorological\napplications, offering insights into their effectiveness, challenges, and\nprospects for future advancements in weather prediction and climate analysis.\n","authors":["Jan MartinÅ¯","Petr Å imÃ¡nek"],"pdf_url":"https://arxiv.org/pdf/2406.04099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17064v1","updated":"2024-08-30T07:49:35Z","published":"2024-08-30T07:49:35Z","title":"Instant Adversarial Purification with Adversarial Consistency\n  Distillation","summary":"  Neural networks, despite their remarkable performance in widespread\napplications, including image classification, are also known to be vulnerable\nto subtle adversarial noise. Although some diffusion-based purification methods\nhave been proposed, for example, DiffPure, those methods are time-consuming. In\nthis paper, we propose One Step Control Purification (OSCP), a diffusion-based\npurification model that can purify the adversarial image in one Neural Function\nEvaluation (NFE) in diffusion models. We use Latent Consistency Model (LCM) and\nControlNet for our one-step purification. OSCP is computationally friendly and\ntime efficient compared to other diffusion-based purification methods; we\nachieve defense success rate of 74.19\\% on ImageNet, only requiring 0.1s for\neach purification. Moreover, there is a fundamental incongruence between\nconsistency distillation and adversarial perturbation. To address this\nontological dissonance, we propose Gaussian Adversarial Noise Distillation\n(GAND), a novel consistency distillation framework that facilitates a more\nnuanced reconciliation of the latent space dynamics, effectively bridging the\nnatural and adversarial manifolds. Our experiments show that the GAND does not\nneed a Full Fine Tune (FFT); PEFT, e.g., LoRA is sufficient.\n","authors":["Chun Tong Lei","Hon Ming Yam","Zhongliang Guo","Chun Pong Lau"],"pdf_url":"https://arxiv.org/pdf/2408.17064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17059v1","updated":"2024-08-30T07:38:28Z","published":"2024-08-30T07:38:28Z","title":"A Survey of the Self Supervised Learning Mechanisms for Vision\n  Transformers","summary":"  Deep supervised learning models require high volume of labeled data to attain\nsufficiently good results. Although, the practice of gathering and annotating\nsuch big data is costly and laborious. Recently, the application of self\nsupervised learning (SSL) in vision tasks has gained significant attention. The\nintuition behind SSL is to exploit the synchronous relationships within the\ndata as a form of self-supervision, which can be versatile. In the current big\ndata era, most of the data is unlabeled, and the success of SSL thus relies in\nfinding ways to improve this vast amount of unlabeled data available. Thus its\nbetter for deep learning algorithms to reduce reliance on human supervision and\ninstead focus on self-supervision based on the inherent relationships within\nthe data. With the advent of ViTs, which have achieved remarkable results in\ncomputer vision, it is crucial to explore and understand the various SSL\nmechanisms employed for training these models specifically in scenarios where\nthere is less label data available. In this survey we thus develop a\ncomprehensive taxonomy of systematically classifying the SSL techniques based\nupon their representations and pre-training tasks being applied. Additionally,\nwe discuss the motivations behind SSL, review popular pre-training tasks, and\nhighlight the challenges and advancements in this field. Furthermore, we\npresent a comparative analysis of different SSL methods, evaluate their\nstrengths and limitations, and identify potential avenues for future research.\n","authors":["Asifullah Khan","Anabia Sohail","Mustansar Fiaz","Mehdi Hassan","Tariq Habib Afridi","Sibghat Ullah Marwat","Farzeen Munir","Safdar Ali","Hannan Naseem","Muhammad Zaigham Zaheer","Kamran Ali","Tangina Sultana","Ziaurrehman Tanoli","Naeem Akhter"],"pdf_url":"https://arxiv.org/pdf/2408.17059v1.pdf","comment":"34 Pages, 5 Figures, 7 Tables"},{"id":"http://arxiv.org/abs/2405.11432v2","updated":"2024-08-30T07:37:25Z","published":"2024-05-19T03:27:31Z","title":"On Robust Reinforcement Learning with Lipschitz-Bounded Policy Networks","summary":"  This paper presents a study of robust policy networks in deep reinforcement\nlearning. We investigate the benefits of policy parameterizations that\nnaturally satisfy constraints on their Lipschitz bound, analyzing their\nempirical performance and robustness on two representative problems: pendulum\nswing-up and Atari Pong. We illustrate that policy networks with smaller\nLipschitz bounds are more robust to disturbances, random noise, and targeted\nadversarial attacks than unconstrained policies composed of vanilla multi-layer\nperceptrons or convolutional neural networks. However, the structure of the\nLipschitz layer is important. We find that the widely-used method of spectral\nnormalization is too conservative and severely impacts clean performance,\nwhereas more expressive Lipschitz layers such as the recently-proposed Sandwich\nlayer can achieve improved robustness without sacrificing clean performance.\n","authors":["Nicholas H. Barbara","Ruigang Wang","Ian R. Manchester"],"pdf_url":"https://arxiv.org/pdf/2405.11432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17053v1","updated":"2024-08-30T07:23:59Z","published":"2024-08-30T07:23:59Z","title":"Estimating Conditional Average Treatment Effects via Sufficient\n  Representation Learning","summary":"  Estimating the conditional average treatment effects (CATE) is very important\nin causal inference and has a wide range of applications across many fields. In\nthe estimation process of CATE, the unconfoundedness assumption is typically\nrequired to ensure the identifiability of the regression problems. When\nestimating CATE using high-dimensional data, there have been many variable\nselection methods and neural network approaches based on representation\nlearning, while these methods do not provide a way to verify whether the subset\nof variables after dimensionality reduction or the learned representations\nstill satisfy the unconfoundedness assumption during the estimation process,\nwhich can lead to ineffective estimates of the treatment effects. Additionally,\nthese methods typically use data from only the treatment or control group when\nestimating the regression functions for each group. This paper proposes a novel\nneural network approach named \\textbf{CrossNet} to learn a sufficient\nrepresentation for the features, based on which we then estimate the CATE,\nwhere cross indicates that in estimating the regression functions, we used data\nfrom their own group as well as cross-utilized data from another group.\nNumerical simulations and empirical results demonstrate that our method\noutperforms the competitive approaches.\n","authors":["Pengfei Shi","Wei Zhong","Xinyu Zhang","Ningtao Wang","Xing Fu","Weiqiang Wang","Yin Jin"],"pdf_url":"https://arxiv.org/pdf/2408.17053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13192v3","updated":"2024-08-30T06:49:51Z","published":"2024-01-24T02:36:52Z","title":"Generative Design of Crystal Structures by Point Cloud Representations\n  and Diffusion Model","summary":"  Efficiently generating energetically stable crystal structures has long been\na challenge in material design, primarily due to the immense arrangement of\natoms in a crystal lattice. To facilitate the discovery of stable material, we\npresent a framework for the generation of synthesizable materials, leveraging a\npoint cloud representation to encode intricate structural information. At the\nheart of this framework lies the introduction of a diffusion model as its\nfoundational pillar. To gauge the efficacy of our approach, we employ it to\nreconstruct input structures from our training datasets, rigorously validating\nits high reconstruction performance. Furthermore, we demonstrate the profound\npotential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely\nnew materials, emphasizing their synthesizability. Our research stands as a\nnoteworthy contribution to the advancement of materials design and synthesis\nthrough the cutting-edge avenue of generative design instead of the\nconventional substitution or experience-based discovery.\n","authors":["Zhelin Li","Rami Mrad","Runxian Jiao","Guan Huang","Jun Shan","Shibing Chu","Yuanping Chen"],"pdf_url":"https://arxiv.org/pdf/2401.13192v3.pdf","comment":"I have submitted to a journal"},{"id":"http://arxiv.org/abs/2408.15545v2","updated":"2024-08-30T06:42:36Z","published":"2024-08-28T05:41:52Z","title":"SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding","summary":"  Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.\n","authors":["Sihang Li","Jin Huang","Jiaxi Zhuang","Yaorui Shi","Xiaochen Cai","Mingjun Xu","Xiang Wang","Linfeng Zhang","Guolin Ke","Hengxing Cai"],"pdf_url":"https://arxiv.org/pdf/2408.15545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15268v2","updated":"2024-08-30T06:18:45Z","published":"2024-08-12T14:23:42Z","title":"Anomaly Detection in Time Series of EDFA Pump Currents to Monitor\n  Degeneration Processes using Fuzzy Clustering","summary":"  This article proposes a novel fuzzy clustering based anomaly detection method\nfor pump current time series of EDFA systems. The proposed change detection\nframework (CDF) strategically combines the advantages of entropy analysis (EA)\nand principle component analysis (PCA) with fuzzy clustering procedures. In the\nframework, EA is applied for dynamic selection of features for reduction of the\nfeature space and increase of computational performance. Furthermore, PCA is\nutilized to extract features from the raw feature space to enable\ngeneralization capability of the subsequent fuzzy clustering procedures. Three\ndifferent fuzzy clustering methods, more precisely the fuzzy clustering\nalgorithm, a probabilistic clustering algorithm and a possibilistic clustering\nalgorithm are evaluated for performance and generalization. Hence, the proposed\nframework has the innovative feature to detect changes in pump current time\nseries at an early stage for arbitrary points of operation, compared to\nstate-of-the-art predefined alarms in commercially used EDFAs. Moreover, the\napproach is implemented and tested using experimental data. In addition, the\nproposed framework enables further approaches of applying decentralized\npredictive maintenance for optical fiber networks.\n","authors":["Dominic Schneider","Lutz Rapp","Christoph Ament"],"pdf_url":"https://arxiv.org/pdf/2408.15268v2.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.19121v3","updated":"2024-08-30T06:17:46Z","published":"2024-06-27T12:05:55Z","title":"Towards Learning Abductive Reasoning using VSA Distributed\n  Representations","summary":"  We introduce the Abductive Rule Learner with Context-awareness (ARLC), a\nmodel that solves abstract reasoning tasks based on Learn-VRF. ARLC features a\nnovel and more broadly applicable training objective for abductive reasoning,\nresulting in better interpretability and higher accuracy when solving Raven's\nprogressive matrices (RPM). ARLC allows both programming domain knowledge and\nlearning the rules underlying a data distribution. We evaluate ARLC on the\nI-RAVEN dataset, showcasing state-of-the-art accuracy across both\nin-distribution and out-of-distribution (unseen attribute-rule pairs) tests.\nARLC surpasses neuro-symbolic and connectionist baselines, including large\nlanguage models, despite having orders of magnitude fewer parameters. We show\nARLC's robustness to post-programming training by incrementally learning from\nexamples on top of programmed knowledge, which only improves its performance\nand does not result in catastrophic forgetting of the programmed solution. We\nvalidate ARLC's seamless transfer learning from a 2x2 RPM constellation to\nunseen constellations. Our code is available at\nhttps://github.com/IBM/abductive-rule-learner-with-context-awareness.\n","authors":["Giacomo Camposampiero","Michael Hersche","Aleksandar TerziÄ","Roger Wattenhofer","Abu Sebastian","Abbas Rahimi"],"pdf_url":"https://arxiv.org/pdf/2406.19121v3.pdf","comment":"Accepted at the 18th International Conference on Neural-Symbolic\n  Learning and Reasoning (NeSy) 2024 [Spotlight]"},{"id":"http://arxiv.org/abs/2310.13019v4","updated":"2024-08-30T05:50:56Z","published":"2023-10-18T18:50:39Z","title":"Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class\n  Manipulation Using DeepFool Algorithm","summary":"  The susceptibility of deep neural networks (DNNs) to adversarial attacks\nundermines their reliability across numerous applications, underscoring the\nnecessity for an in-depth exploration of these vulnerabilities and the\nformulation of robust defense strategies. The DeepFool algorithm by\nMoosavi-Dezfooli et al. (2016) represents a pivotal step in identifying minimal\nperturbations required to induce misclassification of input images.\nNonetheless, its generic methodology falls short in scenarios necessitating\ntargeted interventions. Additionally, previous research studies have\npredominantly concentrated on the success rate of attacks without adequately\naddressing the consequential distortion of images, the maintenance of image\nquality, or the confidence threshold required for misclassification. To bridge\nthese gaps, we introduce the Enhanced Targeted DeepFool (ET DeepFool)\nalgorithm, an evolution of DeepFool that not only facilitates the specification\nof desired misclassification targets but also incorporates a configurable\nminimum confidence score. Our empirical investigations demonstrate the\nsuperiority of this refined approach in maintaining the integrity of images and\nminimizing perturbations across a variety of DNN architectures. Unlike previous\niterations, such as the Targeted DeepFool by Gajjar et al. (2022), our method\ngrants unparalleled control over the perturbation process, enabling precise\nmanipulation of model responses. Preliminary outcomes reveal that certain\nmodels, including AlexNet and the advanced Vision Transformer, display\ncommendable robustness to such manipulations. This discovery of varying levels\nof model robustness, as unveiled through our confidence level adjustments,\ncould have far-reaching implications for the field of image recognition. Our\ncode will be made public upon acceptance of the paper.\n","authors":["S. M. Fazle Rabby Labib","Joyanta Jyoti Mondal","Meem Arafat Manab","Sarfaraz Newaz","Xi Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.13019v4.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.17016v1","updated":"2024-08-30T05:13:11Z","published":"2024-08-30T05:13:11Z","title":"Error-controlled non-additive interaction discovery in machine learning\n  models","summary":"  Machine learning (ML) models are powerful tools for detecting complex\npatterns within data, yet their \"black box\" nature limits their\ninterpretability, hindering their use in critical domains like healthcare and\nfinance. To address this challenge, interpretable ML methods have been\ndeveloped to explain how features influence model predictions. However, these\nmethods often focus on univariate feature importance, overlooking the complex\ninteractions between features that ML models are capable of capturing.\nRecognizing this limitation, recent efforts have aimed to extend these methods\nto discover feature interactions, but existing approaches struggle with\nrobustness and error control, especially under data perturbations. In this\nstudy, we introduce Diamond, a novel method for trustworthy feature interaction\ndiscovery. Diamond uniquely integrates the model-X knockoffs framework to\ncontrol the false discovery rate (FDR), ensuring that the proportion of falsely\ndiscovered interactions remains low. We further address the challenges of using\noff-the-shelf interaction importance measures by proposing a calibration\nprocedure that refines these measures to maintain the desired FDR. Diamond's\napplicability spans a wide range of ML models, including deep neural networks,\ntree-based models, and factorization-based models. Our empirical evaluations on\nboth simulated and real datasets across various biomedical studies demonstrate\nDiamond's utility in enabling more reliable data-driven scientific discoveries.\nThis method represents a significant step forward in the deployment of ML\nmodels for scientific innovation and hypothesis generation.\n","authors":["Winston Chen","Yifan Jiang","William Stafford Noble","Yang Young Lu"],"pdf_url":"https://arxiv.org/pdf/2408.17016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15478v3","updated":"2024-08-30T05:02:12Z","published":"2024-02-23T18:12:53Z","title":"Transformers are Expressive, But Are They Expressive Enough for\n  Regression?","summary":"  Transformers have become pivotal in Natural Language Processing,\ndemonstrating remarkable success in applications like Machine Translation and\nSummarization. Given their widespread adoption, several works have attempted to\nanalyze the expressivity of Transformers. Expressivity of a neural network is\nthe class of functions it can approximate. A neural network is fully expressive\nif it can act as a universal function approximator. We attempt to analyze the\nsame for Transformers. Contrary to existing claims, our findings reveal that\nTransformers struggle to reliably approximate smooth functions, relying on\npiecewise constant approximations with sizable intervals. The central question\nemerges as: ''Are Transformers truly Universal Function Approximators?'' To\naddress this, we conduct a thorough investigation, providing theoretical\ninsights and supporting evidence through experiments. Theoretically, we prove\nthat Transformer Encoders cannot approximate smooth functions. Experimentally,\nwe complement our theory and show that the full Transformer architecture cannot\napproximate smooth functions. By shedding light on these challenges, we\nadvocate a refined understanding of Transformers' capabilities. Code Link:\nhttps://github.com/swaroop-nath/transformer-expressivity.\n","authors":["Swaroop Nath","Harshad Khadilkar","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2402.15478v3.pdf","comment":"18 pages, 17 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.17011v1","updated":"2024-08-30T04:51:19Z","published":"2024-08-30T04:51:19Z","title":"Disease Classification and Impact of Pretrained Deep Convolution Neural\n  Networks on Diverse Medical Imaging Datasets across Imaging Modalities","summary":"  Imaging techniques such as Chest X-rays, whole slide images, and optical\ncoherence tomography serve as the initial screening and detection for a wide\nvariety of medical pulmonary and ophthalmic conditions respectively. This paper\ninvestigates the intricacies of using pretrained deep convolutional neural\nnetworks with transfer learning across diverse medical imaging datasets with\nvarying modalities for binary and multiclass classification. We conducted a\ncomprehensive performance analysis with ten network architectures and model\nfamilies each with pretraining and random initialization. Our finding showed\nthat the use of pretrained models as fixed feature extractors yields poor\nperformance irrespective of the datasets. Contrary, histopathology microscopy\nwhole slide images have better performance. It is also found that deeper and\nmore complex architectures did not necessarily result in the best performance.\nThis observation implies that the improvements in ImageNet are not parallel to\nthe medical imaging tasks. Within a medical domain, the performance of the\nnetwork architectures varies within model families with shifts in datasets.\nThis indicates that the performance of models within a specific modality may\nnot be conclusive for another modality within the same domain. This study\nprovides a deeper understanding of the applications of deep learning techniques\nin medical imaging and highlights the impact of pretrained networks across\ndifferent medical imaging datasets under five different experimental settings.\n","authors":["Jutika Borah","Kumaresh Sarmah","Hidam Kumarjit Singh"],"pdf_url":"https://arxiv.org/pdf/2408.17011v1.pdf","comment":"15 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.17010v1","updated":"2024-08-30T04:50:27Z","published":"2024-08-30T04:50:27Z","title":"Improving Time Series Classification with Representation Soft Label\n  Smoothing","summary":"  Previous research has indicated that deep neural network based models for\ntime series classification (TSC) tasks are prone to overfitting. This issue can\nbe mitigated by employing strategies that prevent the model from becoming\noverly confident in its predictions, such as label smoothing and confidence\npenalty. Building upon the concept of label smoothing, we propose a novel\napproach to generate more reliable soft labels, which we refer to as\nrepresentation soft label smoothing. We apply label smoothing, confidence\npenalty, and our method representation soft label smoothing to several TSC\nmodels and compare their performance with baseline method which only uses hard\nlabels for training. Our results demonstrate that the use of these enhancement\ntechniques yields competitive results compared to the baseline method.\nImportantly, our method demonstrates strong performance across models with\nvarying structures and complexities.\n","authors":["Hengyi Ma","Weitong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.17010v1.pdf","comment":"14 pages,6 figures"},{"id":"http://arxiv.org/abs/2408.17008v1","updated":"2024-08-30T04:40:35Z","published":"2024-08-30T04:40:35Z","title":"Evaluation of Table Representations to Answer Questions from Tables in\n  Documents : A Case Study using 3GPP Specifications","summary":"  With the ubiquitous use of document corpora for question answering, one\nimportant aspect which is especially relevant for technical documents is the\nability to extract information from tables which are interspersed with text.\nThe major challenge in this is that unlike free-flow text or isolated set of\ntables, the representation of a table in terms of what is a relevant chunk is\nnot obvious. We conduct a series of experiments examining various\nrepresentations of tabular data interspersed with text to understand the\nrelative benefits of different representations. We choose a corpus of $3^{rd}$\nGeneration Partnership Project (3GPP) documents since they are heavily\ninterspersed with tables. We create expert curated dataset of question answers\nto evaluate our approach. We conclude that row level representations with\ncorresponding table header information being included in every cell improves\nthe performance of the retrieval, thus leveraging the structural information\npresent in the tabular data.\n","authors":["Sujoy Roychowdhury","Sumit Soman","HG Ranjani","Avantika Sharma","Neeraj Gunda","Sai Krishna Bala"],"pdf_url":"https://arxiv.org/pdf/2408.17008v1.pdf","comment":"10 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.16999v1","updated":"2024-08-30T04:11:35Z","published":"2024-08-30T04:11:35Z","title":"A Tighter Convergence Proof of Reverse Experience Replay","summary":"  In reinforcement learning, Reverse Experience Replay (RER) is a recently\nproposed algorithm that attains better sample complexity than the classic\nexperience replay method. RER requires the learning algorithm to update the\nparameters through consecutive state-action-reward tuples in reverse order.\nHowever, the most recent theoretical analysis only holds for a minimal learning\nrate and short consecutive steps, which converge slower than those large\nlearning rate algorithms without RER. In view of this theoretical and empirical\ngap, we provide a tighter analysis that mitigates the limitation on the\nlearning rate and the length of consecutive steps. Furthermore, we show\ntheoretically that RER converges with a larger learning rate and a longer\nsequence.\n","authors":["Nan Jiang","Jinzhao Li","Yexiang Xue"],"pdf_url":"https://arxiv.org/pdf/2408.16999v1.pdf","comment":"This paper is accepted at RLC 2024"},{"id":"http://arxiv.org/abs/2408.16993v1","updated":"2024-08-30T03:43:37Z","published":"2024-08-30T03:43:37Z","title":"A Scalable k-Medoids Clustering via Whale Optimization Algorithm","summary":"  Unsupervised clustering has emerged as a critical tool for uncovering hidden\npatterns and insights from vast, unlabeled datasets. However, traditional\nmethods like Partitioning Around Medoids (PAM) struggle with scalability due to\ntheir quadratic computational complexity. To address this limitation, we\nintroduce WOA-kMedoids, a novel unsupervised clustering method that\nincorporates the Whale Optimization Algorithm (WOA), a nature-inspired\nmetaheuristic inspired by the hunting strategies of humpback whales. By\noptimizing centroid selection, WOA-kMedoids reduces computational complexity of\nthe k-medoids algorithm from quadratic to near-linear with respect to the\nnumber of observations. This improvement in efficiency enables WOA-kMedoids to\nbe scalable to large datasets while maintaining high clustering accuracy. We\nevaluated the performance of WOA-kMedoids on 25 diverse time series datasets\nfrom the UCR archive. Our empirical results demonstrate that WOA-kMedoids\nmaintains clustering accuracy similar to PAM. While WOA-kMedoids exhibited\nslightly higher runtime than PAM on small datasets (less than 300\nobservations), it outperformed PAM in computational efficiency on larger\ndatasets. The scalability of WOA-kMedoids, combined with its consistently high\naccuracy, positions it as a promising and practical choice for unsupervised\nclustering in big data applications. WOA-kMedoids has implications for\nefficient knowledge discovery in massive, unlabeled datasets across various\ndomains.\n","authors":["Huang Chenan","Narumasa Tsutsumida"],"pdf_url":"https://arxiv.org/pdf/2408.16993v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.16987v1","updated":"2024-08-30T03:22:35Z","published":"2024-08-30T03:22:35Z","title":"From Model Explanation to Data Misinterpretation: Uncovering the\n  Pitfalls of Post Hoc Explainers in Business Research","summary":"  Machine learning models have been increasingly used in business research.\nHowever, most state-of-the-art machine learning models, such as deep neural\nnetworks and XGBoost, are black boxes in nature. Therefore, post hoc explainers\nthat provide explanations for machine learning models by, for example,\nestimating numerical importance of the input features, have been gaining wide\nusage. Despite the intended use of post hoc explainers being explaining machine\nlearning models, we found a growing trend in business research where post hoc\nexplanations are used to draw inferences about the data. In this work, we\ninvestigate the validity of such use. Specifically, we investigate with\nextensive experiments whether the explanations obtained by the two most popular\npost hoc explainers, SHAP and LIME, provide correct information about the true\nmarginal effects of X on Y in the data, which we call data-alignment. We then\nidentify what factors influence the alignment of explanations. Finally, we\npropose a set of mitigation strategies to improve the data-alignment of\nexplanations and demonstrate their effectiveness with real-world data in an\neconometric context. In spite of this effort, we nevertheless conclude that it\nis often not appropriate to infer data insights from post hoc explanations. We\narticulate appropriate alternative uses, the most important of which is to\nfacilitate the proposition and subsequent empirical investigation of\nhypotheses. The ultimate goal of this paper is to caution business researchers\nagainst translating post hoc explanations of machine learning models into\npotentially false insights and understanding of data.\n","authors":["Ronilo Ragodos","Tong Wang","Lu Feng"," Yu"," Hu"],"pdf_url":"https://arxiv.org/pdf/2408.16987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06051v2","updated":"2024-08-30T03:19:26Z","published":"2024-08-12T10:55:42Z","title":"Perceptual Similarity for Measuring Decision-Making Style and Policy\n  Diversity in Games","summary":"  Defining and measuring decision-making styles, also known as playstyles, is\ncrucial in gaming, where these styles reflect a broad spectrum of individuality\nand diversity. However, finding a universally applicable measure for these\nstyles poses a challenge. Building on Playstyle Distance, the first\nunsupervised metric to measure playstyle similarity based on game screens and\nraw actions, we introduce three enhancements to increase accuracy: multiscale\nanalysis with varied state granularity, a perceptual kernel rooted in\npsychology, and the utilization of the intersection-over-union method for\nefficient evaluation. These innovations not only advance measurement precision\nbut also offer insights into human cognition of similarity. Across two racing\ngames and seven Atari games, our techniques significantly improve the precision\nof zero-shot playstyle classification, achieving an accuracy exceeding 90\npercent with fewer than 512 observation-action pairs, which is less than half\nan episode of these games. Furthermore, our experiments with 2048 and Go\ndemonstrate the potential of discrete playstyle measures in puzzle and board\ngames. We also develop an algorithm for assessing decision-making diversity\nusing these measures. Our findings improve the measurement of end-to-end game\nanalysis and the evolution of artificial intelligence for diverse playstyles.\n","authors":["Chiu-Chou Lin","Wei-Chen Chiu","I-Chen Wu"],"pdf_url":"https://arxiv.org/pdf/2408.06051v2.pdf","comment":"TMLR 08/2024 https://openreview.net/forum?id=30C9AWBW49"},{"id":"http://arxiv.org/abs/2408.16981v1","updated":"2024-08-30T03:03:03Z","published":"2024-08-30T03:03:03Z","title":"The Sample-Communication Complexity Trade-off in Federated Q-Learning","summary":"  We consider the problem of federated Q-learning, where $M$ agents aim to\ncollaboratively learn the optimal Q-function of an unknown infinite-horizon\nMarkov decision process with finite state and action spaces. We investigate the\ntrade-off between sample and communication complexities for the widely used\nclass of intermittent communication algorithms. We first establish the converse\nresult, where it is shown that a federated Q-learning algorithm that offers any\nspeedup with respect to the number of agents in the per-agent sample complexity\nneeds to incur a communication cost of at least an order of\n$\\frac{1}{1-\\gamma}$ up to logarithmic factors, where $\\gamma$ is the discount\nfactor. We also propose a new algorithm, called Fed-DVR-Q, which is the first\nfederated Q-learning algorithm to simultaneously achieve order-optimal sample\nand communication complexities. Thus, together these results provide a complete\ncharacterization of the sample-communication complexity trade-off in federated\nQ-learning.\n","authors":["Sudeep Salgia","Yuejie Chi"],"pdf_url":"https://arxiv.org/pdf/2408.16981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16725v2","updated":"2024-08-30T02:53:48Z","published":"2024-08-29T17:18:53Z","title":"Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming","summary":"  Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.\n","authors":["Zhifei Xie","Changqiao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16725v2.pdf","comment":"Technical report, work in progress. Demo and code:\n  https://github.com/gpt-omni/mini-omni"},{"id":"http://arxiv.org/abs/2402.01138v3","updated":"2024-08-30T02:53:24Z","published":"2024-02-02T04:30:58Z","title":"Graph Neural Networks in EEG-based Emotion Recognition: A Survey","summary":"  Compared to other modalities, EEG-based emotion recognition can intuitively\nrespond to the emotional patterns in the human brain and, therefore, has become\none of the most concerning tasks in the brain-computer interfaces field. Since\ndependencies within brain regions are closely related to emotion, a significant\ntrend is to develop Graph Neural Networks (GNNs) for EEG-based emotion\nrecognition. However, brain region dependencies in emotional EEG have\nphysiological bases that distinguish GNNs in this field from those in other\ntime series fields. Besides, there is neither a comprehensive review nor\nguidance for constructing GNNs in EEG-based emotion recognition. In the survey,\nour categorization reveals the commonalities and differences of existing\napproaches under a unified framework of graph construction. We analyze and\ncategorize methods from three stages in the framework to provide clear guidance\non constructing GNNs in EEG-based emotion recognition. In addition, we discuss\nseveral open challenges and future directions, such as Temporal full-connected\ngraph and Graph condensation.\n","authors":["Chenyu Liu","Xinliang Zhou","Yihao Wu","Ruizhi Yang","Zhongruo Wang","Liming Zhai","Ziyu Jia","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.01138v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04261v2","updated":"2024-08-30T02:47:43Z","published":"2024-03-07T06:52:51Z","title":"Advancing Chinese biomedical text mining with community challenges","summary":"  Objective: This study aims to review the recent advances in community\nchallenges for biomedical text mining in China. Methods: We collected\ninformation of evaluation tasks released in community challenges of biomedical\ntext mining, including task description, dataset description, data source, task\ntype and related links. A systematic summary and comparative analysis were\nconducted on various biomedical natural language processing tasks, such as\nnamed entity recognition, entity normalization, attribute extraction, relation\nextraction, event extraction, text classification, text similarity, knowledge\ngraph construction, question answering, text generation, and large language\nmodel evaluation. Results: We identified 39 evaluation tasks from 6 community\nchallenges that spanned from 2017 to 2023. Our analysis revealed the diverse\nrange of evaluation task types and data sources in biomedical text mining. We\nexplored the potential clinical applications of these community challenge tasks\nfrom a translational biomedical informatics perspective. We compared with their\nEnglish counterparts, and discussed the contributions, limitations, lessons and\nguidelines of these community challenges, while highlighting future directions\nin the era of large language models. Conclusion: Community challenge evaluation\ncompetitions have played a crucial role in promoting technology innovation and\nfostering interdisciplinary collaboration in the field of biomedical text\nmining. These challenges provide valuable platforms for researchers to develop\nstate-of-the-art solutions.\n","authors":["Hui Zong","Rongrong Wu","Jiaxue Cha","Weizhe Feng","Erman Wu","Jiakun Li","Aibin Shao","Liang Tao","Zuofeng Li","Buzhou Tang","Bairong Shen"],"pdf_url":"https://arxiv.org/pdf/2403.04261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16978v1","updated":"2024-08-30T02:44:26Z","published":"2024-08-30T02:44:26Z","title":"Training Ultra Long Context Language Model with Fully Pipelined\n  Distributed Transformer","summary":"  Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models.\n","authors":["Jinghan Yao","Sam Ade Jacobs","Masahiro Tanaka","Olatunji Ruwase","Aamir Shafi","Hari Subramoni","Dhabaleswar K. Panda"],"pdf_url":"https://arxiv.org/pdf/2408.16978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16975v1","updated":"2024-08-30T02:36:36Z","published":"2024-08-30T02:36:36Z","title":"Technical Report of HelixFold3 for Biomolecular Structure Prediction","summary":"  The AlphaFold series has transformed protein structure prediction with\nremarkable accuracy, often matching experimental methods. AlphaFold2,\nAlphaFold-Multimer, and the latest AlphaFold3 represent significant strides in\npredicting single protein chains, protein complexes, and biomolecular\nstructures. While AlphaFold2 and AlphaFold-Multimer are open-sourced,\nfacilitating rapid and reliable predictions, AlphaFold3 remains partially\naccessible through a limited online server and has not been open-sourced,\nrestricting further development. To address these challenges, the PaddleHelix\nteam is developing HelixFold3, aiming to replicate AlphaFold3's capabilities.\nUsing insights from previous models and extensive datasets, HelixFold3 achieves\nan accuracy comparable to AlphaFold3 in predicting the structures of\nconventional ligands, nucleic acids, and proteins. The initial release of\nHelixFold3 is available as open source on GitHub for academic research,\npromising to advance biomolecular research and accelerate discoveries. We also\nprovide online service at PaddleHelix website at\nhttps://paddlehelix.baidu.com/app/all/helixfold3/forecast.\n","authors":["Lihang Liu","Shanzhuo Zhang","Yang Xue","Xianbin Ye","Kunrui Zhu","Yuxin Li","Yang Liu","Xiaonan Zhang","Xiaomin Fang"],"pdf_url":"https://arxiv.org/pdf/2408.16975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08994v2","updated":"2024-08-30T02:26:29Z","published":"2024-08-16T19:52:53Z","title":"Model-based RL as a Minimalist Approach to Horizon-Free and Second-Order\n  Bounds","summary":"  Learning a transition model via Maximum Likelihood Estimation (MLE) followed\nby planning inside the learned model is perhaps the most standard and simplest\nModel-based Reinforcement Learning (RL) framework. In this work, we show that\nsuch a simple Model-based RL scheme, when equipped with optimistic and\npessimistic planning procedures, achieves strong regret and sample complexity\nbounds in online and offline RL settings. Particularly, we demonstrate that\nunder the conditions where the trajectory-wise reward is normalized between\nzero and one and the transition is time-homogenous, it achieves horizon-free\nand second-order bounds. Horizon-free means that our bounds have no polynomial\ndependence on the horizon of the Markov Decision Process. A second-order bound\nis a type of instance-dependent bound that scales with respect to the variances\nof the returns of the policies which can be small when the system is nearly\ndeterministic and (or) the optimal policy has small values. We highlight that\nour algorithms are simple, fairly standard, and indeed have been extensively\nstudied in the RL literature: they learn a model via MLE, build a version space\naround the MLE solution, and perform optimistic or pessimistic planning\ndepending on whether operating in the online or offline mode. These algorithms\ndo not rely on additional specialized algorithmic designs such as learning\nvariances and performing variance-weighted learning and thus can leverage rich\nfunction approximations that are significantly beyond linear or tabular\nstructures. The simplicity of the algorithms also implies that our horizon-free\nand second-order regret analysis is actually standard and mainly follows the\ngeneral framework of optimism/pessimism in the face of uncertainty.\n","authors":["Zhiyong Wang","Dongruo Zhou","John C. S. Lui","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2408.08994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16969v1","updated":"2024-08-30T02:07:13Z","published":"2024-08-30T02:07:13Z","title":"Point Neuron Learning: A New Physics-Informed Neural Network\n  Architecture","summary":"  Machine learning and neural networks have advanced numerous research domains,\nbut challenges such as large training data requirements and inconsistent model\nperformance hinder their application in certain scientific problems. To\novercome these challenges, researchers have investigated integrating physics\nprinciples into machine learning models, mainly through: (i) physics-guided\nloss functions, generally termed as physics-informed neural networks, and (ii)\nphysics-guided architectural design. While both approaches have demonstrated\nsuccess across multiple scientific disciplines, they have limitations including\nbeing trapped to a local minimum, poor interpretability, and restricted\ngeneralizability. This paper proposes a new physics-informed neural network\n(PINN) architecture that combines the strengths of both approaches by embedding\nthe fundamental solution of the wave equation into the network architecture,\nenabling the learned model to strictly satisfy the wave equation. The proposed\npoint neuron learning method can model an arbitrary sound field based on\nmicrophone observations without any dataset. Compared to other PINN methods,\nour approach directly processes complex numbers and offers better\ninterpretability and generalizability. We evaluate the versatility of the\nproposed architecture by a sound field reconstruction problem in a reverberant\nenvironment. Results indicate that the point neuron method outperforms two\ncompeting methods and can efficiently handle noisy environments with sparse\nmicrophone observations.\n","authors":["Hanwen Bi","Thushara D. Abhayapala"],"pdf_url":"https://arxiv.org/pdf/2408.16969v1.pdf","comment":"under the review process of EURASIP Journal on Audio, Speech, and\n  Music Processing"},{"id":"http://arxiv.org/abs/2408.16966v1","updated":"2024-08-30T01:56:57Z","published":"2024-08-30T01:56:57Z","title":"UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches","summary":"  Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques.\n","authors":["Chao Wang","Neo Wu","Lin Ning","Luyang Liu","Jun Xie","Shawn O'Banion","Bradley Green"],"pdf_url":"https://arxiv.org/pdf/2408.16966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14520v2","updated":"2024-08-30T01:26:43Z","published":"2024-08-26T06:36:42Z","title":"Towards Graph Prompt Learning: A Survey and Beyond","summary":"  Large-scale \"pre-train and prompt learning\" paradigms have demonstrated\nremarkable adaptability, enabling broad applications across diverse domains\nsuch as question answering, image recognition, and multimodal retrieval. This\napproach fully leverages the potential of large-scale pre-trained models,\nreducing downstream data requirements and computational costs while enhancing\nmodel applicability across various tasks. Graphs, as versatile data structures\nthat capture relationships between entities, play pivotal roles in fields such\nas social network analysis, recommender systems, and biological graphs. Despite\nthe success of pre-train and prompt learning paradigms in Natural Language\nProcessing (NLP) and Computer Vision (CV), their application in graph domains\nremains nascent. In graph-structured data, not only do the node and edge\nfeatures often have disparate distributions, but the topological structures\nalso differ significantly. This diversity in graph data can lead to\nincompatible patterns or gaps between pre-training and fine-tuning on\ndownstream graphs. We aim to bridge this gap by summarizing methods for\nalleviating these disparities. This includes exploring prompt design\nmethodologies, comparing related techniques, assessing application scenarios\nand datasets, and identifying unresolved problems and challenges. This survey\ncategorizes over 100 relevant works in this field, summarizing general design\nprinciples and the latest applications, including text-attributed graphs,\nmolecules, proteins, and recommendation systems. Through this extensive review,\nwe provide a foundational understanding of graph prompt learning, aiming to\nimpact not only the graph mining community but also the broader Artificial\nGeneral Intelligence (AGI) community.\n","authors":["Qingqing Long","Yuchen Yan","Peiyan Zhang","Chen Fang","Wentao Cui","Zhiyuan Ning","Meng Xiao","Ning Cao","Xiao Luo","Lingjun Xu","Shiyue Jiang","Zheng Fang","Chong Chen","Xian-Sheng Hua","Yuanchun Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.14520v2.pdf","comment":"19 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.07000v2","updated":"2024-08-30T01:19:42Z","published":"2024-07-09T16:13:26Z","title":"Etalon: Holistic Performance Evaluation Framework for LLM Inference\n  Systems","summary":"  Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Etalon, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Etalon, discussing their strengths and weaknesses. Etalon is\navailable at https://github.com/project-etalon/etalon.\n","authors":["Amey Agrawal","Anmol Agarwal","Nitin Kedia","Jayashree Mohan","Souvik Kundu","Nipun Kwatra","Ramachandran Ramjee","Alexey Tumanov"],"pdf_url":"https://arxiv.org/pdf/2407.07000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16958v1","updated":"2024-08-30T01:09:32Z","published":"2024-08-30T01:09:32Z","title":"Discovery of False Data Injection Schemes on Frequency Controllers with\n  Reinforcement Learning","summary":"  While inverter-based distributed energy resources (DERs) play a crucial role\nin integrating renewable energy into the power system, they concurrently\ndiminish the grid's system inertia, elevating the risk of frequency\ninstabilities. Furthermore, smart inverters, interfaced via communication\nnetworks, pose a potential vulnerability to cyber threats if not diligently\nmanaged. To proactively fortify the power grid against sophisticated cyber\nattacks, we propose to employ reinforcement learning (RL) to identify potential\nthreats and system vulnerabilities. This study concentrates on analyzing\nadversarial strategies for false data injection, specifically targeting smart\ninverters involved in primary frequency control. Our findings demonstrate that\nan RL agent can adeptly discern optimal false data injection methods to\nmanipulate inverter settings, potentially causing catastrophic consequences.\n","authors":["Romesh Prasad","Malik Hassanaly","Xiangyu Zhang","Abhijeet Sahu"],"pdf_url":"https://arxiv.org/pdf/2408.16958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15991v2","updated":"2024-08-30T01:00:09Z","published":"2023-10-24T16:39:06Z","title":"WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models","summary":"  Compiler correctness is crucial, as miscompilation can falsify program\nbehaviors, leading to serious consequences. Fuzzing has been studied to uncover\ncompiler defects. However, compiler fuzzing remains challenging: Existing arts\nfocus on black- and grey-box fuzzing, which generates tests without sufficient\nunderstanding of internal compiler behaviors. Meanwhile, traditional white-box\ntechniques, like symbolic execution, are computationally inapplicable to the\ngiant codebase of compilers. Recent advances demonstrate that Large Language\nModels (LLMs) excel in code generation/understanding tasks. Nonetheless,\nguiding LLMs with compiler source-code information remains a missing piece of\nresearch in compiler testing.\n  To this end, we propose WhiteFox, the first white-box compiler fuzzer using\nLLMs with source-code information to test compiler optimization, with a\nspotlight on detecting deep logic bugs in the deep learning (DL) compilers.\nWhiteFox adopts a multi-agent framework: an LLM-based analysis agent examines\nthe low-level optimization source code and produces requirements on the\nhigh-level test programs that can trigger the optimization; an LLM-based\ngeneration agent produces test programs based on the summarized requirements.\nAdditionally, optimization-triggering tests are used as feedback to enhance the\ngeneration on the fly. Our evaluation on the three most popular DL compilers\n(i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows WhiteFox\ncan generate high-quality test programs to exercise deep optimizations,\npracticing up to 8X more than state-of-the-art fuzzers. WhiteFox has found 101\nbugs for the DL compilers, with 92 confirmed as previously unknown and 70\nfixed. WhiteFox has been acknowledged by the PyTorch team and is being\nincorporated into its development workflow. Beyond DL compilers, WhiteFox can\nalso be adapted for compilers in different domains.\n","authors":["Chenyuan Yang","Yinlin Deng","Runyu Lu","Jiayi Yao","Jiawei Liu","Reyhaneh Jabbarvand","Lingming Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.15991v2.pdf","comment":"Published in OOPSLA 2024"},{"id":"http://arxiv.org/abs/2304.06496v2","updated":"2024-08-30T00:47:06Z","published":"2023-03-27T12:02:33Z","title":"EEGMatch: Learning with Incomplete Labels for Semi-Supervised EEG-based\n  Cross-Subject Emotion Recognition","summary":"  Electroencephalography (EEG) is an objective tool for emotion recognition and\nshows promising performance. However, the label scarcity problem is a main\nchallenge in this field, which limits the wide application of EEG-based emotion\nrecognition. In this paper, we propose a novel semi-supervised learning\nframework (EEGMatch) to leverage both labeled and unlabeled EEG data. First, an\nEEG-Mixup based data augmentation method is developed to generate more valid\nsamples for model learning. Second, a semi-supervised two-step pairwise\nlearning method is proposed to bridge prototype-wise and instance-wise pairwise\nlearning, where the prototype-wise pairwise learning measures the global\nrelationship between EEG data and the prototypical representation of each\nemotion class and the instance-wise pairwise learning captures the local\nintrinsic relationship among EEG data. Third, a semi-supervised multi-domain\nadaptation is introduced to align the data representation among multiple\ndomains (labeled source domain, unlabeled source domain, and target domain),\nwhere the distribution mismatch is alleviated. Extensive experiments are\nconducted on two benchmark databases (SEED and SEED-IV) under a cross-subject\nleave-one-subject-out cross-validation evaluation protocol. The results show\nthe proposed EEGmatch performs better than the state-of-the-art methods under\ndifferent incomplete label conditions (with 6.89% improvement on SEED and 1.44%\nimprovement on SEED-IV), which demonstrates the effectiveness of the proposed\nEEGMatch in dealing with the label scarcity problem in emotion recognition\nusing EEG signals. The source code is available at\nhttps://github.com/KAZABANA/EEGMatch.\n","authors":["Rushuang Zhou","Weishan Ye","Zhiguo Zhang","Yanyang Luo","Li Zhang","Linling Li","Gan Huang","Yining Dong","Yuan-Ting Zhang","Zhen Liang"],"pdf_url":"https://arxiv.org/pdf/2304.06496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16947v1","updated":"2024-08-30T00:06:29Z","published":"2024-08-30T00:06:29Z","title":"An Empirical Study of Scaling Laws for Transfer","summary":"  We present a limited empirical study of scaling laws for transfer learning in\ntransformer models. More specifically, we examine a scaling law that\nincorporates a \"transfer gap\" term, indicating the effectiveness of\npre-training on one distribution when optimizing for downstream performance on\nanother distribution. When the transfer gap is low, pre-training is a\ncost-effective strategy for improving downstream performance. Conversely, when\nthe gap is high, collecting high-quality fine-tuning data becomes relatively\nmore cost effective. Fitting the scaling law to experiments from diverse\ndatasets reveals significant variations in the transfer gap across\ndistributions. In theory, the scaling law can inform optimal data allocation\nstrategies and highlights how the scarcity of downstream data can bottleneck\nperformance. Our findings contribute to a principled way to measure transfer\nlearning efficiency and understand how data availability affects capabilities.\n","authors":["Matthew Barnett"],"pdf_url":"https://arxiv.org/pdf/2408.16947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.05958v3","updated":"2024-08-30T13:35:31Z","published":"2021-06-10T17:54:21Z","title":"High Probability Complexity Bounds for Non-Smooth Stochastic\n  Optimization with Heavy-Tailed Noise","summary":"  Stochastic first-order methods are standard for training large-scale machine\nlearning models. Random behavior may cause a particular run of an algorithm to\nresult in a highly suboptimal objective value, whereas theoretical guarantees\nare usually proved for the expectation of the objective value. Thus, it is\nessential to theoretically guarantee that algorithms provide small objective\nresidual with high probability. Existing methods for non-smooth stochastic\nconvex optimization have complexity bounds with the dependence on the\nconfidence level that is either negative-power or logarithmic but under an\nadditional assumption of sub-Gaussian (light-tailed) noise distribution that\nmay not hold in practice. In our paper, we resolve this issue and derive the\nfirst high-probability convergence results with logarithmic dependence on the\nconfidence level for non-smooth convex stochastic optimization problems with\nnon-sub-Gaussian (heavy-tailed) noise. To derive our results, we propose novel\nstepsize rules for two stochastic methods with gradient clipping. Moreover, our\nanalysis works for generalized smooth objectives with H\\\"older-continuous\ngradients, and for both methods, we provide an extension for strongly convex\nproblems. Finally, our results imply that the first (accelerated) method we\nconsider also has optimal iteration and oracle complexity in all the regimes,\nand the second one is optimal in the non-smooth setting.\n","authors":["Eduard Gorbunov","Marina Danilova","Innokentiy Shibaev","Pavel Dvurechensky","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2106.05958v3.pdf","comment":"61 pages, 12 figures. Changes in V2: different presentation of the\n  results, different structure, new experiments. Changes in V3: some typos were\n  fixed"},{"id":"http://arxiv.org/abs/2107.04857v2","updated":"2024-08-30T10:43:08Z","published":"2021-07-10T15:14:19Z","title":"Dense-Sparse Deep Convolutional Neural Networks Training for Image\n  Denoising","summary":"  Recently, deep learning methods such as the convolutional neural networks\nhave gained prominence in the area of image denoising. This is owing to their\nproven ability to surpass state-of-the-art classical image denoising algorithms\nsuch as block-matching and 3D filtering algorithm. Deep denoising convolutional\nneural networks use many feed-forward convolution layers with added\nregularization methods of batch normalization and residual learning to speed up\ntraining and improve denoising performance significantly. However, this comes\nat the expense of a huge number of trainable parameters. In this paper, we show\nthat by employing an enhanced dense-sparse-dense network training procedure to\nthe deep denoising convolutional neural networks, comparable denoising\nperformance level can be achieved at a significantly reduced number of\ntrainable parameters. We derive motivation from the fact that networks trained\nusing the dense-sparse-dense approach have been shown to attain performance\nboost with reduced number of parameters. The proposed reduced deep denoising\nconvolutional neural networks network is an efficient denoising model with\nsignificantly reduced parameters and comparable performance to the deep\ndenoising convolutional neural networks. Additionally, denoising was achieved\nat significantly reduced processing time.\n","authors":["Basit O. Alawode","Mudassir Masood"],"pdf_url":"https://arxiv.org/pdf/2107.04857v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.17057v1","updated":"2024-08-30T07:32:19Z","published":"2024-08-30T07:32:19Z","title":"LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality\n  Assessment Model","summary":"  Recent advancements in the field of No-Reference Image Quality Assessment\n(NR-IQA) using deep learning techniques demonstrate high performance across\nmultiple open-source datasets. However, such models are typically very large\nand complex making them not so suitable for real-world deployment, especially\non resource- and battery-constrained mobile devices. To address this\nlimitation, we propose a compact, lightweight NR-IQA model that achieves\nstate-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation\nand test datasets while being also nearly 5.7 times faster than the fastest\nSOTA model. Our model features a dual-branch architecture, with each branch\nseparately trained on synthetically and authentically distorted images which\nenhances the model's generalizability across different distortion types. To\nimprove robustness under diverse real-world visual conditions, we additionally\nincorporate multiple color spaces during the training process. We also\ndemonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks\n(KANs) for final quality regression as compared to the conventional Multi-Layer\nPerceptrons (MLPs). Our evaluation considering various open-source datasets\nhighlights the practical, high-accuracy, and robust performance of our proposed\nlightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA.\n","authors":["Nasim Jamshidi Avanaki","Abhijay Ghildiyal","Nabajeet Barman","Saman Zadtootaghaj"],"pdf_url":"https://arxiv.org/pdf/2408.17057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16990v1","updated":"2024-08-30T03:36:22Z","published":"2024-08-30T03:36:22Z","title":"Video to Music Moment Retrieval","summary":"  Adding proper background music helps complete a short video to be shared.\nTowards automating the task, previous research focuses on video-to-music\nretrieval (VMR), aiming to find amidst a collection of music the one best\nmatching the content of a given video. Since music tracks are typically much\nlonger than short videos, meaning the returned music has to be cut to a shorter\nmoment, there is a clear gap between the practical need and VMR. In order to\nbridge the gap, we propose in this paper video to music moment retrieval (VMMR)\nas a new task. To tackle the new task, we build a comprehensive dataset\nAd-Moment which contains 50K short videos annotated with music moments and\ndevelop a two-stage approach. In particular, given a test video, the most\nsimilar music is retrieved from a given collection. Then, a Transformer based\nmusic moment localization is performed. We term this approach Retrieval and\nLocalization (ReaL). Extensive experiments on real-world datasets verify the\neffectiveness of the proposed method for VMMR.\n","authors":["Zijie Xin","Minquan Wang","Ye Ma","Bo Wang","Quan Chen","Peng Jiang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2408.16990v1.pdf","comment":null}]}}